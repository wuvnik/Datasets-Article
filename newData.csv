title,text,tag,link
Exceptionless Programming,"At its basics, code is a flow of instructions with forks. It is all what it is actually. Nowadays, however, we write a lot of code that tends to get complicated very fast. So we have tools that help us manage the code like functions, switch..case, etc.. Different programming languages offer different tools to help manage our code.Another challenge we have is handling errors. Most programming languages we have today have some sort of exception handling mechanism. This mechanism is essential for handling errors we have throughout the execution of our program.It is important to note that handling code flow and error flow are two very different flows and each has its own purpose, It’s not always clear which flow we need to use in which situation.This post aims to discuss different scenarios in the life of an application and determine which flow should be leveraged, as well as suggest an alternative way to handle the error flow. To do this let’s first discuss what errors are.This article use Typescript and Node.js as a base for examples but the same techniques can be applied to almost every programming language, even if the code looks a bit differentThere are three common error types in an application:In theory, unexpected exceptions are very hard to come by, as with enough tests and design we can anticipate almost every possible error we can encounter. In reality, we all experienced application crashes so we know this is not true and we can expect some unhandled errors for every application we writeSo unexpected errors are ones that we, as developers could not have anticipated or could anticipate but didn’t. One very simple example of such an error can be for example an incorrect value received when it was not anticipated. Let’s suppose we have a function to calculate the tip:In this example, if we receive one of the values from an external source and we do not check it, it can be a string without us knowing it, although we explicitly describe these values as numbers in our code. This error will rightfully bubble up as an exception until it reaches a global exception handler or crushes the process. I personally prefer to crush the process for reasons we will talk about later in this article.Every application has expected errors that if are received making the application execution not relevant anymore. One example can be a DB connection. An application that relies heavily on data received from a database will probably be unusable without a successful connection. For this kind of error, it is also recommended to throw an exception. This is a fatal error, and fatal errors should fail loudly and unless we have a very good reason not to do it.“Normal” errors are errors we do expect and that can either stop the current operation or change execution flow. However, they are not bad enough to take the whole process down. I would argue here that they are also not bad enough to create exceptions out of them.So, these errors are the key subject of this article. Let’s begin with an example:In this example, we are handling messages. Our application can process some messages while others are not relevant. In case of an unknown message received an exception will be thrown to whoever is activating this handler (let's call this code “actor”) and here is where the problems start.The actor should know in advance (or assume) that an exception can be thrown and therefore catch it. If we would catch the exception in the actor, we should then guess what type of object returned in this exception. As in Javascript, you can throw any type of error and you cannot make different catches based on the error type.If the actor decides not to catch exception it is even worse because it is impossible to know at what level it will be caught. It will bubble up and will be handled somewhere and if another layer in between will add try..catch in the future it will be caught there instead of possibly altering the flow.This creates an unstable error handling flow that is prone to fail and potentially will take the whole process down with it. So what can we do?One possible solution is not to handle the error at all. Assuming the unknown message does not break the flow, we can just ignore it and just return null. After updating the code it should look like this:This is better, but still can’t give us information on what happened; we just know that something went wrong.Another option is to use unions in Typescript. We actually already used unions in our previous example where we said that the result can either be status or null. In this case we can say that it is either status or some other error type we’ve described. function handleMessage(message: Message): Status | ErrorThis will work but looks a bit dirty since we will need to understand what type of object we’ve received before we will act on it.If you are familiar with Nodejs you have probably seen how async functions work there. Most of the time, an async function will call a callback with 2 properties, the first is a potential error, the second is the result:So how can we make it work in our favor? Typescript has support for tuples. Tuples can easily be mistaken for arrays as they behave very similarly. The main difference between them is that their length is predefined, as well as the type of every element in order. To describe a tuple, we need to explicitly define its length and types. So with that, let’s refactor again.As we can see here, we now only use tuples to return an error and a result at the same time. We also make use of unions to note that both error and result can be null.Glad you asked :) Actually it is very easy to refactor this code to support asynchronous flow. All we need to do is wrap the code with Promise like this:Please note that: in node.js unhandled promises do not behave (by default) as exceptions, meaning that they do not stop the process. This will change in the future, but, until then to make it behave the same way as normal exceptions do we will need to add a little piece of code as explained here: https://medium.com/@dtinth/making-unhandled-promise-rejections-crash-the-node-js-process-ffc27cfcc9ddSo here we have it, “exceptionless programming” approach that leverages typescript union and tuples.It is important to note that every tool and every method has its own place, and we should not treat this programming method as the law of the instrument. From my experience, when we write multi-layer applications, with services, controller/components, db-accessors we can benefit from this a lot.There is one downside for this method, because we described both error and result as possibly null, we will need to have two if statements, first to check that there is no error, and the second one to check that result is not empty unless you disable the strictNullChecks flag in typescript.",javascript,https://medium.com/att-israel/exceptionless-programming-80b3e06b1f4e?source=tag_archive---------40-----------------------
"Beginner's guide on Vite, fastest dev environment for VueJS","If you are a JavaScript Developer, you must have worked with Grunt, Gulp, Webpack, and other traditional tools to build & bundle the whole application codebase. We have a new similar tool Vite, which can drastically reduce our development time, by refreshing the dev-server as soon as you blink your eye. Interesting? Let us understand it deeply…Well, Vite is a french word meaning ‘quickly’ or ‘fast’ and pronounced as ‘Vit’.“The world is already moving fast, so why not our development” — Evan You.While working on Vue3, Evan thinks of a solution for a major problem, we had since starting of the web. He decided to make web development, as quickly as possible and started innovating the Vite. This is still experimental, but the community is intended to make it suitable for production.Vite is a no bundler DEV environment for Vue.js, created by Evan You. Vite serves your code via native ES Module imports during development, allowing you to develop Vue.js single file components without bundling them. Vite also bundles it with Rollup for a production build.While Vite is primarily designed to work with Vue 3, it can also be used with other frameworks, including React, Preact & other major frameworks.Primarily Vue developers are using Vue CLI (includes Webpack internally) to compile their projects during development and for production. This comes with a few disadvantages:Vite tackles these issues by compiling code on-demand, only compiling the code imported on the current screen and HMR performance is decoupled from the total number of modules, making HMR consistently fast no matter how big your app is. The resultant page reloads as quickly as you blink your eye.To get started with Vite, you need not to clone any particular boilerplate. Simply open your terminal and navigate to your project directory. From here run the Vite create command:NPX is an NPM package runner that makes it really easy to install any sort of node executable that would have normally been installed using NPM.Change directory to the project:and install the node modules:You can then start the Vite Dev environment by running:Your new Vite powered Vue 3 application should be running in localhost:3000Your dev environment is properly configured with minimal packages.Now you can start developing your application as you were doing with Vue3. The output will get HMR as soon as you change any code. Hit save and watch your changes instantly appear in the browser.",javascript,https://medium.com/simplejs/beginners-guide-on-vite-fastest-dev-environment-for-vuejs-29dd0aa5b004?source=tag_archive---------33-----------------------
Creating an Application Performance Monitor Using Node 14 New and Experimental Features,"Erick WendelJun 1, 2020·13 min readNode v14 came with lots of new exciting features. Let’s dive into some of those features by creating a real app.On April 21st, Node.js 14 has been announced as a current release. It came not only with platform improvements but also new keywords available from the new Javascript.In this post, you’re going to create a complete Node.js program. Keep in mind we are using a few experimental features which means that this code could break at any time. If you see something not working here, leave a comment below and I’ll take a look as soon as possible.Our goal is to create an Application Performance Monitor (APM). I’ll cover the following Node.js features in this app:However, you might be thinking about how you can install the Node.js in this specific version. You’ll need to install and configure the Node Version Manager or Install it from the Node.js website. I highly recommend you install the NVM so you can go back and forth into versions without the need to change the entire environment. Also, you'll run Unix commands in this tutorial. If you're on Windows, you may use Windows Subsystem for Linux (WSL) to use Unix commands as well.Should you have NVM installed, on a terminal runnvm install 14.3and thennode -vto check your current version. If you're familiar with Docker and prefer using it I wrote a Dockerfile and that may help you.As I said before, you’re going to create an app to monitor a Web API. Our app will follow the requirements below:It's time to code! Going to your environment, you'll need an empty folder to create the programs. I'll start from scratch, running mkdir appGoing through the app folder, we'll initialize a Node.js project, install nodemon for hot reload, uuid to generate unique ids and debug for logs, with the following commands:cd app && npm init -y && npm i -D nodemon@2 && npm i uuid@8 debug@4Node.js version 13.2.0 announced core support for ECMAScript Modules. Since then, ESModules has been evolving. In Node.js 14.x the experimental warning for ESModules was removed. That means they’re taking a huge step towards the stable stage. However, it's still in the experimental stage.Our projects will run ESModules and Javascript files using the .jsextension to see how it works but you can also use .mjs files.At first, we'll change the package.jsonfile by simply adding the key-value property type: ""module"". This flag tells Node.js runtime that we're going to use ESModules.External databases won't be our focus today. To simplify our lives, we'll use a JSON file as a data source. Create a file database.json on your app folder with the information below.On the root folder (app) you'll create a folder and a file as follows:mkdir agent && touch agent/agent.jsIn the agent.jsfile, we'll implement a function that extends the HTTP module behavior. Copy the file below and fill your agent.jsIt’ll perform the actions below. In parenthesis, I put the code line for easier understanding:Before we move on, let me introduce you other Node.js experimental feature. If you're familiar with test Spies you'll love this feature. I've been using sinon.JS for it. Spies are functions that record function arguments, returned values, and exceptions thrown from function requests.Yet, assert.CallTracker only checks the function's amount calls. It still in the experimental stage and it has only three functions: .calls to record the function's amount calls, .report and .verify to validate assertions. We'll use it to inspect if the .setHeader function was called to make sure our custom header will be added.Creating a fileagent.test.js, we'll implement our test runner for our APM Agent. Copy the code below and fill agent.test.jsAs I did before, In parenthesis the code line for easier understanding:Excited to see the result? Let's go back to our package.jsonand add scripts for testing this app.On the 08th line, I added a few flags on thenodemoncommand to use experimental Node.js features as follows:Running npm run test:agent you may have seen the following result:Have you ever thought about how to track a user request?Node.js Async Hooks is an API that allows us to attach functions to track async events on the Node.js lifecycle. Before Async Local Storage API was introduced we could create Javascript objects and handle data in-memory. Certainly, a bad idea for something single-threaded, isn't it?Using the Async Local Storage API it can store individual contexts adding data for requests and threaten them independently. As we must track different customers in individual requests, it'll be perfect!That can be easier! Using the Performance Hooks API we can mark when some action started, ended, and then measure duration.Instead of incremental code, I'll implement all agent.jscode and then explain it later, ok?. Create the agent.js file into the agent folder then paste the code below on it.Let's take a look at the order that actions have happened here.As we did before, running npm run test:agent you may have seen the following result in your console:Accordingly the Node.js announcement, the Diagnostic Report came to the stable stage. It brings huge power for debugging Node.js programs. We just need to add a few command lines and any unexpected error will be stored in a file for further analysis.On our agent.js, you'll add the following instructions at the head of your file. Now, if a non-handled exception has happened or for some reason, the application has crashed we'll have a new file at the ./reports folder.Ok, to test this feature isn't an easy task. We must raise an error and catch it via process.on global events. Below, I put how the agent.test.js file must look.Yeah, my friend if you realized there's a top-level await statement, congrats for you!! It was released as experimental on Node.js v14.0.3 and it enables using the await keyword without being on an async function by (47). Now, adding the --experimental-top-level-await flag on thenode command will make it happens.Before running this project again, we must have created a folder called reports on the approot folder. I'm gonna add this command on thepackage.json right at the beginning of thetest:agent script. Also, add a rule to the nodemon package for ignoring report files.As we did before, running again npm run test:agent you may have seen the following result:Finally, our Application Performance Monitoring app is ready to use! Let's create a Web API to see how it works in practice.The Web API project must follow a list of requirements:Let's create our api folder on app and empty Javascript files running the following code:mkdir api && touch api/index.js api/index.test.jsLet's import the APM Agent and create a simple server using the code below:On the package.json, let's add a script for starting the API and other for testing scripts.Notice that theDEBUG environment variable is set for app:* that will ignore all logs from the Agent. If you wanna see logs from the agent as well, you need to write them as DEBUG=app:*,agent:* and then your console will show complete logs.I'm gonna use the cURL, for now, to test if the agent is logging out our requests. Also, I'll choose either 1 or 2 for thex-app-id to see different outputs in thelogger.log file. Opening two terminal sessions on the left run npm start and on the right the cURL given on the index.jsfile.Using the same idea of our cURL request. We'll need to create mock data to request APIs. On theapi create a folder called mocks. Thought this folder we'll create two files: request1.json and request2.json as following data:The index.test.js should request the index.js API and validate the outputs. I'll put the whole index.test.js file below. Create the index.test.jstest file, copy the data below, and paste it there.Ok, I know there's a lot of code here. Shall we dig into some of them?4. (67) — After all, it closes de app.Just a quick pause to introduce more one important improvement on the Node.js Ecosystem.These changes are intended to improve consistency across the Streams APIs to remove ambiguity and streamline behaviors across the various parts of Node.js core — https://medium.com/@nodejs/node-js-version-14-available-now-8170d384567eWe usually handle streams with the Transform, Writabe, Readable, and Duplex functions from the streams core module. Now, before a Node.js stream function ends we can add custom behavior which will be called per default.And last but not least, on the V8 Engine Version 8.1 of Node.js v14 many new Javascript keywords were introduced and I'm so excited to talk about it.If you've been working with the C# language you may have seen the following code:ob1?.ob2()?.obj3 ?? ""That's not enough 😨""Optional Chaining is a feature that avoids unnecessary if statements. See how is the Before and after it was added below:Let's copy the code below and I'll explain later what's going on there:Once we have our package.json ready, let's run the app tests with npm run test:app as shown below:We're going to install the autocannon package using npm i -g autocannon@4Do you remember we left a comment in agent.js on the 58th line? I'm going to comment on the 57th line and then uncomment the 58th line. I'll run a load test using the autocannon package and see if our logger.log file makes sense. For that, copy the command below and paste it on your console after starting your app using npm start.Ok, Let's see in practice if it does work! Running all statements shown above you may have seen a similar output as following:We made a complete Application Performance Monitor using the top newest features of Javascript and Node.js. Also used Node.js advanced concepts such as concurrence, Node.js Streams, testing, and so on. It was amazing being with you and sharing a few knowledge I have.I wish you the best and hope you liked this content. Please share it on your social media channels and help me keep growing leaving your comment below.I've been presenting conference talks and creating online training courses in Brazil and overseas and it's being buuuusy!I'm gonna release soon a free training course about Mastering Node.js Streams check this out🎉Also, if you wanna follow my latest content, I have some links that you should be interesting to you:erickwendel.comSee ya 🍻",javascript,https://medium.com/@erickwendel/node-v14-x-is-up-deep-diving-into-new-features-ace6dd89ac0b?source=tag_archive---------6-----------------------
How A Single JavaScript Library Can Break The Internet In Two Lines Of Code,"Almost everyone who develops JavaScript is aware of the 2016 “unpublish-disaster” where one developer yanked all his repositories from NPM and broke the internet. If you have never heard about it give it a read, it’s quite fascinating how fragile the things we rely on often are.However this story is about a much more recent case, the “isPromise-disaster” of April. The author of this actually published a pretty good post-mortem about the case here on Medium, I highly suggest you read that one.There are also news articles (this one for example) describing the actual problem, but for summary:isPromise is a JavaScript library used in millions of projects either directly or indirectly and when an erroneous update broke it many other programs broke in direct response. This included big ones like Angular, Firebase and many others.While this does nothing against a complete unpublish like in 2016 it would have prevented this case — completely. That is precisely what version pinning is for, you tell NPM which exact build you want and only upgrade when there is a real reason to do so.Of course you would only push that update to your own server as soon as it’s thoroughly tested.That, then creates the reverse problem for whenever there is a security update that fixes critical bugs that don’t necessarily turn up on everyone’s radar — but I feel this can be circumvented by warnings and for example Github’s repository vulnerability scanning.Without version pinning you hand complete control over your code to strangers on the internet, by pinning you trade your freedom back and pay in added responsibility. With how many warnings often arise during the build stages of many packages and programs it’s easy to miss some, which then is a good reason to treat warnings like errors — but that is another topic altogether. I feel like version pinning creates the least hassle in the long run.The world of tech has long grown too-big to build everything yourself, everything depends on other things in one way or another. That being said there is merit to the question of “do I really need to import this?” — the package in question boils down to just two lines of code.That being said those two lines are pretty complex and include error checking and bool casting that could easily be spread over more lines of code — you gotta love modern syntax sometimes.So it will always be a tradeoff — and if you develop highly important applications it’s worth thinking about this problem more thoroughly. On hobby level projects you can probably deal with a broken build for a couple hours — but if that’s something other people rely on for their work you quickly waste a lot of time and money or might even create life-threatening problems.Anyone still remember the Washington State 911 outage that was caused by a small server routing the calls?While it is probably obvious it should also be noted in particular: If someone manages to slip any kind of undected backdoor into a package manager update we can be sure that thousands of projects will import that code voluntarily.Again: With how big the internet and world of tech have gotten there is no real way to check everything, especially not those dependencies that your dependencies rely on.I would hope that direct dependencies used in the bigger corporations’ projects are validated before upgrading, but then I also work in IT and know how things often are ignored until something breaks — which may be too late.According to an article I read recently the maintainer of CoreJS just went to prison for vehicular manslaughter — leaving his library unmaintained.There’s also Hans Reiser who went to prison for outright murder after building a file system contained in the Linux kernel.While those are the cases that make the news it is always worth thinking about the people behind the code you import: They are people and can do or don’t what they want with their code.This can pose a big problem, maybe not at first but certainly once you need to upgrade or fortify that component and no one is there to update or fix anything on the other end. That’s why forks get created, who then become maintained by a single person which just shifts the problem to the next guy.No one who works in programming, or IT in general would be where we are without package managers, libraries and standardized code.It has its problems, quirks and outright dangers, but on the upside it saves us all hours of time each week — each day, really — and brought the whole world light years ahead.All those one-click-installations of static websites on netlify, the thousands of beginner projects and pretty much anything you ever build — they all rely on imported third-party code.I hope this article serves to make you think about just how fragile our world of tech really is, how easily things can break even when everyone is doing their best job errors like this can still happen.Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/how-a-single-javascript-library-can-break-the-internet-in-two-lines-of-code-1f47936a499c?source=tag_archive---------12-----------------------
Applying Atom Design Methodology and Hexagonal Architecture Using React,"⚠️ Important:⚠️ Note:A few weeks ago I read an article from Netflix about Hexagonal Architecture. It was really interesting how this architecture separates everything from the domain, having different layers where each of them has their own responsibilities avoiding coercion between different parts of the software. Days before reading this article I was asked by the architect of a new project I’m working on to apply this architecture to a React project. In the beginning I struggled a lot because this is not my area and I have a lack of knowledge related to architectures. As I was reading, in addition to the architect explanation of some of the concepts of this architecture, I could have some kind of mental model of how this could work in a frontend project using React.You can take a concept and apply it or spark an epiphany from it. That was what I did. From the hexagonal architecture I took the next concepts:The idea of Hexagonal Architecture is to put inputs and outputs at the edges of our design. Business logic should not depend on whether we expose a REST or a GraphQL API, and it should not depend on where we get data from — a database, a microservice API exposed via gRPC or REST, or just a simple CSV file. — NetflixThe hexagonal diagram that I came out with would be something like this:I think this looks good for being my first time doing this. Of course everything could be improved or maybe it is a bad idea to use this in a frontend project (as I said this is an opinion article 😄). The final project structure should be something like this:The Domain Layer was the the easiest part of all of this because it contains only the state of the application and the entities. Below is how the code looks like. (I’m using one file for practical case example.)Here I added all the actions to interact with my application and then interact with some services in the infrastructure layer. To create the actions I use createSlice from Redux Toolkit. Redux Toolkit allows to reduce the amount of code you could have done a time ago. One of the cool features is that it makes use of Immer — which I’ve been using for a year — Immer is a fast library to work with immutable objects. It make use of the Proxy object.⚠️ Note: If you create an action using createAction don’t pass it right away to the reducer object. As the createSlice function assigns the state and the payload to the given functions, the created functions with createAction will receive the state as it was the actual payload. This might cause some undesired behavior. To avoid this, add it to the final action object making use of the spread operator.As you could see in the code, I like to keep the functions related to the Sagas together and to keep the rest of the actions above them.Now the configuration of the store. This is a very clean and simple configuration with the basic things you might need for a project. I recommend that you read the API doc from Redux toolkit as this code is using some of the functions from that library. I also really recommend that you remove your old code and start using the functions from Redux toolkit.Before continuing with the Redux-Saga configuration, I’m gonna show you some tests. I’m not an expert writing tests and I am still learning, so If you find any mistake or you know a better way to do them, please, let me know by posting the answer below 😃. For me the tests must be simple and I just need to test the final expected output from a given input. So, from the actions, I see the next things that I should test.Using the code from above as an example, I have two actions: fetchUsers and fetchUsersSuccess.On this test I’m taking advantage of the nice function toMatchSnapshot from Jest.Remember in this layer we will have all the modules and services needed to communicate with external resources. The first thing I did was create an abstraction of all the commonly-used HTTP verbs method (POST, PUT, DELETE, GET) You can use any library to do this. In the next code I made use of Axios. I loooove Axios 😍, it comes already with some security configuration that I don’t have to worry about most of the time and it has a huge community behind it. The code below could be improved to cancel the request and to add a more personalized configuration, but for a practical example this one is ok.The next code would be how I tested this code form above. I think I needed to test the successful and failure responses of each verbs from my API module implementation.It’s time to show my Redux-Saga configuration. For those who doesn’t know about Redux-Saga, this is a great library to manage the side effects of your application. One of the cool things about Saga is that it makes use of function generators, this mean that you can pause your Promises, avoid callback hells, and you can make multiple calls in parallel as well. It’s well integrated with Redux making use of a middleware. If you did pay attention to the store configuration, the saga middleware is already there. What I usually use to mount all my Sagas is to make use of one of the following Saga patterns.And then I have a single saga file that will interact only with the corresponding actions. Following the example from above, my actions are in the user folder, there for my saga will also be in the user folder and it will contain only requests from those actions.I think that keeping the Sagas this way makes it easy to test and to maintain. What I test from saga is:With those points in mind, the code I came up with using the example from above it would be something like the following:An extra mention about Sagas. There’s another pattern that you could use when working with Sagas. This is something you might not find on the internet because I did it for a company I was working for. This pattern allows you to inject the Sagas before your container is mounted. This way, if you have a lot of Sagas, the starting point of your project wont be slow.First of all, I needed to made use of the context from Redux. (You can make your own from React context API.As you can see I’m passing as parameters the injectedSagas, tasks, and the addTasks function. The addTasks will fork all the Sagas when the connected container is called. The injectedSagas will hold all the injected Sagas name to keep track on them. The tasks array is also needed and it might be useful, for example, if you logout from your app and you left some Sagas calling an external API, you could cancel those sagas preventing any undesired behaviors.The store configuration file must change a little bit as well. I’m gonna take the example from above and update it.To call the Sagas by the respective container you must connect the container to the Saga. To do so you have to add the Saga and give a unique name.Finally fork the Sagas. I’m going to use the same example from above updating it a little bit.This ways you have “async” Sagas. You could mix this pattern with the rest of the patterns. Play with them as you wish!.Part II: Atom Design MethodologyThat is all. As always, I hope you enjoyed the article and found it useful. Thanks for your time and for reading, and any feedback is very welcome! 😃.Thanks to J. Drake, my husband, for helping me review this article. ❤️Cheers!Did you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!",javascript,https://javascript.plainenglish.io/applying-atom-design-methodology-and-hexagonal-architecture-using-react-6dbb1863a5d5?source=tag_archive---------18-----------------------
How to Write Effective Code that Every Programmer Loves to Maintain,"As a programmer, you may have heard about SOLID. Have you ever applied that principle to your project?In this article, I’ll walk you through SOLID. And trust me, once you master it, your coworkers should happily maintain your code.Multitasking kills your productivity. You can’t produce ten quality tasks at a time, let alone you have time to finish them. The point is just pick one and do it well.The same mindset should be applied to programming. Whenever you write a function, make it do merely a thing. There’s no and/or in a function’s name. If you write something like validateAndLogin(), you’re doing it wrong.Keep your function short and to the point. Use your common sense for every function you write, if you feel a part of a function can be extracted to another one, don’t hesitate to do it.A does-everything function is ugly, hard to read and you definitely want to go to hell instead of maintaining it.Let’s take validateAndLogin() for example.To meet SRP, break it into two separate part — validate and login.Every function/module you write should be open for extension and closed for modification. It means whenever you want to make a change, something like a new feature, you should add new code rather than modifying the existing one.For example, you want to show different badges according to user type.What if you want to show a vip badge? There’re no other ways than editing the showBadge function for adding an if to it. And the if/else keeps going on and on whenever you need to show a new badge.Here is how you can make this situation meet Open-Closed principle:Need to show a new badge? Just add a new function without modifying the existing ones.You see. You don’t need to edit the existing function. Every time you need to show a new badge, just add a function to extend the code base.Every subclass must be substitutable for their parent class.As Uncle Bob said, “To build software systems from interchangeable parts, those parts must adhere to a contract that allows those parts to be substituted one for another.”For example:You can see in getProgrammersSkills() function, getSkills() method’s implmentations are interchangeable. That way, you’re adhering to Liskov Substitution principle.I don’t know why some of my colleagues often force a client to depend on interfaces or methods that it doesn’t use.There’s no native interface in JS, but it should not a matter. The deal is if you don’t need to rely on a function or a module, then don’t.Let’s take a look at the example below:It makes no sense to have both showJuniorSalary() and showSeniorSkills() in the parent class. We will never use neither senior.showJuniorSalary() nor junior.showSeniorSkills().I know this example is ridiculous but you should get the point.This principle’s also known as Inversion of Controls.It means abstractions must not depend on details. Details must depend on abstractions. And high-level modules must not depend on low-level modules.Abstractions shouldn’t know how details are implemented. Otherwise, you break the rule.For example, do not do this:calculateProgrammerSalary() should not know how senior is implemented. Let’s do this instead:It’s better. The calculator doesn’t depend on the concrete implementation of Programmer class. You now can pass either senior or junior as a parameter.When writing code, you should always keep SOLID in mind.It’s not only about the quality of the code, but also show that you care about it and put your heart into every line of code — a kind of programmer you should become.medium.commedium.comDid you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!",javascript,https://javascript.plainenglish.io/how-to-write-effective-javascript-code-that-every-programmer-loves-to-maintain-a490b42b9b9f?source=tag_archive---------21-----------------------
Little Unknown Way To Get Rid Of Switch Statements,"Like me, you have probably learned to avoid large series of if statements inside your source code. They are hard to read; code is hard to follow and, in general, simply ugly.Take a look:Of course, this is just an example extrapolated from the context of a real app. Just imagine we perform function calls inside our statements. Or maybe some more conditions checking.Let’s analyze the example provided. Comparing an expression to a bunch of values, then executing some code when one of them is true. You’ve got it right? Just use a switch statement.I’ve intentionally left break ‘s even if they weren’t necessary here because of the return statements.Can you spot what’s wrong with this example?By writing this, you haven’t really cut down on code verbosity. Not even on complexity to be honest. Your code will still look weird and hard to manage. Probably it will just be easier to understand what you’re trying to do.What I think you should be compelling curious about, is how the refactoring process works. And why this example can be illuminating. Most of the people tend to write code in this manner:The last part is normally considered refactoring. And this shouldn’t be a once-in-a-lifetime issue. Don’t quickly refactor your code once you’re done and then leave the file.This process doesn’t work like this. It’s more like a loop. You implement something. Try to make it look good. Then make it shorter. Then separate the concerns of your code.“Code is made to be flexible. And you keep it flexible by performing this continuos acts of kindness to it”.So try to dig deeper into your refactorings. Don’t stop in the first round. And when you’re acting on some pre-existing code, try to make it better as soon as you touch it. Small, gradual steps are needed.Let’s conclude with the last act for our conditions. Why not using a function instead of a switch?Note how we define the ingredients object to avoid creating it every time we call getIngredients. The or || instead is used for managing the default case where we return lemon.Easier, better looking, and more maintainable. This is what refactoring done well can do. Simply don’t stop at the first glance.And remember, you can also use maps for this:The point of this article was not only showing you how to improve your code. But tho shine a light on the refactoring process. And why most of us get it wrong. I hope this helped!— PieroDid you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/a-little-unknown-way-to-get-rid-of-switch-statements-15c2584f51b9?source=tag_archive---------4-----------------------
Create a Simple Expense Manager with JavaScript,"Building an Expense Manager is probably one of the best introductory JavaScript projects.And you never really learn a language until you build something of your own.So, let’s begin. This article assumes you have basic knowledge of HTML, CSS, Bootstrap 4, and JavaScript. A few helpful references can be found at the end of this article if you need them.All the code we’re going to use in this tutorial is on Github. I would urge you to complete this tutorial and use the final code for reference until then.Meet John. John earns a decent amount by working as an auto mechanic at one of the largest auto repair chains in the country. But he ends up spending his monthly income on things he can’t keep track of.John wants us to help him track his finances and understand how much percentage of his salary he ends up saving, investing and spending.The first task we need to do is settle on a design for our Budget Manager. I usually try creating my designs or if nothing works, head over to Dribbble or Behance to find inspiration from a variety of designs. I use Figma to create designs for my projects and you can find a lot of great tutorials online.The reason why I always begin with the design is that it makes writing code simpler. Selecting a design helps you think in a structured manner which leads to quicker development.Let’s start by creating an index.html in your project folder.Follow the steps here to include bootstrap in your project. To make our design look better, we are using the Open Sans font. This is completely optional.Now that our scripts are ready, let’s start by analyzing our design. To begin with, we need two containers, the blue one on the left which takes up about 40% of the screen and the right one, which fills the rest of the screen. Thank god for the Bootstrap Grid!Let’s start with the blue left container.Insert code from the below block inside your body tag. This will create the following:Oh and, please feel to use your local currency. John the mechanic would like to use Indian Rupee (INR) for this example. 😃Great! Now let’s move on the part where John will add his savings or expenses to create a list.This container will contain a title, a dropdown to select the type of expense, a couple of input fields for John to record his expenses and a list which will display the entries with the date. Add this code below the left-container.That’s it for our HTML! We just completed 1 out of 3 files required for our Budget Manager. Excellent job reaching here. Next up, we will be giving our HTML some style. See that style.css we imported in the <head> above? Let’s style our HTML to create a simple and elegant design.Psst, a friendly reminder: I see you are following along for the last 15–20 minutes. How about a glass of water to keep you hydrated?As we use Bootstrap, most of the CSS-stuff is taken care of for us. Bootstrap handles a lot of positioning and design, so there’s not really much left to do except work with margins, font-sizes, and colors, the look, and feel.Although one interesting property of CSS is creating gradient backgrounds, and can really come in handy when you need to throw in some color into your project.Check out the CSS for the left-container , the background property can take a liner-gradient function which accepts a direction, and the colors you want your gradient to use.The direction has a default value of top-to-bottom .Sometimes, a linear-gradient is not exactly what you have in mind. CSS also allows a radial-gradient, a great reference can be found on W3Schools.Okay, so we are now done with most of the HTML & CSS part of the expense manager. Next, we’ll set up our app.js to add interactions and functionality to our project!JavaScript is the most important part of this project. The HTML CSS defined how our expense manager will look, but now we need to work in the logic. The app.js is where all the magic happens!Before we begin, take a few moments to think over what functionalities we need to add in the app.js . Currently, nothing happens when we select an expense type or when we add a description and expense value and click on the button. Yes, you guessed it right, we need to add eventListeners . We also need to show the current month.We will be writing different functions, functions that are solely responsible to handle the UI and the logic to calculate the month’s budget. Let’s call them controllers.Our project will have 3 controllers.Create an app.js file and add the script tag to the bottom of your index.html just before closing the body tag.Let’s go over an important concept of JavaScript functions before we start writing our controllers.Immediately Invoked Function Expressions (IIFE)IIFEs are functions in JavaScript which run as soon as they are defined. They are “immediately invoked” when your script file is run. Hence, they do not need another function to call them.Pre ES6:ES6:Awesome! We are now ready to add the magic to our well designed, gorgeous looking Expense Manager project.Let’s jump right into creating our controllers for the project.The main controller will take care of the following:The UI controller will focus on:The expense controller will focus on:Create the three controllers as outlined below. The main controller takes in two parameters, the UI controller and the expense controller. This will help the main controller control the flow of data between them.The HTMLStrings function is an object that keeps track of the class names and element ids used in the HTML files. I do this because it makes referencing HTML elements easy in the project and reduces potential hours of debugging spent to find a spelling mistake.Now, let’s set up the eventListeners . List down the elements that will require a click event listener. The expense type dropdown options and the submit button.Alongside, we’ll also create the functions which will be called when the eventListener is triggered.Let’s dig deeper into each function.The setupEventListeners function selects the dropdown elements and the submit button to add a click listener to each of them. These listeners are activated when you click on them. Each click listener performs a specific task, such as setting the expense type or instructing the other two controllers to perform some tasks.The addExpense function asks the UI controller to get the input from the HTML, verifies if the input is valid (not null or 0) and asks the UI controller to add a new list item with the correct input. It also asks the expense controller to re-calculate the values from the new input. Then it asks the UI controller to update the overall expense for the month.You might wonder, we can directly let the UI and expense controllers talk to each other, and yes we can, but it may not be considered a good practice. Controllers should remain independent of each other except for the main controller. This makes it easier for the developer to understand and debug the flow.Great, that’s it for the main controller.The UI controller is the biggest of the three as it works with HTML and CSS classes to provide the right look and feel to our project.Let’s take a look at all the functions:The expense controller has a simple task. It maintains four values: savings, investments, expenses and the total monthly budget.That’s it! You have just completed the basic functionality for the expense manager. By now, you should be able to log multiple entries in the savings, investments and expense types and your budget should be calculated accordingly. Great job!Now, it’s time for our bonus section!Michael is always ready for a doughnut! 🍩Source: GIPHYThere are multiple libraries out there that help you create beautiful charts. Libraries help you quickly achieve your goal and are always a better option than writing the same functionality all over again.We will be using the Chart.js library which is a simple and flexible library for designers and developers. The library uses the HTML <canvas> element to render your charts.Insert a new div which will hold our chart at the bottom of the left-container.We need to link our controllers to update the chart whenever a new record is inserted into our expense manager.In the addExpense function in the main controller, add a new function call which will instruct the UI controller to update the chart with new values.The displayChart in the UI controller method will create a new chart. We need to pass the type of chart, the data (labels and the dataset), and some options in case we need to customize the chart.Awesome! Now try entering a few values into the system to see your beautiful doughnut graph!This is how our expense manager should look like at the end.The full code for the project can be found on Github.Thank you for staying through the end of the two parts! I hope you found this article helpful in your programming journey!Happy Coding! 😃",javascript,https://levelup.gitconnected.com/create-a-simple-expense-manager-with-javascript-4e2cf2097fba?source=tag_archive---------11-----------------------
Top 19 frequently asked TypeScript interview questions,"TypeScript is everywhere on the web these days so if you're a front developer or even a full stack developer it's important to know the key concepts of TypeScript. Some of the below questions probably could be used for other OOP languages like c# or Java.The below questions can be used for all levels. When interviewing I usually give the same test to all the different levels as this can give you a baseline to what level you think the developer is.One or many of the below you should be looking for.The tsconfig.json file specifies the root files and the compiler options required to compile the project.You’re looking for of the below in the example.Example:extends is the keyword you should be looking for.Example:Yes, you can use the keyword extendsExample:No, you can’t in TypeScript.mixins create partial classes which we can combine to form a single class that contains all the methods and properties from the partial classes.Super is a TypeScript keyword which can be used by developers in expressions for base class constructor and base class properties reference.Example:A namespace is simply a way to logically group related classes or interfaces in a wrapper.Example:public is the defaultpublic - All the members of the class, its child classes, and the instance of the class can access.protected - All the members of the class and its child classes can access them. But the instance of the class can not access.private - Only the members of the class can access them.You're looking for with ? or | undefinedExample :Decorators are simply functions that modify a class, property, method, or method parameter.The syntax is an “@” symbol followed by a function.Example:Function overloading is the ability to create multiple functions of the same name with different implementations.Example:Yes, you can!ExampleA constructor is responsible for initializing the properties/variable of the class.ExampleThe as is additional syntax for Type assertion in TypeScript.Example:Enables a developer to create a component that can work over a variety of types rather than a single one. This allows developers to consume these components and use their own typesA generic class/interface can be defined using <T>Example:I think the questions above give a well-rounded TypeScript interview and allows the developer to show of their skills. With some questions being very specific on TypeScript and other questions focusing on OOP principles.I think questions alone are not the best way to go about interviews. Other ideas to go along with questions:medium.comDid you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/top-19-frequently-asked-typescript-interview-questions-dac4ff30c017?source=tag_archive---------7-----------------------
Amazon Chime SDK Whiteboard with Data Messages for Real-time Signaling,"dannadoriJun 1, 2020·6 min readThis article is also available here.(Japanese)https://cloud.flect.co.jp/entry/2020/06/01/115652(NEW ARTICLE 20th/Oct./2020): Faceswap and Virtual Background on your browerIn the last article, I explained how to create a virtual background for Amazon Chime SDK.medium.comI’d like to continue talking about the Amazon Chime SDK in this post.Well, did you know that Amazon announced a new feature addition to their Amazon Chime SDK the other day?aws.amazon.comThis feature allows participants in a conference to exchange data messages by using the data communication channel used by Amazon Chime. As mentioned in the announcement, this allows us to easily implement the whiteboards and emojis among participants in the conference room. And it can also be used to control the state of the conference room, such as forcing participants to mute.So I’d like to show you how to make a whiteboard using this feature.This is the behavior of the whiteboard I made this time.This additional features of the Amazon Chime SDK uses the signaling communication already existing in Amazon Chime. Amazon Chime’s video conferencing is achieved using a technology called WebRTC, and in WebRTC, signaling communication is used to control the session.Specifically, WebRTC is used for P2P communication between browsers, and the signaling communication is used to identify the destination of the other party or to exchange keys for cryptographic communication in order to start this communication.And, even though it is called P2P communication, it is necessary to go through a relay server called TURN when communicating over a firewall. The exchange of information about these routes is also done through signaling communication.If you want to know more about WebRTC and its relationship with signaling, please refer to this page.Amazon Chime provides managed relay servers and signaling channels to make it easy to start video conferencing in a variety of network environments. The new feature leverages the managed communication path for this signaling to allow arbitrary data messages to be exchanged. So developers can easily add things like shared whiteboards to their video conferencing systems without having to provide a server for messaging.The three new methods offered are as followsThis function sends data messages with “Topic”.First of all, each client registers a callback function that defines the process for each Topic. Then, when the sender sends a data message with Topic, the client receives the data message and calls the callback function corresponding to Topic. We don’t know the details of the internal processing, especially the data flow, but it’s probably running on a general publish/subscribe model.After using it this time, I found it to be a very easy to use feature.Note that this function may not be able to receive the data message even if the Publisher of the data message has subscribed to the topic of the data message, so you may need to be careful. I think the advantage of the publish/subscribe model is that it allows publishers and subscribers to completely ignore each other’s relationships, so the fact that publishers can’t receive data when they’re in the same software (session) was just a little off.Here’s the general process flow of the shared whiteboard we created.As mentioned earlier, Publisher cannot receive the data messages it sends, so it must draw on its own canvas before sending the data messages. When creating an application that wants to reflect user operations in the UI without delay, such as a whiteboard, it is better for the user experience to reflect them in the UI before sending data messages, so I think it will be similar regardless of whether Publisher can receive data messages or not.Furthermore, since Publisher cannot receive the data messages it sends, it may be easier to implement because Publisher does not have to discard the received data.SubscribeHere is an example of a wrapper function that registers a topic and a corresponding callback function to be subscribed by realtimeSubscribeToReceiveDataMessage. Here, we define a callback function that calls app.app.receivedDataMessage when we receive a data message and use it as an argument. Please note that app.app.receivedDataMessage itself can be defined elsewhere for arbitrary processing.Send DataMessageThis is an example of how to send a data message using realtimeSendDataMessage.In order to draw on the whiteboard, the coordinates of the starting and ending points, stroke information, line thickness, etc. are JOSNized and sent.WhiteBoardThis is how the whiteboard function you created works. This demo will be a simulated classroom whiteboard. You can see that what you draw on the right side is reflected on the left side of the screen.WhiteBoard with SharedDisplayYou can also create this whiteboard as an overlay so you can use it with the Amazon Chime SDK’s screen sharing feature to give a presentation.The features described in this article are built into a test bed of new features using video conferencing.If you are interested in it, please visit the following repository.github.comThis time, I tried to create a whiteboard using the latest features of Amazon Chime SDK.In Japan, it was recently announced that the state of emergency has been lifted. However, it still seems difficult to get many people in the classroom to teach a lesson. Also, face-to-face customer service can be risky and difficult to do in the same way. I think that video conferencing and shared whiteboards may be an option to address these issues.",javascript,https://medium.com/@dannadori/amazon-chime-sdk-whiteboard-with-data-messages-for-real-time-signaling-c0740575a6c0?source=tag_archive---------25-----------------------
Is Deno the Node.JS killer?,"On June 2, 2018, the creator of NodeJS Ryan Dahl landed on the JSConf scene with maximum stress. He no longer supports NodeJS and starts talking about Deno. But what’s wrong with NodeJS? And why would Deno be our savior? Today I’m giving you the reasons behind this decision.This operation via package.json which will download modules via the require() brings an even bigger problem. The extreme complexity of solving modules when you type npm install. It’s a kind of huge algorithm that will make 1 billion calls to download as many modules and module dependencies as you are going to store in a bottomless folder.Fortunately that day the creator of NodeJS did not just come to self-flagellate in front of everyone. No, he was there with a solution! And this solution is called Deno.Deno is a secure runtime for executing Javascript and Typescript. Like NodeJS, Deno is built on Chrome’s Javascript engine: V8. It is written entirely in Rust (Nodejs is written in C and C ++) which promises a lot of performance.Note that adding these flags without values means that you give the full access to your application.Deno is a simple, modern, and secure runtime for JavaScript and TypeScript that uses V8 and is built in Rust. It is secure by default, supports TypeScript out of the box, ships only a single executable file, and many other features.Deno today is officially available to use, it successfully removes many of the drawbacks from JavaScript development and fixes many NodeJS problems.It may not completely replace NodeJS just yet, but it’s already a fantastic programming environment for daily use and it promises a bright future.Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/is-deno-the-nodejs-killer-5fdf903191f7?source=tag_archive---------24-----------------------
Custom Decorators in Angular,"How often have you come across similar situations?Routine actions, repeated from time to time, lead to duplication of code and leave a bitter aftertaste of bad smell as the project grows.In this article, I am going to highlight the solution to the above problems by the example of the achievements of our team on the use of decorators in Angular. Let’s look at them in more detail.I am sure that you have repeatedly encountered situations during dumb components development when you need to react to the changes in input properties. So this seemingly ordinary action can lead to excessive code duplication while polluting and inflating the ngOnChanges hook.To avoid this, will mark ngOnChanges hook with our custom decorator, passing in it the necessary parameters.The idea is to put conditions into the decorator, making code easier to read. At the same time, we can add the ability to determine how exactly we should track changes by specifying strategy parameter.Let’s consider in more detail. There are the following input parameters:View at StackBlitzAs you know, there are several events trigger the change detection mechanism:Each time an event occurs, NgZone notifies Angular, which triggers a new change detection cycleFor example, you want to highlight a block by scrolling to the specified coordinates using scrollTo. You can use runOutsideAngular to trigger an event outside the Angular zone, optimizing change detection.An alternative approach is to use a decorator which makes code cleaner and declarative.On our project, we are dealing with a sufficient number of various graphs and maps, are trying to put all the calculations that do not require updating UI outside of Angular zone, thereby improving performance.It is worth noting that this implementation includes checking for the presence of ngZone. In other words, to use this decorator, you need to inject NgZone.We cannot pass an instance variable as an argument to the decorator because of the fact that decorators are called when the class is declared, and at this time we do not have an instance that could be passed.View at StackBlitzSometimes we have to deal with the Web Storage API. This can be organized in different ways: someone directly accesses the Storage API from the same place where they need to get the value, someone uses various abstractions and then calls them, etc. One way or another, this requires too much extra code. Let’s look at the decorator approach.This is a property decorator. Here we are dealing with the defineProperty method and the ability to modify the decriptor. By defining our accessor properties, we get more flexibility and control over real property values, and then we introduce all the necessary logic for getting/setting values.I would like to pay special attention to StorageService. In the current implementation, our decorator is completely unaware of what this service is, how it is arranged, and where it writes data. It only performs its purely “decorating”/”decorator” task. This is the essence, it is an abstraction, which at any time can be replaced depending on your needs.View at StackBlitzCannot read property ‘X’ of undefinedHow many of us have caught this, working with the “heavy” backend API responses that contain a large degree of nesting or sophisticated data structures, or both?To avoid this, if…else chains or try…catch constructs are usually used. When it grows, this turns into a big mishmash of logic and a lot of checks, methods are bloated and become inconvenient in support and understanding. Apply decorator:Here we wrap the original method in a try…catch block, while allowing us to add tracking and handling of various errors. First, the native method is called. If an error occurs, we process it in the way you need and return the default value, which can be passed as an argument to the decorator.As a result, we get a clean and simple structure, avoiding many checks and increase of methods, as well as the ability to track and respond to various errors.View at StackBlitzThere are almost always streams that you need to unsubscribe from. Painfully familiar situation, isn’t it?Subscribing to a stream, a Subscription is created and keep on living until it is completed or until we unsubscribe manually. But what if there are two, three, five streams …? When we use multiple subscriptions, this can get pretty messy.Let’s use our decorator to automate the process:In terms of implementation, everything looks quite simple. The decorator creates a Subject — _takeUntilDestroy$, which will continue to govern our unsubscription. The next point is overriding the ngOnDestroy hook, in which we initially transfer control to the original method, and then call next() and complete() on our Subject. Once these methods have worked, takeUntil fires and unsubscribes.View at StackBlitzDecorators are really powerful tools that can bring beauty, elegance, and purity to your code.We examined some of the most common and interesting scenarios for them that can be useful to any of us. Of course, you should not limit yourself only to these. There are many more cases where such force would be useful:Good luck with using and writing your decorators!stackblitz.com",javascript,https://itnext.io/custom-decorators-in-angular-c54da873b3b3?source=tag_archive---------1-----------------------
How to Build a To-Do List With React,"Original article published on my blogThis article was originally published using React’s class-based components. But given the fact that functional-based components are now the norm, I would be providing the functional equivalent of the App component with the useState React hook. Enjoy!😊Building a To-do list application is a great way to challenge yourself to get going after grasping the fundamentals of React Js. It is a starter project that would help solidify basic concepts like State, Components, Virtual DOM, and what have you.I would be walking you through the steps in creating a simple To-do application and would encourage you to build on it and add any features you may like. Before we get started, here are few things you must know:For those who would like to check out the real code, here’s the link to that.You could also access the live app here.The repo for the functional-components version of this app has been added to this article.Since this is just a React tutorial, I am just going to drop the links to the CSS files for each component:For the sake of this example, we would be using the create-react-app method to set up our app. This is to make everything easy for us.If you do not have create-react-app set up on your computer, you can get started by checking out the Create React App Documentation.The following steps will get you started setting up your app:Note: In this article, I will be using only the term terminal.2. Change path to the directory you want to app to be created on, using the cd command:3. Type create-react-app <name of the app> in your terminal and hit Enter. This installs and the app and voila: We can start coding our app.Note: If you get this kind of error in the message below, try using the sudo command to switch to root.4. Go into the app folder and open in VS Code or any other IDE that you use:Before we dive into coding our app, we need to have a visual representation or structure of what we want to achieve. I have a quick sketch of what we want the app to look like :Most especially in React, we want to know the following:Looking at the image, there are three basic components we will need to come up with:Just as these components have been set out, this is how they would look in our app. We will have to include a to-do component for each task, and finally, our App.js file will contain all of these components (this is already provided to us by create-react-app.)After the whole setup, we have our app’s src structure looking like this:As you can see here, we created a component folder under the src folder to house all individual and reusable components. This is one of the major advantages of React; it helps to break down our entire app into chunks (reusable components). We tend to see that advantage more when building larger applications.Finally, it’s time to code our app. With all our files in place, let’s start with the App component since it will house all of our remaining components.This particular App component will be a class component so that we can have access to React’s state.I recommend we clear the whole content inside our App.js file and start coding it from scratch. However, you could just modify it to the code below:Okay, let’s explain what we’ve done so far. The first image just describes what we have to import. As you know, before writing any functional or class component in React, we have to import React. Since this is a class component, we also have to import React’s component class as well.We then import the TaskIndicator function from its respective file and also the CSS file for the App component.After this, we start writing our App class components, which will extend the Component class of React and has access to its methods (by calling super()). This gives us access to a state property where we can declare what we want to change.In the case of our app, what we want to change is our tasks (lists) property, with object values containing each task and its respective id.The render method is then called, which will return the components we want in HTML-like format (JSX). The first thing we render for now is the TaskIndicator component, which we pass the number props into, indicating the number of tasks left in our to-do list.Let’s see how our TaskIndicator component will look:We make it a functional component(since it doesn’t contain any state) that just returns text with the number of tasks we have (using the number props we passed to it).After applying the component’s CSS file, we should see this in our browser:As you can see, it indicates that we have three tasks, which is the number of objects present in our lists array.That was pretty easy, wasn’t it? Now let’s move on to adding our ToDoList component. which will take the lists array as its props and also have a handleDelete method that will help delete any task we are done with. The handleDelete method is as follows:This method takes in an index argument that would help to filter out a task from the list with id not equal to the index. It then modifies the list’s state to the filtered form, using the this.setState() method.We then add the ToDoList component to the render method of our App.js file:Don’t forget to import the ToDoList function from its file and then go on to write the ToDoList function:This functional component iterates through the lists array given as props and returns a ToDo component for each list. We then give each ToDo component the following props:Of course, we import our ToDo component and write the function in our ToDo.component.jsx file.This part of our app finally returns a div with each task and a Font Awesome minus (-) icon that has the onDelete props passed to it. When this icon is clicked, the corresponding task is removed. The component is then exported.To access the Font Awesome icon, we add the script tag below to the end of our body tag in our index.html file, which is located in the public folder of our app.After all this, our app comes out looking like this:Now for the final stage of our app, which is creating the input form that will take in new tasks and add them to the list. From our sketch, this form will contain an input element (of type=text) and another Font Awesome icon with the plus (+) sign.To help us achieve this, we need the following methods:First, we add an input state in our App component:After that, we proceed to write our handleChange and handleSubmit methods after the constructor.The handleChange method, as mentioned earlier, just takes note of the user types and sets our input to that value.Our handleSubmit method, on the other hand, does nothing if the user’s input is empty. If the user eventually does write something, it creates an object with the task and id property and adds this object to our to-do list. When the task is added, the form is cleared or reset.As we have done for the previous components, we import the InputForm component from its file and render it in our App component:Remember that we have to pass in the two methods we just wrote, as props into the InputForm component.Here is the InputForm function:We make use of all necessary props and export our component.As you would notice, switching our App component from Class-based to functional-based comes with a few modifications.The first and obvious thing we do is to change from a Class to function.Doing this, we lose the this context; hence, we initialize all handle functions using const. We also no longer need the constructor, and can go on to remove it.Lastly, we use the useState hook to initialize our state in place of this.state object in the constructor. useState allows you to initialize a state and also pass in a function to change the state later on — just like this.state and this.setState would. The difference is, it is used in conjunction with functional components and can be used as many times as possible to specify individual states just like we did for lists and input.Every other thing is pretty much straightforward from here.And that’s it. We have our to-do list app ready. Just add the necessary styles, and we have our app looking exactly like the finished product at the beginning.Understanding a little project like this can help you grab some basic concepts in React and prepare you to learn more challenging concepts as you proceed.",javascript,https://betterprogramming.pub/how-to-build-a-todo-list-with-react-d2d5dd9f6630?source=tag_archive---------27-----------------------
Sparse and Dense Arrays in JavaScript,"Have you ever heard of elision in arrays? It is the act of the verb “elide” which means to not pronounce a particular sound in a word. As the Cambridge Dictionary tells, the “t” of “acts” is often elided if someone is speaking quickly.When speaking of JavaScript, an elision occurs when multiple trailing commas are used in arrays and this causes the generation of holes which makes that array, a sparse array. On the contrary, a dense array has no holes.Throughout this discussion, the main focus will be on the array constructor function to form and initialize arrays, the problems encountered when sparse arrays are of concern, finding different solutions on creating dense arrays, and a number of practices applicable on arrays.According to the related article in the language specification, array elements may be elided at the beginning, in the middle or at end of the element list. Whenever a comma in the element list is not preceded by an assignment expression (a comma at the beginning or after another comma), the missing array element contributes to the length of the array. Also if an element is elided at the end of an array, that element does not contribute to the length of the array. In order to visualize this fact simply, open up a Console (that’s why I love JS), please write the following, and press “enter”.As you see, there are two holes created and the related indexes are not formed. Elided array elements are not defined and the length of the array is five.On the other hand, use the array constructor to create an array with a specified length and observe the output.From this perspective, it is clear that a sparse array has a length which is greater than the total number of elements inserted.Please note that array initializer behaves according to the provided parameter count and type:Let’s iterate over a sparse array with some Array.prototype methods.Array.prototype.forEach skips the holes as seen above and the callback (expected to operate) is not executed at that index.Array.prototype.map behaves in a slightly different way, such as the callback is not operated for the holes but they are preserved and available in the new array.As the Array.prototype.filter method preserves the elements on which the callback returns truthy, the holes are evaluated to be falsy (as they are not defined) and removed from the produced array. Array.prototype.reduce does not execute the callback function either where the accumulation is not achieved at the specific indexes.As a solution to the problem of sparse arrays, there are a number ways to produce a dense array either with a given length or by plugging the holes if present.fill() method fills the array with a provided value (no value provided in the example, hence undefined), starting from an index (zero by default) to an end index (array length by default). Spread operator and from() method treats the holes as undefined.Have you noticed the fancy practice above? You should have watched out how the array-like object (with supplying only the length) was converted to a dense array. Array-like objects can be manually generated by providing a length property of a non-negative integer and optional indexed properties.Another method is to use apply() inherited from the Function.prototype by arrays. Function.prototype.apply lets to supply arguments to a function in the form of an array.Please note that the last case is dangerous as the array() function evaluates the single parameter (if it is number) as the array length, not as array elements. It was pointed out at the beginning of the discussion. Consequently, two holes are created instead of creating a dense array.Careful handling of the potential sparse arrays, arrays with holes, might be really a matter since it can lead to unexpected results and performance degradation. Provided with a number of alternatives stated within this discussion, I hope those issues can easily be solved.Thanks for reading!Did you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!",javascript,https://javascript.plainenglish.io/sparse-and-dense-arrays-in-javascript-4ba874fc243c?source=tag_archive---------45-----------------------
Using Docker environment variables at JavaScript runtime,"One of the greatest benefits provided by Docker is portability.We can take the same image and run it in a number of different ways, and different places — one of the features that enable this portability is the ability to configure environment variables when launching a container, i.e. at runtime.While tools like Webpack allow us to inject environment variables into our JavaScript applications, web browsers do not have direct access to these environment variables — meaning these variables can only be injected at build time.This is a problem - we have now lost the portability that Docker provided us. So, let’s get it back.There are two steps I’ve used to achieve this.Most JavaScript applications (e.g. React apps) will include some static assets such as logos or other media in a folder, often called public. So what we do - is simply add our runtime environment variables to a file that will be served along with our other public assets.A good way to perform tasks at docker runtime is to use an ENTRYPOINT script, in this case — I have a shell script which writes the environment variables I need to a JSON file.A quick example:2. Use JavaScript to read vars from fileWhen the application initialises in the web browser, it can simply fetch the config.json with JavaScript much the way it loads other build assets, and use it to supplement the application config.Bonus pointsIt can also be a good idea to do some validation on these variables to ensure your application has all of the required information it needs to bootstrap successfully.For example, if you application must know the value of the $MY_API variable in order to operate — you can also check that it has been defined in Step #1, and abort container execution at runtime if validation fails — this way the issue can be detected and handled at the container orchestration layer, rather than potentially deploying a JavaScript app that is in a bad state (and running in an unknown environment, i.e. someones browser).",javascript,https://levelup.gitconnected.com/using-docker-environment-variables-at-javascript-runtime-cd6d77822608?source=tag_archive---------49-----------------------
Deep Learning in JavaScript (Part 2),"In the first part of this series, I introduced deep learning in JavaScript—we explored why you should consider using Javascript for deep learning, and then went on to create a neural network to predict areas acutely affected by forest fires.As you might have noticed, if you read part one or are otherwise familiar with TF.js, both training and inference happened directly in the browser. While training in the browser can be fast and effective for small datasets, it quickly becomes intractable as the data scales. This is mostly because the amount of storage assigned to a browser is minimal.In order to leverage the power of deep learning, especially for perceptual tasks like image recognition, object detection, or pose estimation, you’ll need access to more compute power.To solve this challenge, we need to perform training at the backend (server-side). But instead of using Python, we can leverage the TensorFlow.js node version (tfjs-node).This TensorFlow.js (node) version can be installed in a node environment and, according to the creators, has access to a low-level C++ runtime, the same runtime used by Python TensorFlow.In this article, we’re going to create an application that can recognize hand-drawn digits in the browser. Below is the end product of what we’ll create:In order to create this demo, we need to download the MNIST handwritten digits datasets. You can download it from Kaggle here.The dataset contains 42,000 grayscale images that have been converted into pixel values and stored in a CSV format. Each row of the dataset contains 745 columns, with the first column as the label, and each pixel value has a range of [0–255]. We’re going to create a convolutional neural network (CNN) to help us classify these digits, which run from 0–9.Before we move on, I’ll assume you’re familiar with CNNs in general and understand some of the basic concepts around convolution, dropout, and so on. If you don’t understand these concepts, I’ll advise you to take a bit of time to familiarize yourself with them before proceeding to the next sections. You can find good tutorials on the subject here:heartbeat.fritz.aimachinelearningmastery.comNow let’s get started!Machine learning is rapidly moving closer to where data is collected — edge devices. Subscribe to the Fritz AI Newsletter to learn more about this transition and how it can help scale your business.First, you need to have Node.js installed on your computer. If you don’t, you can find installation details on the official website here.Next, open a terminal in your project directory and install express-generator.express-generator will help us quickly create a simple scaffold Node app. We’ll build on top of this so that we don’t bother ourselves with directory structures.To use express-generator, run the following command in your terminalYou can configure the express-generator by setting the view engine with the — view command. Although we won’t be rendering views in this application, I prefer to use handlebars as my default.The directory structure generated by express-generator is shown below:├── app.js├── bin│ └── www├── package.json├── public│ ├── images│ ├── javascripts│ └── stylesheets├── routes│ ├── index.js│ └── users.js└── viewsOur application is going to be composed of two parts. The backend for training, and the frontend for inference. The app.js file and other scripts (model.js and data.js) which we’ll create shortly will handle all data ingestion and model training, while the files inside the public folder will handle the UI and model inference.Now go ahead and create the two extra scripts (model.js and data.js) and a dataset folder (dataset) in the root directory. Your directory structure should now look like this:├── dataset├── app.js├── bin│ └── www├── data.js├── model.js├── package.json├── public│ ├── images│ ├── javascripts│ └── stylesheets├── routes│ ├── index.js│ └── users.js└── viewsBefore we continue, go ahead and download the train dataset from Kaggle and move it to the dataset folder.├── dataset│ ├── train.csvNext, let’s add the TensorFlow.js package to our app. Open the package.json file and add the @tensorflow/tfjs-node and argparse dependencies, as shown below:The argparse package will help us parse command-line arguments used for model training customization, which we’ll see shortly.Next, let’s look at the model training scripts and understand their respective functions.The app.js file is the entry point of our app at the backend. It receives and parses training arguments like the number of epochs, model saving path, etc., and calls the corresponding functions to start model training.The data.js file handles everything involved with data ingestion and preparation. Here we’ll leverage tf.data.csv in TensorFlow to read in our data. Copy and paste the code below:Note that each OS hsd a different file path format. Ensure to add the one appropriate for your setup. I used Ubuntu for this tutorial.First, we sliced out the train and test sets from the feature array, then we reshaped both set to have a shape of [size, 28,28,1], where the size depends on the numbers we specified earlier. In my case, I specified 38000 for training and 4000 for testing. This means the training dataset will be a tensor of shape [48000, 28,28,1] while the test set will be of shape [4000, 28,28,1].Then we one-hot encoded the targets. Since we have 10 classes, we’ll be using a softmax output function in our CNN. As such, the target has to be one-hot encoded to have 10 classes.The model.js file houses our CNN. Here we create the model, perform training and evaluation, and then save it to a specified folder. Let’s walk through the code below:Note the input shape. That’s where we specify the image width, height, and channel, which is [28,28,1]. This architecture is definitely not optimal, and can be tweaked or tuned.You should only set the train_mode to 1 when you’re okay with your result from the partial mode training.The partial training mode function contains the following:In the full training mode, we perform training on all available datasets, print the training logs at the end of every epoch, and finally, save the trained model to the specified folder. We’ll use this saved model for inference in the frontend of our application.And that’s it! All the code you need to train and save your model is now ready. Next, let's start training.In your terminal opened in the root directory of your application, enter the following commands to start the partial training:Experiment with different numbers for the epoch and batch size to see which gives higher accuracy. When you’re satisfied with the accuracy, you can move on to full model training mode.Before you start full training, you first have to create a new folder in the public directory called assets. In the assets folder, create another folder called model. We’ll save our trained model file here. This also ensures we have access to the model files in the front end.├── public│ ├── assets│ │ └── modelNext, obtain the full path to the model folder. In most OS’s, you can get this by right-clicking on the folder and copying the path link.If you’re using VSCode like me, you can get the full path by right-clicking on the model folder and clicking Copy Path.Next, in your terminal, set the train_mode to 1 and specify the model path you’ve copied, as shown below:It’s important to add the “file://” prefix to your model_save_path; otherwise TensorFlow will throw an error.Now sit back, relax, and watch your model train.When it’s all done training, the model will be saved to the specified path. You should see a model.json file and a weights.bin file.Congratulations! You’ve just trained a convolutional neural network on a large dataset of handwritten images using TensorFlow.js in a Node environment.In the next post, I'll show you how to connect the frontend of this application and leverage the saved model for real-time and quick inference. As an extra feature, we'll create an input canvas where we can draw a number and predict its class with TensorFlow.js.If you have any questions, suggestions, or feedback, don’t hesitate to use the comment section below. Stay safe for now, and keep learning!Connect with me on Twitter.Connect with me on LinkedIn.Editor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to exploring the emerging intersection of mobile app development and machine learning. We’re committed to supporting and inspiring developers and engineers from all walks of life.Editorially independent, Heartbeat is sponsored and published by Fritz AI, the machine learning platform that helps developers teach devices to see, hear, sense, and think. We pay our contributors, and we don’t sell ads.If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Fritz AI Newsletter), join us on Slack, and follow Fritz AI on Twitter for all the latest in mobile machine learning.",javascript,https://heartbeat.comet.ml/deep-learning-in-javascript-part-2-a2823defd3d9?source=tag_archive---------32-----------------------
Top JavaScript Code-Tuning Techniques,"JavaScript is a very forgiving language. It’s easy to write code that runs but has mistakes in it.In this article, we’ll look at some common code tuning techniques for JavaScript code.JavaScript does short circuit evaluation. So we should take advantage of that so it only tests the conditions that it needs to.For instance, the following is redundant:We check both if x is bigger than 5 and if x is less than 10.If we know that x isn’t bigger than 5, then we don’t have to check that x is less than 10.Therefore, we can rewrite that as:Then we check that if x is less than 10 only when x is bigger than 5We should put the more frequently encountered cases first so that the earlier cases don’t have to be checked as often.For instance, if we have:Assuming that addition is encountered more often, then we put that first so that the other cases won’t have to check as frequently, reducing the amount of code that has to be run.If we have lots of cases to check for and we’re only checking for some constants, then we can replace if or switch statements with lookup tables.For instance, we can write:instead of:If we just run the same if or else in every iteration of the loop, then we shouldn’t put the if and else blocks in the loop.For instance, if we have:and each iteration has either type equal to SUM or otherwise, then we should move the if and else outside the loop body as follows:Then there’s no boolean check in every loop and makes the code run faster.Jamming is combining 2 loops that operate on the same elements.Since they operate on the same elements, we can combine them easily.For instance, if we have:Then we can combine them into one as follows:Unrolling is separating the cases of the loop into their own code.For instance, we can write something like:In the code above, we have a while loop that runs up until i is count - 1 .Then below it, we have an if statement that only runs when i is count — 1 , which may not always be the case since i is incremented by 2.Reducing the number of iterations makes our code faster.If there’s less code inside the loop body, then it should perform faster.Therefore, we should try to make our loops simple to make them perform faster. Simpler loops are also easier to read.We should aim to simplify our loops to make them run faster.If there’re cases that aren’t always run in a loop, then it should be separated out of the loop.We should also take advantage of short-circuit evaluation to reduce the number of cases that have to be checked for conditional statements.Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/top-javascript-code-tuning-techniques-806f8d780a3b?source=tag_archive---------19-----------------------
"Create an Image slider with HTML, CSS and JavaScript","I am currently learning web development with the OdinProject curriculum. There’s a task which is to create a simple image carousel. It should contain arrows on each side to advance the image forward or backward. It should automatically move forward every 5 seconds. It should contain the little navigation circles at the bottom that indicate which slide you are on (and they should be click-able to advance to that particular slide).First, let’s create an html file named index.html.In the html file, we have a container that serves as a frame for each slide and each slide contains an image.Let’s add the styles. I’ll assume you have basic knowledge of CSS for you to want to build an image slider. I’ll try to make the styling basic and simple.The slide display property is set to none which makes them not visible now. The slide container and heading are centered too. We will add the functionality in the JavaScript to make the slides visible.Now, let’s style the next and previous buttons and the dots for navigation. Also, add an active class to style the dot for the slide that is currently being displayed.All we have displaying now is the heading, next and previous buttons, and the four dots.It’s time to add the functionality. Create a file named index.js and this to it.We created a variable named currentSlide that stores the index of the current slide to determine the current slide.We also created a variable called slides to store each slide into a array which enables us to iterate over them and another variable named dots to store all the dots in an array.Then we created a function named init that accepts a parameter n. The parameter will be currentSlide passed into it. Inside the function, we iterated through slides and set each slide’s display property to none. While iterating through the slides, we also iterate through dots and remove the class active from each do. When done setting each slide’s display property to none and removing the class active from each dot, we then set the display of current index according to the currentSlide, to block and add the active class to the dot of the current index using currentSlide variable.And lastly, we add an event to the window to run the init() function when the HTML content is done loading.We add this to the index.js file.We created a function named next to change the current slide to the next one. Here, I used the ternary operator instead of if-else statement. Inside the function, we checked if the currentSlide is greater than or equal to the last index of the slides (4 -1 = 3) which is an array. If it is true, we reset the currentSlide to 0, else we increment currentSlide variable and we run the init() function with currentSlide value.For the prev() function, we check if the currentSlide variable is less than or or equal to zero. If it is true, we set currentSlide to last index of the slides (4 -1 = 3), else we decrement currentSlide.And finally we add click event on next and previous button. When you click on the next button, it runs the next() function and when you click on the previous button, it runs the prev() function.To make the slide change automatically, we set a timer that runs the next() function every 5 seconds. Add this to index.jsWe also want to make the dots clickable to advance to the the next slide. Add this to index.jsHere, we iterate through the dots variable and for each dot, we add an click event and run the init() function passing the index of the dot that is clicked as the parameter and also setting currentSlide to that index.Yes, that is all. We now have a working image slider.This is my first ever article. Let me know what you think about it by leaving a response.Thank you for reading.",javascript,https://levelup.gitconnected.com/create-an-image-slider-with-html-css-and-javascript-3bf2c3e84060?source=tag_archive---------0-----------------------
Handling data at the edge of your Vue.js application,"TLDR: In this article, we discuss how to handle data at the application boundary. Code available at vinicius0026/handling-application-boundaryThis is the fifth article in our Structuring Large Vue.js Applications series. Here is the full list of released and planned articles:In the previous articles in this series, we have dealt only with data that was local to the application, meaning that our data’s entire life cycle was under control. The vast majority of the applications will have some interface with an external service, the app’s REST API, more often than not, so that the data is generated outside of the application scope.Today, we will discuss how we can handle data at the boundary of our application, making sure we have well-formed objects of the proper type, whenever data comes into our Vue.js app from external sources.Whenever we reach out for external APIs, we can’t know exactly the shape of the data that will be delivered to us. We have to rely on the API’s documentation, if available, to build our front-end app interface with the API. Still, as HTTP JSON APIs don’t enforce a particular schema on the data, we should keep some distrust regarding the incoming data.So, to increase our confidence in the data we are handling in our apps, we should establish a clear boundary between our app and the consumed APIs, making sure that all the data that comes in through this boundary is in the expected shape.In this article, we will build a service layer that will act as the sole interface to external APIs, making sure that everything that flows across it will be in a known type.Our objective is to have a service layer that handles all external communication, making sure that the data that comes into our application is in proper shape.We will expand the Invoice model we have discussed in previous articles and will build the service layer for the CRUD operations for invoices.In a previous article, we have defined our Invoice types as:We added the id field for the invoice, with type number or null (for when the invoice was still not created on the server-side).Now we want to build a service layer that allows us to do CRUD operations in the invoices. Let’s define the service skeleton now.As we did when building the core logic of our app, we will use the Module pattern to build our service layer. We can scaffold it like this:We are assuming a REST API accessible in the same origin as our app, with all CRUD endpoints available.With our application accessing external data exclusively via this service, we can be sure it will receive valid objects, with a known type. Notice how we have left comments saying we need to parse the response from the API. We still need to write these parsing functions. They will be part of the app’s core logic, in the modules, with each module being responsible for parsing their related data type. Let’s see what that looks like:In the last article, we introduced a Partial type, for when the object didn't have all the required attributes of a type. We can reuse it here to treat the data coming from the API as a possibly partial object of the type we want.The data provided by the API is likely to have some of the same fields (ideally all) as our front-end type definitions. But, to be safe, we will assume any of the fields might be missing, using the Partial type to wrap all of the data coming from the API.Let’s write the invoice parsing function. We will put it in the Invoice module we have created before:The parse function takes an unknown data object, that can also be undefined. We then build a valid Invoice object by giving it a null id by default and by parsing the other fields appropriately. For the totalAmount field, for example, we call the Decimal constructor to build a valid decimal, whereas the data coming from the API would most definitely not be a decimal object - it would be a string or a number, depending on how the server serializes the data.We have delegated the User and Line Item handling to their own modules’ parse function, which we didn’t implement yet. Let’s do that now.As our User model is very simplistic, the User parse function is straightforward. We just need to provide default values for the user name and avatar, in case the data coming from the API is missing these fields.Let's move on to the Line Item parse function:For the Line Item parse function, we are converting the rate into a valid Decimal object (as we did for the Invoice total amount); we are giving the quantity field a default value of 0 and are delegating the product parsing to its own module. Let’s see how that looks:Similar to the User parse function, parsing the product is very straightforward.With that, we have written parsers to all our data types, and we can now plug the Invoice parse function into the Invoice service.Below is our final implementation of the Invoice service:Notice how simple the service has turned out, as we are delegating the parsing logic to the Invoice module. The service responsibility is to talk to the API and then pipe the API’s response to the parsing logic into the module. This will ensure that the return type of the service is known.In the preceding examples, we assumed the shape of objects coming from the API is similar to the form of objects in our Vue.js application. If we control both sides of the equation, we can (and probably should) enforce that. But sometimes we either don’t control the server-side or can’t change it for whatever reason.If that is the case, we can declare a specific type for the data coming from the API, wrap it in a Partial type, and then convert it to a proper type inside our application.For the sake of building an example, let’s say that the Product object in the payload has different field names: product_name instead of name and product_desc instead of description. We can handle this situation as follows:We have declared the ApiProduct interface, to represent the shape of the Product sent by the API.We don’t need to export this interface to the rest of the application, nor declare it inside our Types namespace. It is intended to be used solely in the parse function. In the rest of the app, we should use only the proper Types.Product type.If we need to convert back from the app Product type to the API's ApiProduct type (when handling Product CRUD, for example), we should add a function to the Product module that does that conversion. We can call it whatever we want, as long as we are consistent with naming across our modules.In the examples we built in this article, we are ignoring the fact that the API didn’t send a full object of the expected type. Sometimes that might be all we need to do. But on most occasions, we should be notified about and fix our API (if that’s something we control) or our parsing functions.The parse functions we wrote are the perfect place to do that. If you are using something like Sentry to collect front-end exceptions, you can raise an exception if there is any missing field, which will be caught by Sentry. Or you can use the Capture Console integration and log an error and return a valid default (like we did in the examples) so that the application is still usable, even if there's some data missing.We have created a clear boundary for our application, using services to convert data coming from external sources (such as our backend API), making sure that the data coming into our app will have a known shape.We have created parse functions in each of our modules to handle the transformation from a potentially incomplete object into a known and type-complying object.We discussed how to parse data in a completely different format, without polluting our global Types with transformation-only types.The resulting service and parse functions are short, easy to reason about, and easy to test, making us more confident of the application state.Originally published at https://viniciusteixeira.tk on June 1, 2020.",javascript,https://levelup.gitconnected.com/handling-data-at-the-edge-of-your-vue-js-application-1872782d391a?source=tag_archive---------35-----------------------
CSS Fundamentals: The CSS Grid Guide,"CSS Grid is a modern layout system that we can use when laying out pages.It’s often compared with Flexbox. And whilst they are both excellent systems for working with complex layouts, there is one major difference: CSS Grid works on 2 dimensions (rows and columns), while Flexbox works on a single dimension only (rows or columns).If you only need to define a layout as a row or a column, then flexbox will likely suit your needs. When working in both dimensions — it’s time for CSS Grid!🤓 Want to stay up to date with web dev?🚀 Want the latest news delivered right to your inbox?🎉 Join a growing community of designers & developers!Subscribe to my newsletter here → https://easeout.eo.pageWe activate the grid layout by making an HTML element a grid container:Our HTML:In our CSS, we simply set its display property to grid:A grid layout consists of a parent element, with one or more child elements.There are a set of properties which can be applied to the container element, as well as any child elements (being each individual item in the grid).Throughout this guide we’ll work with the following code:HTML:And our CSS styles:The most common container properties are grid-template-columns and grid-template-rows. With these properties we define both the number of columns & rows as well as the width of each.For example, let’s tell our grid to layout its items (child elements) in 4 columns at 200px wide, and 2 rows with a height of 150px each.And lets now make it a smaller 3x3 grid:Often you’ll be working with elements with no fixed size. For example, you could have a fixed navbar followed by a flexible content section, then a fixed footer section. For this we can use auto and the layout will adapt to the size of our content:We can add spacing between grid items using grid-column-gap and/or grid-row-gap:We could also use the shorthand grid-gap to set both at once:We can control how much space each grid item takes up in the column or row with the following properties:Let’s see an example:Here’s we’ve added classes to the first & sixth items in our grid.The numbers correspond to the vertical line separating each column. So by setting grid-column-start to 1 and grid-column-end to 3, we’re telling our element to start at the first line & end at the third.Similarly we’ve told our sixth element to start at the 3rd line and end at 5.This of course also applies to grid-row-start and grid-row-end, with the cells expanding across rows instead of columns.We can repeat the above using the shorthand properties of grid-column & grid-row, like so:And we could take this even further by using grid-area as a shorthand for grid-column and grid-row. This would only apply in cases where we need an item to span both rows & columns:Would become:With the order being: grid-row-start > grid-column-start > grid-row-end > grid-column-end.Another option we have when positioning our items is span:With grid-column: 1 / span 2 starting at line 1 and spanning across 2 columns.One of the great benefits of grid is the ability to easily create highly flexible layouts.Fraction units give us the ability to build layouts without needing to specify fixed dimensions.For example, lets divide a grid into 3 columns of equal width, each taking up 1⁄3 of the available space:Too simple!We can use any of the CSS length units. So feel free to use a mix of percentages, pixels, rem, em and fractions:We can use repeat() to specify the number of times a row or column will be repeated, and the length of each.It’s a handy way to quickly put together a layout & it also reduces lines of code! For example, you could define 3 columns of equal width as follows:We use minmax() to specify a minimum or maximum width for a grid track.Let’s say you want a column to be between 100px and 300px, followed by a 1fr column:The value for min has to be smaller than the value for max. And fr units can’t be used for the min value, but they can be used for the max!By using a 1fr as the max value, you’ll ensure that the track expands and takes up the available space:Used this way, minmax() allows us to create grid tracks that adapt to the available space, but that don’t shrink narrower than a specified size.Now if the browser is resized, the 1st column won’t shrink to less than 250px.You can also use the auto, min-content and max-content keywords as the min or max values.We use justify-content to align the whole grid inside the container.There are a number of values we can work with:Keep in mind that the grid width has to be less than the container width for the justify-content to work!Let’s see an example of each:We use the align-content property to vertically align the whole grid inside the container.Our grid height needs to be less than the container height for this property to work.We can use grid-template-areas to define named areas & move them around inside the grid, and also to expand grid items across multiple rows and/or columns.Let’s use grid-template-areas to build a typical layout with a header up top, a sidebar to the left of the main content, followed by a footer:And the code used:HTML:CSS:Notice that despite the header being the last element in our HTML, it’s still at the top of our page. This is because we’ve defined it’s position in CSS with grid-template-areas using the grid-area property.If we want the sidebar to move below our main content on mobile devices, we can easily do so using a media query:Are you ready to take your CSS skills to the next level? Get started now with my new e-book: The CSS Guide: The Complete Guide to Modern CSS. Get up-to-date on everything from core concepts like Flexbox & Grid, to more advanced topics such as animation, architecture & more!!Thanks for reading! 🎉🎉🎉",javascript,https://itnext.io/css-fundamentals-the-css-grid-guide-1efe31542cfe?source=tag_archive---------41-----------------------
Offline Notice In React Native,"Have you ever seen the red “No Internet Connection” sign in mobile apps. It looks something like this:I will show you how to create this in your React Native application.Step 1:NetInfo exposes info about online/offline statusStep 2:Add the below snippet into main file where you import Netinfo:Our componentDidMount should look like this:Also it is good practise to remove event listeners when your component is about to be unmounted to avoid any memory leakage, so we would do that in the componentWillUnmount lifecycle method.In render:Thanks for reading this article ♥I hope you find this article helpful, Let me know if you have any question in below comment section.🌟 Twitter | 👩🏻‍💻 suprabha.me",javascript,https://medium.com/swlh/offline-notice-in-react-native-a2f679570245?source=tag_archive---------15-----------------------
Janky animations? Web Workers to the rescue,"JavaScript being a single threaded language has to run all the code in the browser as well as perform reflow, paint and garbage collection. This can lead to unresponsive UI if the script contains CPU intensive tasks.In this article we will work with an example where there’s some heavy calculation which results in a bit of a bad performance and later provide a solution with web worker.Let’s see the code first for the main thread so that you get an idea of what we are doing:What’s going on in the above code:You can visit the demo to see how it behaves. Attaching an image of the UI below:You can test by clicking on “Animate” and then clicking on “Calculations” to see the performance issue till the calculation is performed. It might not seem that significant at first but it actually interferes with the animation as I will show you by some profiling.Now lets analyse the performance of our site from developer tools(I am using Chrome for this). Let’s open up the developer tools and head over to Performance tab.We will perform the following steps to record the performance-What you will notice is a waterfall view of our UI which will look something like below:Green bar shows CSS animation that’s running throughout the profile. The dip that is shown with yellow bar is the place where we did our calculations.I have highlighted one of the sections that shows the drop in our frame rate. We can see that frame rate is pretty healthy for most of the recording, but collapses completely whenever we press the button.Let’s try improving this issue with web worker next.Web worker try to run the script in a separate thread which doesn’t interfere with the main thread. The main thread and the worker thread can’t call each other directly, but communicate using an asynchronous messaging API.Here’s our main thread code:The main difference here, compared with the original, is that we need to:Code for our worker thread will look like:In the worker, we have to listen for a message telling us to start, and send a “done” message back when we are done. The long running calculation is actually the same loop as we had before.Let’s perform some analysis for this version. You can visit here to see the new site with service worker code, and capture a new profile.And here’s how that looks:In this profile we pushed the button two times. Each button-press is visible in the overview as two yellow markers.You can notice that there isn’t any frame rate drop in this case as the computation is handled on a separate thread. Once it’s done we communicate back to our main thread. This greatly reduces the load on our main thread and enhances performance.Consider using web workers for such intensive tasks where the main JS thread is blocked leading to bad performance.developer.mozilla.orgcaniuse.comDid you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!",javascript,https://javascript.plainenglish.io/janky-animations-web-workers-to-the-rescue-b650d8f18db3?source=tag_archive---------42-----------------------
"Publish and Reuse React Components to Build Gatsby Sites, Faster","Reusing components between Gatsby sites is a great way to deliver faster and provide your users with a consistent look and feel at every touchpoint.In this demo, I’ll use Bit to publish React components from a marketing site built with Gatsby. I’ll then use my published components to quickly compose a brand new marketing page.These are the steps we’ll go through:When we share and reuse our components across projects — we can build sites faster and safer. We don’t have to waste time reinventing the wheel and can make sure we create a consistent experience for our users.Bit is a platform that makes it easy to publish and document components from any project. It provides both a CLI tool for isolating and publishing components and a cloud hub to host and display them. Pretty useful when building different Gatsby websites with the same components.bit.devI’ll build a landing page for my up-and-coming goldfish business. I’m positive this will draw interest from many goldfish enthusiasts around the world.github.comThe styling in this project is done using styled-components. Every component is themed using the theme object (in the theme.js file) which will be published as well.I’ve also classified my components into four groups: common, elements, blocks, and sections.In this demo, I decided to publish my blocks, elements, and common components — and leave out the more concrete ‘sections’.In general, there’s no absolute right or wrong to which components should be published. It all depends on the way you intend to use your collection. For example, it wouldn’t make much sense to publish very large compositions to a collection that serves as a design system but it does make sense for us since we’re using our collection to compose other, similar, websites.As mentioned earlier, there’s no reason why you shouldn’t share anything that you might find useful. It doesn't have to be solely pure and “dumb” UI components. It should be anything that’s useful for you and your team.For example, in this demo, I’ve published the SEO component which queries data from the site metadata and tags the website accordingly. It may not seem like a very reusable component but in the context of Gatsby, it actually is.The query should be structured the same across our different Gatsby projects. That’s also the case if we were to use the same headless CMS for multiple projects. If a specific logic is something we often use, then it should be a reusable component.To get started I’ll first install Bit on my machine:I’ll then head over to my project’s root directory and initialize a Bit workspace:It’s time to start tracking my components. I’ll add all three groups and tag each of them with the corresponding namespace to make them easier to find and manage both in my local project and in their shared collection. I’ve used the * sign to select all components under each directory.I’ll then set up a compiler for the tracked components. That would essentially decouple them from my build setup and make sure they run in other projects.I’ll then ‘tag’ them (the -a or --all flag is used to select all tracked components that have been changed since the last tag).It’s time to head over to Bit.dev and create a new collection for my soon-to-be-published components. It’s free and takes about 1 minute.I can now publish (“export”) my tagged components:And here’s the end result (notice I’ve also added example code so that I can now visually see and try-out the components):bit.devThe response to my marketing page was good — I’ve received many emails that show me, without a doubt, people are looking for a convenient golden pet 🙂So, I've decided to launch a new marketing page with a simple “Buy Now” button and a different message.To make things as simple as possible, I’ll start a new Gatsby project using the gatsby-starter-default starter.I’ll use a few of my recently published components to create a simpler version of my previous header:We can install Bit components using NPM or Yarn (we can also clone them into our repository using bit import so that we could modify them and push them back to their collection with a bumped version — but that’s beyond the scope of this tutorial).As you can see, for the demo, I’ve used NPM.I’ll then use these components in the index.js page.I’d like to show some features on my new marketing page. For that, I'll install my Feature component. As you can see, it was built as a compound component so that it would be easy to add and customize new features listed on it (using JSX).That’s great but that also creates much boilerplate. To make it easy on me and save me some time I’ve copied the example code presented on its page on Bit.dev.I now have a new marketing page, composed in no time 🎉github.comGatsby used to be thought of only as a static site generator but lately, it’s become obvious that it’s much more than that. Gatsby makes our dev life simpler and easier with its great eco-system of plugins, starters, etc. Thanks to it, we build with optimized code that not only makes good apps and websites but also allows us to deliver faster than we would otherwise.It is in the same spirit that we look for solutions that would make our code more reusable across projects. We’d love to be able to write good code and never have to do it again.That’s where Bit comes in. It does most of the “dirty job” of publishing and sharing components, for us. It helps in component isolation, auto-generates documentation, renders our components in its playground and notifies us of any change, both in the component itself and its usage (for example, if a new project has recently installed one of our components).Put together, Gatsby and Bit create a great development experience where I can build components and use them to compose new websites faster.Thanks for reading, I hope you enjoyed it!blog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.io",javascript,https://blog.bitsrc.io/publish-and-reuse-react-components-to-build-gatsby-sites-faster-7c08c63e6198?source=tag_archive---------17-----------------------
JavaScript Best Practices — Code Organization,"JavaScript is a very forgiving language. It’s easy to write code that runs but has issues in it.In this article, we’ll look at how we should organize our JavaScript code.We should organize code that must be run in a specific order so that the dependencies are obvious.For instance, we can write the code that needs to be run first by writing something like:With the code above, we know that we need to calculate the subtotal before we can use that to calculate the taxes.Then we need both to calculate the total.Also, the parameter for calcTaxes is subtotal so that we know that we know subtotal is a dependency for calcTaxes .If dependencies aren’t clear, then we need to clarify them with comments.For instance, if we didn’t have the parameter for calcTaxes , then we may have to explain that in the comments.We may write:If our sequence of code have dependencies, then we want to check for them before we move on to the next step.For instance, we may want to write assertions or error handling code to check if the required values are available before proceeding.We may write something like:We check that length and width are 0 or bigger before we calculate the area by multiplying length and width .If we’re writing a Node app, we may also use the assert module to check the conditions that we want to check instead.If we have code where the order does matter, then we can make the code read from the top and bottom.This way, we can read the code easily without jumping up and down the page.For instance, we may write something like:This way, we can read them in sequence by grouping the quarterly calculations together and the annual ones together.Grouping related statements are also important. Putting related statements together lets us read them together.Reading unrelated code that overlaps with each other is hard because we’ve to jump around to read it.We should organize our code in a way so that we don’t have to jump around the page to read our code.Also, we should group related statements together so that we can read related statements together.Assertions and error handling code should also be added so that we know what’s required to run the code that comes after the assertions.Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/javascript-best-practices-code-organization-e49e78132f66?source=tag_archive---------16-----------------------
Fibonacci series in BigQuery,"Bence KomarniczkyJun 1, 2020·4 min readBigQuery has some neat functionality to support user-defined functions written in SQL and JavaScript so I thought I’d be fun to tackle a little a challenge with Fibonacci numbers.I’m pretty sure I saw this challenge on Project Euler or somewhere similar so please don’t use this as a way to cheat. The challenge is:Find the sum of the last 6 six digits for the first 1000 Fibonacci numbers. Now, this might sound a bit complicated so we will start out slow by calculating the full Fibonacci numbers and then go from there.The sequence is defined as:So each element is the sum of the previous 2. Here it is in action:0, 1, 1, 2, 3, 5, 8, 13, 21…As you can imagine these numbers can get pretty big pretty quickly. For example the 50th number is 12,586,269,025, that's more than 12 billion, with a b! (The 100th number is: 354,224,848,179,261,915,075 😬)Now that we know what the Fibonacci series is, the obvious question is how do we do a loop in BigQuery. Well, there are ways to do this with BigQuery scripting but here I will use JavaScript user-defined functions instead.Here’s the JavaScript function to calculate the nth Fibonacci number:I know zero to none JavaScript so please call me out in the comments if I’m doing something silly here!Here it is in action if you want to play around with it.In order to create this function in BigQuery we need to use the CREATE TEMP FUNCTION declaration:We can use the same JavaScript function body as above. All we need to do is remove the function fib(n) bit from the JavaScript bit and add the input and output types to the SQL bit. Notice how we say fibonacci(n INT64) to declare that the input must be an integer and RETURNS INT64 to specify the output type.Let’s get some numbers in BigQuery:If you don’t know what’s happening with the array generator and the unnesting in the withstatement then check out my article on FizzBuzz with BigQuery 🍾— it’s all explained there.Here are the results:This took 0.5s to run. Not too shabby and it looks like it’s working just fine since we got the same 50th element as above.If you go back to the top, you can see that the original challenge was to calculate the sum of the last 6 digits for the first 1000 Fibonacci numbers. Doing that will be easy now. The trick here is to realize that in order to accurately calculate the last 6 digits you only ever need the last 6 digits, so we can throw the rest away by using the modulo operator. We need this trick as otherwise, our INT64 would get too big and overflow.We just need to add: var new_num = (numbers[0] + numbers[1]) % 1000000; inside our JavaScript function and then sum up all the results. Here’s the SQL for this:This ran in 0.9s so BigQuery is really sweating now… And the result is:I would recommend you change the above SQL and look at the last 6 digits of the 100th Fibonacci numbers to see if that’s correct!To sum up, JavaScript functions are really easy to add into BigQuery and you can hack some interesting calculations together. Even better, you can even use JavaScript libraries if you include the .js file on a Google Cloud Storage bucket. How awesome is that?towardsdatascience.comtowardsdatascience.com",javascript,https://towardsdatascience.com/fibonacci-series-with-user-defined-functions-in-bigquery-f72e3e360ce6?source=tag_archive---------46-----------------------
"GraphQL — What, Why and How","In this post, I am going to explain what problems GraphQL solves, setting up of a basic GraphQL project using graphql-express, what are root types and how to best structure the code in GraphQL projects.GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. — graphql.orgThe key here is, GraphQL is a query language for “APIs”. Note the word “APIs”. It’s important to know that GraphQL is NOT a query language for Databases. This is a common misconception and there have been numerous debates about this on the internet and social media.Ok, so what do you mean by a query language for APIs?In order to understand this, let’s take a step back and understand how a client(webApp/mobile app) calls APIs to get information from a server. For the purpose of this explanation, let’s assume that these APIs are RESTful (the most common API pattern).For instance, let’s say we are building a medium.com for remote workers. Our backend is a NodeJS server with a Postgres database deployed on some cloud. We define APIs so that our ReactJS client application can consume them. Nothing fancy so far. Let’s think about how these APIs might look like,Again, pretty basic right? Now, let’s think a little bit about front end development for this project and what kinds of use cases we might encounter.Publish reusable React components from any codebase to a single component hub. Use Bit to document, organize, and even keep track of component updates and usage.Share components with your team to maximize code reuse, speed up delivery and build apps that scale.bit.devuse-case #1: As a medium.com user, I need a page where I can see all my stories.In order to achieve this, the front end needs to do the following:This approach works and we are able to gather the information needed to build this page. But, in order to do this, we made in total 4 API calls(1 to get the user meta data and 3 API calls to get post meta data for 3 posts.) and also we dropped the extra information that we received on the floor with the GET /api/post/:id endpoint which probably looked like:We just needed the post summary from this object for each one of these posts. Also, for a user with 100 posts, the client needs to make 100 calls, which is expensive and also the amount of information transferred between the database, server and client is much more than what the client needs.One way to optimize this is to introduce a new endpoint that looks like,GET /api/posts/summary/:user_idAs you have probably guessed already, this endpoint fetches only the information needed(with properly constructed SQL queries) to display the user’s summary page on the backend and returns only what the client needs.With cleverly constructed APIs that fits the needs of the client with hyper-optimized DB queries, we can build fast APIs for this project. But, software is hard and things don’t always go according to the plan. We are all prone to blind spots and API design can often go wrong with ambitious projects like these. No matter, how well we think about API design, there is a good chance we will run into under-fetching and over-fetching (of data) problems. The solution with REST is mostly,GraphQL exists to solve this very problem with REST APIs.Now, coming to the question of how to implement a GraphQL solution and what options are available for me. Before we look at the list of available options/projects, let’s look into building a simple GraphQL solution ourselves.For the purpose of this explanation, we are going to build an ExpressJS server that uses GraphQL as the API layer. Let’s get started,So, what is going on here. I am deliberately starting off in an unknown land and going to try to work it backwards so that you get a full picture of this.On lines 17–22, we are instantiating the familiar express app. But, on top of that, we are also adding a middleware called “graphqlHTTP” which has a bunch of properties. Our endpoint in this case is, http://localhost:4000/graphql. Let’s see what is going on in the browser:We get a pretty interface which is nothing but a web based client for graphQL.Before we go further, let’s explore a little bit into graphqlHTTP middleware and lines 6–17.Now, let’s look at the properties to the middleware,hello is a query type which is a special type in GraphQL. All query types should have a resolver function that tells GraphQL how to resolve this query.Query types can be called like this,orNow, let’s think about our medium.com example. Let’s go ahead and define our graphQL types.For the sake of this example, I have defined two variables, users and posts for storing the data instead of using a real database. On lines 26 and 35, I have defined the corresponding schema types for users and posts. These are basically wrappers around the actual structure using the graphQL APIs.Notice carefully, how I have explicitly declared the types for each field as GraphQLString or GraphQLList(in the case of posts). Also, the posts field in userType is a GraphQLList of postTypeAs we know, users to posts is a one-to-many relationship. And, ideally we want our API to return the user object based on the id we pass. We would also like to populate our user data with the actual posts associated with the user and not just the post id. So, we need data that looks like,Let’s go ahead and write a queryType that resolves to the data that looks like the above code block.Ok what’s going on here?Let’s go ahead an run the query from the browserAs you can see, we asked the data of user with id, “user1” and we asked only for name and the posts. We got exactly what we asked for. Let’s go ahead and add id of the user too,Again, we got what we asked for. Let’s pause for a moment and think about how we could have achieved the same without graphQL. In this example, the postgreSQL database has two tables that look like:Notice how we map 1-to-many relationship between users and posts by having a user_id column in posts table. Each user can have more than one post. But, each post is created only by a single unique user.Typically, in order to fetch a user object by it’s id along with all the related posts, we can write a SQL query that looks like,Or, we can access the same data by using an ORM sequelize which is nothing but a wrapper on top of low level SQL queries.This query is wrapped under a function call which we may call as, getUserInformation(id). In a REST architecture, we will have an endpoint that looks like,This end point will internally call the getUserInformation(id) function on the server to resolve the data and provide it back to the client.Notice, the list of things we do to achieve the same with REST,In REST, we follow the above 3 steps repeatedly for almost every endpoint.This is approach is great and works really well. But, some of the downsides to this approach include,Although, it’s nice to have the freedom of writing specific SQL queries for specific needs,With great freedom, comes great responsibility“Ok! So, now you may be tempted to ask, how does GraphQL talk to the DB? Does it have a mechanism to translate GraphQL queries to SQL queries(for MySQL or PostGreSQL)? If so, am I transferring too much responsibility to a new technology without knowing exactly how it is going to impact my performance?”All of these questions are very valid. The answer is yes and no. Let me elaborate.Does GraphQL translate GraphQL queries to SQL queries?The short answer is, it depends! It entirely depends on the GraphQL solution you adopt. For instance, in the case of medium.com for remote workers, we have written resolvers for user and post . If you notice carefully, resolvers do the heavy lifting of reading information from the database, slicing and dicing it and providing it back to the GraphQL server. So, in this case you have two options:As you can see, this is no different than what we do with REST. This gives you the control and freedom to talk to the DB and also layer it with a GraphQL stack on top to reap its benefits.But, if you consider other options like,These solutions are GraphQL engines that have a compiler that automatically learns your database schema and generates efficient queries to fetch data.Again, it depends on the needs and requirements of the project and you alone can determine what works the best for you.So far, we have seen how to read using GraphQL. But, there are 4 operations associated with any database.C R U D -> Create, Read, Update and DeleteLet’s move on to Create, Update and Delete. Enter MutationsLet me remind you again that Query is a special graphQL type. Similarly, Mutation is another special graphQL type.Let’s go ahead and create a mutation that let’s us create a user and create a post for the user.As you can see, it’s follows a very similar pattern as queryType. We have defined a new variable mutationType, that has two fields, createPost and createUser.Again, the resolver is hand crafter with business logic that involves,As you can see, we are able to create a new post and associate it with the user. And as per our logic on line 6 where we tell our mutation, createPost to return a userType, we get the corresponding user object with up to date posts information after we create a new post.It’s important to note that Mutation encompasses Create, Update and Delete as we control what the operation does inside the resolver function.So far, we have seen the two main root types of GraphQL, Mutation and Query. I have shown how to create GraphQL types using GraphQLObjectType and other GraphQL types. And also, how to create schema using GraphQLSchema.But, this code is very tightly coupled with the type definitions, resolvers and schema all defined together. There is another elegant approach to structuring a GraphQL projects.For keeping the schema separate and flexible, we can create a new file type called schema.graphql which looks like this,This file defines the types, Query, Post and User.Now, let’s move our resolvers to a separate file too.As you can see, the query resolver we wrote initially has been modified and refactored to the resolver that we see on line 25. Also, notice carefully, we have defined a resolver for each field of the User type. And user resolver simply returns the ‘id’ argument down to its child types — id, name and posts. The child types can access the ‘id’ passed down from its parents through the root argument and thereby resolve it separately.This way we can resolve each field separately. Structuring resolvers this way makes the code cleaner, debuggable and has clear separation of concerns.Now, let’s look at how we can import the resolvers and type definitions from resolvers.js and schema.graphql into our app entry file, index.jsWe make use of the “graphql-tools” (which is part of apollo-graphql) package that provides us with the makeExecutableSchema API. This API takes in typeDefs and resolvers as arguments and creates a schema that can be directly passed to the graphqlHTTP middleware.The end result of this code restructuring gives us the exact same functionality with a code base that is much more flexible, re-usable and scalable.GraphQL has been implemented in various projects that cater to different stacks. Some of the popular ones include:I hope you enjoyed reading and learning the basics of GraphQL and bootstrapping a GraphQL project from scratch. Feel free to leave comments/feedback if you have any questions. Thanks for your time!blog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.io",javascript,https://blog.bitsrc.io/graphql-what-why-and-how-cf151eaa25b5?source=tag_archive---------3-----------------------
Creating your first News CLI app using Deno,"Deno is a secure runtime for JavaScript and TypeScript.It was originally created by Ryan Dahl, the same person who developed NodeJS. On May 13, 2020 first stable version of Deno, i.e. v1.0.0 saw the dawn of the day. Since then, it has received a lot of hype and a bunch of early adopters. Some even coined DENO as “DEstroy NOde” or the “Node Killer”.Deno is a secure runtime for TypeScript and JavaScript. Being created by the creator of NodeJS, it aims to solve the bad design choices in NodeJS. Similar to NodeJS, Deno also runs on Chrome’s V8 JavaScript engine. But unlike NodeJS, which is written in C++ and JS, Deno is written in Rust and TypeScript.The concept of Deno was first revealed to the world during JSConf EU 2018, where Ryan Dahl gave a talk on “10 Things I Regret About NodeJS”. You can find recording of the talk here. I’d highly suggest you to go and watch the conference first to get a better understanding of what issues does Deno aims to solve and the motivation behind it.No need to get overwhelmed if you don’t understand the features yet. We will divert from the development and talk about the feature as needed. It will give you a better idea about the meaning as well as the usage of the feature.In this tutorial, we will be developing Newzzer: a simple CLI app to show us the latest news using Deno. This article is focused on people who have basic knowledge of JavaScript and TypeScript. During this process, we will learn and use different features of Deno.The app will have following 2 major features :-Note:- I will be using TypeScript throughout this article. If you are not comfortable with TypeScript, you can remove the types and follow through.Let’s kick things off with the official ‘Welcome’ program. In Deno, we can directly run a program from the file URL. Just run deno run https://deno.land/std/examples/welcome.ts in your terminal.Bravo! You just ran your first Deno program. First time when the command is run, the file is downloaded, compiled and run. The next time when the same file is run, it is neither downloaded, nor compiled. This is due to the fact that Deno caches the dependency and the compiled files.What if we want to refresh the cache and force it to use the new version? --reload to the rescue, you can run the file using this flag to reload source code cache.Let’s go to the code URL and sneak what’s inside the file.It’s a simple console.log , but what’s with the styling of the page? The Deno website can detect that the traffic is coming from a web browser and renders the file in a styled way, but when accessed from a non browser environment, it returns back the raw document. Don’t believe me? Let try to download the file using curl and check the content.We will be creating 5 files as depicted above with each file serving a specific purpose.mod.ts : This is the entry point of our app. According to conventions in famous Deno programs and standard libraries, the name of main file is generally mod.ts, but it can be named anything.types.d.ts : This file will house our custom interface definitions.deps.ts : This file will store all our imports from external dependencies and will serve as an centralised store to import all the dependencies from.api.ts : This file contains the logic for interaction with NewsAPI.error.ts : This file will contain the logic for error handling in our app.If you are coming from a NodeJS background, you’d notice that there is no package.json file.Deno has no centralised/ official package managerThis is an intentional design choice of Deno. Deno uses URLs to import external modules just like a browser would. Because there is no official package manager, anyone can host their module on internet and can be made available for download from the URL.Note: If you are using VSCode, I’d suggest you to download vscode-deno extension to help with auto completion, intellisense and the formatting.Type DefinitionsFirstly, let’s define our type definition in types.d.ts. We will create type definitions for the NewsAPI response using the example provided on their site as reference. We will also create an interface for the config file, which will host the secret-key required to access the NewsAPI.Let’s first define our objectives once again. The app will:-Let’s define all out dependencies first and get them out of the way:-As you can see all the dependencies are in formstd/* which means they are Deno standard modules. The Deno website maintains the list of all the modules in the standard library. You can view the full list here.You can also view a list of third-party modules created by the community here. In this tutorial I’ll be restricting myself purely to the standard modules.Let’s import the modules in deps.tsBehind the scenesDeno uses ECMAScript Modules(ESM) standard. If you are coming from NodeJS, you must be familiar with CommonJS module system.When NodeJS was built, there was no standard for browser modules and NodeJS decided to adopt the CommonJS way. Later ECMAScript standardised ESM modules as the module system for browsers. The support for ESM modules is now built into major modern web browser. For more info on ES modules check out flaviocope’s article.Why did Deno and ECMAScript divert from the CommonJS way? Well, it had issues — majorly complex module resolution. I’ll try my best to explain it. Consider an import inside NodeJS as, const abc = require(""abc"") .While resolving the dependency abc, NodeJS has to checkThese features, while being useful, makes the module resolution a complicated task. ESM standard saves some of the complexity by only allowing absolute and relative URL.Every CLI is incomplete without a help message accessible through -h or --help or when some invalid config is passed. Let’s create the basic help message in error.tsdisplayHelpAndQuit function accept string error as argument. If the error is passed, it shows the error and then the help message, else it just shows the help message. The help message contains information about the supported flags and their purpose.Adding colours to the logs is pretty easy and straight forward, thanks to std/fmt/colors standard module we imported in deps.ts . To create a red bold text with green background, the code is as simple asAt the end of displayHelpAndQuit , we exit the Deno process using Deno.exit()Now that we’ve completed the help message part, let’s integrate it with our entrypoint, i.e. mod.ts and test it.import.meta.main checks if the current file has been executed as the main input and not imported and executed inside another file. It is quite similar to if __name__ == ""__main__"" in python and require.main === module in NodeJS.args in Deno namespace returns the argument vector. parse is used to parse these flags and return key value pairs of flag in a Map like structure.Now let’s run our file and check the outputdeno run mod.ts --help should show the help messageNow let’s try with an invalid flag deno run mod.ts --blahWe want user to configure their own API key from NewsAPI to get the news. For this, we will save the API key in a file .newzzer.json in the home directory of the user.But every operating system has a different home directory, how do we get a path to the home directory? We can get it from the “HOME” environment variable inside the terminal. Deno provides a method Deno.env.get to get the environment variables.Let’s edit our mod.ts and include the logicGet the API key from NewsAPI and let’s set it to our config file.deno run mod.ts --config ""API KEY HERE""Looks like there was an error. We are unable to access the environment variables, but why?Behind the scenesRemember one of the Deno features which was mentioned in the starting?Security — Code runs in a Sandbox by defaultSecurity is one of the major USP of Deno over NodeJS. In Deno, your code runs in a sandbox and you have no access to:-These permission has to be explicitly given while running the code using flags. You can find all the flags from deno run --helpLet’s give the permission and set our config key using deno run --allow-read --allow-write --allow-env mod.ts --config ""API KEY HERE""We can check if the API key is set using deno run --allow-read --allow-write --allow-env mod.tsNow let’s add the juicy part. Getting the news from the API and displaying it. All the logic of the API will be nested in app.tsDeno tries to adhere to W3C standards as much as possible. As a result, Deno comes with a support for fetch API to interact with the web servers.Now let’s use the API in mod.ts . This is the final file along with some error handling.Now let’s run our app, don’t forget to add --allow-net to allow internet access.Yayyy! We built our first fully functional CLI app together.Who doesn’t love a little bonus? You might ask that the app is fully functional, now what’s left to do?Making our app installableWe forgot about making our app installable. Also running the whole command deno run --allow-net --allow-env --allow-read --allow-write mod.ts is a bit time consuming. Let’s handle this.To make our app installable, you just have to addat the top of our mod.ts file. This specifies that the script should be run using Deno and since we have passed all the flags in the script, we don’t have to explicitly define it. Now you can upload the files to anywhere on the internet (I prefer GitHub or a CDN) and install it usingFor example, the final app is hosted on my GitHub repository. It can be installed usingNow the app can be run using newzzer followed by the filters.Some extra commandsAll the codes and snippets can be found at my GitHub repository:A dump of codes and snippets used in my ArticlesWanna check out a slightly advanced version on the app? Check out News-cli at :github.comThe advanced version contains:1. Search for all news with a query word, not just the headlines2. Robust error handling3. Better UI and UXEverything comes to an end, and so did this journey of exploring Deno for the first time and creating your first ever CLI app using it. Here are my thoughts on Deno.With the ever evolving JavaScript ecosystem, Deno has emerged as major competitor to NodeJS. Being created by the person who created NodeJS, it focuses on correcting all the shortcomings of NodeJS. But at the current point of time, I thing it is still premature. The version 1 is just released and there are lot of bug fixes and features to be implemented. Thanks to the active community and early adopters, these bugs are getting reported and fixed. Deno has a lot of potential and may overtake Node in near future, but it is still too early to predict.Tip- Deno is new in the market. While you are developing anything with Deno, you might face issues on how to implement a functionality. Due to being majorly new, you might not be able to find answers on Stack Overflow. I’d suggest you to look at Deno runtime API and Deno manual. The docs are very well written and would help you most of the time.Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/creating-your-first-news-cli-app-using-deno-e1470398c627?source=tag_archive---------13-----------------------
Queue Data Structure in JavaScript,"Read this article in dark mode, easily copy and paste code samples and discover more contents like this on Devjavu.devjavu.spaceYeah, Data Structure. A Data Structure in computing is a “structure” (the arrangement of and relations between the parts or elements of something) of data values, that outlines the relationships that exist between these values, and the functions or operations that can be applied to all or some of these values.Well, Queues are a type of data structure that holds a collection of data in a linear/sequential (a set of related events, movements, or items that follow each other in a particular order) form, where modification is made by the addition of entities at one end of the sequence and the removal of entities from the other end of the sequence.where modification is made by the addition of entities at one end of the sequence and the removal of entities from the other end of the sequence.This might sound familiar to you if you have a basic computer science or accounting knowledge. It’s the concept of FIFO (First In First Out).FIFO in computing and in systems theory is a method for organizing the manipulation of a data structure — often, specifically, a data buffer — in which the first items entered are the first ones to be removed.So far we’ve only talked about the theoretical definitions. Let’s take a Pizza shop as a practical illustration. Que-minos is the name of this shop — they make pizza. Ideally, they take and deliver orders sequentially on a “first come first serve” basis (FIFO). Each customer walks in, heads over to the counter, places an order, and sits to wait, once the first order is complete, the pizza is delivered and the next is line is handled.With JavaScript, it’s almost obvious that queues would have to be implemented using the Array object (Arrays are JavaScript’s strongest data structure — arguably of course).We would be implementing a Queminos class to represent our Queue Structure. This class would have some important methods to:We’ll have a simple folder structure, one file to hold our Queminos class, another to import and use the class, and one more to hold orders data.Here we’re using the class declaration to create the Queminos class. We then declare a private class property “orders” and set the value to an empty array. This would hold all the orders received by Queminos.In ES2019, private class fields are defined using a hash # prefix: This is required so certain fields/properties of a class would not be mutable or accessible outside that class (e.g from an instance of the class). This is designed around the idea of Encapsulation in OOP.Alright, Let’s implement each of the methods:Since we’re unable to access/mutate the “orders” property outside the class (because it’s private), we’ll implement a getter method “orders” that returns the private “orders” property.The placeOrder method takes a single parameter “order”, and adds it to the array of orders. This is basically our enqueue method.The deliverOrder method which serves as an implementation of the dequeue method removes the first element in the orders array (remember FIFO?). First, we use the hasOrders method to check if there are orders existing in the queue before attempting to remove any.medium.comHere’s a list of practical examples of Array methods in JavaScript.The upNext method as mentioned earlier returns the next order in the queue to be addressed. First, we need to check if there are orders, then check if there’s an order right behind the current one.Finally, the hasOrder method simply checks if there are orders in the #orders array.Our Queminos class should now look like this:Note that we’re exporting the Queminos class so we can use it outside of this file.Our JSON data is pretty basic. It’s a list of received orders in object format. You could make it simpler and use strings if you wish.We’ll import our class into the app.js file as well as our JSON data. From here we can test our Queueing system.There you have it, a practical implementation of Queues in JavaScript.Did you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!Here’s a link to the source code.github.comCheers. ☕️",javascript,https://javascript.plainenglish.io/queue-data-structure-in-javascript-ea6601e41201?source=tag_archive---------39-----------------------
"Ionic Framework with VueJS: Split-View Menu with Authentication Flow Using, Vuex & Vue Composition","Clearly InnovativeJun 1, 2020·4 min readIonic VueJS Support is Still in BetaIonic Framework with VueJS build a Split-View user interface with Side menu. The application uses the official vuejs state manager, vuex, for authentication state management in the login flow. We also use information from the store to protect routes and hide the side-menu when the user is not authenticated.The second part of the blog post, we show how to do the same but using the new Vue Composition API to manage state and implement the same functionality.Since Vue Composition API is for v3.x, we re using a plugin to provide that functionality to this v2.x applicationThis post is to cover the important pieces of the code; the bulk of the details are contained in the two video showing each of the specific implementations📺 You can jump to end of post to see the videos…Import the store in main.jsChecking for user when app starts upWe need to get access to the state information in our beforeEnter handler to protect routes. Since we are using namespaces, the user state is at store.state.user and the actual user is store.state.user.user in this case we are checking for the existence of a user to determine if we should allow access to the specific routeFor logging into the application we can access the store using $store and dispatch the login function the combination of the namespace and the action and passing the payload.we use a computed property to get the currentUserFor logging out we dispatch the logout action in the same manner that we dispatched the login action aboveTo hide the menu we use currentUser computed property from the component, we could have used isLoggedInSince we we are using namespaces we need to do a bit more setup on the store.We need to get access to the state information in our beforeEnter handler to protect routesFor logging into the application we don’t need to use the setup approach as we did above, you can just import useAuth and call the function on the moduleIn this component, we are using the new setup functionality to incorporate the information returned from the vue composition api in the the component as data properties.Now to call the logout function you must use this.logout. To hide the menu we can get the loggedIn state from the component nowI tried to keep the store straightforward with no real code for authentication, this is really to demonstrate the approach.So just calling the login function will log the user in and set the appropriate state values.The logout function clears the user object and sets loggedIn to false.",javascript,https://medium.com/@c-innovative/ionic-framework-with-vuejs-split-view-menu-with-authentication-flow-using-vuex-vue-composition-30fafd85202c?source=tag_archive---------44-----------------------
"Create a Custom Web Editor Using TypeScript, React, ANTLR, and Monaco-Editor","Hello and welcome to part 2 of my article on how to create a custom web editor using Typescript, React, ANTLR, and Monaco-Editor. If haven’t read the first part, here is a link to it.In this article, I am going to show you how to implement the language service that will take care of the heavy work of parsing the current typed text in the editor. We then use the generated Abstract syntax tree (AST) returned by the parser to detect any syntax or semantic errors, format the typed text or suggests defined TODOS to the user as they start typing (auto-completion, I’m not going to implement this one. I’ll just give a hint on how to do it and the required API).Basically, this service will expose three functions:So let’s get started.I’m going to add the ANTLR library and add a script to generate parser and lexer from ourTODOLang.g4 grammar file.So first things first, add the necessary libraries: antlr4ts and antlr4ts-cli.antlr4ts is the run-time library for ANTLR in typescript, antlr4ts-cli in the other hand as the name suggests is the CLI which we will use to generate a parser and a lexer for the language.Now, add the following file containing TodoLang grammar rules in the root directory:Now we add a script in the package.json file to generate the parser and the lexer for us by antlr-cli:The files will be generated in the directory ./src/ANTLR.Let’s run antlr4ts script and take a look at the generated files:As we can see, there is a lexer and a parser. If you check the parser file, you will find that it exported a class TodoLangGrammarParser; this class has a constructor constructor(input: TokenStream) that takes as argument the TokenStreamthat theTodoLangGrammarLexergenerates for a given code.TodoLangGrammarLexer has a constructor constructor(input: CharStream) which takes the code as a parameter.The parser contains a method public todoExpressions(): TodoExpressionsContext that returns the context of all TodoExpressions defined in the code. Guess where the TodoExpressions name came from? it’s from the name of the first rule in our grammar rules:TodoExpressionsContext is the root of our AST; each node inside it is another context for another rule. There are Terminals and Node Contexts, terminals hold the final token (maybe an ADD token, TODO token, or “name of the todo” token).TodoExpressionsContext contains the addExpressions and completeExpressions, which comes from the following three rules:On the other hand, each of the context classes contains the Terminal Nodes, which basically holds the text (pieces or tokens of code like “ADD”, “COMPLETE”, “The string representing the TODO”). The complexity of the AST depends on the rules of your grammar; in our case, it’s very simple.As we can see in TodoExpressionsContext, it contains ADD, TODOand STRINGterminal nodes, which correspond to these rules:The string terminal node holds the text of the Todo we are trying to add.Let’s parse a simple TodoLang code to see how the AST looks.In the direcotry ./src/language-service create a parser.tsfile with the following content:All this file does is exports a function parseAndGetASTRoot(code), which takes the TodoLang code and generates the corresponding AST.Parsing the following TodoLang code:will result in this AST:In this section, I’m going to walk you through how to add syntax validation to the editor, ANTLR, which generates lexical and syntax errors for us out of the box. We just need to implement a ANTLRErrorListnerand provide it to the lexer and parser so we can collect the errors as ANTLR parses the code.Create a classTodoLangErrorListener that implements ANTLRErrorListner in the ./src/language-servicedirectory:Every time ANTLR encounters an error during code parsing, it will call this listener providing it with information about the error.We return a list of errors, which holds the position in which the error occurs within our code and a generated error message.Let’s now add the listener to the lexer and parser in the parser.ts file, change its content to:Now inside the ./src/language-service directory, create a file LanguageService.ts which exports the following:We are good to go now and add errors to the editor. For that, first I’m going to create the web worker I talked about in my previous article and add our worker service proxy which will use the language service to do the work.First let’s create TodoLangWorker, which is the worker that will be proxied by monaco. TodoLangWorker will use the language service methods to execute the editor functionalities, those methods that are executing in a web worker will be proxied by monaco, so calling a method inside the web worker is just a matter of calling the proxied one inside the main thread.Inside the ./src/todo-lang folder, create a file TodoLangWorker.ts with the following content:As you can tell, we make an instance of the language service and add a doValidation() method that calls validate on the language service. The other stuff is just for getting the document text from the editor. There are many things that should be added if you want to support multiple file editing._ctx: IWorkerContext is the context of the editor, it holds models (open files…Now let’s create a web worker filetodolang.worker.ts in the ./src/todo-lang directory, with the following content:We used the built-in worker.initialize to initialize our worker and make necessary method proxies from the TodoLangWorker.That’s a web worker, so we must tell webpack to bundle its own file. Head right to the webpack config file and add the following:We named our worker file todoLangWorker.js.Now we should go the editor setup function and add the following:This is how monaco will get the URL of the web worker. Notice that if the label of the worker is the ID of TodoLang, we return the same file name that we used to bundle the worker in webpack.If you build the project now, you could find that there is a file called todoLangWorker.js (or in the dev-tools, you will find in the thread section both workers).Now let’s create a WorkerManager that manages the creation of the worker and helps us get a client of our proxied worker so we can use it later to make the calls.We usedcreateWebWorker to create or run the web worker if it isn’t already created. Otherwise, we get and return the proxy client.We can use workerClientProxy to call the proxied methods.Let’s create DiagnosticsAdapter class that will adapt the errors returned by the language service to the errors that monaco needs to mark them in the editor.What I did there is add a onDidChangeContent listener for every change the user makes. We debounce the changes for 500ms and then call the worker to validate the code and add the markers after adapting them.The API onDidCreateModel is called when the file (model) is created, so at that moment we add the listener for changes.setModelMarkers tells monaco to add error markers, or simply put, underlines the given errors.To apply these validations, make sure to call them in the setup function, and notice that we are using WorkerManager to get the proxied worker.Now everything should work fine. Run the project and start typing some bad TodoLang code; you should see that the errors are being underlined.Here is the project so far:github.comLet’s now add semantic validation to our editor. Remember the two semantic rules I mentioned in the previous article.To check if a TODO is defined, all we have to do is iterate over the AST to get every ADD expression and push them to a list. We then check the existence of a TODO in the defined TODO list. If it exists it’s a semantic error, so get the position of the error from the context of the ADD expression and push the error to an array. The same goes for the second rule.Now call this function and concatenate semantic errors with syntax errors in the validate function.We now have the editor supporting semantic validation.github.comFor auto-formatting, you need to provide and register the formatting provider for Monaco by calling the API registerDocumentFormattingEditProvider. Check the documentation for more details. Calling and iterating over the AST will give you all the information you need to re-write the code in a pretty format.Here is the format method in LanguageService. It takes the code and checks if there are errors in the code, and returns a formatted code:Now let’s add the formatting provider to monaco, and use this service. Here I added format method to the todoLangWorker:Now let’s create a classTodoLangFomattingProvider that will implement the interface DocumentFormattingEditProvider.All it does is get the code and format it using the worker, and then provides monaco with the formatted code and the range of the code that we want to replace, in our case all of the code. You can change the code to support partial formatting.Now go to setup function and register the formatting provider using registerDocumentFormattingEditProvider API.If you run the app now, you should see that it supports formatting.Try clicking on Format document or “Shift + Alt + F”, you should get the following result:github.comTo make auto-completion supports defined TODOs, all you have to do is get all defined TODOs from the AST, and provide them in the completion provider by calling registerCompletionItemProvider in setup. The provider gives you the code and the current position of the cursor, so you can check the context in which the user is typing, if they are typing a TODO in a complete expression, then you can suggest pre-defined TO DOs. Keep in mind that by default Monaco-editor supports auto-completion for pre-defined tokens in your code, you may want to disable that and implement your own to make it more intelligent and contextual.Here is the project:github.comFeel free to contact me if you have any questions, suggestions, or feedback.Thanks a lot for your time.",javascript,https://betterprogramming.pub/create-a-custom-web-editor-using-typescript-react-antlr-and-monaco-editor-bcfc7554e446?source=tag_archive---------9-----------------------
Amazon Interview Question: Unique Staircase Combinations,"The dreaded whiteboard coding interview. We all hate it, yet we all must face it.It’s become the standard for companies to test a candidate’s programming prowess — especially for junior to mid roles. So, let’s sharpen up our skills and try one today.Having spent time in several different countries, I’ve developed a strong network of friends in software-related roles in London, New Delhi, San Francisco, Chicago, Singapore, Berlin, and Munich.On this occasion, I’m sharing something shared in my network — a question asked in an Amazon interview.Given a staircase with n number of steps and provided you can only climb 1 or 2 steps at a time, write a function that returns the number of unique ways to climb the staircase.For e.g. If n is 4, then the answer is 5 unique ways to climb. Here they are:If we think about the problem, the number of steps needed to climb the staircase depends on the sum of the number of steps needed for its previous 2 predecessors. I.e. to calculate uniqueWays(3) = uniqueWays(2) + uniqueWays(1). To calculate the number of unique ways you would climb a staircase with 3 steps, you can simply add uniqueWays(1) (1; since a 1 step staircase only has 1 unique way) and uniqueWays(2) (2; since a 2 step staircase has two unique ways — 1, 1 and 2).So to simplify:uniqueWays(2) === 2uniqueWays(1) === 1Thus: uniqueWays(3) === 2 + 1.This is confirmed since sum of all unique ways to climb a 3 step staircase is 3:Ok, let’s confirm this theory by looking at uniqueWays(4). From the “Question” section of the blog post we know that uniqueWays(4) === 5. And so far we know:Thus if we do uniqueWays(3) + uniqueWays(2) we do come up with 5, which we know to be uniqueWays(4). So we have confirmed that we simply need the answers to the previous 2 values, sum them and we get the value for the current number. Does this remind you of something? Let me write this pattern out for you as a hint;0, 1, 1, 2, 3, 5, 8, 13, 21That’s right! It’s the fibonacci sequence!: f(n) = f(n — 1) + f(n — 2).This makes the problem super easy to solve now. We can simply use recursion to answer this question:This works but it is really slow. It has a time complexity of O(2^n) — we are doing a lot of repeated computations! We can do it a lot faster by just computing iteratively:Now we have cut down the time complexity to O(n) since we are just looping through the range up to the total number of steps, n, once.If you would like to test this out, simply open the console in your browser, paste the code from the function in it and run console.log(uniqueWays(4)) or replace 4 with any number you want the answer of.We have now solve the problem to a decent time complexity. How about I told you that now instead of taking only 1 or 2 steps you could take 1, 3 or 5 steps. Or 1, 2 and 3 steps at once. How can you make this function more general so it also takes a set of steps as a second argument and comes up with an answer.Think about it, it’s not as hard as it looks. Hint: you would have to loop through the number of possible steps you can take at once too and as you compute the answers, save them in a data structure like an array where you can retrieve values based on their index. I’m looking forward to seeing your answers in the comments!If you like questions like these, you may like a previous question such as this one.Did you know that we have four publications and a YouTube channel? You can find all of this from our homepage at plainenglish.io — show some love by giving our publications a follow and subscribing to our YouTube channel!",javascript,https://javascript.plainenglish.io/amazon-interview-question-unique-staircase-combinations-87e6c400ba64?source=tag_archive---------23-----------------------
"Building a Realtime Full Stack ToDo App in 15 Minutes with Vue, Vuetify, VueFire, and Firebase","In this tutorial, we’re going to be building a realtime todo app extremely quickly using Vue and Firebase. For the styling, I have chosen Vuetify. We’ll also be using a package called VueFire that will set up all of our realtime functionality with minimal coding.The first thing we’ll need to do is set up a project in Firebase. Head over to https://firebase.google.com/ create an account and then click “Go to console”. Next, click on the “Add project” button and name it whatever you’d like. Click through the other options choosing the defaults.Next click on “Database” in the left-hand menu.Then, at the top of the screen click “Create database”.For now, choose Start in test mode and pick the default server location.Warning: This will leave your database open for anyone to read and write to it. If you are going to use this in a production setting, PLEASE familiarize yourself with Firestore rules and implement authentication.If you don't have the Vue CLI installed, install it by running the following command in your terminal:Next, let’s create our Vue Project. Open your terminal, cd into the directory of your choosing, and run this command:Chose the default setup.Now, cd into the root directory of the project (cd vue-firebase-todo) and run the following command:You will be prompted to choose a preset. Choose: Default (recommended)After Vuetify has finished installing run the following command:Now we need to head back to Firebase and get the config for our app. On the main screen of your Firebase project, click the web icon.Give your app a Nickname and click next. You will then be presented with some code. Inside of the <script> tags copy everything.Inside of the src folder, create a new folder called firebase. Then add a file called db.js. Inside of this file, add the following code:Next, inside of src/main.js add the following underneath the existing imports:Now inside of src/App.vue, let's remove both of the Vuetify logos and replace them with: <h2> VueFire ToDo</h2>. Next, remove the v-btn on the right side of the v-app-bar. Lastly, we’ll remove the HelloWorld component from the file and delete it from the project. You can find that in components/HelloWorld.vueWhen that is all done your App.vue file should look like this:TemplateNext, inside of the components folder, create a new file called ToDoList.vue. In the template tag, we’ll create a text-field that will allow us to add new ToDos. We’ll also add a v-card that contains a row. Inside the row, we’ll have two columns. One for the name of the ToDo and another for a delete button. We’ll wrap each card in a transition-group so it’ll fade in and out.We’ll add a v-for to the card that will generate a new card for every ToDo we have in our ToDo list. When a user clicks on the delete icon button, we’ll fire logic that will delete the ToDo from our collection.ScriptAt the top of the script tag, we’ll import the database. Next, we’ll add our export default. Then we’ll add a data property with 2 variable properties. One for our list of ToDos and another as a placeholder for new items. Then we’ll add our methods. One to add items and another to delete items.Last, we’ll add VueFire by adding a firestore property and adding our ToDos collection to it. By adding this property we get realtime capabilities without having to add a lot of code to subscribe to our firestore collection.StylesFor our transition-group, we’ll need to add some styles to fade items in and out.As you can see, vue-fire coupled with Firebase makes it tremendously easy to create a realtime application. Add a robust styling package like Vuetify, and you can code an app in record time.Let me know what you think in the comment section below.Bitbucket Repo: https://bitbucket.org/TheDiligentDev/vue-fire-todo/src/masterDid you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",javascript,https://javascript.plainenglish.io/building-a-realtime-full-stack-todo-app-in-15-minutes-with-vue-vuetify-vuefire-and-firebase-e9b3cac2b190?source=tag_archive---------28-----------------------
How to build a sitemap with a node.js crawler and D3.js (Part 2/2),"This tutorial is based on this d3.js example.Create an index.html, main.js and a data.js. That’s all that is needed.Fill the index.html with this few lines of contentYou see, we’re importing d3js from the original source. Also, don’t worry about the path to the main.js file because as we will compile es6 with webpack, the compiled file will then be stored in the build folder.Inside of the body we only need an image tag to fill it with svg source later and to be able to download the sitemap.For the data.js file, paste the output of the crawler into a javascript object like this:In the main.js we first need to import our data and set our final svg width:In your console typeand your main.js is being watched for changes and automatically compiled to build/main.js.First, we need to prepare the data, find out which elements are child elements of the main domain and which are located below another child. Create a function that will process our data:This will create the base of the tree later. Also, we assume that the very first entry of our data array is the base url of the website that we crawled. When remembering the crawler code, this is absolutely logical because we only inserted the domain and added a slash as the first entry of the output. So in case you followed the first part of the tutorial, this assumption should be fine for you.Our newData object will contain the name of the base and — so far — an empty array with children. Next, we’re going to loop through all links in our data array and sort them into children or even grandchildren.Short explanation of the upcoming steps. A URL consists of “domain — slash — path — slash — path” and so on. So our divider for finding children and grandchildren can easily be the slash. As we assume our base is the “domain + slash”, we’re going to remove this part for finding the children. Hence, the path to check will always start with letters: domain.com/my-products is going to be my-products. domain.com/my-products/awesome-product is going to be my-products/awesome-product. This makes it easier to sort it into children and grandchildren.Next, we need to check if there’s still a slash in the path. If not — it’s a direct child of the domain, if yes — it includes grandchildren.Here, we’re going to push the whole dataset entry to the children array for not losing any information. But we’re going to change the name of dataset entry to only store the path. This is relevant for the tree diagram later to only show the paths instead of full URLs.For the grandchildren, we need to cut the path again in parts, finding out which path is the parent path.Then we need to check if this parent path is already stored inside our children array. and if not, we need to add it — because we don’t know if this path will be added automatically later. For example if there is no complete product overview on the page but all products are stored as /products/product.Here we check for the parent of the current path. If the parent was already added to the newData children array, then we’ll use this object. Otherwise we will check in our original data to find an object with the parent path or else create a new parent. Either way, we push the current path to the children of the parent object.After the whole data loop, simple return the new data.Now, we need a function that uses the d3 hierarchy logic and generates the sizes for our svg.At this point, it’s important for all that to work, to stick to the data structure:Otherwise d3 hierarchy won’t work.The last function is huge and it’s mainly some SVG adjusting, styling and adding texts. I won’t go into much detail here because its mainly base svg knowledge — although it is based on d3 which brings some special functions such as data, joins, selections and so on.At the end of our main.js we simply attach the chart function to the window object:Now, head back to the index.html and insert a script part below the image. I use to set type=“module“ because I am only starting Chrome to display this image and download it. I do not plan to make it work for several browsers.So, what are we doing here? First of all, we store the tree svg in a local chart variable. Then we serialize this string and create a bas64 out of it to put as image source. And that’s it. The sitemap will show up.Additional notes: I haven’t tried the JavaScript code on deeper level than 2. And as always: if you think, I did a part too complicated, let me know 😉So, in case of the website of my employer (mediaman.com), the current sitemap looks like this:Again, you find the complete code that I used in Gist:",javascript,https://medium.com/create-code/how-to-build-a-sitemap-with-a-node-js-crawler-and-d3-js-part-2-2-44c425ef4871?source=tag_archive---------43-----------------------
The Top 6 JavaScript Features From ES2020,"The ECMAScript 2020 candidate was approved in early April. Now the ECMA General Assembly has to approve it in June.Let's discover the most important features in this new release!Dynamic import in JavaScript enables you to import JS files only when you need it. It is known as code splitting.Previously, this was possible using Webpack with your project. But with this new version of ES2020, the feature will be handled natively.The dynamic import is asynchronous. Therefore, you will have to use async/await or promise:This feature is going to make our source code a lot more readable!Optional chaining syntax allows you to access nested object properties without having to verify if the parent property exists every time.See the comparison:Instead of declaring a let variable and including some condition to affect its value, I can do it in one line without any condition and by using a const.If the property is not found, undefined will be returned. Super helpful!Nullish coalescing allows you to do an OR operation with nullish values instead of falsey values.We call a variable nullish if it is either undefined or null. On the other hand, a falsey variable can have many other values: undefined and null of course, but also '' (empty string), NaN , 0 , and false.See the difference between || and ??:The || operator returns a truthy value, while the ?? operator returns a non-nullish value.BigInt is a new type of variable that allows you to store very big numbers.Currently, the largest number you can store in JavaScript is 2^53 — 1, which is 9,007,199,254,740,991. It seems some people need more. BigInt allows developers to store numbers without any limits!To declare a number as BigInt, you can use the constructor BigInt() or just add an n at the end of your number:You can compare values between BigInt and Number, but not the strict equality:However, you cannot make an operation between BigInt and Number:The Promise.allSettled method accepts an array of promises and resolves when every promise inside this array is finished, regardless of whether they were resolved or rejected.It can be useful when you don't care if some promises are rejected — you just want to know when they’re finished:Today, JavaScript is used in different environments: Node, browser, web-workers, etc.And when you have to access the global variable this, depending on your environment, you would do it using window (browser), global (Node), or self (web-workers).If you want a cross-platform code, you would have to handle it yourself with something like:globalThis has been created to refer to the global object regardless of the environment. Pretty cool, huh?In this article, I listed the top six features of ES2020 in my opinion. There are others you can easily find, though.Thanks for reading!",javascript,https://betterprogramming.pub/the-top-6-javascript-features-from-es2020-9beba927a9ae?source=tag_archive---------36-----------------------
Use Immer to Manage Your Immutable React State,"If you’ve worked with React, or frankly any JavaScript framework, you’ve probably come across the idea of keeping state “immutable”. In other words, when working with state, you want to avoid mutating the state object when modifying a particular value. Immutability is a core concept of both object-oriented and functional programming, so before I dive into the awesomeness of Immer, I just want to take a brief moment to expound upon what immutability is and why it’s important in the applications we build.The root of what it means to be immutable, regardless of programmatic context, is to be unchangeable. Basically, what’s done is done, there’s no going back. Where the textbook definition and programmatic definition differ is that it isn’t a matter of can you, rather, should you?Because my knowledge of programming is very much limited to the web domain, any further discussion of immutable programming practices will be framed in the context of JavaScript.I think a common misconception of what mutating something in JS is, is reassigning an already declared variable.For example:However, this is incorrect because mutability deals strictly with the object primitive type, and therefore reassigning name to age is not a mutation. As a side note, if this is a behavior you’d like to prevent, either consider using TypeScript, assigning name as const, or both (my preference).If you’ve worked with React at all, you likely already know that the framework is an advocate for and adopter of immutability. The documentation discusses this further, but I can give you the TL;DR. The ultimate goal of a JS framework like React is to be “reactive”. In other words, when a JavaScript value changes, react to it! This can be changing a button color, a numerical value, or perhaps an item within a list of items. Let’s take this state structure as an example:If you wanted to add a hobby for Bob, given this structure, you could just push to his hobbies array:The downside to this approach, however, is that you have no way of knowing which specific properties have changed in that state object since the previous copy of that object was overwritten. This is what happens when you drill down into an object to update a property without caring about other properties of that object.The modern hooks era has certainly changed the way we handle state in our React apps. Most notably, we no longer have to store all of our component’s state in a giant object. The useState hook allows us to itemize our state objects within a given component. If you’re like me, I’m very keen on grouping related component state together within a useState hook. In other words, I often use multiple useState hooks to deal with objects rather than singular values.Here’s a look at one of my most common approaches to updating state for a given state object defined within a useState hook:Any time I update a value in that state object, I have to be sure not to mutate the object as a whole. The initial argument provided to the second of two values returned from useState is the previous state based on the value(s) passed as argument(s).This isn’t that bad, right? Pretty clean cut, fairly readable if you’re familiar with arrow functions, implicit returns and object spreading, but this is the simplest of cases. What if, instead of a form, we’re dealing with an array of user objects like so:And when a user confirms their email, we want to update the emailConfirmed property for their user object.So when mapping over the previous user state, when we find the user with an id equal to the confirmedUserId, we want to spread over that object and update their emailConfirmed property. Again, not crazy, but you can see how this can become cumbersome the more nested your state gets and more generally how complex your app becomes.I know, it took me a second to get to Immer, but I wanted to set the scene for why I like to use it. It has grown increasingly popular in the React space over the last couple of years. Immer operates on a copy-on-write paradigm, which basically just means it’s able to create a draft copy of state and for any modifications made to that copy, a new state will be published based on those modifications. I’m going to steal an illustration from the Immer documentation because I can’t think of a better visualization of this process:So the crux of Immer is its `produce` function that accepts your original state copy as an argument and then a callback with a drafted version of that state. Within this callback, you’re able to hack and slash the drafted copy anyway you like and Immer will be sure that the end result stays immutable. Pretty cool, right? I think so. Let’s look at how we’d go about updating a user’s emailConfirmed property using produce:And within some click handler we’d make use of this producer like so:And just like that we’re able to be much more direct with our state update and, from our perspective, mutate the target user’s object (even though Immer has our back in this regard). This is a bit clunky, in my opinion, though. Another way we could achieve the same effect and be bit more concise is by writing a curried producer. I won’t go into what currying is in JavaScript, but basically a curried producer will only require a state to produce a new value from and any additional arguments relating to the operation. A curried version of updateUsers defined above would be:Note the signature difference between a regular and curried producer:This can be nice if you want to extract functions outside of components for unit-testing, but I personally don’t use this approach very often. There is a tiny package called use-immer created and maintained my the author of Immer, which provides a useImmer (and useImmerReducer) hook that allows you to work with your component state the same way you would with useState. In most cases that I’m using Immer, I’m making use of the useImmer hook. So, replicating our initial approach to updating the users array with useImmer would look something like this:Personally, I like to name the values returned from this hook different from how I would with a regular useState hook just to distinguish the two. Also, with this hook I no longer feel the need to itemize/group state as much seeing as regardless of how nested out state object becomes, updating it will be relatively trivial with Immer.Immer also allows you to track incremental changes to your state with patches. Not every state object will need this, of course, but for some use cases this can be pretty convenient, specifically if you desire having undo/redo functionality. This doesn’t necessarily relate to immutability, but I think it’s a pretty neat opt-in feature of Immer. Here’s a brief example of how patches could be incorporated in the above state set up using Immer’s produceWithPatches. Keep in mind you’d have to restructure the function a bit to handle the tuple that it returns:Note: in attempts to keep the bundle as small as possible, Immer disables patches by default. If you wish to enable them, call Immer’s enablePatches() method at the root of your appHere, our nextState would be the updated state object, patches would be an itemized list of changes that were applied to the state, which would look something like this if Kevin’s emailConfirmed property was updated:and inversePatches would be a function that would do exactly that — undo all the changes contained in patches. Pretty neat, right? I think so, at least. Check out this sandbox if you want to play around with produceWithPatches.My favorite part about Immer is how little it does. It provides such a minimal API and it’s quite easy to go from 0 knowledge to up and running after just breezing through the documentation. It has definitely made my development experience in React a lot more pleasant and I hope that after this you’d be willing to give it a try! Unless, of course, you like handling immutability yourself ;)Also, regarding Immer, Sebastian Markbåge has this to say:If that’s not an endorsement I’m not sure what is.Thanks for listening 👋🏻",javascript,https://levelup.gitconnected.com/use-immer-to-manage-your-immutable-react-state-519f4555fb8c?source=tag_archive---------20-----------------------
Open the Black Box: Understand What Drives Predictions in Deep NLP Models,"Michal MuchaJun 1, 2020·9 min readArticle written together with Josh CasswellObjective - Understand and sense-check model decision making through inspecting attributions- Visualize token attributions- Provide easy to use code so that anyone can apply this to their PyTorch modelsDeep neural networks have done incredible things for NLP — state of the art records broken every year and whole new subdomains of research and applications blossoming.There is no question the models enable researchers to create impressive results. From AI writing poems to shifting the tone of a message while maintaining meaning. How exactly? The results are so impressive that it’s almost too easy to forget to ask.The amounts of parameters, the abstract architectural approaches, the publications written to impress, and the sheer pace of innovation can be distracting — keeping up with deep learning is quite the full time endeavor.Those are hard questions. It’s wonderful to witness some of the leading researchers devoting time and work to provide answers. But there are many practitioners applying the technology, who need ways to understand and reliably explain what it does and why.The objective of this post is to provide a quick and pragmatic introduction of a great, axiomatic approach to model explainability. We focus on NLP, and provide code examples to let you jump straight in and get your own, independent experience.Explainability is key. How are senior stakeholders or compliance expected to put their names behind something, when they don’t what new risks can it bring?A machine learning model is a result of the model architecture, the data set used for training, and the training procedure. That means each model is different, and you need ways to evaluate your own model that you created with your own data set.We’ll focus on a method called integrated gradients, and use a pre-trained review sentiment classifier coming from the well known fast.ai deep learning for coders course.Integrated Gradients is an axiomatic attribution method introduced by the authors Mukund Sundararajan, Ankur Taly, Qiqi Yan in Axiomatic Attribution for Deep Networks (2017).It aims to show what parts of the input to a deep neural network affect the output, and how strongly.Critically, the attribution is calculated as compared to a baseline input. In other words, it doesn’t rely on the discrete immediate gradient wrt. to changes in input, but requires a reference baseline that is assumed to be neutral. Blank canvas vs. painted image.IG is a path method -Its key advantage is in the axiomatic origin. It has desirable traits that come from mathematical principles. That’s in contrast to attribution methods that originate from empirical (read “try and see what happens”) approaches, which can also be great by the way.The two axioms at the heart of the approach:Other desirable natural properties are implied to work as well. One of them is completeness — attributions add up to the exact difference in model (network) outputs for given input vs. baseline.We highly recommend reading the paper, it’s clear, comes with proofs and has carefully prepared examples.The version we are using has been implemented in the Captum library.For an NLP model the input features are words, punctuation marks and other text information ( e.g. a capital letter ) converted to tokens. Tokens are discrete and don’t work with this path method.As tokens are entered into the network via an embedding layer, they gain a continuous representation of meaning. We need to hook into that layer — this interface of words and numbers — to compute meaningful attributions.The model, originally presented in Regularizing and Optimizing LSTM Language Models (2017) by Stephen Merity, Nitish Shirish Keskar and Richard Socher, is taught as part of the free fast.ai deep learning for coders course.AWD-LSTM, very broadly, consists of two stages:It combines a number of carefully applied regularization techniques, and is a great demonstration of understanding of the inner workings of LSTMs.The encoder can be pre-trained on a language modeling task, and in fact this is the approach taught by the fast.ai team. The architecture does really well, while being quite a lot smaller than the various newer transformer models. This can be highly advantageous in situations with constrained resources, and indeed fast.ai deserve high praise for their impactful work to make deep learning accessible.While this article is based on this particular model, the method will apply for any NLP architecture utilizing embedding layers.In this section, we’ll cover 3 topics:The fast.ai AWD LSTM implementation does not work directly with Captum.When collecting gradients, we need to hook into the correct layer. This does not happen automatically and we need to dig into the model structure to find that layer.It appears as if there’s duplication, but in reality, those are nested modules, and the reason for that is the custom embeddings dropout implemented by the fast.ai team.On the other end, Captum IG class needs the model to output a prediction without any extras attached to it. What we get from the AWD-LSTM model is a tuple, where the prediction is accompanied by hidden states coming from the encoder. Additionally, we have to apply softmax.Fortunately, we can wrap the model in a function that presents output in the way we need it.At this point we are free to create a LayerIntegratedGradients instance, which comes from Captum.Calling the attribution method happens like so:We provide:We receive:Next, let’s sum over the embedding dimension and normalize — the attributions are ready to display.To view, clone and run this complete example, check out this notebook.Attributions come out as normalized floating point numbers — one value for each token. There’s a wide choice of options when it comes to visualizing this format. In this section we provide code for you to let you quickly generate bar charts and color-coded tokens (the “saliency map” approach).The objective for appropriate visualization here is to enable immediate understanding of prediction explanations, to let you judge performance quickly and easily.Follow the links below the images to access code examples you can run in Jupyter!Link to the notebook with codeExamples of converting the normalised output using the Chart.js library can be seen here:One nice benefit of specifying the target, is you can get an answer to “Why not?” just as easily as “Why yes?”Going after the answer to this question is as beneficial as calculating the attributions.Baseline is the prediction-neutral input to your model, and is model-specific. It describes your model.Is it really neutral? What makes it budge? Isn’t it great to know those things about the model you use in production?Some approaches you may wish to try for working out a baseline:This can be attached to the documentation of your model. Integrated gradients is a method that works with any predictor function, therefore it can even be included in model tests, looking out for things like unfair bias.We include an example discovery process in the example notebook with IMDB reviews. The baseline sequence is constructed using the <BOS> token and followed by repetitions of a token that shows high ambiguity in class probabilities. The token is kept as a keyword argument to enable quick experimentation.An early issue we found when applying IG to the fast.ai implementation was the failure on longer input sentences. Usage showed that sentences that evaluated to greater than 70 tokens were generating errors. Careful debugging showed this to be caused by the default setting of the Back Propagation Through Time (BPTT) parameter to 70 in the AWD-LSTM model.The bptt parameter is the value used (by the algorithm of the same name) to specify how long a sequence an RNN based language model will consider in trying to predict the next word. In this case a maximum of 70 tokens were considered before trying to predict the next token.BPTT is a useful algorithm for training RNNs, it increases the speed of training significantly. However internally is breaking sequences into smaller sequences of 70 which was causing issue with other parts of the model when trying to create mask sizes (even completely empty masks) of the correct size.The solution was to set the model’s bptt property higher than the sentence length as part of setting the model to evaluation mode.Explaining very large deep neural networks with tens of millions of parameters requires new tools and approaches. We hope this has presented another tool in the Data Scientist’s kit and helps to form the basis of their own test approach.An axiomatic explanation method, coupled with a simple, iterative and visual inspection process, makes it easy to “get to know” the ML model. As data scientists do that, they can simultaneously create test cases.The authors are members of the London fast.ai study group. We run free, informal course iterations focused on practicing modern deep learning and building projects. If you’d like to join our group, do get in touch!Josh is a senior data scientist working in the finance sector, interested in the uses of NLP techniques in a business context and python in a snowboarding context.Michal delivers data science and AI projects through his consultancy create.ml He provides training in data science, machine learning, and Python to institutions and individuals.",python,https://towardsdatascience.com/open-the-black-box-understand-what-drives-predictions-in-deep-nlp-models-833f3dc923d0?source=tag_archive---------34-----------------------
Capstone Project — The Battle of Neighborhoods (Week 1),"cahyati sangaji (cahya)Jun 1, 2020·18 min readA Visual Approach to determine Strategic Locations for Masks and Medical Devices Distribution for COVID-19 treatment based on confirmed cases on May 28,2020 at red zone areas to measure “new normal” readinessCahyati S. SangajiApplied Data Science Capstone by IBM/CourseraSince the beginning of 2020, Jakarta and many other cities around the world have been under attack by an invisible army called ‘Novel Corona Virus’, also known as ‘Covid-19’. Every effort has been focusing on solving or minimizing problems, including Data Scientists. Data Scientists assessed the situations in places around the world, such as availability, amount, and geographical distribution (i.e. locations) of health infrastructures, such as virus testing centers and authorized hospitals to treat affected patients. In this article, we would like to present a simple analysis for determining strategic locations for the distribution of masks and medical devices for COVID-19 treatment, based on confirmed cases on May 28, 2020, and the red zone areas for “new normal” condition analysis.A few Identified factors that influence our decision are:The following data sources are needed to extract/generate the required information:Let’s start the Project by importing necessary Python libraries.Import necessary librariesOutput:Make sure that we have created a Foursquare developer account and have our credentials handy.Output:Read and show all data used.Read and show data Covid-19 cases per district.Read and show the top 5 data rows from Covid-19 cases per district.Read and show the bottom 5 data rows from Covid-19 cases per districtRead and show the total population data in DKI Jakarta 2020.Total population in Jakarta.Read and show the top 5 data rows from total population in DKI Jakarta, 2020.Read and show the data from 10 districts most pupulated in DKI Jakarta, 2020.Read and show the top 5 data rows from 10 most populated areas in DKI Jakarta, 2020 per district.According to the information update from Kompas.com (megapolitan.kompas.com), the following hospitals are the existing reference hospitals for Covid-19 testing in Jakarta area:Construct a Pandas data frame for subsequent data analysis.Read and show Hospital data that provide treatment Covid-19.Read and show the top 5 data rows from Hospital data providing treatment Covid-19.This sums up our data mining and data exploration section. In the following METHODOLOGY section, we will describe the process of how to do a ‘Visual’ approach to better understand our data using data science and data analytics tool kits.First, we create a new dataset of only positive cases from the Covid-19 Case table on May 28, 2020.Output:Remove / drop irrelevant columns for this analysis.Check if there are any missing or null values.Output :Out[153]:From all these processes: data mining, preparation, and exploration, the total number of Covid-19 confirmed positive cases in Jakarta is 5,061 per 28 May 2020, distributed across 6 main municipalities or cities in Jakarta, across 268 districts (or ‘Kelurahan’) out of just over 92.736 population of Jakarta.East Jakarta (Jakarta Timur) has the highest number of total POSITIVE cases with 1162 confirmed positives. Just like any other city, each city/municipality has many neighborhoods that can be used to pinpoint the location of the new proposed Covid-19 testing center along with further analysis of the neighborhood using FourSquare API and Folium map visualization technique.To assist in the analysis, we will use the ‘’free services” provided by Open Cage Geocode (https://opencagedata.com/) to get the latitude and longitude of cities, districts, particular venues, or neighborhoods. We will start by opening an account and downloading the required dependencies for our analysis. Terms and condition applies. Please refer to their website for further details.Output :Similarly, we can use the API service from OpenCage Geocoder to obtain the latitude and longitude of all districts in Jakarta.Besides, we also need to get the latitude and longitude of all Covid-19 testing centers in Jakarta that we have checked from the source www.kompas.com.We then need to know how to get a map of the city that we are interested in (i.e. Jakarta) to present our data to the stakeholders using a ‘Visualization’ approach.We have downloaded all the required dependencies earlier in the report, and now we are ready to use the FOLIUM API service as described in the following section.The map shows the main outer ring roads surrounding the city of Jakarta. It does NOT, however, show the official territorial boundary of the city concerning other administrative regions in the east, west, and south of Jakarta.However, because the author is from Indonesia, we know roughly which neighborhood belongs to Jakarta and which does not. In this scenario, we want to propose a strategic locations (i.e. neighborhood) for the investing group within the Jakarta governmental area.The chart below show the population density in Jakarta.The chart below show the population density in Jakarta, per districtBased on the graph results shown that areas need the distribution of masks the most is Central Jakarta (Jakarta Pusat) with the most populated areas. Then 5 districts that mostly need for a distribution of masks are Kali Anyar, Kampung Rawa, Galur, Tanah Tinggi, and Kerendang.To better understand and estimate the territories or areas that are within the administrative government of Jakarta city, we need to plot all the districts that we have downloaded from the riwayat-file-covid-19-dki-jakarta-jakartagis.hub.arcgis.com site together with their latitude and longitude values. The following lines of Python code will execute the task using Folium API.As we can see from the above map, most of the districts are within the main outer ring roads surrounding the city, and others are situated outside the main ring roads. To solve in our business challenge, we need to show the extent and the distribution medical devices for treatment of COVID-19 positive case-patients within the city of Jakarta based on the number that we obtained from the government site. The following lines of Python code will achieve the task and present the data in a clear visual approach.This is a similar map plot that we can see from the government task force for Covid-19 cases in Jakarta. Their graph can be seen in this link: https://corona.jakarta.go.id/id/peta-persebaran. As we can see, most of the regions in Jakarta are now in the ‘RED’ zone, with the radius of the circle represent the relative extent of Covid-19 distribution in the City of Jakarta.A better presentation of the data would be to use a ‘slider’ in the map that shows the growth of the circle day by day or simply an animation that shows the daily growth of Covid-19 cases in the city. An app developer might develop an App that alerts vehicles/road users that alerts that they are not allowed to pass the RED zone within the city. This App could save lives! The next set of problems that we need to solve is to show the location of existing and approved Covid-19 testing centers (or reference hospitals) and see how well they are distributed to each other within the city and in which regions of Jakarta. The following lines of Python code show how. We will first try to plot the hospitals WITHOUT the RED circles as that might cause distraction.As you can see, the hospitals are quite sparsely distributed within each other except the two hospitals in the south are relatively close to each other (i.e. Fatmawati and Pasar Minggu hospitals). Let’s see how strategic they are in accomodating the extent of positive cases patients in the city. We can do this by overlaying the two data within a single map as shown in the following codes:We can see from the results of the distribution of COVID-19 cases and the location of hospitals, almost all hospitals require a lot of medical equipment for COVID-19 treatment. In addition to Fatmawati hospital and the Pasar Minggu hospital, the distribution of the COVID-19 case is not as extensive as other hospitals.We will try to analyze locations in the red zone based on the location of the hospital in the middle of the red zone. We determine based on the location of the Tarakan Hospital, Central Jakarta.Let’s begin by trying to get the top 100 venues that are within Tarakan Hospital neighborhood and are within a radius of 500 meters of our candidate Covid-19 testing center using FOURSQUARE API. First, let’s create the GET request URL. Name that URL, url.Output:Get URL for the API in Tarakan Hospital neighborhood.Next, let’s make a request using REQUEST library, and name our query results for Tarakan Hospital area, results.Next, we will use the above function (get_category_type) to extract information from the JSON file related to venues in the Tarakan Hospital neighborhood. The following line of code should do the trick:Output:Based on the results generated by the FOURSQUARE API, we can locate the business site around Tarakan hospital and identify affected business locations in the red zone.The next set of challenges that we need to tackle is to gain slightly more insights (profile) of the Tarakan hospital area. To simplify our analysis, we will just use the Euclidian (distance-based) clustering technique which is part of the unsupervised machine learning technique. In particular, we will use K-means clustering.To start, we need to decide the best K-value for our analysis. We will let the K-means clustering algorithm to calculate this for us. The following lines of code will carry out the task.The X-axis of the plot shows various number of K-values that we can use for our clustering analysis. As we can see from the chart, the curve starts flattening out at K=3. Therefore, we will use a K=3 to cluster neighborhoods surrounding our proposed Covid-19 testing center. The following lines of code assign Cluster label to all venues that are within a 500-meter radius of our Covid-19 testing center in Tarakan Hospital area:Output:To better visualize the clustering of our neighborhood, we will need to create a custom function that we call ‘regioncolors’ that will assign a color to each area within a 500-meter radius of our proposed facility. The following line of code should help us with this task.At this stage, we have assigned cluster labels to all of our neighborhood venues, and we have assigned unique colors to each cluster. Next, we can then visualize our clustering analysis to a Folium map to see how all of these venues are geographically distributed within the 500-meter radius that we specified surrounding the proposed facility.Then we compiled a map of the results of this business location with a map of the distribution of COVID-19 cases.The result of analysis is the location of the business which is in the Tarakan hospital neighborhood and is within a radius of 500 meters. Then, we also get the most congested cluster if businesses apply normal conditions in the red zone, potentially increasing cases of contracting the COVID-19 virus within the area.The project aims to provide information to local people who must be alerted to go out of the house from the distribution of the COVID-19 case in Jakarta. It also aims to provide information on areas that are most needed for a lot of mask distribution, according to population density in the area.Further, it provides information on which hospitals that need the most medical equipments for COVID-19 treatment, possibly even additional medical personnels (doctors and nurses). It also provides information on the business neighborhood which shall implement Covid-19 health protocol with a high discipline when “new normal” comes.This project helps mask sellers to understand potential distribution areas according to population density in Jakarta. It also helps the distribution of medical devices for corona care to hospitals that are estimated to have a large number of patients or even helps analyzing which hospitals need additional medical personnel (doctors and nurses).It will also provide awareness to help business owners who run businesses surrounding the adjacent clusters to be better informed, with the density of people within the business neighborhood.",python,https://medium.com/@cahyati2d/capstone-project-the-battle-of-neighborhoods-week-1-1f2035d6f06a?source=tag_archive---------26-----------------------
Python Database API,"Yashi AgarwalJun 1, 2020·6 min readFirstly, Let us understand What is Database..A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques.The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a “database system”. Often the term “database” is also used to loosely refer to any of the DBMS, the database system or an application associated with the database.Now Lets understand the meaning of API(Application Programming Interface) API is a language and message format used by an application program to communicate with the operating system or some other control program such as a database management system (DBMS) or communications protocol. APIs are implemented by writing function calls in the program, which provide the linkage to the required subroutine for execution.Thus, an API implies that a driver or program module is available in the computer to perform the operation or that software must be linked into the existing program to perform the tasks.SQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle.The sqlite3 module was written by Gerhard Häring. It provides a SQL interface compliant with the DB-API 2.0 specification described by PEP 249. A complete SQL database with multiple tables, indices, triggers, and views, is contained in a single disk file. The database file format is cross-platform — you can freely copy a database between 32-bit and 64-bit systems or between big-endian and little-endian architectures.SQLite natively supports the following types: NULL, INTEGER, REAL, TEXT, BLOB. The following Python types can thus be sent to SQLite without any problem:cursor(factory=Cursor)-The cursor method accepts a single optional parameter factory. If supplied, this must be a callable returning an instance of Cursor or its subclasses.commit()-This method commits the current transaction. If you don’t call this method, anything you did since the last call to commit() is not visible from other database connections. If you wonder why you don’t see the data you’ve written to the database, please check you didn’t forget to call this method.rollback()-This method rolls back any changes to the database since the last call to commit().close()-This closes the database connection. Note that this does not automatically call commit(). If you just close your database connection without calling commit() first, your changes will be lost!2. Cursor Objects — Database Queriesexecute(sql[, parameters])-Executes an SQL statement. The SQL statement may be parameterized (i. e. placeholders instead of SQL literals).executemany(sql, seq_of_parameters)-Executes an SQL command against all parameter sequences or mappings found in the sequence seq_of_parameters. The sqlite3 module also allows using an iterator yielding parameters instead of a sequence.fetchone()-Fetches the next row of a query result set, returning a single sequence, or None when no more data is available.fetchmany(size=cursor.arraysize)-Fetches the next set of rows of a query result, returning a list. An empty list is returned when no more rows are available.The number of rows to fetch per call is specified by the size parameter. If it is not given, the cursor’s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned.Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next.fetchall()-Fetches all (remaining) rows of a query result, returning a list. Note that the cursor’s arraysize attribute can affect the performance of this operation. An empty list is returned when no rows are available.close()-Close the cursor now (rather than whenever __del__ is called).arraysize-Read/write attribute that controls the number of rows returned by fetchmany(). The default value is 1 which means a single row would be fetched per call.Creating a Connection object that represents the database. Here the data will be stored in the example.db file:2. Create a TableCreating a Cursor object and a table name material which contains date , trans, symbol, qty and price attributes.3. Insert commandcall its execute() method to perform SQL command -execute.4. Reading the Data5. Update the Records (Optional)Now consider that qty changed to 500 on 2008–05–106. Delete a record from a table (USE it carefully)Lets delete the record of date — 2006–01–057. Closing the ConnectionThe PRAGMA statement is an SQL extension specific to SQLite and used to modify the operation of the SQLite library or to query the SQLite library for internal (non-table) data.PRAGMA command syntaxThe PRAGMA statement is issued using the same interface as other SQLite commands (e.g. SELECT, INSERT) but is different in the following important respects:Here comes the end of the article.I hope you like it and gain something from it.",python,https://medium.com/@er.26yashiagarwal/python-database-api-beb8e61109cf?source=tag_archive---------27-----------------------
Chunking and Extracting information using NLTK — PART -6,"Jeffy samJun 1, 2020·5 min readCongrats making it to this article, we have already come a long way but still, we still got few more topics to cover. Check out my previous article if you haven’t.In this article, we will learn about the pipeline architecture of Information extraction of any data and along with it, we will also learn about different concepts like chunking and chinking.Most of the data on the web are in the form of unstructured data, obtaining information from that data is difficult. So we must find a way to convert such unstructured data into a structured one.We are given structured data of different colleges from different locations in tabular form. This is an example of structured data. If we want to query the colleges from a particular location, we can make use of data structures.This data can be stored as a list of tuples in the format (college, relation, location).We can do something like this as shown above. It becomes trickier if we are trying to obtain such information from unstructured data.In the case of unstructured data, we can follow a simple information extraction system, where we process the data through a series of steps and at the output we obtain a list of tuples in the format (entity, relation, entity).In machine learning, these whole block of steps is known as preprocessing.We have already gone through the first 4 points in my previous articles.If we combine them all in one single function, it will look like thisI will use two sentences so that you can observe what happens at each step.In part of speech tagging, we tag individual words. Chunking works on top of POS tagging and it chunks together set of tokens like Verb phrase or Noun.It is a very important concept if you are working with unstructured data and you want to obtain information from it.In Noun Phrase chunking it follows a rule which determines if the context it takes into consideration represents a Noun phrase. If the function finds a Determiner followed by an Adjective and then a noun then the chunk will be tagged as a Noun Phrase.First, we will have to represent a grammar which will be used to chunk.Then I will specify the sentence I want to chunkNLTK comes with RegexParser(gram) function and we have to parse the regular expression grammar inside the parenthesis.After tagging them individually, we can then consider tagging them as a whole using the parse() function.For a visual representation of the tree, you can use the following function.Chinking is somewhat similar to chunking but instead of taking chunks as a whole, we have to define a chunk that we want to remove. This piece of chunk is called a chink and the process is known as chinking.The only difference is in specifying the grammar just like we did before. By grammar, I mean by specifying the regular expressionI will be using the same sentence for this example tooIOB is a widely used format to represent chunks. Inside the chunks the tokens have these 3 special tags i.e. I [inside], O[outside],B[begin].We have this NLTK corpus module which contains almost 300k words of WSJ which are divided into “train” and “test” data.These data are chunked in IOB format and we can easily access them through conll2000 submodule.We can use this train data to train our chunkersBy specifying the chunk_types argument as “NP”, we are only considering NP chunks for our purpose.Thank You for taking the time to read this article.Have a fantastic day :)",python,https://medium.com/@jeffysam02/chunking-and-extracting-information-using-nltk-part-6-5ecceeb4aac4?source=tag_archive---------47-----------------------
Customer Segmentation in Python,"Irfan Alghani KhalidJun 1, 2020·9 min readSuppose that we have a company that selling some of the product, and you want to know how well does the selling performance of the product.You have the data that can we analyze, but what kind of analysis that we can do?Well, we can segment customers based on their buying behavior on the market.Keep in mind that the data is really huge, and we can not analyze it using our bare eye. We will use machine learning algorithms and the power of computing for it.This article will show you how to cluster customers on segments based on their behavior using the K-Means algorithm in Python.I hope that this article will help you on how to do customer segmentation step-by-step from preparing the data to cluster it.Before we get into the process, I will give you a brief on what kind of steps we will get.In this step, we will gather the data first. For this case, we will take the data from UCI Machine Learning called Online Retail dataset.The dataset itself is a transactional data that contains transactions from December 1st 2010 until December 9th 2011 for a UK-based online retail.Each row represents the transaction that occurs. It includes the product name, quantity, price, and other columns that represents ID.You can access to the dataset from here.Here is the size of the dataset.For this case, we don’t use all of the rows. Instead, we will sample 10000 rows from the dataset, and we assume that as the whole transactions that the customers do.The code will look like this,Here is the glimpse of the dataset,After we sample the data, we will make the data easier to conduct an analysis.To segmenting customer, there are some metrics that we can use, such as when the customer buy the product for last time, how frequent the customer buy the product, and how much the customer pays for the product. We will call this segmentation as RFM segmentation.To make the RFM table, we can create these columns, such as Recency, Frequency, and MonetaryValue column.To get the number of days for recency column, we can subtract the snapshot date with the date where the transaction occurred.To create the frequency column, we can count how much transactions by each customer.Lastly, to create the monetary value column, we can sum all transactions for each customer.The code looks like this,Here is the glimpse of the dataset,Right now, the dataset consists of recency, frequency, and monetary value column. But we cannot use the dataset yet because we have to preprocess the data more.We have to make sure that the data meet these assumptions, they are,The data should meet assumptions where the variables are not skewed and have the same mean and variance.Because of that, we have to manage the skewness of the variables.Here are the visualizations of each variable,As we can see from above, we have to transform the data, so it has a more symmetrical form.There are some methods that we can use to manage the skewness, they are,Note: We can use the transformation if and only if the variable only has positive values.Below are the visualization each variable and with and without transformations. From top left clockwise on each variable shows the plot without transformation, log transformation, square root transformation, and box-cox transformation.Based on that visualization, it shows that the variables with box-cox transformation shows a more symmetrical form rather than the other transformations.To make sure, we calculate each variable using the skew function. The result looks like this,Here is how to interpret the skewness value. If the value is close to 0, the variable tend to have symmetrical form. However, if it’s not, the variable has skew on it. Based on that calculation, we use variables that use box-cox transformations.Based on that calculation, we will utilize variables that use box-cox transformations. Except for the MonetaryValue variable because the variable includes negative values. To handle this variable, we can use cubic root transformation to the data, so the comparison looks like this,By using the transformation, we will have data that less skewed. The skewness value declines from 16.63 to 1.16. Therefore, we can transform the RFM table with this code,It will look like this,Can we use data right now? Not yet. If we look at the plot once more, each variable don’t have the same mean and variance. We have to normalize it. To normalize, we can use StandardScaler object from scikit-learn library to do it. The code will look like this,The data will look like this,Finally, we can do clustering using that data.Right after we preprocess the data, now we can focus on modelling. To make segmentation from the data, we can use the K-Means algorithm to do this.K-Means algorithm is an unsupervised learning algorithm that uses the geometrical principle to determine which cluster belongs to the data. By determine each centroid, we calculate the distance to each centroid. Each data belongs to a centroid if it has the smallest distance from the other. It repeats until the next total of the distance doesn’t have significant changes than before.To implement K-Means in Python is easy. We can use the KMeans function from scikit-learn to do this.To make our clustering reach its maximum performance, we have to determine which hyperparameter fits to the data. To determine which hyperparameter is the best for our model and data, we can use the elbow method to decide. The code will look like this,Here is the result,How to interpret the plot? The x-axis is the value of the k, and the y-axis is the SSE value of the data. We will take the best parameter by looking at where the k-value will have a linear trend on the next consecutive k.Based on our observation, the k-value of 3 is the best hyperparameter for our model because the next k-value tend to have a linear trend. Therefore, our best model for the data is K-Means with the number of clusters is 3.Now, We can fit the model with this code,By fitting the model, we can have clusters where each data belongs. By that, we can analyze the data.We can summarize the RFM table based on clusters and calculate the mean of each variable. The code will look like this,The output from the code looks like this,Besides that, we can analyze the segments using snake plot. It requires the normalized dataset and also the cluster labels. By using this plot, we can have a good visualization from the data on how the cluster differs from each other. We can make the plot by using this code,And here is the result,By using this plot, we know how each segment differs. It describes more than we use the summarized table.We infer that cluster 0 is frequent, spend more, and they buy the product recently. Therefore, it could be the cluster of a loyal customer.Then, the cluster 1 is less frequent, less to spend, but they buy the product recently. Therefore, it could be the cluster of new customer.Finally, the cluster 2 is less frequent, less to spend, and they buy the product at the old time. Therefore, it could be the cluster of churned customers.In conclusion, customer segmentation is really necessary for knowing what characteristics that exist on each customer. The article has shown to you how to implement it using Python. I hope that this article will be useful to you, and you can implement on your case.If you want to see how the code is, you check on this Google Colab here.[1] Daqing C., Sai L.S, and Kun G. Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining (2012), Journal of Database Marketing and Customer Strategy Management.[2] Millman K. J, Aivazis M. Python for Scientists and Engineers (2011), Computing in Science & Engineering.[3] Radečić D. Top 3 Methods for Handling Skewed Data (2020), Towards Data Science.[4] Elbow Method for optimal value of k in KMeans, Geeks For Geeks.towardsdatascience.comtowardsdatascience.comtowardsdatascience.com",python,https://towardsdatascience.com/customer-segmentation-in-python-9c15acf6f945?source=tag_archive---------1-----------------------
Ultimate Guide to Python Debugging,"Martin HeinzJun 1, 2020·6 min readEven if you write clear and readable code, even if you cover your code with tests, even if you are very experienced developer, weird bugs will inevitably appear and you will need to debug them in some way. Lots of people resort to just using bunch of print statements to see what's happening in their code. This approach is far from ideal and there are much better ways to find out what's wrong with your code, some of which we will explore in this article.If you write application without some sort of logging setup you will eventually come to regret it. Not having any logs from your application can make it very difficult to troubleshoot any bugs. Luckily — in Python — setting up basic logger is very simple:This is all you need to start writing logs to file which will look something like this (you can find path to the file using logging.getLoggerClass().root.handlers[0].baseFilename):This setup might seem like it’s good enough (and often it is), but having well configured, formatted, readable logs can make your life so much easier. One way to improve and expand the config is to use .ini or .yaml file that gets read by logger. As an example of what you could do in your config:Having this kind of extensive config inside you python code would be hard to navigate, edit and maintain. Keeping things in YAML file makes it much easier to set up and tweak multiple loggers with very specific settings like the ones above.If you are wondering where all these config fields came from, these are documented here and most of them are just keyword arguments as shown in the first example.So, having the config in the file now, means that we need to load is somehow. The simplest way to do so with YAML files:Python logger doesn’t actually support YAML files directly, but it supports dictionary configs, which can be easily created from YAML using yaml.safe_load. If you are inclined to rather use old .ini files, then I just want to point out that using dictionary configs is the recommended approach for new application as per docs. For more examples, check out the logging cookbook.Continuing with the previous logging tip, you might get into a situation where you need log calls of some buggy function. Instead of modifying the body of said function, you could employ logging decorator which would log every function call with specific log level and an optional message. Let’s look at the decorator:Not gonna lie, this one might take a bit to wrap your head around (you might want to just copy-paste it and use it). The idea here is that log function takes the arguments and makes them available to inner wrapper function. These arguments are then made adjustable by adding the accessor functions, which are attached to the decorator. As for the functools.wraps decorator - if we didn't use it here, name of the function ( func.__name__) would get overwritten by name of the decorator. But that's a problem because we want to print the name. This gets solved by functools.wraps as it copies function name, docstring and arguments list onto the decorator function.Anyway, this is the output of above code. Pretty neat, right?Easy improvement to your code that makes it more debuggable is adding __repr__ method to your classes. In case you're not familiar with this method - all it does is return string representation of an instance of a class. Best practice with __repr__ method is to output text that could be used to recreate the instance. For example:If representing object as shown above is not desirable or not possible, good alternative is to use representation using <...>, e.g. <_io.TextIOWrapper name='somefile.txt' mode='w' encoding='UTF-8'>.Apart from __repr__, it's also a good idea to implement __str__ method which is by default used when print(instance) is called. With these 2 methods you can get lots of information just by printing your variables.If you for whatever reason need to implement custom dictionary class, then you can expect some bugs arising from KeyErrors when you try to access some key that doesn't actually exist. To avoid having to poke around in the code and see which key is missing, you could implement special __missing__ method, which is called every time KeyError is raised.The implementation above is very simple and only returns and logs message with the missing key, but you could also log other valuable information to give you more context as to what went wrong in the code.If your application crashes before you get a chance to see what is going on in it, you might find this trick quite useful.Running the application with-i argument ( python3 -i app.py) causes it to start interactive shell as soon as the program exits. At that point you can inspect variables and functions.If that’s not good enough, you can bring a bigger hammer — pdb - Python Debugger. pdb has quite a few features which would warrant an article on its own. But here is an example and a rundown of the most important bits. Let's first see our little crashing script:Now, if we run it with -i argument, we get a chance to debug it:Debugging session above shows very briefly what you could do with pdb. After program terminates we enter interactive debugging session. First, we import pdb and start the debugger. At that point, we can use all the pdb commands. As an example above, we print variable using p command and list code using l command. Most of the time you would probably want to set breakpoint which you can do with b LINE_NO and run the program until the breakpoint is hit ( c) and then continue stepping through the function with s, optionally maybe printing stacktrace with w. For a full listing of commands you can go over to pdb docs.Let’s say your code is for example Flask or Django application running on remote server where you can’t get interactive debugging session. In that case you can use traceback and sys packages to get more insight on what's failing in your code:When run, the code above will print the last exception that was raised. Apart from printing exceptions, you can also use traceback package to print stacktrace ( traceback.print_stack()) or extract raw stack frame, format it and inspect it further ( traceback.format_list(traceback.extract_stack())).Sometimes you might be debugging or experimenting with some function in interactive shell and making frequent changes to it. To make the cycle of running/testing and modifying easier, you can run importlib.reload(module) to avoid having to restart the interactive session after every change:This tip is more about efficiency than debugging. It’s always nice to be able to skip a few unnecessary steps and make your workflow faster and more efficient. In general, reloading modules from time to time is a good idea, as it can help you avoid trying to debug code that was already modified a bunch of times in the meantime.Debugging is an Art.Most of the time, what programming really is — is just a lot of trial and error. Debugging, on the other hand, is — in my opinion — an Art and becoming good at it takes time and experience — the more you know the libraries or framework you use, the easier it gets. Tips and tricks listed above can make your debugging a bit more efficient and faster, but apart from these Python-specific tools, you might want to familiarize yourself with general approaches to debugging — for example, The Art of Debugging by Remy Sharp.If you liked this article you should check out other of my Python articles below!This article was originally posted at martinheinz.devtowardsdatascience.comtowardsdatascience.comtowardsdatascience.com",python,https://towardsdatascience.com/ultimate-guide-to-python-debugging-854dea731e1b?source=tag_archive---------8-----------------------
Getting Amazon Price Drop alert using this Python script,"Vishal SharmaJun 1, 2020·3 min readPython does wonderful things. And, there is hardly anything this beautiful language can’t do. Bundles of libraries have boosted this language to outshine every other when it comes to data analysis, web scraping, or task automation.Just imagine, you want to buy a new iPhone. You are sick of checking Amazon website again and again. But, it’s price isn’t dropping and you always get disappointed.What if, there is an automatic price checker that will give you an email alert if there is a price drop. Exciting, isn’t it? Let’s do the same by writing a Python script.Library IngredientsThere are mainly three libraries that will do the job for us. Requests library will communicate with HTTP server, Beautiful Soup will communicate with the webpage and extract the web elements, and smtplib will send you email alerts.Meat CodeFirst things first, I have requested the web server the Amazon’s iPhone webpage. Then, I created a soup object which parses the webpage. We only want the price of the product. So, using the “select” method, we can get the price of the product.Now, whenever the “new price” becomes less than the “scraped price”. An email must be sent to the user regarding the price drop.We will be using smtplib library to send emails to the user. First, we will create an SMTP object to communicate with the domain’s SMTP server.Once all is done and set, we are good to log into our email account. And, then send the email to the recipient.NOTE: Parameters might change in smtplib.SMTP() depending on the domain’s SMTP server. If you have Gmail account, you will have to generate Application-specific password for your email address. Else, you will get an Application-specific password required error message when your program tries to log in.These 40–50 lines of code will do the job for you. Next time, you don’t have to check the website again and again for price drops. Make programs like this for other e-commerce website and save your time and effort.For discussions and queries, contact me on Linkedin!",python,https://towardsdatascience.com/getting-amazon-price-drop-alert-using-this-python-script-616a98bcba6b?source=tag_archive---------25-----------------------
Lovecraft with Natural Language Processing — Part 1: Rule-Based Sentiment Analysis,"Mate PocsJun 1, 2020·10 min readI’ve been considering doing a Natural Language Processing project for a while now, and I finally decided to do a comprehensive analysis of a corpus taken from literature. I think classical literature is a really interesting application of NLP, you can showcase a wide array of topics from word counts and sentiment analysis to neural network text generation.I picked H. P. Lovecraft’s stories as a subject of my analysis. He was an American writer from the early 20th century, famous for his weird and cosmic horror fiction, with a huge influence on modern pop culture. If you want to read more about why I think he is a great choice for a project like this, have a look at my previous post in which I also describe how I prepared the text we are going to analyse now.In the first post of the NLP series, we are going to have a closer look at rule-based sentiment analysis. The wider scope of sentiment analysis is definitely not the best first step to take in the world of NLP, however, the good thing about rule-based sentiment analysis is that it requires minimal pre-work. I wanted to start the project with something that can immediately be played with, without being bogged down by the technical details of tokenisations and lemmas and whatnot.We are going to apply VADER-Sentiment-Analysis to rank the writings of Lovecraft, from most negative to most positive. According to its GitHub page, VADER is “specifically attuned to sentiments expressed in social media”. In other words, the library is trained on a modern, slang-heavy corpus. This should be interesting because there were three things Lovecraft abhorred from the bottom of his heart:Let’s start with a quick summary. Sentiment analysis is a type of NLP, and deals with the classification of emotions based on text data. You input a text, and the algorithm tells you how positive or negative it is. There are two main approaches one can take:We are going with the rule-based approach in our current project, let’s have a closer look at our library next!VADER stands for Valence Aware Dictionary and sEntiment Reasoner. At the core, it’s a sentiment lexicon, they started with over 9,000 features (words, expressions, emojis), and asked people to rate them on a scale of -4 to +4, from most negative to most positive. Then they took the average of those ratings, excluded the ones that were found to be neutral, and collected them in a dictionary of 7,520 elements. (The GitHub repo also has a funny, comprehensive emoji collection.)In addition to this lexicon, there are a number of interesting tweaks. For example, the word “extremely” has no intrinsic negative or positive sentiment, it simply increases the intensity of the word it’s attached to. There are also words that decrease the intensity, punctuations have meanings, negations like “not” in “not very good” cancel out or inverse the original sentiment, etc.Let’s have a look at a couple of examples first.Install the library, and import the method that we are going to use:Next, we create a SentimentIntensityAnalyzer object:Remember that text file with the 7,520 tokens? You can view them in Python, let’s see how the word “adorned” is rated:The result is 0.8. That is how positive this word is going to be, regardless of the context. We are not going to use the individual scores, that would be tedious, the polarity_scores method aggregates the text information for us.For example, this sentence:is rated as:The first three values are the portion of the text that is negative, natural, or positive. On our case, 0.592 is deemed to be positive, 0.408 is neutral. The compound score is the overall negativity or positivity score, normalised, ranging from -1 to +1.Now, there is a chance that you, similar to me, would assume two things: 1) the portions are somehow connected to either the number of words or the number of characters and 2) the compound score can be calculated from the portions and the individual scores. You would be wrong on both accounts.I spent a considerable amount of time trying to figure out how the portions and compound score are calculated exactly. For example, if you run the polarity_score method on “adorned”, which had a score of 0.8 in the lexicon, you get this result:The first three numbers look OK, after all, there is only one word in the text, and it is a positive one. But where does the compound score come from? It’s close to 0.8 / 4, which you would get from normalising the original 0.8 score to the scale between -1 and +1, but it’s not exactly. Similar issues with the previous example of “I feel good”, you would assume that “I” and “feel” are the neutral words, and “good” is a positive one, but where does the 0.408–0.592 split come from?There is a long description of how the scores are calculated on StackOverflow, in case you are interested. I for one drew the line here as to how much time I wanted to spend on investigating the code behind something that is ultimately just an interesting thought experiment.However, while I was there, I had a closer look at the code on GitHub, there are a number of interesting little details:And just to drive home the absurdity of measuring the nuances of human communication by such simple rules, guess which sentence is rated more negatively: “I feel nothing inside” or “The soup tastes bad”.Yes, the one with the soup is rated to be more negative. In fact, the first sentence’s score is completely neutral.Don’t get me wrong, VADER is a very impressive library, and works quite universally, as we will soon see. But like with many machine learning algorithms, the closer you look, the less magical it seems.All right, on to Lovecraft now!We are going to do a few test runs just to make sure that the library works with text that is from the early 20th century.Take this description from The White Ship (1919):Then came we to a pleasant coast gay with blossoms of every hue, where as far inland as we could see basked lovely groves and radiant arbours beneath a meridian sun.Running the polarity_scores returns this result:Sounds OK, it’s a very positive text. However, I was curious to see what the basis of this score was in the lexicon. By splitting the text, we can gather the words in the sentence that are in the lexicon:After printing result we get:Interesting, right? All that writing, and these three words determine the sentiment, like there was nothing else there, no “gay with blossoms of every hue”.Let’s see a more typical Lovecraftian sequence, from Dagon (1917):I have said that the unbroken monotony of the rolling plain was a source of vague horror to me; but I think my horror was greater when I gained the summit of the mound and looked down the other side into an immeasurable pit or canyon, whose black recesses the moon had not yet soared high enough to illumine. I felt myself on the edge of the world; peering over the rim into a fathomless chaos of eternal night. Through my terror ran curious reminiscences of Paradise Lost, and of Satan‘s hideous climb through the unfashioned realms of darkness.The estimated sentiment of this segment:0.186 of the text is deemed to be negative, and 0.121 is positive. However, the overall compound score, -0.8074, is close to the negative limit, which means the intensity of the negative segments were much stronger.Once again, we can gather the individual words with an assigned non-neutral score:Like with the first example, we can see how most of the information is not processed. No “unbroken monotony” or “eternal night” in the list. We can also see that there are some misclassifications. For example, the word “greater” in the expression “my horror was greater” gets a +1.6 score, but it is meant to be an increase in negativity. But the main thing is, it somehow got the overall sentiment right.In general, I am quite satisfied with the performance so far: the overall score seems to be reflective of the sentiment a human would assign to the text. The only thing left to do is to apply this to the complete corpus!Before we jump into the ranking, I would like to emphasise once more that we are heavily in thought experiment territory. I think rule-based sentiment analysis is a high-level approximation at its best, and VADER was created with the intention of analysing modern, short, informal texts, none of which is true for Lovecraft’s writing.With the warnings out of the way, without further ado, I used the code below to process the writings and save the results in lists. The list filenames contains the individual story names.Then, I created a pandas DataFrame object from these lists. Statistics of the 4 scores:One clear conclusion we can draw is that the writings are mostly negative. the 75th percentile of the compound score is still negative, and the average sentiment is -0.5025. Only 16 out of 63 writings got an overall positive score.All right, let’s see the list. The 5 most positive writings are, based on the compound score:There is a very interesting common factor here: all of these stories deal with the protagonist “dreaming” themselves into magical fantasy worlds, they are mostly descriptions of landscapes and cities, and not much action takes place in them. They undoubtedly use a rather positive language, we saw one example from The White Ship earlier, but the overall tone of these stories is rather melancholic, and no human reader would categorise them as “happy”. However, it’s quite interesting to see that the sentiment analysis algorithm managed to separate this group of stories from the “really dark” ones.(By the way, in case you are interested, you can read more about the stories in this bibliography.)The 5 most negative writings are:The common theme here is that these stories are more plot-heavy (relatively speaking), longer, more complicated writings. They are also quite famous, three out of these five is among Lovecraft’s most influential works. In case you are wondering, most of his other famous stories like At the Mountains of Madness (1931), Herbert West — Reanimator (1922), The Case of Charles Dexter Ward (1927) are also among the Top 10 darkest stories. The Call of Cthulhu (1926) is #16.It’s also interesting to see in the statistics table that the majority of the text in all the writings is neutral, the neutral portions range from 71.4% to 90.5% of the texts. This would support the concept that despite the overall depressive tone, the narration in Lovecraft’s writings is usually calm and distant, apart from the occasional “The horror, the horror, the indescribable horror!” sections. But it could also be caused by the majority of the dictionary not being present in the VADER lexicon.In case you want to see all the results, you can access them here in a CSV file. There are also large charts for the overall compound score and the text portions.There are a number of additional analysis one could do with VADER on this corpus. For example, you could split the stories into sentences, and see how negative they are over the course of the story. Are there relatively neutral starting points, and then a peak in the middle, or the other way around? It would definitely be interesting to see that.I hope you found this post helpful, in the next one, we are going to have a closer look at tokenisation and word analysis.Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.Hobson, L. & Cole, H. & Hannes, H. (2019). Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python. Manning Publications, 2019.The Complete Works of H. P. Lovecraft:arkhamarchivist.com",python,https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-1-rule-based-sentiment-analysis-5727e774e524?source=tag_archive---------23-----------------------
Convolutional Neural Network Champions —Part 1: LeNet-5 (TensorFlow 2.x),"Amir NejadJun 1, 2020·17 min readConvolutional neural networks are a special type of neural network that is used for modeling data with strong spatial correlations such as images, multivariate time-series, earth science studies (seismic classification and regression) among many other applications. Convolutional networks have gone under significant changes since 1998 and in this series of articles, I aim to reproduce the famous model architecture champions i.e. LeNet, AlexNet, Resnet. My aim is to share my findings and studies with wider audiences and deliver reproducible Python notebooks.Part 2: AlexNet classification on ImageNet and Tensorflow:towardsdatascience.comPart 3: VGGnet classification on ImageNet and Tensorflow:towardsdatascience.comThe Python notebook for this study is located on my Github page.The Tensorflow version used in this study is 1.15.0.The machine used in this study has an Intel core I7 @ 1.8 GHZ with 16 GB memory.Applying convolutional layers (aka “ConvNet”) on images and extracting key features of the image for analysis is the main premise of ConvNets. Each time “Conv-layers” is applied to an image and divides this image into small slices known as receptive fields, hence extracting important features from the image and neglecting less important features. The kernel convolves with the images using a specific set of weights by multiplying its elements with the corresponding elements of the receptive field. It is common to use “pooling layers” in conjunction with Conv-layers to downsample the convolved features and to reduce the sensitivity of the models to the locations of the features in the input.Finally, adding dense blocks to the model and formulating the problem (classification and/or regression), one can train such models using the typical gradient descent algorithms such as SGD, ADAM, and RMSprop.Of course, prior to 1998, the use of convolutional neural networks were limited and typically support vector machine was the preferred method of choice in the field of image classification. However, this narrative changed when LeChun et al. [98] published their work on the use of gradient-based learning for handwritten digits recognition.LeNet models are developed based on MNIST data. This data-set consists of the hand written digits 0–9; sixty thousand images is used for training/validation of the model and then a thousand images are used to test the model. The images in this data-set have a size of 28×28 pixels. An example can be seen in the following figure. The challenge of using a MNIST data-set is that the digits often have slight changes in shape and appearance (for example, the number 7 is written different way).Looking at the labels in the MNIST data set, we can see the number of labels are balanced, meaning there is not too much disparity.The proposed model structure of LeNet-5 has 7 layers, excluding input layers. As described in the Data section, images used in this model are MNIST handwritten images. The proposed structure can be seen in the image above, taken from the LeChun et al. [98] paper. The details of each layer are as follows:The following code snippet demonstrates how to build a LeNet model in Python using Tensorflow/Keras library. Keras sequential model is a linear stack of layers. Then, we need to define each layer as seen below. Finally, the model needs to be compiled and the choices of optimizer, loss function, and metrics need to be explicitly defined. The optimizer used in this work is sgd or Stochastic Gradient Descent. The loss function is optimized to train the machine learning model. The loss function used here is cross-entropy, or log loss, and measures the performance of a classification model whose output is a probability value between 0 and 1. An accuracy metric is used to evaluate the performance of training. Loss function is a continuous probability function while accuracy is a discrete function of the number of correctly predicted labels divided by total number of predictions (refer to appendix 3).Note that in the above code snippet, we have not specified anything about how the weight of neural network is initialized. By default, Keras uses aglorot_uniforminitializer. Weight values are chosen randomly in a way to make sure that information passed through the network can be processed and extracted. If the weight is too small, the information shrinks as a result. If the weight is too large, the information grows as a result and becomes too big to process. The Glorot uniform algorithm (also known as Xavier algorithm) chooses appropriate random weight values from a multivariate random normal scaled by the size of the neural network [refer Glorot 2010].The summary of LeNet-5 network constructed with Tensorflow is given below (Using model.summary()) :Now that we have constructed the LeNet model using Tensorflow and Keras, we need to train the model. Using model.fit() and feeding training and validation sets, the model is trained. Additional parameters needed for training are the number of epochs, batch size and verbose:Once the model is trained, we can use a testing set that we have set aside to evaluate the performance of model training using model.evaluate() command:The result of training the model for 10 epochs can be seen in the following figure. Initially, the weights of neural network are chosen randomly, but after 2 epochs of presenting 48,000 pictures to the model, the model loss reduced from 0.38 to 0.1. After 10 epochs of model training, the model accuracy surpassed 95% on testing set. This is a substantially improved accuracy compared to previous models at the time (mainly support vector machines), and as a result LeNet-5 cemented its legacy as one of the earliest champions of computer vision.Using model.optimizer.get_config() we can interrogate the optimizer parameters. Note that we only specified the type of optimizer, the loss function, and accuracy metrics. As it can be seen from the following snippet, the optimizer used to train LeNet model is a Stochastic Gradient Descent (SGD) optimizer. The learning rate by default is set at 0.01. The learning rate controls the change in the model parameters in response to observed error measured by loss function. Imagine the model training process as traversing from a hill to valley, and the learning rate defines step sizes. The larger step size traverses to the solution faster but it might result in jumping over the solution. On the other hand, smaller step sizes take too long to converge. In the more complex problems, step size decay is commonly used. In this problem, however, no decay is sufficient to get good results. Another parameter using SGD is momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. Momentum, therefore, can be used to improve converge the speed of the SGD. Another parameter is nesterov. If set to true (boolean), SGD enables the Nesterov accelerated gradient (NAG) algorithm. NAG is also a closely related algorithm to the momentum in which step sizes are modified using velocity of the learning rate change (refer to Nesterov [1983]).There are multiple ways to assess the performance of a classifier. Accuracy performance on the test data set that held back from any of the model development tasks is the obvious choice. However, the confusion matrix can provide a detailed report of classifier and better assess the classification performance. Furthermore, the Classification accuracy can be misleading if an unequal number of observations in each class are present.A confusion matrix is a detailed report of the number of samples classified correctly/incorrectly. The number of samples along the diagonal of the confusion matrix are correctly predicted samples. All other samples are miss-classified. The higher the number of samples on the diagonal, the higher the model accuracy. As it can be seen from the confusion matrix of LeNet-5 on MNIST data set, most of the classes are classified correctly. However, there are a few cases the classifier had trouble correctly classifying the label such as label 5,4, and 8. For example, there were 16 cases the classifier incorrectly classified number 2 as number 7. Those aforementioned cases are depicted in the following image.In the previous section, it is mentioned the SGD optimizer is used for this optimized neural network model. However, because of the slow convergence of SGD and problems of getting stuck at local minima, this method is not popular. Since its introduction, Adaptive Moment Estimation aka Adam (refer to Kingma et al. [2014] for more details) enjoys significant popularity in the field of deep learning. The Adam optimization algorithm is an extension to stochastic gradient descent in which momentum by default is applied to gradient calculation, and separate learning rates for each parameter. Using Adam optimizer and retraining the LeNet-5 from scratch, model accuracy can be increased to 98% as seen in the following learning curves:The batch size is one of the most important hyper-parameters in neural network training. As discussed in the previous section, the neural network optimizer during each training epoch randomly selects data and feeds it to the optimizer. The size of the selected data is called the batch size. Setting the batch size to the entire size of the training data may cause the model to be unable to generalize well on data it hasn’t seen before (refer to Takase et al. [2018]). On the other hand, setting batch size to 1 results in higher computational training time. The proper choice of the batch size is particularity important as it leads to model stability and increases in accuracy. The following two bar charts demonstrate testing accuracy and training time of various batch sizes from 4 to 2,048. Testing accuracy of the model for batch sizes 4 to 512 is above 98%. However, the training time of batch size 4 is more than four times the training time of batch size 512. This effect can be more severe on more complex problems with a large number of classes, and large numbers of training samples.As discussed before, the pooling layer is required to down sample the detection of features in feature maps. There are two most commonly used pooling operators: average pooling and max pooling layers. Average pooling layer operates by calculating average values of the selected patch in the feature map, whereas max pooling layer calculates maximum value of the feature map.Max pooling operation as it can bee seen in the following figure, works by selecting the maximum feature value from the feature map. Max pooling layers discriminate against features with less dominant activation functions and only select the highest values. This way only the most important features are fed through pooling layer. The major drawback of max pooling is that the pooling operator in the regions with features of high magnitude, only the highest value feature is elected and the rest of features are ignored; the discerning features disappeared after performing max pooling operations and results in loss of information (the purple region in the following figure).Average pooling, on the other hand, works by computing average values of the features in the selected region of the feature map. All parts of the selected region in the feature map are fed through using average pooling. If the magnitude of all the activations is low, the computed mean would also be low and rise due to reduced contrast. The situation will be worst when the most of the activations in the pooling area come with a zero value. In that case, feature map characteristics would reduce by a large amount.As indicated earlier, the original LeNet-5 model uses average pooling strategy. Changing average pooling to the max pooling strategy resulted in approximately the same testing accuracy on the MNIST data set. One can argue the point of different pooling layers. However it should be noted that the MNIST data set is rather simple compared to other complex data sets such as CIFAR-10 or Imagenet, hence the performance benefits of max pooling in such data sets can be by far more beneficial.Thus far we have explored different aspects of LeNet-5 model including the choice of optimizer, effect of batch size, and choice of pooling layer. LeNet-5 model is designed based on MNIST data. As we have seen so far, the digits are centered in each image. However, more often than not, the location of the digits in the image in real life is shifted, rotated, and sometimes flipped. In the following few sections we will explore the effects of image augmentation and sensitivity of the LeNet-5 model to image flipping, rotation, and shifting. Image augmentation is done with the help of the Tensorflow image processing module, tf.keras.preprocessing.Effect of FlippingIn this exercise, images are flipped along the horizontal axis using ImageDataGenerator (horizontal_flip=True). Applying ImageDataGenerator tests image results in the new data set with the images horizontally flipped as seen in the following image. As it can be seen, it is expected the model have low accuracy on the flipped images data set. As seen from the testing accuracy table, the accuracy of the LeNet-5 model dropped from 98% to 70%.A closer look at the confusion matrix of the flipped images data set reveals few interesting takeaways. The highest accuracy labels are 0,1,8, and 4. The first three labels are symmetrical (0,1,8) and as a result model has good accuracy of prediction on such classes. But it is interesting that the LeNet-5 model has good classification accuracy on label 4. Another interesting aspect of this test is how the model identifies digits. For example one of the labels that the model suffers from accuracy in the flipped data set is 3. The model almost half the time misclassified it as the number 8. It is very useful to understand how the model identifies each digit in order to build a robust classifier. Packages like SHAP can provide means of understanding input and output mapping of any deep neural network model (look for DeepExplainer module in SHAP library).Image RotationImage rotation is another possible scenario in real life. Digits can be written in an angle with respect to the image boundaries. With the Tensorflow image augmentation module, one can produce randomly rotated images using the following line of code: ImageDataGenerator(rotation_range=angle). The testing result of LeNet-5 model with various randomly rotated images can be seen in the following figure. The more rotation, the worse the prediction of the model. It is interesting to note that model predictions are rather satisfactory for up to 20 degrees of rotation, then the predictions degrade rapidly.Effect of ShiftingOne final image augmentation effect is shifting digits along the horizontal or vertical axis within the image. This effect can be easily applied to MNIST test dataset using ImageDataGenerator(width_shift_range=shift). Note that, in this section I am demonstrating the result of width_shift generator. The sensitivity of LeNet-5 network to width shift is much higher than image flipping and image rotation. As can be seen from the following figure, the accuracy degrades much faster than other discussed image augmentation processes. Only a 10-degree width shift results in accuracy drop from over 95% to about 48%. This effect might be attributed to the filter size and kernel dimensions of the model.As we have seen from all the previous sections, LeNet-5 model achieved a significant milestone in the hand written digit recognition. Due to its superior performance on classification problems, LeNet-5 model used in banks and ATM machines for automatic digit classification in mid-1990s. However, the next frontier for this model was addressing image recognition problem to identify various objects in the image.In this final section, we aim to train the LeNet-5 on CIFAR-10 dataset. CIFAR-10 (Canadian Institute For Advanced Research) is an established computer vision data set with 60,000 color images with the size 32×32 containing 10 object classes as it can be seen from the following picture. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. As can be seen from the images below, the complexity of images is much higher than that of MNIST.Applying LeNet-5 structure to this data set and training the model for 10 epochs, results in an accuracy of 73%. The testing accuracy of this model is 66%. Considering human level accuracy on this data set is about 94% (according to Ho-Phuoc [2018]), LeNet-5 type structure is not efficient enough to achieve high recognition capability.The LeNet-5 provided important framework for training and classifying handwritten digits. As we saw in previous sections, the LeNet-5 structure even though achieved many successful milestones for optical character recognition tasks, it did not perform well in the task of image recognition. However, Yann André LeCun’s and many contemporaries paved the way for more sophisticated neural network model structures and optimizing techniques. In the next article, I am going to explore and discuss another convolutional neural network structure champion, ALexNet.Thanks for reading! My name is Amir Nejad,PhD. I’m a data scientist and editor of QuantJam , and I love share my ideas and to collaborate with other fellow data scientists. You can connect with me on Github, Twitter, and LinkedIn.QuantJam:medium.comYou can see my other writings at:amirnejad.medium.comFormula to calculate output dimension of a convolutional neural networks:Formula to calculate number of parameters:Using the two formulas above, one can calculate the output dimensions and number of parameters of LeNet5 model as follows:Loss function:Accuracy functions:Optimizer:Batch Size:Pooling Strategies:Initialization:Confusion Matrix:MNIST:CIFAR-10:Image Augmentation with Tensorflow:SHAP",python,https://towardsdatascience.com/convolutional-neural-network-champions-part-1-lenet-5-7a8d6eb98df6?source=tag_archive---------18-----------------------
Migration to Airflow: One year feedback,"At Maisons Du Monde, we used to create and schedule our data pipelines with Rundeck, which is an automation server like Linux Cron for the veterans. Rundeck had been available on our company’s network, so we simply used it to get the pipelines started.At the beginning, this solution was convenient as long as workflows finished in time. But “winter is coming !” The platform kept on growing, the pipelines became increasingly bigger and much more complex. So Rundeck’s maintenance has become much harder, because of an unconvenient user experience and some performance limits on the local server.That’s why we moved to Airflow !In this article, we will express our feedback after one year of Airflow usage, comparatively to Rundeck, in the data team of Maisons Du Monde.In the first section of this article, we will discuss about the main issues that we have encountered with Rundeck.In the second section, we will explain how did we integrated Airflow to satisfy our needs. The best solution for us, is the one which is sustainable and user friendly !In this automation system, our tasks were organized over tree folders, similarly to files over an OS system. Moreover, the scheduler executes jobs inside folders sequentially, following a depth-first search execution priority. So, there is no concrete dependencies, only some execution rules in the task scope.However, separate folders could be run in parallel, but without dependencies. If needed, we shifted adequately the start time for the dependent workflows, with a safety margin.Fernando here doesn’t know if Michael has reached the nursery. He’s just expecting it, and puts faith on the plan to have a chance escaping from the prison. It is 3 am now, Fernando shall get started. Hopefully Michael didn’t get caught.So this is making the user experience harder for exploring or troubleshooting the workflows. Thus, catching up the failing jobs tends to be a nightmare when it comes to replay the time dependent workflows.But there were many other reasons to quit Rundeck, here are some examples:Apache Airflow is a popular open-source solution with an active community around it. It came up with a new spirit of automation suited for data use cases, also it could be integrated on a scalable architecture.The solution could resolve all the previously mentioned issues and Airflow’s documentation first lines bring some answers to that. To sum up, here is some good reasons that led us to choose Airflow: it uses DAGs, has a rich user interface for monitoring and troubleshooting, could be scalable, workflows are Python coded and last but not the least, Google Cloud Platform — which is our cloud provider — has a managed service for Airflow named Composer. It helped us a lot to quickly study and deploy Airflow on our infrastructure.Moving from folders to DAGs was a relief, simply because a DAG is less restrictive: it unlocks the possibility to make new dependencies and to multi-task.We took this advantage to rethink the existent pipelines and dependencies. As a result:Airflow has a rich user interface to handle troubleshoot and to manually catch up tasks over time and over dependencies.This is a very attractive aspect of Airflow. Each DAG in Airflow has an equivalent python code which has three steps:And here is the best part, with Airflow plugins you could create your own operators or sensors by overriding the classes BaseOperator or BaseSensor.Before discussing about Airflow integration, I would like to share with you some useful operators and sensors: LatestOnlyOperator, BranchOperator, ExternalTaskSensor, KubernetesPodOperator and also the DummyOperator that could be useful even if it is a dummy.We use Google Cloud Composer as a managed Airflow’s service.Google Cloud Composer instance has a specific bucket dedicated to put Airflow’s code in production. So the deployment is operated by Gitlab using CI/CD pipelines in two steps:Once CI/CD pipelines are achieved, it takes only few seconds to notice the changes in Airflow’s web-server.An Airflow docker image was developed by the team for this purpose.A volume is mounted, to make the container access to the Airflow’s git local repository.Airflow’s configuration inside the container is similar to the production configuration. Thus, if the workflow works locally, technically, it should be working fine on production as well.Furthermore, we have developed a “dry run” feature for a testing purpose, that could be set to True or False through the UI. With an active “dry run” mode, the execution would be simulated by just logging the steps instead of launching their execution.It is possible to mount a volume on the context folder (~/. kube/config) with the running container in order to test the pipelines in a Kubernetes environment. This could be useful to test the new developed plugins before their releases.We are using KubernetesPodOperator because most of our python projects are containerized and could be run in Composer’s GKE cluster. KubernetesPodOperator’s input parameters are quiet similar to a Kubernetes job’s Yaml file. Here is an example of a program which runs a transformation in the datalake.So now you will not only require some Airflow skills, but also some additional Kubernetes skills for any job deployment. That’s why it was important to create custom plugins to apply some minimalism to the operators.Do we need to change all the Kubernetes parameters anytime we would like to reuse a specific program like a transform operation ? Luckily not!Actually, we only have three changing parameters for a transform operation which corresponds to its Docker entrypoint:So an Airflow plugin at Maisons Du Monde will expose only the changing parameters as inputs, and will preset the environment configurations. For example, the preset would contain a hard-coded Docker image URI and a predefined version value, “Latest” by default. Moreover, we used plugins to automatically define task_id by combining the required inputs, to generate a task_id which looks like: “OPERATOR_NAME-my_dataset.my_table” .Let’s take a look under the hood !For the record, the purpose of our plugins is to have the fewest required inputs in order to use our custom operators. So, every Docker image we use has a specific plugin, which is basically a tiny class extending the abstract class MdmKubeOperator, this one is a child class of the original KubernetesPodOperator. The middle class (MdmKubeOperator) is very important to define some common methods and attributes that could be reused over all the plugins.Here is a class diagram representing, among others, two of our plugins: QueryLauncherOperator for transform operations and DatabaseCollectOperator for databases collect operations.Now that we know how Airflow could fit on our infrastructure, it remains to normalize the separation between DAGs in order to define some common task regrouping rules. In simple words, what defines a DAG ?We ended up choosing the following organization:As Airflow doesn’t allow regrouping DAGs inside folders, we prefix every DAG by its level in order to see the collects first in the UI, then the central tables, then the output DAGs.We didn’t end up with this solution at the first step of the integration. At the beginning, we had migrated only few workflows from Rundeck, in order to test & learn. So we went for a “DAG per use case” separation for a couple of months but we quickly found out that it wasn’t really sustainable. For example, a collected table that is used in two use cases DAGs could be problematic to place, hence the necessity to define what is a DAG in your company ! However this transition period was very crucial to raise our knowledge about Airflow operators and tips.DAGs dependencies could be implemented using the ExternalTaskSensor. It periodically checks for another task’s execution state and turns on to green when it succeeds. This wasn’t possible with Rundeck, remember ?Fernando has a cell phone now to check. He could precisely start right after the first job is done by Scofield.However, we don’t recommend using sensors to directly listen on the tasks one by one, because the dependencies will get complicated, like too many octopus hugging each others.Instead we do recommend to put some dummy tasks as checkpoints, to create tasks clusters inside each DAG. As a result, we listen on clusters instead of elementary tasks. In our case, checkpoints are based on execution duration.So we have agreed on 3 duration levels inside each DAG:In the second DAG, the tasks A3, A4 and A5 depend on the first DAG’s level 1 tasks, which refer to the quick dependencies ! So this separation allows to execute urgent workflows first.More than one year after Airflow’s integration, only time could prove the final solution’s sustainability through real scenarios.Indeed, it was a success because the current system satisfied our initial requirements:Integration success key is based on good decisions and scenarios anticipation which takes into consideration the infrastructure constraints. This required the entire team’s engagement to avoid forgetting any scenario.So after more than one year of Airflow’s integration, the system was exposed to real situations which have revealed some strengths and some weaknesses as well.Let’s take a look to a real scenario that witnessed the following key strength points: A trigger campaign sending a welcome email to the new Maisons Du Monde’s clients, scheduled on Airflow.Scheduling easiness: Every member of the team is able to know how and where to schedule just by applying the rules. For the trigger campaign example, the task should be set in an output DAG which depends on two DAGs.The interesting thing is that selecting dependencies remains abstract and keeps workflows simple.Troubleshooting convenience: It is handy to identify and manage incidents for workflows execution. Let’s assume that our email campaign has failed, we know that it depends on 2_Transactions and 2_Users. So we are able to find and fix the root cause error and its dependencies easily through the user interface. However, most incidents were related to network problems or data sources themselves, hence the interest of having separate DAGs per collected source.Responsibilities separation: every team member could enrich the production pipelines by adding new tasks to the workflow that runs on Kubernetes. However, Kubernetes skills are not required in the pipelines making. The interesting point, is that responsibilities could be divided between two types of users:Time gain: the pipelines were optimized thanks to the fitting workflows and sensors which have reduced dependencies waiting time. As a result, this trigger campaign is most of time ready at 8 am instead of 5pm before: 9 hours optimization.It is True that Airflow made our scheduling routines easier since we have began using it. Nevertheless, some weaknesses should be highlighted.The scheduler: As you might now, the scheduler manages DAG-runs and orchestrates tasks executions across the workers and pods. So for every heartbeat, the scheduler loops over all the DAGs tasks and query the Airflow’s database, to orchestrate all the executions. But when the scheduler is down, Airflow is out of service meanwhile Composer will be trying to pop up a new scheduler to recover. So there is two recommendations to minimize scheduler’s incidents:Zombies tasks: Some tasks keep in a running state on Airflow even if they have been completed. That happens likely for three reasons, when Airflow database is overwhelmed, when an Airflow worker is down or when we use a sub-dag (that we strongly advise you not to use). The only way we found so far to recover is to re-run the task manually.Sub-DAGs: We used to encapsulate the reusable workflows inside sub-DAGs. It simplifies more the pipelines but it have caused so many bugs that we have abandoned using this feature.DAG folders: DAGs are listed on a paginated list, and it is not possible to organize them on folders for a better navigation on the UI.Timezone handling: We have encountered some issues to handle timezone, like the time inconsistencies between the UI and the execution context, or the summer time switch. Even though it is configurable in “airflow.cfg”, we managed to programmatically adjust the context’s timezone to Europe/Paris inside plugins, in the parent class MdmKubeOperator, because it doesn’t work fine in our current version 1.10.0 of Airflow. Fortunately, this problem was tackled by the community since the version 1.10.10 of Airflow.Trigger task with input parameters: It was the missing features that we had with Rundeck before. So a new Airflow page was developed internally, to launch a task with custom parameters to fill in the form. To know more details about this internal feature, check this Maisons Du Monde’s article to do the same. https://medium.com/maisonsdumonde/road-to-add-form-for-airflows-dag-1dcf2e7583efBeyond the features which are currently developed in this project, we have a to-do list of new features to improve the user experience a little more. However, we stay alert to Airflow’s Roadmap and the new versions changelogs for a better prioritization.We want to add a new button in the ExternalTaskSensor’s window, which will redirect to the corresponding DAG.We want to automate the creation of a top level DAG named “0_ALL_DAGS”, which displays all the relationships between DAGs. Each task of “0_ALL_DAGS” is actually the same sensors which have been used for DAGs dependencies. So dependencies between DAGs would become concretely represented and we will avoid the pitfall of creating cycles between DAGs.Airflow is a really good tool that could fit, basically, on every architecture. However, there is no common integration standards to integrate your solution. Furthermore, we noticed that companies have a different integration logic, but a lot of them have encountered the same pitfalls. I hope that this feedback will inspire you. Your solution would be definitely different but remember: the better solution you have, is the one which is “sustainable” and “user friendly”.I would like to thank you for your reading, a special thanks to the entire data team that made this success story happen, also to Benjamin Berriot and Guillaume Duflot for this article review.We are looking forward for your comments, hoping that this feedback was useful and feel free to reach us for further discussions and experience sharing.",python,https://medium.com/maisonsdumonde/migration-to-airflow-one-year-feedback-91a8a72cb4a1?source=tag_archive---------24-----------------------
Document scanner and private data sharing,"Simeon EmanuilovJun 1, 2020·11 min readThe problem: we want to take a picture of a document with our phones, and send it privately to a recipient. With the current article, we are trying to solve the current issues, which can appear in real life:Is this a real-life problem? taking a picture of an image and “repair” the perspective is not something new, but it is something that appears very frequently in the real wife. It is a common thing — someone to ask: “Please scan this contract and send it to me today” or “Please scan this declaration and send it to me in an hour”, but what we do if we don’t have a scanner? Of course — take a picture with our phone and send a messy picture. We then have a response — “Hey, do you have a scanner? I don’t want to have your shoes on the picture” and so on.How we will solve the problem — of course with Math. We will use some steps to go throw the process. We will use detect angles, change of perspective, and crypto libraries to hide the important information and easily transmit.What libraries we will use — we will use and combine various type of libraries:What cryptographic algorithms we will use for securing data?Why changing the perspective is so important in this text? In the current article, we will take a look at the practical change of perspective. But getting deeper into this concept and mathematical fundamentals is crucial to understand the whole picture.Here you can find an article, which gives a deep overview of mathematics, behind the perspective transformation: https://www.math.utah.edu/~treiberg/Perspect/Perspect.htm Those mathematical ideas that occur in art and computer graphics.This question prompted the development of a new subject, projective geometry whose exponent was Girard Desargues (1591–1661).The perspective transformations that describe how a point in three space is mapped to the drawing plane can be simply explained using elementary geometry. We begin by setting up coordinates. A projection involves two coordinate systems. A point in the coordinate system of an object to be drawn is given by X=(x, y, z) and the corresponding in the imaging system (on the drawing plane) is P=(u, v). If we use the standard right handed system, then x and y correspond to width and depth and z corresponds to height. On the drawing plane, we let u be the horizontal variable and v the vertical.We can measure the distances between pairs of points in the usual way using the Euclidean metric.Ifandand so on, then:The projection from X to P is called a parallel projection if all sets of parallel lines in the object are mapped to parallel lines on the drawing. Such a mapping is given by an affine transformation, which is of the formwhere T is a fixed vector in the plane and A is a 3 x 2 constant matrix. Parallel projection has the further property that ratios are preserved. That is if X (1, 2, 3, 4) are collinear points in the object, then the ratio of distances is preserved under parallel projectionOf course denominators are assumed to be nonzero.It is always a pain to start a Python scripts, when you don’t know the required libraries and version. That’s way I create a requirements.txt file (link):At this stage we need to make the imports, we will use them further in this article. Please don’t forget to make the imports for everything to work as expected. Also, we define some of the functions, which will be useful for use in the future. Those are basic operations with OpenCV, which can be repeated many times and it is good practice to have them in functions (like read_image, show_image_opencv, save_image_opencv, etc). We also make a function get_current_dir, which can help us if we don’t know current dir, or we want to include the image from a different location.Please keep in mind, that for *nix systems (like Mac), show_image_opencv can not work very well. It can “freeze” in the part of destroyAllWindows();We read our input file, called bulsatcom.png, which is placed in the same directory as the course project files. Then we can make a variable holding the input file + one copy.Original file:The expected result on this step: We now have the OpenCV object, holding the image. We also have a copy of the image in input_image.pngEvery image has some noise and our goal in this step is to perform a cleaning. One of the approaches for doing so is to convert the colored imaged into a gray one. After that, we apply a blur function to blur the image with (3, 3) filter. Blurring reduces any high-frequency noise and makes detection of contours easier.We have only one function here detect_edges, it accepts the input image and returns an instance with edges.Maybe the most interesting part here is the Canny Edge Detection. Canny Edge Detection is a popular edge detection algorithm. It was developed by John F. Canny in 1986. It is a multi-stage algorithm and the steps in short are:So what we finally get is strong edges in the image. The first argument is the image instance (already gray and blurred), second and third arguments are our minVal and maxVal respectively.The expected result on this step: We have only one function here, but a very important one. We perform some cleaning of the noise in the image, applying filters.Additional methods, articles & approches for edge detection:One of the most interesting parts is to find the contours in the image. It is also a challenge (but very important) to find out the contour with the highest area. On that, we will exclude some big letters or images inside the paper. We only need the largest are, a.k.a the whole document.We make a function calculate_draw_contours where we use some of the functions, built-in OpenCV, like findContours. This function returnsThe expected result on this step: We have contours of the image.This is one of the hardest moments in this article. We have the coordinates of all the corners of our document and it is crucial to arrange them and know which coordinate to correspond to a corner.Images are composed of pixels. When we have a gray picture, we don’t have a depth of color, which is a dimension also. So we can work with such pictures in two dimensions — width and height.When we have the dimensions, we can construct the destination points. We can use getPerspectiveTransform function from OpenCV, which calculates a perspective transform from four pairs of the corresponding points. After that, we can use warpPerspective, which applies a perspective transformation to an image.The expected result on this step: An almost scanned image, which better perspective to show.Base64 is a way in which 8-bit binary data is encoded into a format that can be represented in 7 bits. This is done using only the characters A-Z, a-z, 0–9, +, and / in order to represent data, with = used to pad data. For instance, using this encoding, three 8-bit bytes are converted into four 7-bit bytes.The term Base64 is taken from the Multipurpose Internet Mail Extensions (MIME) standard, which is widely used for HTTP and XML, and was originally developed for encoding email attachments for transmission.Base64 is very important for binary data representation, such that it allows binary data to be represented in a way that looks and acts as plain text, which makes it more reliable to be stored in databases, sent in emails, or used in text-based format such as XML. Base64 is basically used for representing data in an ASCII string format.It is good that Base64 can do some important things for us, but we must keep in mind that we should not use base64 for every place, especially in web development. Here you can find an interesting article about this.BlurHash is a compact representation of a placeholder for an image. I find it useful in projects, where I want to save bandwidth and show a placeholder until the image is actually loaded. Also, it can be a good fit for this article, as we can calculate the BlurHash value of a picture and store it in a DB. We can after that show “preview” in the browsers of users, which are not allowed to view the full picture/document.It can be used for something like a secret variant of an image with some data on it, but not enough to read or identify patterns.More links information about itThe example below will illustrate a simple password-based AES encryption (PBKDF2 + AES-CTR) without message authentication (unauthenticated encryption). I find this useful for this article, as we will want to encode the base64 equivalent of the image and make it “password protected”, without the ability someone to see the content, event he owns the servers, or read our message somehow.Useful links for such operations: https://cryptobook.nakov.com/symmetric-key-ciphers/aes-encrypt-decrypt-examplesThis step is optional and we are not going to go deep inside this topic. The idea is that when we have encrypted image + blurhash to show in the browser (short preview), the user with the password can encrypt the ciphertext and see the base64 string. He can also convert it to an image. It is very easy to make a JavaScript library, which accepts BlurHash value + ciphertext and after a successful password entry — it visualizes the base64 image (natively in HTML).Example library, that can be used for such operations (AES decrypt in browser) can be found here: https://github.com/ricmoo/aes-jsWhat do we want to make in short in this article?It will solve some problems with private document/picture sharing + repairing perspective of a picture of a document. We use various techniques to obtain this, this approaches can be easily made to an API, I tried to make it in the biggest part like functions, which can be transformed to endpoints.What we have more in this article?",python,https://towardsdatascience.com/document-scanner-and-private-data-sharing-2fb82b3f0fbb?source=tag_archive---------32-----------------------
Still using Accuracy as a Classification Metric?,"Pranav KaushikJun 1, 2020·4 min readAccuracy is the most common evaluation metric for classification models because of its simplicity and interpretation. But when you have a multiclass classification problem in hand, say, for example, with 15 different target classes, looking at the standard accuracy of the model might be misleading. This is where “top N” accuracies might be of some use, and in this post, I’ll take you through the basic intuition and python implementation of top N accuracies.Before we get into top N accuracy, a small refresher on standard accuracy metric:Accuracy is the fraction of total records that are correctly predicted by the model. Accuracy = (Number of correctly predicted records / Total number of records) or Accuracy = (TP+TN) / (TP+TN+FP+FN)When you have a lot of different classes, the classification model might not be able to predict the right class exactly. This especially becomes a problem in NLP cases like text classification, where you have a vast number of features, and the data is not adequately clustered across classes. So looking at your standard accuracy metric might be misleading at times. As I said earlier, measuring the top N accuracies might help in overcoming this issue.Top N accuracy is not any different metric, but it’s just standard accuracy of the true class being equal to any of the N most probable classes predicted by the classification model.Top 1 accuracy is the accuracy where true class matches with the most probable classes predicted by the model, which is the same as our standard accuracy.Top 2 accuracy is the accuracy where true class matches with any one of the 2 most probable classes predicted by the model.Top 3 accuracy is the accuracy where true class matches with any one of the 3 most probable classes predicted by the model.And in the same way, we can measure the top 4, top 5, top 6, and so on.Let me show you an example to understand better. Consider having to classify records into respective animals (dog, cat, lion, tiger, and elephant). The table shows the true class and predicted class of 6 records.From this, we’ll be getting an accuracy of 50%, which is not so satisfying. Now let’s find out the top 2 accuracies for this problem. The table shows the true class and 2 most probable classes for the same set of records.With this, we get an accuracy of 83%, which is a significant increase compared to the previous one. This is the basic intuition behind finding top N accuracies.I hope you would have understood the significance of measuring the top N accuracies. Even if the classification model cannot predict the exact class, looking at the top 2 or top 3 accuracies might be of use in many situations, especially if there are a lot of different classes like 15 or 20.Now I’ll show you some code to find top N accuracies using python. Unfortunately, sci-kit learn doesn’t have any inbuilt function for this. So I’ve defined a function here that you can directly copy and use in your problems.After importing the required libraries and preprocessing the data, run the provided function.You can call the function by providing the following parameters:I’ve defined this function for text classification, but you can simply remove the lines having vectorizer instance and use it for standard classification.",python,https://towardsdatascience.com/understanding-top-n-accuracy-metrics-8aa90170b35?source=tag_archive---------36-----------------------
How to secure Python Flask Web APIs with Azure AD,"René BremerJun 1, 2020·7 min readPython Flask is a popular tool to create web applications. Using Azure AD, users can authenticate to the REST APIs and retrieve data from Azure SQL. In this blog, a sample Python web application is created as follows:The code of the project can be found here, architecture can be found below.In the remaining of this blog, the following steps are executed:To learn how to access an Azure Function backend using delegated or application permissions, see my follow-up blog which shares the same git repo as this blog.This sample shows how to build a Python web app using Flask and MSAL Python, that signs in a user, and get access to Azure SQL Database. For more information about how the protocols work in this scenario and other scenarios, see Authentication Scenarios for Azure AD. In this step, the following sub steps are executed:Step 1 focusses on the follow part of the architecture.To run this sample, you’ll need:Create and configure an app registration as follows:You will need to install dependencies using pip as follows:Run app.py from shell or command line using the following command:When the app is run locally, it can be visited by localhost:5000 (not 127.0.0.1:5000). After step 1, users can login using their Azure AD credentials. In the next step, the user roles are set that can be used to verify if user is allowed to retrieve data using the API.In this step, the claims in the tokens can be set which can be just be the web app to verify whether a user is allowed to call an api. See this link for more information on token claims. The following sub steps are executed:Step 2 focusses on the follow part of the architecture.Claim verification is an optional step and can be enabled using the following setting in app_config.py file: AAD_ROLE_CHECK = True .Follow the steps in this tutorial to add roles to app registration created in step 1.2. As manifest, the following appRoles shall be used:The assignment of users is explained in th link. As a test, two users can be created. User 1 is assigned the basic_user_access, whereas user 2 gets the premium_user_access role.In the next step, the Azure SQL database is created and the application identity is used to retrieve data form the database.In this step, the managed identity of the app is used to retrieve data, which is linked to the app registration created in step 1. The following sub steps are executed:Step 3a focusses on the follow part of the architecture.Create an Azure SQL DB using this link in which the cheapest SKU (basic) can be selected. Make sure the following is done:The backend_settings needs to be set to database. Make also sure that connection is filled in with your settings. Since the MI of the app is used, application_permissions need to point to “https://database.windows.net//.default"" in the app_config.py file, see also below.Now the app can be run as described in step 1.4. When you click on the link (Premium users only) Get Customer data from Database , customer data is retrieved. Subsequently, when there is clicked on the link Get Product data from Database, product data is retrieved (provided that claims are set correctly for user in step 2 or check is disabled)In this step, the identity of the app is used to retrieve data. However, the identity of the user can also passed (AAD passthrough) to retrieve data from the database.In the step, the identity of the user itself is used to retrieve data. This means that the token created in step 1 to login in the web app is also used to authenticate to the database. The following sub steps are executed:Step 3b focusses on the follow part of the architecture.Since AAD passthrough is used in this step, the users themselves shall have the appropriate roles in the SQLDB as external user and datareader. See example how to do this below.In case you want to be more granular in rolemembers in the database, read_customer reads data from SalesLT.Customer, whereas read_product reads data from SalesLT.Product)AAD User passthrough authentication can be set in the app_config.py file by setting delegated_permissions to [“https://sql.azuresynapse-dogfood.net/user_impersonation""], see also belowNow the app can be run as described in step 1.4, in which data can be retrieved from the database using the identity of the logged in user.In this blog, a Python web application is created that retrieves data from SQLDB. Users claims, managed identities and signed-in user passthrough tokens are discussed to authenticate and authorize users to retrieve data from Azure SQL, see also overview below.",python,https://towardsdatascience.com/how-to-secure-python-flask-web-apis-with-azure-ad-14b46b8abf22?source=tag_archive---------10-----------------------
Building a Simple Neural Network from Scratch,"Akarsh SaxenaJun 1, 2020·8 min readIn this article, we will look into the working of a simple neural network having only one neuron and we’ll see how it performs on our “Cat v/s Non Cat dataset”.By the end of this article, you will be able to-We’ll be coding our neural network and then using the trained network to determine whether an image contains a cat or not. This type of problem is known as a “Binary Classification Problem”. It consists of classifying the input into two classes, in our case, ‘Cat’ or ‘Not cat’.A single neuron in the neural network works as a straight line which has the following equation:This is the fundamental equation around which the whole concept of neural networks is based on. Let us break down this equation:y: Dependent variable (Output of the neural network)m: Slope of the linex: Independent variable (Input features)b: y-interceptIn terms of neural networks, we specify the slope as Weights, intercept as Bias and the output(y) as z. So the equation becomes:Here we have only one feature which we are giving to the model. To input multiple features, we’ll have to scale up the equation.The above equation can be scaled to ’n’ number of features which can be written as:Here we have ’n’ input features fed to our model. Corresponding to each input feature, we have a weight which specifies how important the feature is to our model to predict the output. The bias term helps in shifting our line on the axis to better fit the training data or else the line will always go through the origin (0, 0).We can make use of matrices to multiply all the weights with the inputs and adding biases to them. This can be done as follows:Here, each row represents a single training example (image, in our case) and each column represents an array of pixels.In python, we will use Vectorization to implement the above concept.Note: In the above equation, we have used “X.w+b” because our input matrix is of the shape (mXn) where ‘m’ is the number of samples and ’n’ is the number of features.The goal of training the neural network is to update the weights and biases to get as accurate predictions as we can.A neuron is a single unit in the neural network. It mimics the neuron in our brain having ‘Dendrites’ as inputs, ‘Nucleus’ as body and ‘Axon’ as output. Each neuron takes some input, processes it and gives an output based on an activation function.If you are unable to grasp any of these concepts, bear with me, it’ll all make sense once we’ll start to put things together.From the creation phase to getting the predictions, the whole process is defined in the following parts:The input that we have is the collection of images of ‘Cats’ and ‘Not Cats’. Each input image is a coloured image of 64x64 px dimension. We have a total of 209 images in our train dataset and 50 images in our test dataset. To feed these images into our neural network, these must be reshaped into a vector of pixels. So each image will be arranged in a row-wise order into a 1-dimensional vector.Initially, the shape of our inputs was (209, 64, 64, 3) but now after conversion, it will become (209, 64x64x3) i.e. (209, 12288). Now this ‘12288’ is the number of inputs to our neural network and ‘209’ is the number of training examples.Our images are in 8-bit so each pixel in our image is having a value in the range [0, 255] i.e. a range of total 256 values (2⁸=256). Therefore, we have to normalize our images by dividing each pixel by the maximum value i.e. 255.Neural networks are very sensitive to the input scale. We don’t want our inputs to vary too much or else larger inputs might dominate the smaller ones. So it’s always a good practice to normalize our inputs if they are in a large range.We will have to initialize weights and biases to some small value to be able to start the process of training.Here, we have multiplied the weights with 0.01 so that they will not explode (get very large) in the whole training process.Note: In the code, ‘b’ is only a ‘float’ number because of the broadcasting in python. It will automatically convert this ‘float’ number to the required vector shape.So now that we got our weights and biases initialized, let’s move forward to the next step.Here, we will calculate the output ‘z’ using the equation that we established above and then using an activation function on the output.We will use the sigmoid activation function in case of binary classification because sigmoid function gives output by transforming the input to [0, 1] interval. So, we can easily find the target class by making values greater than 0.5 to 1 and less than 0.5 to 0.The formula for sigmoid is:where,Let us look at the code to implement these two steps:So now that we got the output from the neuron, we can calculate loss and see how good/bad our model is performing.In binary classification, the loss function used is Binary Cross Entropy / Log Loss. It is given by the formula:where,m: Total number of samples in the datasetyᵢ: True label for iᵗʰ sampleaᵢ: Predicted value for iᵗʰ sampleThis can be implemented in python as:This loss will tell us how far are we from predicting the correct output. If the loss is 0, then we have a perfect model. But practically, if the loss is 0, then our model might be overfitting to the data.This is the part where all the magic happens. In every iteration, based on the model output and the expected output, we calculate the gradients. The gradient is how much weights and biases we need to change in order to reduce the loss.To calculate the gradient, we use the following formulas:In python, the above equations can be implemented as:Now, the ‘dw’ and ‘db’ contains the gradients that we need to adjust in our weights and biases respectively.We now need to adjust the weights and biases with the gradient that we just calculated. For this, the following equations are used:where ‘α’ (alpha) is the learning rate which defines how big/small our update should be.The code for updating the parameters is:Now we just have to repeat these steps for multiple times to train the neural network.In order to train the neural network, we must run the above steps for some epochs (number of times to repeat these process).Now that our neural network is trained, we can predict the output for new input.To get predictions from our neural network, we have to convert the output ‘a’ of the neural network such that the output value less than 0.5 becomes 0, otherwise 1.Although we only have one neuron, we are still getting 96% accuracy on the training data and 76% accuracy on the test data which is not bad.Congratulations! We have just made our first neural network from scratch. In the next article, I will tell you how to develop a shallow neural network which will have one hidden layer containing multiple neurons.You can find the complete code on Github:github.comSo, this was the whole process of creating and training a neural network from scratch. I hope you understand everything. Still, if you haven’t understood anything, drop a comment here and I’ll try to resolve your query.Follow me on Github and LinkedIn for more.",python,https://towardsdatascience.com/building-a-simple-neural-network-from-scratch-a5c6b2eb0c34?source=tag_archive---------28-----------------------
Applied Graphical Network Analysis using Python,"Saiteja KuraJun 1, 2020·6 min readNetworks are everywhere. Networks or Graphs are a set of objects (called nodes) having some relationship with each other (called edges). But how can graphs be used in our day-day analysis? I will explain the real-time usage of network analysis using two examples. Don’t missout on the case study at the end! I promise you, it is an interesting one!Firstly, consider the current challenge our world is facing. It’s the Covid-19 Pandemic. Let us consider we have a network comprising of various airports across the world as shown below. We can find the airports having more number of connections and identify them as hotspots.Secondly, let us consider a twitter network. We can find out the most influential people by identifying important nodes using network analysis. Nowadays, this is how most of the marketing agencies approach the problem of reachability and spread.We can clearly notice that network analysis has many applications across various fields like Social networks, financial networks, biological networks, transportation networks, and many more. We will be using the NetworkX library to create graphs in this series of articles. In this part, let us try and understand the basics of Network Analysis.As I already mentioned networks (are called ‘graphs’ mathematically) comprised of nodes and edges. These nodes and edges can have metadata too. Let us consider an example of two friends A and B who met on May 31st, 2020. Let us create a network now.Undirected NetworksEdges have no direction. Example — A Facebook social graph. (A sends a friend request to B and both become friends)Directed NetworksEdges have direction. Example — A Twitter social graph. (A follows B but it is not necessary that B follows A)Weighted NetworksNot all relationships(edges) are equal. Some have more weight.Example — Let us consider a network of employees in an organization and the weights on edges represent the number of emails sent to each other. In the above image, A sent 6 emails to B and so on.Signed NetworksSome networks carry information about friendship and enmity which is depicted on the relationship as an attribute. Example — In a website named ‘Epinions’ people can declare a connection as either a friend or a foe.Multi NetworksIn some networks, there may exist more than one edge between nodes. Example — Consider the relationship network shown in the image.Consider the following network? Which node do you think is important?You may answer that the central node is important. How did you arrive at this answer? You see that the central node has more neighbors than any other nodes.It is one of the many metrics used to evaluate the importance of a node and is simply defined as —Examples of nodes with high degree centrality are 1. Twitter broadcasters — (have many followers)2. Disease Super-spreaders3. Airport Hubs(AbuDhabi, NewYork, and London)As expected we can notice that the degree centrality of node 1 is the maximum. The neighbors()method returns the neighbors of that particular node.All shortest paths is a set of shortest paths between all pair of nodes in a given network. Betweenness Centrality of a node is defined as —This metric captures a different view of the importance of a node. Instead of finding nodes with more number of neighbors, it finds the bottleneck nodes in a network. We can find betweenness centrality of nodes of graph G using the nx.betweenness_centrlality(G) method. Consider the metro rail map of Hyderabad city shown below. The nodes with higher betweenness centrality are circled.I guess you understood the intuition behind betweenness centrality. It assumes that the nodes which connect most of the other nodes in the shortest possible way are most important.A question may pop up in your mind while reading this article. How is the dataset going to be for us to build a network? Until now we have explicitly created nodes and added edges between them. But that's not the case in real life. Let us take an edge file and try to build a network.Mark Sageman studied the lives of some terrorists who belong to The Hamburg Cell (which was behind the 9/11 attacks). He found the most common factor driving them was the social ties within their cell. He created an analysis which consisted of social ties among terrorists. The data is not 100% complete and accurate.The first two columns denote the persons who are connected and the following two numbers signify the strength of the connection(5=strong connection, 1=weak connection); and the last column denotes level to which the connection has been verified by the government officials (1 = confirmed, 3 = possible and unconfirmed connections ). The last two columns will not be used in our analysis.We can convert the above edge file into a graph using the code below.Now let us find the top three nodes having the highest degree centrality and betweenness centrality.Similarly, we can find the top 3 betweenness centrality nodes too. We can see the top three betweenness centrality valued nodes in the trap shown below. It is clear that without these nodes the network would become disconnected.You may think, without the two red circled nodes (in purple) too, the graph is pretty much connected. ie. the node named Imad Eddin Barakat Yarkas still connects the graph. But without the nodes circled in red, time taken for transmission of a message from the center part to each node would be longer. The nodes with high betweenness centrality act as connections that deliver messages in the shortest possible path. Hence the nodes circled in red have the highest betweenness centrality when compared to all other nodes.In this article, we tried to understand the importance of network analysis across various fields and the basics of networkxAPI. We also learned many real-life situations where network analysis is applied. In continuation, we will discuss about network connectivity and robustness. Stay tuned!Thanks for reading. Do feel free to share feedback.",python,https://towardsdatascience.com/applied-network-analysis-using-python-25021633a702?source=tag_archive---------7-----------------------
10 Smooth Python Tricks For Python Gods,"Emmett BoudreauJun 1, 2020·6 min readAlthough on the surface Python might appear to be a language of simplicity that anyone can learn, and it is, many might be surprised to know just how much mastery one can obtain in the language. Python is one of those things that is rather easy learn, but can be difficult to master. In Python, there are often multiple ways of doing things, but it can be easy to do the wrong thing, or reinvent the standard library and waste time simply because you were not aware of a module’s existence.Unfortunately, the Python standard library is quite a vast beast, and furthermore, its ecosystem is absolutely terrifyingly enormous. Although there are probably two-million gigabytes of Python modules, there are some useful tips that you can learn with the standard library and packages usually associated with scientific computing in Python.Though it might seem rather basic, reversing a string with char looping can be rather tedious and annoying. Fortunately, Python includes an easy built-in operation to perform exactly this task. To do this, we simply access the indice ::-1 on our string.In most languages, in order to get an array into a set of variables we would need to either loop through the values iteratively or access the dims by position like so:In Python, however, there is a way cooler and quicker way to do so. In order to change a list of values into variables we can simply set variable names equal to the array with the same length of the array:If you’re going to spend any time whatsoever in Python, you will definitely want to get familiar with itertools. Itertools is a module within the standard library that will allow you to get around iteration constantly. Not only does it make it far easier to code complex loops, it also makes your code both faster and more concise. Here is just one example of a use for Itertools, but there are hundreds:Unpacking values iteratively can be rather intensive and time consuming. Fortunately, Python has several cool ways in which we can unpack lists! One example of this is the *, which will fill in unassigned values and add them to a new list under our variable name.If you’re not aware of enumerate, you probably should get familiar with it. Enumerate will allow you to get indexes of certain values in a list. This is especially useful in data science when working with arrays rather than data-frames.Slicing apart lists in Python is incredibly easy! There are all sorts of great tools that can be used for this, but one that certainly is valuable is the ability to name slices of your list. This is especially useful for linear algebra in Python.Grouping adjacent loops could certainly be done rather easily in a for loop, especially by using zip(), but this is certainly not the best way of doing things. To make things a bit easier and faster, we can write a lambda expression with zip that will group our adjacent lists like so:In most normal scenarios in programming, we can access an indice and get our position number by using a counter, which will just be a value that is added to:Instead of this, however, we can use next(). Next takes an iterator that will store our current position in memory and will iterate across our list in the background.Another great module from the standard library is collections, and what I would like to introduce to you today is Counter from collections. Using Counter, we can easily get counts of a list. This is useful for getting the total number of values in our data, getting a null count of our data, and seeing the unique values of our data. I know what you’re thinking,“ Why not just use Pandas?”And this is certainly a valid point. However, using Pandas for this is certainly going to be a lot harder to automate, and is just another dependency you are going to need to add to your virtual environment whenever you deploy your algorithm. Additionally, a counter type in Python has a lot of features that Pandas Series don’t have, which can make it far more useful for certain situations.Another great thing coming out of the collections module is dequeue. Check out all the neat things we can do with this type!So there you have it, these are some of my favorite Python tricks that I use all the time. Though some of these might be used a little more rarely, these tricks tend to be very versatile and useful. Fortunately, the Python tool-box of standard library functions certainly doesn’t start to become bare there, and there are certainly more tools inside of it. More than likely there are some that I don’t even know, so there’s always something to learn which is exciting!",python,https://towardsdatascience.com/10-smooth-python-tricks-for-python-gods-2e4f6180e5e3?source=tag_archive---------0-----------------------
Python Library Hijacking on Linux (with examples),"In this article, we will discuss three effective methods to hijack the Python library in a Linux environment.Let’s see… What happens if a Python script runs with sudo privileges, but you have got write permissions on the imported module?Exactly! You can escalate your privileges by editing the imported functions to call system commands or even spawn a shell, that will have root rights.I am going to share three scenarios where anybody can exploit this vulnerability (or better call it a “security misconfiguration”):Each scenario has a specific requirement in common. The Python script must meet one of the following conditions for a successful privilege escalation attempt:Or you can search for files that have both SUID and SGID permissions:Let’s suppose we have a Python script that imports the base64 module and calls the b64encode function from this module, in order to encode a specific string.Now, let’s locate the base64 module path within the Linux file system.Got it under many paths (because I have more Python versions installed), but our attention should fall on the Python3.7 path:Let’s check the permissions of the Python module.We can clearly see that it got read, write, and execute permissions for everyone.Next, the b64encode function within that file must be edited in order to execute a system command such as whoami.Save the file, return to the terminal and type the following command to check which files we can run with sudo:And here we are, after running the script:We can notice that whoami system command got executed and returned expected results.When importing a module within a script, Python will search that module file through some predefined directories in a specific order of priority, and it will pick the first occurrence.For example, on Ubuntu, the default searching priority is the following:You can find your order, based on your Python version, with the following command:Maybe you already figured out how the attack works, if not, I am going to explain it in the paragraph below.The searched module will be located in one of the defined paths, but if Python finds a module with the same name in a folder with higher priority, it will import that module instead of the “legit” one.The requirement for such a method to work, is to be able to create files in folders above the one in which the module resides.We will use the same hijack.py script, but I moved the base64.py module to a lower priority folder within the directories hierarchy, so now the /usr/lib/python3.7/ folder doesn’t contain the module anymore.The above screenshot provides the actual folders priority hierarchy for Python Library search, on my local machine.Next, I’ll check the privileges for /usr/lib/python3.7.And there it is, the folder has read, write and execute permissions, so we can create our custom base64.py module inside.The malicious module must contain a function definition as the one called through the Python script (in our case b64encode), and with the exact number of arguments (in our case, just a string is parsed as an argument, so we will define the function with a single one, called “a”).Now, save the module, run the script, and notice that the payload was successfully executed.The PYTHONPATH environment variable indicates a directory (or directories), where Python can search for modules to import.It can be abused if the user got privileges to set or modify that variable, usually through a script that can run with sudo permissions and got the SETENV tag set into /etc/sudoers file.In our example, I moved the Python module to the /tmp/ folder.Let’s check if the SETENV tag is set, through the “sudo -l” command:And now, we can run the script like this:There it is!Do not set write permissions for users, on folders where Python modules are located.Restrict access to specific modules through virtual environments rather than letting Python search through the folders.I really hope this article was useful and wish you all a great day!",python,https://medium.com/analytics-vidhya/python-library-hijacking-on-linux-with-examples-a31e6a9860c8?source=tag_archive---------12-----------------------
A Simple Genetic Algorithm from Scratch in Python,"Joos KorstanjeJun 1, 2020·7 min readGenetic Algorithms are optimization algorithms that mimic the process of natural selection. Rather than using “mathematical tricks”, they simply copy a logic of which we know that it works.This process of natural selection is founded on the Survival of the Fittest: the process in nature that makes the best individuals (animals, plants, or other) survive. Those fittest individuals then mate with each other, giving rise to a new generation. Nature also adds a bit of randomness in the form of mutations to the genome.The new generation is a mix of good and bad individuals, but here again, the good ones will survive, then mate and then give rise to a new generation.The result is a consistent improvement from generation to generation.Staff Planning is a topic of optimization research that comes back in many companies. As soon as a company has many employees, it becomes hard to find planning that suits the business needs while respecting certain constraints. Genetic Algorithms are one optimization method to solve this, among other existing solutions.In a previous article, I have shown how to use the DEAP library in Python for out-of-the-box Genetic Algorithms. In this article, I am going more into the specifics to show how to understand the different parts of the genetic algorithm.The below code is a simplified version of what a production code for a genetic algorithm could look like. It is optimized for a better understanding of the example rather than for speed and reusability. It contains each of the listed steps, applied to example data.If you want to follow along with the notebook, you can download it here.In this code, we will be working with two different shapes of the same staff planning.Type 1 Planning — Per EmployeeThe first shape will be the staff planning for employees, the detailed view. This total weekly planning is a list that contains a list per day (5 days in our case). Each daily list contains a list of shifts (11 shifts for the employees in our case). Each shift is a list of an employee id (from 0 to 11, just for information), a start time (between 0 and 24 o’clock), and a shift duration (between 0 and 10 hours).This type of planning will be needed for our employees to know when they work.Type 2 Planning — Totals Per HourThe second type of planning is the total number of employees that have been staffed per hour. This planning will be used by the shop owner to decide whether the planning corresponds to the estimated needs of the shop.In order to evaluate an hourly staff planning, we need to have defined a goal situation. Defining this goal is not a part of the optimization: it would be a question for another project.We do need to define how to evaluate the differences between the planning proposed and the goal planning. This will be done based on the hourly planning, by summing the total number of employee-hours too much and the number of employee-hours missing. This will be a cost function, that we need to minimize.We could add weights for overstaffing or understaffing, but in this example I made them weigh equally.There are two key steps in the Genetic Algorithm: mating (also cross-over or recombination) and mutation.In the Mating step, the new generation is formed from the offspring of individuals of the parent population, as it is in natural selection.To apply this to our example, consider that later on, we will be generating many not so good staff plannings and trying to combine the best ones together. So we need to define a way to “mix” two individuals (staff plannings) with each other.In this example, I have decided to code this as follows:This is one way to do it, and there are many other approaches possible. In order for the genetic algorithm to work, it is important to have randomness in the combination code. Of course the combination must fit with the data structure that you chose in step 1.The second important step in the Genetic Algorithm is Mutation. It consists of adding a completely random change to the new generation. This random change allow to add a new value to the population that was not present anymore.For example, consider a case where the algorithm has done a few iterations, and due to randomness in the selection and combination process, it has deselected all starting times before 10 am. Without mutation, the algorithm would never be able to get this value back, while it may be actually giving a better solution later on.The random insertion of (a very small number of) new values helps the algorithm to escape from such situations.It is coded here as adding replacing either a shift duration or a starting time of one shift by a random value between 0 and 10. This can be repeated if we specify an n_mutations value.The selection process is quite simple:The last part of the code is to add all the previous building blocks into an overall code that iterates.To make the Genetic Algorithm work perfectly, it is important to select the right parameters: generation_size, n_mutations, and n_best are important in this.Tuning those three would allow for finding the best combination that both:If after this tuning your algorithm still gets stuck, another axis of improvement would be to adapt the mating and mutation functions and see what happens then.As the goal of this article was to have a simple and applied Genetic Algorithm from scratch, I will not go into the specifics of how to find those best parameters: that will require another article.Thank you for reading. Don’t hesitate to stay tuned for more!",python,https://towardsdatascience.com/a-simple-genetic-algorithm-from-scratch-in-python-4e8c66ac3121?source=tag_archive---------2-----------------------
"The Unscented Kalman Filter, simply the best! Python code","Jaroslaw GoslinskiJun 1, 2020·10 min readIn almost every project of data science, we face one of the three problems: filtration, prediction, or smoothing. All of these can be solved by the use of the Kalman Filters. I guess you read about or work with Kalman Filters before, but just to clarify on that: we speak about the powerful tool of data filtering, which was invented by the Rudolf Kalman in the late ’50s, used in the Nasa’s Apollo program and extended to handle nonlinear problems in ’70s. It is also a subject of multiple articles during its way of neverending improvements. You can find a bunch of information here on medium as well. Today I’m going to explain how to work with that tool by simply giving you a working example with a minimum theory as that was shown and explain millions of times. Remember this article is more about the practice and I assume that the reader knows the basis of KF filtering, the process, idea, etc.When a lot of improvements were introduced to the KF, also new generation was presented, called the Unscented Kalman Filter. It is the filter of the choice when it comes to filtering of nonlinear systems data. There are three big pros: the first, it makes approximation valid to the third order of Taylor series expansion, the second: it uses minimal computation in realizing the approximation via sigma points, the third: it is simple in use as it does not need Jacobian like the Extended Kalman Filter or Hessians (the Second-order Extended Kalman Filter). If you’ve seen KF in action or read articles covering estimation theory I bet you’ve also seen a nice set of graphs, like the ones below:In the figure above, the first column depicts the operation of data sampling; we have many data points describing x — the state of the system (in this case x is two dimensional), then each point goes through the nonlinear function f, yielding new data point y (also two-dimensional). Now, we can simply compute the mean and covariance of the x and y. In this case, the mean is a single data point (2-dim vector) while the covariance is a matrix of size 2x2. It is easy to draw a data point, but to draw covariance we need to find eigenvectors and eigenvalues of the covariance matrix, and then draw an ellipse which is called “confidence ellipse”. Based on the ellipse size we show how much data is covered, e.g. in case of Gaussian distribution, we may choose 2 sigma spread and cover ~95% of data. In the second and third graphs, we show how good the covariance and mean are captured by the EKF and UKF respectively. The picture above shows the main reason why the researchers improve KF. The better it approximates statistical Moments, the better estimation it provides. I think it would be beneficial to show this in a real case scenario. For this purpose, we need a model with two-dimensional state and two-dimensional output. I’ve modified the univariate nonstationary growth model (UNGM) to obtain multivariate nonstationary growth model MNGM:As you can see, the model is given in a discrete form, so it can be used directly in a UKF. The model is a nonlinear system, as there are two nonlinear functions, f and g, the first one takes the system input (here i) and previous state, and generates a new state. The second function takes the current state and forms the output. This model is what always is needed to compose a KF. The model is recursive as the KF. Not always f and g are nonlinear, sometimes there are linear, sometimes f is nonlinear and g is linear. But you need to form the system of equations, otherwise, there is no way to create a working estimation tool based on Kalman Filter.We use a more general description of the model given above i.e. model with additive noise, which can be given in a state-space representation:where noises are given by u,v~N(0,1).Having this information in mind, let’s introduce the model class:We use three functions/methods: generate_data — generates all vectors of data with noise added, state, and output — these are an f and g functions, respectively. The latter ones are used in the UKF.The UKF is a well-known algorithm, and all things that work under its hood have been developed for many decades. Yes, there are many variations of the UKF (e.g. Square Root Unscented Kalman Filter), but nearly all of them aim at computation efficiency improvement, not the core idea which is the use of sigma points.In the UKF given above, there are f and g functions used recursively. The algorithm is divided into three parts init, time update, and measurement update. The measurement update phase is being realized only in the last two equations so the division given above is not sticking to the original one. The code below is my python implementation of the algorithm:If the code above is not readable in this block (can’t do it better on medium), don’t worry, at the end of the article you will find a link to the Github project.Now we can generate noisy data, provide it to the UKF, and estimate the state:To run the above, type: estimateState()Base upon the chosen settings i.e. Q and R the obtained results may vary. For the same values of variances of process and measurement noises, the following was achieved:You can find easily the rule: the smaller the value of variance Q, the greater the confidence in the model, and: the smaller the value of variance R, the greater the confidence in the measurements. In the end, what’s matters is the variance ratio R/Q or Q/R.The UKF can be used for various problems and models. The presented idea of ​​combining the model class in the UKF class should be treated as a framework. In the coming post, I will tell you more about the ellipse of confidence as this is closely related to the UKF and its superiority over derivate-based Kalman Filters. Stay tuned.https://github.com/jaroslav87/UKF-MNGM.git",python,https://medium.com/@jgoslinski/the-unscented-kalman-filter-simply-the-best-python-code-5cd5ebaebf5f?source=tag_archive---------20-----------------------
How Much Math do I need in Data Science?,"If you are a data science aspirant, you no doubt have the following questions in mind:Can I become a data scientist with little or no math background?What essential math skills are important in data science?There are so many good packages that can be used for building predictive models or for producing data visualizations. Some of the most common packages for descriptive and predictive analytics include:Thanks to these packages, anyone can build a model or produce a data visualization. However, very solid background knowledge in mathematics is essential for fine-tuning your models to produce reliable models with optimal performance. It is one thing to build a model, it is another thing to interpret the model and draw out meaningful conclusions that can be used for data-driven decision making. It’s important that before using these packages, you have an understanding of the mathematical basis of each, that way you are not using these packages simply as black-box tools.Let’s suppose we are going to be building a multi-regression model. Before doing that, we need to ask ourselves the following questions:How big is my dataset?What are my feature variables and target variable?What predictor features correlate the most with the target variable?What features are important?Should I scale my features?How should my dataset be partitioned into training and testing sets?What is principal component analysis (PCA)?Should I use PCA for removing redundant features?How do I evaluate my model? Should I used R2 score, MSE, or MAE?How can I improve the predictive power of the model?Should I use regularized regression models?What are the regression coefficients?What is the intercept?Should I use non-parametric regression models such as KNeighbors regression or support vector regression?What are the hyperparameters in my model, and how can they be fine-tuned to obtain the model with optimal performance?Without a sound math background, you wouldn’t be able to address the questions raised above. The bottom line is that in data science and machine learning, mathematical skills are as important as programming skills. As a data science aspirant, it is therefore essential that you invest time to study the theoretical and mathematical foundations of data science and machine learning. Your ability to build reliable and efficient models that can be applied to real-world problems depends on how good your mathematical skills are. To see how math skills are applied in building a machine learning regression model, please see this article: Machine Learning Process Tutorial.Let’s now discuss some of the essential math skills needed in data science and machine learning.Statistics and Probability is used for visualization of features, data preprocessing, feature transformation, data imputation, dimensionality reduction, feature engineering, model evaluation, etc.Here are the topics you need to be familiar with: Mean, Median, Mode, Standard deviation/variance, Correlation coefficient and the covariance matrix, Probability distributions (Binomial, Poisson, Normal), p-value, Baye’s Theorem (Precision, Recall, Positive Predictive Value, Negative Predictive Value, Confusion Matrix, ROC Curve), Central Limit Theorem, R_2 score, Mean Square Error (MSE), A/B Testing, Monte Carlo SimulationMost machine learning models are built with a dataset having several features or predictors. Hence, familiarity with multivariable calculus is extremely important for building a machine learning model.Here are the topics you need to be familiar with: Functions of several variables; Derivatives and gradients; Step function, Sigmoid function, Logit function, ReLU (Rectified Linear Unit) function; Cost function; Plotting of functions; Minimum and Maximum values of a functionLinear algebra is the most important math skill in machine learning. A data set is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, dimensionality reduction, and model evaluation.Here are the topics you need to be familiar with: Vectors; Norm of a vector; Matrices; Transpose of a matrix; The inverse of a matrix; The determinant of a matrix; Trace of a Matrix; Dot product; Eigenvalues; EigenvectorsMost machine learning algorithms perform predictive modeling by minimizing an objective function, thereby learning the weights that must be applied to the testing data in order to obtain the predicted labels.Here are the topics you need to be familiar with: Cost function/Objective function; Likelihood function; Error function; Gradient Descent Algorithm and its variants (e.g. Stochastic Gradient Descent Algorithm)In summary, we’ve discussed the essential math and theoretical skills that are needed in data science and machine learning. There are several free online courses that will teach you the necessary math skills that you need in data science and machine learning. As a data science aspirant, it’s important to keep in mind that the theoretical foundations of data science are very crucial for building efficient and reliable models. You should, therefore, invest enough time to study the mathematical theory behind each machine learning algorithm.Linear Regression Basics for Absolute Beginners.Mathematics of Principal Component Analysis with R Code Implementation.Machine Learning Process Tutorial.",python,https://pub.towardsai.net/how-much-math-do-i-need-in-data-science-d05d83f8cb19?source=tag_archive---------5-----------------------
Machine Learning Algorithm — RFM Model,"Sivaranjani PrabasankarJun 1, 2020·5 min readIn this tutorial, we will be learning how to implement customer segmentation using RFM (Recency, Frequency, Monetary) analysis using Python.Customer segmentation is the process of dividing customers into groups or clusters based on common characteristics.In Business to Business model, the company can segment customers based on various factors like1) Demographic (Age, Gender, Occupation, Marital Status)2) Geographic (Location, Region, Urban/Rural)3) Behavioural (Spending, Consumption, Usage)4) Psychographic (Lifestyle, Social Status, Personality)To identify the most potential customersTo easily communicate with a targeted group of the audienceTo improve the quality of service, loyalty, and retentionBetter understanding needs of the customer in each segmentFor better upselling and cross-selling of productsTo identify new products that customers could be interested inCustomer Segmentation using RFM AnalysisRFM (Recency, Frequency, Monetary) analysis is a behaviour-based approach grouping customers into segments. It groups the customers based on their previous purchase transactions considering the factors like· How recently· How often· How much did a customer buyRFM filters customers into various groups for the purpose of better service. It helps managers to identify potential customers to do a more profitable business.There is a segment of customer who is the big spender but what if they purchased only once or how recently they purchased? Do they often purchase our product?Also, it helps the company to run an effective promotional campaign for personalized service· Recency (R): Who have purchased recently? Number of days since last purchase (least recency)· Frequency (F): Who has purchased frequently? It means the total number of purchases. (high frequency)Monetary Value(M): Who have high purchase amount? It means the total money customer spent (high monetary value)1) Load DataData source: https://www.kaggle.com/jihyeseo/online-retail-data-set-from-uci-ml-repo?2) Data Explorationa) Unique Valuesb) Grouping Country wise3) Data Cleaninga) Removing Rows with negative values for quantityb) Checking for Duplicate valuesc) Checking for Missing values and Dropping them4) Data Pre-processinga) Calculating Total price and adding it as a new featureb) Calculating Snapshot date using recent invoice date5) RFM Customer SegmentationI. Calculate RFM valuesRecencyi) Group customer based on their Idii) Find the difference between invoice date and snapshot date to know their recencyFrequencyi) Group customer based on their Idii) Count the no. of. invoice for each customer to know their frequencyMonetaryi) Group customer based on their Idii) Sum the total purchase using invoice amount for each customer to know their monetary valueII. Plot RFM distributionsThis plot provides us with some very interesting insights and how skewed our data is. The important thing to take note here is that we will be grouping these values in quantiles. However, when we examine our customer segmentation using K-Means in the next, it will be very important to ensure that we scale our data to centre the mean and standard deviations.III. Create R, F and M groups based on QuartilesIV. Calculate RFM score — Add a new column to combine RFM scoreV. Adding customer in each segment bins to base on RFM scoreVI. Calculating mean in each segment bins for better segment statsVII. Plotting a map based on segment stats for better understandingConclusionIn this tutorial, we covered a lot of details about Customer Segmentation, RFM analysis and Implementation of RFM in python. Also, we learned some basic concepts of pandas such as handling duplicates, groupby, qcut() and squarifyplot for bins based on sample quantiles.Hopefully, this would help to analyze your own datasets.Happy Learning!Sivaranjani",python,https://medium.com/@sprabasankar/machine-learning-algorithm-rfm-model-efd543228a1e?source=tag_archive---------44-----------------------
10 Steps to Setup a Comprehensive Data Science Workspace with VSCode on Windows,"Gong NaJun 1, 2020·9 min readTo start a data science project, choosing an effective development environment is always the first step. Jupyter Notebook/Lab is a common choice. I like it very much. But it’s really not enough.When I started my first professional data science project, I used many tools together: Jupyter Notebook for model training, PyCharm (free Community version) for structured Python script, Anaconda Prompt for python package management, MobaXterm for remote SSH connection, Excel for raw data review and so on. Everyday, I jump between different windows all the time.Fortunately, I found a single tool which supports all above functions and most importantly, it’s for free: VSCode.If you are suffering from the same problem as above, or you are a beginner in the data science field, this guide may help you build your own effective data science workspace and have a good start-point.Now, Let’s start ~🚀[following setup is based on Win10]You can install the latest edition from here, or the archived version from here. If you are going to use the integrated terminal in VSCode, I highly recommend you to install the version of Anaconda ≥ 4.6.After the successful installation, you have also installed Jupyter Notebook, Python and some common python packages for data science usage. And you can now find Anaconda Navigator, Anaconda Prompt, Jupyter Notebook icons in the Start Menu.To avoid the mutual interference between different projects, it’s better to develop in an isolated virtual environment. If you really prefer to develop in the defaultbase env, you can skip this step.Open Anaconda Prompt from Start Menu:Check your current env list :Create your own virtualenv with a name and specified python version (example name: ssl):Activate the virtualenv:Check what packages exist in this virtualenv (python should have been installed during the virtualenv creation):After finishing the setup in VSCode, you can directly mange your virtualenv in VSCode terminal.For more conda environment management, please check here.Option 1: open Anaconda Navigator > launch VSCode from the Home page.Option 2: you can also install VSCode from the website. In this way, you can get a shortcut, then launch VSCode quickly from the desktop or your taskbar. From my own experience, this way is faster than Option 1.Create a new project folder as your root directory in anywhere you like. Open VSCode > click the File in the top left > click Open Folder > click the target folder name. Now, you can put all your project files and scripts here.VSCode is a source-code editor supporting a bunch of languages. In order to enable Python, we need to install the Python extension.Click Extension icon in the left bar > search Python > click install:Now, you can create standard python scripts simply with .py file extension:If you also like the cute python icon in front of the helloWorld.py, you can get it by adding VSCode Great Icons extension. It can help you easily distinguish different types of file and folder:In VSCode, press ctrl+shift+p, a Command Palette will pop up in the top middle. Click Python: Select Interpreter > you will see a list containing all your virtualenv and the base env > choose the proper virtualenv.If you cannot see the Python: Select Interpreter, just type and search it.Afterwards, the python interpreter with the env name will show in the bottom-left of the Status Bar:Open a new VSCode terminal by clicking top left ‘Terminal’ or the shortcut ctrl+shift+`. The terminal will be opened in the right bottom.Choose a default shell based on your personal preference (default: cmd):If you want to use Linux command on Windows just like me, Powershell is a good choice for you.After selected, just open another new terminal to catch your updates. If you have successfully selected a python interpreter in the last step, the virtualenv now will be automatically activated in your new opened terminal via conda activate. And the virtualenv name will show in () at the front:Now, you can execute .pyfiles in the terminal by💢 Two common errors you may encounter:1. Fails to run conda activate in Powershell.Solution: conda starts to support conda activate in Powershell only from the version 4.6. So if you want to use Powershell as your default shell in VSCode, make sure that your conda version is at least 4.6 or higher (it may differ in different Windows systems)2. Unrecognized term error when you run conda or pythonin the terminal Solution: Get your python and conda paths by running below command in a terminalCopy&Paste both the python and conda paths to the Path of Environment Variable:Install the Jupyter package which includes the notebook, qtconsole, and the IPython kernel viaCreate a new jupyter notebook file: pressctrl+shif+p > search Create New Blank Jupyter Notebook > click it > save the created.ipynbfile with a name:In a few seconds, the top right Jupyter Server will be automatically set as local, and the Kernel as the conda environment chosen in Step 5.If not, or you want to change the kernel/env: press ctrl+shif+p > search Select Interpreter to start Jupyter server > click it > choose a proper env from the shown list:Now, you can use Jupyter Notebook as usual:Another awesome feature which I like the most is the remote SSH connection which supports opening any remote folder with full VSCode’s feature set.Install Remote-SSH extension:Then you will see a little connection icon shown in the bottom-left of the Status Bar:Click the icon > select Remote-SSH: Connect to Host… > add the standard ssh connection command ssh username@hostname > hit Enter:An empty window will be opened which will setup the remote SSH connection automatically. After connected, you will see the host name in the bottom-left of Status Bar with SSH:username@hostname:Now, you can open any folder under your home directory on that connected remote machine and work on it just as you are locally:💢 If you get the corrupted MAC on input error during the remote connection as below, you can solve it by adding the availabel MAC cryptographic algorithm in ssh . I post the detailed steps in this story.VSCode supports Git version control in-the-box so that you can track your code in a very pleasant format and push it easily by a few clicks.If your opened folder is under a git repository, the branch name will be shown in the bottom left with a * in VSCode. And the total number of changed files will be shown with the Source Control icon in the left bar.In the below example, it’s in the master branch with one changed file in Untracked status.Click the Source Control icon > click the file name > you can check the detailed code changes in a comparison view:Stage changes by clicking + icon after the file > add a commit message in the above box > click ✔ on the above to commit:A Synchronize Changes action will then show in the Status Bar with a little icon of 1↑ which indicates that you have 1 commit not pushed yet:Just click it to push all committed changes. After the successful push, the 1↑ indicator will disappear:Now, your Data Science workspace in VSCode is ready!VSCode is far more powerful than I shared above. There are a lot of diversified extensions which support different developing requirements. If you are new about them, and feel little bit lost in the large extension pool at the beginning, the following essential extensions may help you to start. Just keep trying!",python,https://towardsdatascience.com/10-steps-to-setup-a-comprehensive-data-science-workspace-with-vscode-on-windows-32fe190a8f3?source=tag_archive---------17-----------------------
Common Image Processing Techniques in Python,"Renu KhandelwalJun 1, 2020·8 min readIn this article, you will learnPIL, OpenCV, and imutilsWhy do you need to learn image processing techniques?Deep learning is used to analyze, identify, and generate an intelligent understanding of the objects in an image. Some of the common applications are Image Classification, Object Detection, Instance Segmentation, etc. Hence, it is imperative to have a strong knowledge of image processing techniques like image enhancements that include cropping the image, removing the noise present in the image, or rotating the image, etc., to build a better training dataset.Image processing techniques are equally helpful for Optical Character Recognition(OCR)Image processing techniques increase the interpretability of the image to classify or detect objects present in the image by helping to identify key features or to read text from images.Code and images available hereImporting Required LibrariesLet’s first display the image using OpenCV and PILReading and Displaying the Image using OpenCVIf the image is too big, then the window in which the image is displayed will not fit the screen.So how do I display the full image on the screen?By default, you get a cropped display when displaying an oversized image. To view the full image, you will use OpenCV’s namedWindow(name, flag) to create a new window. The first parameter, name, is the caption for the window and will be used as an identifier. When you set the flag to cv2.WINDOW_NORMAL, then the full image is displayed, and you can resize the window. There are other options for the flag parameter.Resizing imageWhen we resize the image, we change the height or width or both of the image and maintain the aspect ratio. The aspect ratio of the image is the ratio of the image’s width to its height.Reading and Displaying the Image using PILYou will load the image using open() and then use show() to display.image. show() creates a temporary file.What if you are interested in identifying edges or other features in an image?A grayscale image is used to identify edges. Grayscale images are also helpful to understand contrast or shading gradients present in an image and understand their role to analyze the features of an image.RGB images have three channels Red, Green, and Blue, compared to a 2D channel for the grayscale image. There will be less information per pixel for the grayscale image compared to a color image, and hence the processing time for the grayscale image will be faster.We are converting a color image to grayscale using cvtColor(), which will save an image from one color space to another.convert() returns a converted copy of this image; you use an “L” mode for grayscale and “RGB” mode for color.Edge in an image is detected using a canny edge detector. A Canny edge detector uses a grayscale image using a multi-stage algorithm.Canny(): the first argument is the input image, and the second and third arguments are threshold1 and threshold2 values.Edges with intensity gradient more than threshold2 are considered as the edge and those below threshold1 as non-edges. Non-edges will be removed. Any gradient intensity value between the two thresholds is classified as edges or non-edges based on their connectivity.What if the image is skewed or slightly rotated, and you want to deskew the image?OCR on skewed text does not work well, and hence we need to deskew. Images can be deskewed using rotate() of OpenCV and PIL.rotate() will rotate the image based on the rotationCode, which is specified as the second parameter to the rotate function.Values for rotationCode areWhat if I want to rotate the image by only a specific angle?Rotating image based on a specific angleIn the code below, the image is rotated in increments of 60-degrees using rotate() of imutilsHere the image is rotated by 110 degrees using PILHow to enhance the image quality when it is deteriorated by the presence of noise and impacts the image analysis?Noise is an unwanted signal, and in terms of an image, it is a random variation of color.Minimizing the noise present in an image using OpenCV.There are different methods in OpenCV to remove noise from images. The one used below is cv.fastNlMeansDenoisingColored(), which is to remove noise from a color image.Common arguments for the fastNIMeansDenoising methods areHow to extract certain regions of interest from an image?Cropping the image lets you extract the area of interest in an image.We will crop the image of TajMahal and only keep the TajMahal and remove other details from the image.Cropping image using OpenCVCropping in OpenCV is done by slicing the image array where we pass the start and end of y co-ordinates followed by the start and end of x-coordinates.image[y_start:y_end, x_start:x_end]Cropping image using PILcrop() of PIL allows us to crop a rectangular area of the image, The parameter to the crop() is left co-ordinate, top co-ordinate, right co-ordinate and bottom pixel co-ordinate.You can provide a template and search that template in an image and extract its location using matchTemplate() of OpenCV.The template slides over the entire image like a convolution neural network and tries to match the template to the input image.minMaxLoc() is used to get the maximum/minimum value, which will be the top left corner of the rectangle along with the width and height.There are 6 methods available for the template matchIn the example below, I have cropped a small portion from the main image for creating the template.The method used for the template match is TM_CCOEFF_NORMED. The threshold for the match is set to 0.95. It localizes the location with a higher matching probability greater than 95% and then draws a rectangle around the area corresponding to the match.We have discussed the most common image processing techniques that can be used to analyze images for Image classification, Object detection, as well as OCR.docs.opencv.orgpillow.readthedocs.iohttps://opencv-python-tutroals.readthedocs.io/en/latest/",python,https://towardsdatascience.com/common-image-processing-techniques-in-python-e768d32813a8?source=tag_archive---------16-----------------------
Space Science with Python — A comet in 3 D,"Thomas AlbinJun 1, 2020·10 min readThis is the 12th part of my Python tutorial series “Space Science with Python”. All codes that are shown here are uploaded on GitHub. Enjoy!It’s the 12th November 2014. The last couple of days were overwhelming. So overwhelming that I did not have much sleep … now … I am waiting, sitting in front of screens with several other scientists to wait and witness a scientific and engineering milestone in human history: The landing of a spacecraft on the comet 67P/Churyumov–Gerasimenko. The lander’s name: Philae; an essential part of ESA’s mission Rosetta/Philae.Spacecraft missions are indeed generation projects. I have seen documents of the mission’s ideas and objectives that were dated back when I was visiting elementary school. During secondary education the mission launched (2004), during by Bachelor’s studies the mission was still flying in interplanetary space (2009–2012). Then, during my Master’s in 2014, I was sitting now with scientists in the lander control centre in Cologne (Germany), working in the Dust Impact Monitor (DIM) team, as a part of the Surface Electric Sounding and Acoustic Monitoring Experiment (SESAME) collaboration [1, 2, 3]. DIM was a rubric’s cube sized and shaped instrument on the lander. It’s objective was to measure impacts of millimetre sized dust particles during the landing (descent) phase and on the comet’s surface itself. The measurement principle was simple: DIM analysed and derived the particles’ properties from impacts on piezo-electric elements … these are the same electronic components that convert swinging guitar strings to electric signals … making it an electric guitar.The landing of Philae and months of close observations with the Rosetta orbiter revealed a ancient cosmic world formed at the very beginning of our Solar System: consisting of water ice, minerals and an abundance of organic compounds. Rosetta/Philae, and my personal story that is linked to it would probably be a nice addition for another less data scientific Medium article. For now, what matters is that the mission gathered a huge amount of data that is analysed or still waiting to be analysed and also published. All data are publicly available, either on NASA’s Planetary Data System’s site (PDS) and / or on ESA’s Planetary Science Archive (PSA).In the last sessions, we learned a lot about comets, their origin, their relation to Jupiter and observational biases. Today, we will have a look at the shape of this exemplary comet core.Preusker et al. (2015) [1] used the image data from a camera system on board the orbiter Rosetta called Optical, Spectroscopic and Infrared Remote Imaging System (OSIRIS) to define a coordinate system for this weirdly formed comet as well as to derive a so called shape model. A shape model contains 3 dimensional information of an object that allows one to render, visualise and work with 3 D objects. Shape models contain so called vertices and additionally edge or face information:Shape models of the comet 67P have also been derived by the so-called Navigation Cameras (NAVCAM) that were used to determine the spacecraft’s orientation in space. NAVCAM was intentionally an engineering device that allowed one however to derive scientific insights.NASA’s mighty SPICE toolkit that we used in the previous tutorial session has some great functionalities to work with shape models. These models are abbreviated as so-called dsks (digital shape kernels). We will get back to the dsk functions in the future.Today, we will need to install a new package for 3 D rendering: visvis. visvis works with miscellaneous graphical backends like: FLTK, GTK, Wx and also PyQT5. For this tutorial we will be working with QT and install it via:Now we can install visvis via:Please note a few things: The installation and testing for this session has been done on Mac OS Catalina. If you face any issues with Linux or Windows, do not hesitate to ask here. We will try to figure it out.The creator of visvis provides a nice wiki page if you are interested in more examples. The creator also states that there are similar libraries out there like vispy.First, let’s import the already known libraries (also imageio and tqdm as used in a supplementary article). We will also need our custom function func (line 10) to download the shape model. Due to its size of around 85 MB the shape model cannot be stored on the tutorial corresponding GitHub repository.An extra folder (line 2) shall be created for the shape model. The download link of the model is listed in line 3 and 4. You can see that the link contains the dsk SPICE notation. The file name of the shape model appears somehow cryptic so let’s take a look at the folder’s corresponding readme file. It states:With the abbreviations:In our case, we have a shape model of the comet 67P/Churyumov–Gerasimenko (CG) from the Rosetta mission (ROS). There are at least 1 million plates (faces) in the data set (M0001), derived from the OSIRIS camera (O) with a so-called Stereo Photo Clinometry (SPC) method by the Laboratoire d’Astrophysique de Marseille + Planetary Science Institute (LPS): OSPCLPS. The object is named (N) and it is currently version 1 (V1). The data are stored in a so called OBJ file.After the download, we load the shape model with the pandas read_csv function. The file has 4 columns. The first one indicates the type with a v for vertex or f for face. The last three columns are the corresponding coordinates (vertex) or vertex indices (face). According to the printed results (see below) we have almost 600,000 vertices and over one million faces, as the file name already indicated.Now, we can take a look at the vertex (line 2 and 3) and face (line 4 and 5) data, by printing the first 5 rows with the pandas head function:We can see the coordinates for the vertices and the indices for the vertices (faces). The coordinates of the vertices are given in km and represent the actual physical 3 dimensional size of the comet’s core!We can easily assign a Python list with the vertices (line 2 and 3), by converting the vertices sub-set to a list with tolist. The faces however (line 4), need some more data parsing. We will see it in the next code snippet.Let’s print the minimum (line 2) and maximum (line 3) vertex index in the faces sub-set …… We see, that the minimum index is 1; further, the index appears to be a float. For our Python routine, the indices start at 0. We need to subtract the indices by 1:The subtraction is simply done in line 2. Afterwards, we convert the floats to integers and convert the faces sub-set to a Python list.We are now ready to go! We should be able to adjust the resolution of the window, to change the settings according to the computer's performance. For this purpose, the creator of visvis provides a nice example for QT4. We need to change some imports for QT5:And we are ready to explore our comet! First, we create the visvis application (line 2 and 3). We create the main window that was defined before in line 6 and set a proper resolution in line 7. Now, with the function mesh the shape model is read. The vertices and faces are committed as well as a parameter that tells the function how many vertices define a face (here: 3). Line 14 gets the axes objects (similar as in matplotlib) that allows us to set a background colour (line 17) and to adjust some parameters (line 20 and 21). Now, we set a camera setting (3d) in line 26. Note: if you want to “fly” along the comet, like in a game (with w-a-s-d (translation) and i-j-k-l (tilt)), replace this parameter with “fly”. The field-of-view is defined in 29 as well as the initial camera’s azimuth and elevations angle (line 32 and 33). And finally, we can run it (line 36).Below, you can see a short movie. Feel free to run the application locally on your computer, adjust the settings and explore this cosmic world. 67P is weirdly shaped. It consists much likely of a binary system that merged at some point in the past. The “rubber duck” consists of two bodies with crater and basins, while the “neck” in between is much smoother. The object (486958) Arrokoth has a similar shape that was being visited by NASA’s New Horizons mission.Finally, let’s finish our exploration with the very first animation at the beginning of the tutorial. For this purpose, we set a smaller resolution, since the resulting GIF should not exceed 25 MB (Medium limit; line 9). We change the material properties of the comet in line 14 and 15; the resulting object is less shiny and appears more realistic. Further, per default, our camera view has an “attached spotlight” that illuminates the object. We turn this light source off (line 36 and 37) and add a fixed light source in line 38. Now, in line 44 to 57 we change the camera’s azimuth angle dynamically in a for-loop (line 47) and re-draw the figure in each step (line 50 and 51). The resulting image is extracted in line 54 and line 57 converts the image data to an 8 bit matrix and appends the result to a list that was defined in line 41. Finally, in line 60, we create the animation as a GIF.Enjoy the 3 D power of Python and visvis! Scientists have worked hundreds of hours to derive this scientifically highly accurate shape model after waiting for years to reach the comet! It is now up to you to explore this world and compare the model with actual observations stored in the ESA image database. We will work with this model and derive some more parameters like the entire surface of the comet and we will also work with the coordinate system of 67P. What do we know about 67P?ThomasBy the way. The methods that I am showing have actually been used for scientific insights and publications! Take a look at [2]: Link. In figure 3, you can see the recent model back then. With the Philae corresponding spk kernel you could try to reproduce this figure (without the lander)![1] Flandes, Alberto; Krüger, Harald; Loose, Alexander; Albin, Thomas; Arnold, Walter. Dust Impact Monitor (DIM) onboard Rosetta/Philae: Tests with ice particles as comet analog materials. Planetary and Space Science, Volume 99, p. 128–135. September 2014. DOI: 10.1016/j.pss.2014.05.014[2] Krüger, Harald; Seidensticker, Klaus J.; Fischer, Hans-Herbert; Albin, Thomas; Apathy, Istvan; Arnold, Walter; Flandes, Alberto; Hirn, Attila; Kobayashi, Masanori; Loose, Alexander; Péter, Attila; Podolak, Morris. Dust Impact Monitor (SESAME-DIM) measurements at comet 67P/Churyumov-Gerasimenko. Astronomy & Astrophysics, Volume 583, id.A15, 13 pp. November 2015. DOI: 10.1051/0004–6361/201526400. arXiv: arXiv:1510.0156[3] Hirn, Attila; Albin, Thomas; Apáthy, István; Della Corte, Vincenzo; Fischer, Hans-Herbert; Flandes, Alberto; Loose, Alexander; Péter, Attila; Seidensticker, Klaus J.; Krüger, Harald. Dust Impact Monitor (SESAME-DIM) on board Rosetta/Philae: Millimetric particle flux at comet 67P/Churyumov-Gerasimenko. Astronomy & Astrophysics, Volume 591, id.A93, 7 pp. June 2016. DOI: 10.1051/0004–6361/201628370. arXiv: arXiv:1605.06291[4] Preusker, F.; Scholten, F.; Matz, K. -D.; Roatsch, T.; Willner, K.; Hviid, S. F.; Knollenberg, J.; Jorda, L.; Gutiérrez, P. J.; Kührt, E.; Mottola, S.; A’Hearn, M. F.; Thomas, N.; Sierks, H.; Barbieri, C.; Lamy, P.; Rodrigo, R.; Koschny, D.; Rickman, H.; Keller, H. U. Agarwal, J.; Barucci, M. A.; Bertaux, J. -L.; Bertini, I.; Cremonese, G.; Da Deppo, V.; Davidsson, B.; Debei, S.; De Cecco, M.; Fornasier, S.; Fulle, M.; Groussin, O.; Güttler, C.; Ip, W. -H.; Kramm, J. R.; Küppers, M.; Lara, L. M.; Lazzarin, M.; Lopez Moreno, J. J.; Marzari, F.; Michalik, H.; Naletto, G.; Oklay, N.; Tubiana, C.; Vincent, J. -B. Shape model, reference system definition, and cartographic mapping standards for comet 67P/Churyumov-Gerasimenko — Stereo-photogrammetric analysis of Rosetta/OSIRIS image data. Astronomy & Astrophysics, Volume 583, id.A33, 19 pp. November 2015. DOI: 10.1051/0004–6361/201526349",python,https://towardsdatascience.com/space-science-with-python-a-comet-in-3-d-3774b1d71d9b?source=tag_archive---------49-----------------------
A Simple Method to Calculate Circular Intensity Averages in Images,"In experimental materials science, we are often given images such as the one below, called an X-ray diffraction pattern.In this case, the symmetry of the sample measured causes the images to have rings of constant intensity, knows as Debye-Scherrer rings. For us, this is redundant data, we need to reduce this image down to a simple plot of radial distance (from the center of all the rings) vs. the intensity at each radial distance. The simplest possible way to do this is to just take a line that begins at the center and goes radially outward, recording intensity at each radial point. A more robust method, however, is to use the average intensity from the entire ring, to ensure that variability gets averaged out.It turns out that this is very simple to do! The example shown here will be done using the Python package numpy and visualized using matplotlib. Additionally, to import the TIFF file in this example, I will use a package called tifffile, which can be found here.First, we import the package we will use:Now, we can load our image by using tifffile.imread. We do so as following:If we want to now show our image to inspect what it looks like, we can use the following lines of code:From inspection, we find that the center of all the circles is at the pixel value of (1242, 642) — the 1242nd row and 642nd column. So the first thing that we want to do is redefine the pixel coordinates so that the radial distance is 0 at this point. This transformation will involve 2 steps:We will now initialize our variables for the averaging calculation. Here, we need to set the resolution of our averaging. The following line of code creates an array called rad with radial values that go from 1 pixel to the maximum radial value in our grid. The third argument defines how dense or sparse we want our radial points to be (in this case, every 1 pixel). This array will serve as the x-values in our final plot:Now we initialize array for our y-values (intensity). We create an array of zeros with the same length as rad (which we will change as we go through), and an index variable called index, which we will use to keep track of where we are changing the intensity:Now, we can use a loop to calculate the average at each radial distance. Here, another important parameter that we set is the bin size. This sets how many pixels we include in addition to the exact radial distance we are looking for. In this example, we will a bin of 1 pixel less than and greater than our radius of interest:Our loop will proceed by the following steps:As simple as that seems, we are now done!The last thing left to do is view our new, circularly-averaged image. We can do that as follows (I have changed some of the plot parameters, for a more detailed explanation, you can refer to my earlier article):I was fascinated by how elegant and simple it was to do a circular averaging with a few lines of code, so I wanted to put together a step-by-step article to show — hope you enjoyed it! This script is part of a larger set of tools I developed for analyzing X-ray diffraction data, and can be found at this Github repository.Thank you for reading! I appreciate any feedback, and you can find me on Twitter and connect with me on LinkedIn for more updates and articles.",python,https://levelup.gitconnected.com/a-simple-method-to-calculate-circular-intensity-averages-in-images-4186a685af3?source=tag_archive---------33-----------------------
Analysis On Tweets Using Python and TWINT,"Aditya Kousik CotraJun 1, 2020·5 min readEver heard of Twint?Twint is an advanced web scraping tool built in Python which scrapes the web instead of collecting data through the twitter API like tweepy. It is short for Twitter Intelligence Tool. You can download it using:The twint documentation can be found here.In this article, we will use Donald Trump’s tweets since the start of the year 2019. We can download tweets of a given user with this simple command in the command line:This will download all the tweets of @realDonaldTrump since 2019 into a single csv file trump.csv .Here I have converted the csv file to xls format for convenience. Lets dive in!Looks like the president’s tweets are lengthy early in the morning (3am to 10am).How about when coupled with the sentiment of those tweets? (calculation of sentiment shown later.)First, let’s clean the tweets. For this, we will create two functions, one for removing urls, mentions and hashtags (store them in a separate column) and the other for cleaning the remaining text (removing stop words, punctuations).I will use the cleaned_tweets column for the tweets with stripped content, stopwords and punctuation and tweet column with just the content removed, to calculate sentiment and subjectivity.Now let’s build a word cloud to get an idea of frequent phrases.More frequent words/phrases appear in larger font.Now let’s define a function to plot the top n occurrences of phrases in the given ngram range. For this, we will use the CountVectorizer function.Most of the work is done, now let’s plot the frequent phrases.Sleepy Joe Biden? Seriously?We use the tweet column to analyze the sentiment and subjectivity of the tweets. For this, we will use TextBlob.Given an input sentence, TextBlob outputs a tuple of two elements: (sentiment, subjectivity)Let’s look at the sentiment distribution of tweetsMost of the tweets might not be subjective. The tweets might be a fact, like bad news. Let’s find out the sentiment distribution of tweets which were subjective. For this, let’s filter out the tweets with subjectivity greater than 0.5 and plot the distribution.Looks like the proportion of negative sentiment increased when only subjective tweets were analyzed.Now let's look at the polarity of subjective tweets of the 20 most mentioned users.Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as ‘unsupervised’ machine learning because it doesn’t require a predefined list of tags or training data that’s been previously classified by humans.We will use the gensim LDA model for topic modelling.There are some clear topics like topic 5 during the early stages of impeachment trial, topic 8 containing phrases related to the China trade deal and topic 6 regarding his plans to build the wall.Let’s look at the topic distribution.Let’s look at the distribution of topic 5 and 6.These plots make sense since the tweets in topic 5 significantly increase during the month when the whistleblower complaint was released and topic 6 has more tweets during the first month of 2019 when Trump was planning to build the wall.You can find a more detailed analysis here.If you liked this article please do leave a clap. Thank you for reading!",python,https://towardsdatascience.com/analysis-on-tweets-using-python-and-twint-c7e6ebce8805?source=tag_archive---------15-----------------------
2: Python Smart Contract Dev Server,"Part 1 in this tutorial Series: Smart Contracting with PythonFull stack developers are used to developing both the backend and frontend in tandem. The problem with using blockchain as a backend is, they are generally complicated to setup and even more complicated to interact with. This slows down what should be a simple interaction between storage and user interaction.A dApp (Decentralized Application) is an application that interacts with a blockchain. The most common these days is a web app with these basic interactions:During development it would be nice to streamline this process so that I didn’t have to wait for the blockchain to process my transactions every time. I just want to get some state to render a route so I can mess with layout. I also want to design my user interactions with the blockchain across different routes without mocking data.In my last blog post I explored how to write and test smart contracts using a Python package called contracting.https://blog.usejournal.com/smart-contracting-with-python-2af233620dcaIn this post I’m going to setup a simple Python web server that will process mock transactions to my smart contracts. This will allow me to easily create a frontend for my smart contract using modern RESTful API patterns.Lamden Masternodes already serve up the public blockchain via an API. This is a good place to start if we want our contracting server to mimic the blockchain.Here is a link to the masternode web serverThe Masternode API has a bunch of routes relating to blocks, nonces and transactions hashes; but our contracting web server isn’t going to mint blocks or do consensus, so we won’t need any of those.We are only concerned with endpoints that let us view and change state, so I paired down the list to these five.submit transactionsMake sure our server is respondingView the names of all contracts on our blockchainView specific information about a smart contract on our blockchainGet the current state of a variableIf you have finished the previous tutorial just load it up and we will continue where you left off.If you are just starting here, then you will need to do the setup from the previous tutorial and then copy the files from the previous project.Next, we will need to install Sanic Server which will be our Python webserver.Create a new server directory and a file called contracting_server.py.Let’s start by copying the sanic quickstart code and making sure we can start a simple server.Paste this into contracting_server.py.Then from project directory runAnd you should get this output indicating your web server is active and listening on port 3737.Let’s also test that our API’s ping endpoint is functioning correctly. Open your internet browser and navigate to http://localhost:3737/ping. You will get a JSON status object indicating that the server is online!Perfect, we can now add some endpoints to return the information about our contracting environment.This endpoint will return all contracts currently submitted to the contracting client.By default, the contracting package only has 1 contract available (the submission contract). So we will need to add our contract for contracting to know about it. Let’s do that by adding the contracting client and calling the submit method like we did in our tests form the previous tutorial.Now when the contracting server is started it will add our contract by default.client.get_contracts() will return a list of all contracts that are loaded into the contracting client.Create an endpoint by adding the below code to serve out the result of calling client.get_contracts().Restart the web server by opening the terminal, stopping the server by pressing control + c and start it with python3 tests/contracting_server.py. From now on this tutorial will simply call this process “restarting the web server”.** If you get an error here, check that mongodb is install and startedBrowse to our new endpoint http://localhost:3737/contracts and you will now see our my_token smart contract listed on the network.We can use this endpoint to get the metadata about a contract. This can be useful for debugging our own smart contract or displaying dynamic information about a smart contract.We need the ast package to walk the code, so add that import at the top of contracting_server.pyNow add the code for this new route and restart your server.Browse to our new endpoint http://localhost:3737/contracts/my_token and you will see all the metadata about our smart contract.Let’s take a look!The first method “____” was our CONSTRUCTOR method called “seed”. Contracting renames it after submission so it cannot be called ever again.The second method is our transfer method and we can see it takes 2 arguments, AMOUNT and RECEIVERThen we see that our contract has no state variables, but it does have one state hash called “S”.It would be helpful to be able to query the state of the S hash to get our balances. We did this already in our tests file, so let’s make that dynamic and serve it out from a new endpoint so that it’s available to our webapp later.We need the encode function from contracting to properly format our values for transmission.Add that import statement at the top of contracting_server.pyNow add the code for this new route and restart your server.This endpoint is going to need some more information than the previous ones.To supply this information, we parameterize our URLBrowse to our new endpoint http://localhost:3737/contracts/my_token/S?key=me to get the balance for “me”.Changing to a key that does not exist. like “you”, will return a null value because it does not exist in state.Our API is shaping up pretty good so far. Now all we need to do is accept a transaction into our network.On a real blockchain a transaction result isn’t immediate as it needs to get in line to be processed by the nodes. When you submit a transaction the blockchain will reply with a receipt (called a transactions hash) which you can then use to lookup the success of your transaction later.Our contracting web server doesn’t need to do any of that. We don’t need blocks or consensus so we can simply write our value to state. This helps with rapid development.Add the code for this new route and restart your server.To hit this endpoint, we need to create an http POST request. The simplest way is to use a curl statement like the one below. With the following information needed to call our method:Of course you can use any program you choose to test the API (like Postman or Paw). Just make the body JSON and send the request to http://localhost:3737/ you should get back this response.If you try and send too many tokens you will get the assertion error back.Then we can use the variable endpoint we made earlier to check that our state has changedVoila!Here is the entire dev server we just created.Thank you for following along!The git repo for this part of the tutorial can be found here.In my next tutorial We will setup a frontend to handle sending tokens from person to person. This new dev server will assist us in the creation of the frontend.I’m available to chat on twitter, telegram or linkedIn",python,https://blog.lamden.io/python-smart-contract-dev-server-f81bd605a92c?source=tag_archive---------43-----------------------
Building an Automated Machine Learning Pipeline: Part Three,"Ceren IyimJun 1, 2020·12 min readIn this article series, we set our course to build a 9-step machine learning (ML) pipeline and automate it using Docker and Luigi — just one step left to that article 😉. We will assemble the steps eventually so that it can run on any production system without requiring specialized configurations and setups.We are following an intuitive ML pipeline to build wine rating predictor:The problem itself is a supervised regression problem because we are trying to determine points (target variable) using the defined feature set. Points is a number between 80 and 100 indicating the quality of the wine.Our raw data looked like this:We started by formulating the business problem and explaining our objective:The goal of the project is to build a wine rating predictor using a sample dataset to show good prediction is possible, perhaps as a proof of concept for a larger project.Through the first three steps of the pipeline covered in the first notebook, we cleaned, understand and formatted the dataset. Moreover, we added and transformed features which prepared the training and test datasets to be used in any ML model ultimately:In the second notebook, we selected the evaluation metric as the mean square error (MSE) which is an appropriate one for our problem. Then, we calculated the baseline (resulted in 9.01) using the mean square error computing logic in the Set Evaluation Metric & Establish Baseline.Then, we tested several different ML algorithms and decided to use random forest regressor form scikit-learn for building the wine rating predictor in the Select an ML Model based on the Evaluation Metric step. We performed hyperparameter tuning on the selected algorithm and improved our model performance further in the Perform Hyperparameter Tuning on the Selected Model step.We have significantly lowered the baseline MSE, from 9.01 to 4.99 which resulted in 45% improvement!In this article, we are going to complete the remaining last three steps: We are going to evaluate our model on the test set and interpret the predictions. We will be concluding the overall pipeline and reporting the highlights in the last step.The code behind this article can be found in this notebook. The complete project is available on GitHub:github.comP.S: I encourage you to read previous articles if you haven’t already before we start!The datasets that we are going to use is available in notebooks/transformed. The model that we saved in the previous notebook is available in the same directory as this notebook.We are going to need the following libraries of Python in this notebook:Let’s load the datasets and the model:Convert them to arrays using the same functions that we used in the second article:The training set​ will be used to map patterns between the features (X_array) and target (y_array) and to build the wine rating predictor. This process is called ​training the model​ or building the model​. The mapping will be done with the fit method called on our random forest model.Before evaluating the model, the actual target values (y_test_array) are held-out from the test set (X_test_array) to utilize as a comparison to the predicted values. Both will be used to generate points/ratings of wine using the patterns found during the model fitting.Feeding the test set into the model and generating predictions is called ​evaluating the model. When we call the predict method on our random forest model, the array of predictions are generated.Let’s see how our model performed on the test set (part of the training dataset that our model has never seen before):As data scientists, we are the interpreters of machine learning technologies. We should be able to explain how a solution works and what are the further improvement areas. This happens at the Interpret Model Predictions step.We are going to interpret the model predictions from below perspectives:We had defined how the random forest algorithm works in the previous notebook:Random forest regressor ​is an ensemble algorithm that builds multiple decision trees at once and trains them on various sub-samples and various subsets of the features of the dataset.Our random forest algorithm consists of 200 decision trees. It may not provide us much information if we plot all trees at once. However, visualizing a single decision tree is sufficient to understand how wine rating predictor generates predictions.Let’s visualize the first 3 nodes of a decision tree from our random forest regressor algorithm to understand further how our algorithm works:Looking at this picture, you can think of a decision tree as a series of yes/no questions asked and answered like a flowchart. This specific tree is built using 5645 data points out of a total of 8948 data points — a random subsample of the complete dataset.The root node is initiated by asking if the region_1 is less than 320.5. It can be easily realized that the initial MSE is the same as the baseline estimate of MSE. Predicted points is 88.425.The root node is split into two as true and false child nodes consisting of 5018 (88% of the initial subsample) and 627 data points (12% of the initial subsample) respectively.The false child node of the root node asks if the price is less than 30.5 and answers this question using the 12% of the initial subsample. The MSE improved slightly compared to the previous node, as an indicator of a more coherent node. Predicted points is 87.818.This process continues iteratively until a leaf node is reached. When a node’s MSE does not improve further or when the specified criteria for reaching a leaf node (having a minimum of 2 samples in a leaf node for our model) is met, the process stops.To understand how actual points and predicted points differ let’s visualize them side by side with a histogram:Recall that points show a normal distribution and 95% of the points lie in the 82.5–94.5 range. Our model successfully predicted the points within this range but failed to predict the points below 84 and points above 94 which are less-frequent values of the distribution. This is also the reason for the narrower distribution of the predictions.This is a further and interesting improvement area for our wine rating predictor, but we will not touch it for now.Another way to evaluate the model is to look at which features that the model considers most important. Following is the number for the relative contribution of each feature in determining a point:According to the model, the strongest predictor is price. It is followed by the region_1, year, variety and the taster_name for predicting the points of wine.These are in line with our preliminary observations developed during the Exploratory Data Analysis step.Price is an indicator of wine’s quality and age. Therefore, we expect it to be an essential feature of the model.Top 5 features form 80% of the total importances. Isn’t it another beautiful example of Pareto Princinciple?From the features extracted, year is the most important among them and whether a wine is sparkling or not is the least important predictor of points.Another significant observation is the note and taste-related features extracted from variety and description became moderately-important features for our predictor. A comprehensive use of NLP and sentiment analysis may help to find more valuable features from the description and variety features.So far, we have interpreted the predictions of our wine rating predictor from an aggregate perspective. Now, let’s examine an individual prediction from our model.We are going to get some help from an explainer library called LIME (locally interpretable model-agnostic explanations) to observe a prediction. This library creates an approximate local model in a place where data points are condensed and uses this model to explain individual predictions of any ML algorithm.We will observe the most incorrect prediction detected from the maximum of the residuals. Then, we are going to create LimeTabularExplainer object.Following code block will plot how the wine rating predictor concluded a wrong prediction. It predicted 90.3 points whereas the actual point was 82. This is the most incorrect prediction of the model:This plot shows the contribution of each feature to the wrong prediction example. Positive contributions are colored in green and negative contributions are colored in red.As the price is the strongest predictor and this example has a price above 40, those significantly increased the model’s prediction of points. On the other hand, country, is_red and is_white balanced the predictions by decreasing it. This is a great example of how machine learning algorithms may also fall into the common pitfalls as we humans do. Don’t you ever buy an expensive wine to guarantee the high quality 🙂As we are the interpreters of machine learning technologies, it is important to tailor our message and deliverables according to our audience. Especially, if you are presenting your results to your manager or client you must be very concise and relevant. You might expect that they will be more interested in your reasoning and outcome, so focus on them while presenting your findings 😉If you remember from the first article, I built this project as a take-home assignment of the ML Engineer interview process at Data Revenue. The objective of the assignment is to decide whether to build a full-production solution by looking at the results of the wine rating predictor. So, I prepared a report to present my decision with the most crucial points of the 9-step ML pipeline and draw conclusions.I proposed building the full-production solution counting onGiven the sample dataset, determined set of features and the tuned random forest regressor, the answer is ​yes, a​ ​​full-production solution is applicable with those.The model significantly lowered (45%) the baseline MSE and resulted in 4.93 MSE on the test set. This serves as evidence to build the full-production solution... and highlighted the most two most important findings:- The wine rating predictor can infer the points of wine with a reasonable variance (MSE) of 4.9.- The most useful predictors of points are determined as price, region_1, year, variety and the taster_name... and I pointed out the further improvement areas:Extending prediction range: so that the less-frequent target values can be predicted by the model. The current sample dataset is dominated by wines from the most dominant countries (US, Italy, France, Spain), feeding more data from non-present countries might help the model to learn better the determiners of the less-frequent points.Using NLP to extract stronger predictors: currently, the strongest predictor is “price”. Extracting more features from the description related to wine taste and notes can help create strong predictors like price, decreasing prediction error of the model.You can find the complete report here.It ended up as a thorough (a habit I carry from consultancy) but succinct report. Besides, I kept the code aside because the report was prepared for a hypothetical client whose statistics and ML knowledge is limited.You can prepare a similar summary report (I prepared in a PDF format) for your own projects or learning purpose by converting your Jupyter notebooks into documents. You can use various download options available in the File > Download As. HTML or LaTeX formats are pretty straightforward to use. (if you have the LaTeX Studio installed in your machine, then you can convert it to pdf format later on). You can read more on that here.We should always keep in mind that the “science” in data science serves a purpose, ultimately for a non-scientific purpose. Our work is mostly used to support decision-making processes in businesses or solving real-world problems. Presenting the results (verbally or written) and adjusting the language according to your audience will be a valuable skill you have.We came a long way from structuring the raw data to making the meaning out of them. We started by cleaning and formatting the data, then we trained and tuned the selected model (random forest regressor). We concluded the 9-step pipeline by evaluating the model and interpreting its results. Finally, we have highlighted the most important outcomes of the project in a report.We have followed these steps sequentially in the article series. However, I went back and forth between these steps 20% of the time when I was working on this project. All in all, I believe building a machine learning solution is an iterative and experimental process, so never hesitate to improve and revisit the previous steps!I want to thank Will Koehrsen here. He is one of the inspirers for me to write about my data science/machine learning projects.I first learned this pipeline from one of his articles and adjusted it for my own objective and problem. Now, I am using this pipeline as a guide when building any kind of machine learning/data science project. It helps me to organize my thought process and see how individual elements of machine learning is connected!We have one last article to go which will automate this pipeline with Docker and Luigi. Stay tuned in the next week!towardsdatascience.comThanks for reading 🙂 Please feel free to use this pipeline, the code and the repository for your own projects.For comments or constructive feedback, you can reach out to me on responses, Twitter or Linkedin!Stay safe and healthy until then 👋",python,https://towardsdatascience.com/building-an-automated-machine-learning-pipeline-a74acda76b98?source=tag_archive---------40-----------------------
Predicting Hourly Energy consumption of San Diego — I,"Prathamesh PawarJun 1, 2020·12 min readPart 1 of this post will deal with the basics of energy (electricity) consumption, how to import, resample, and merge datasets gathered from different sources, EDA, and draw some basic inferences from San Diego’s energy consumption data. If you are only interested in the modeling part then please feel free to jump directly to Part 2 of this post.towardsdatascience.comIn these posts (Part 1 and Part 2) we will cover both short term (an hour ahead and week ahead) and long term (months ahead) forecasting.Links to Jupyter notebooks on nbviewer: 1. Data import and EDA (covered in this post)2. Building ML models and Predictions (covered in Part 2 post)Github link for the entire project: Hourly_Energy_Consumption_Prediction.Electrical utilities need to diligently plan ahead of time the allocation of generating units in their power plants to match their regional energy demand because if the demand is higher than the generation, it can cause several blackouts resulting in a huge loss to the economy; on the other hand, if the generation is higher than the demand the extra electricity will be wasted and it can also create an unnecessary load on the transmission lines.So, it is very important for the utilities to have a forecast of the energy consumption to be able to allocate appropriate resources to meet their demand. A year, month, or day-ahead forecast can help the utilities plan for a larger time scale, but for smoother daily operations an hourly (or even better) forecast can prove very useful. For example, if the plant operators get a high energy forecast for the next hour, they can ramp up the energy supply by switching on more power plants.In this project we will analyze past 5 years of hourly energy consumption data of SDGE utility to find trends in energy consumption around hour of the day, day of the week, season of the year, etc., and also to check if factors like outside temperature and solar installations in the region affect the energy consumption.The developed prediction model can be utilized by the electrical utilities to effectively plan their energy generation operations and balance the demand with appropriate supply.Enough with the terminology, let’s put the rest of ‘our energy’ into building some time series models.Importing data:Similarly, we can check the 2014–2017 data as well. Note: At the time of the project, data was available only till July 2019, but if you check now (May 2020), data is available on the website for all the months of 2019. The analysis shown in this post considers only the full years’ data from 2014–2018 but the remaining months of 2019 can be added easily using the same code.The above code will give us the past 5 years of hourly energy consumption values for 4 of the CAISO utilities, namely PGE, SCE, SDGE, and VEA. It will also give us the CAISO total. Later, we will extract only the SDGE data for this project.Below are some of the operations that were carried out on the above hourly1418 dataframe:The original energy time series for the San Diego region can be observed below.The above hourly1418 dataframe was saved as ‘hourly1418CA.csv’ file for future use. We will continue using only the SDGE energy consumption values.We can see how energy consumption has higher peaks during summer and has strong multi-seasonality patterns. What that means is, the energy consumption has daily, weekly as well as seasonal patterns. We will later also analyze the trend of the data.Other date-time parameters were added to the dataframe as shown below:non-working: the day is either a federal holiday or a weekendseason: November-May: winter and June-October: summer (as per SDGE link)To see the impact of weather (specifically the temperature) on energy consumption, we’ll import the hourly temperature data from gov.noaa.ncdc. There are two or more weather stations in San Diego at which the weather data has been recorded. Since weather data at airports is generally more accurate and not biased by nearby buildings and industries we’ll use the San Diego International airport data for this project. (from this link, select San Diego County > San Diego International Airport and Add to cart, then you can download the required data).The downloaded weather data has many columns but we will focus only on the ‘HourlyDryBulbTemperature’ which is the outside temperature in deg F.The weather data is captured every 7 minutes and the energy consumption data is hourly data, so I had to resample the weather data in hourly intervals by calculating the average temperature for each hour before merging it with energy data.It was observed that the energy consumption of San Diego has overall decreased over the years, and the decrease was more prominent over the daylight hours. So, we can test the theory that maybe solar capacity installed in the San Diego region has increased over the years, and thus more and more customers are using less energy when the sun is out, causing a total reduction in energy consumption. We can test this theory by importing the solar installations data in SDGE territory.The solar installations data was imported from California DistributedGeneration Statistics site (direct data link). This dataset contains a lot of parameters but we will focus on,- ‘approval date’: which represents the date when the system was connected to the grid became active and,- ‘System Size AC’: which represents the total kW power in AC of the solar panels installed at a site.To get more details about the data, you can check this workbook in my repo.Basically, the idea is, if the customers have more PV at their end, they will rely less on the grid for the power supply (at least when the sun is out) thus decreasing the overall energy demand. And so we are considering PV installed at the customer sites (like residential housings, commercial buildings, etc.).Let’s merge the PV dataset to our previously merged weather+energy ‘wehSDGE1418’ dataframe.Plotting some graphs to get insights from the data. Some of the questions that can be answered are:2. Hourly vs weekday energy consumption to see how the consumption varies across any particular weekFrom SDGE’s website: “If customers can shift some of their energy use to lower-cost time periods outside 4 p.m. to 9 p.m., they can lower their electricity bills and make better use of cleaner, renewable energy sources, like wind and solar, when they are more available.”3. Visualizing the distribution of energy values for different yearsThis peak shift to the left can be assumed to be caused by increasing renewable energy installations at customer sites (called behind-the-meter) and increasing participation of customers in demand response programs resulting in lower peak demand on the grid.3.1. Plotting the same distribution but for 12 pm only4. Exploring energy and weather data togetherpearson correlation coefficient and pvalue for winter: 0.32pearson correlation coefficient and pvalue for summer: 0.65As observed in section 3.1 above, the energy consumption for daylight hours seems to have decreased from 2014 to 2018. Now, this can be due to stricter energy efficiency measures being mandated, or increase in incentives for different categories of ratepayers (residential, commercial, agriculture, etc.) to install self-generation resources like solar, wind, etc. and/or energy storage on their premises to avoid the higher energy demand. It was observed that the energy consumption decrease over the years is more dominant over the daylight hours, so we can test the theory that maybe solar capacity installed in the utility region has increased over the years, which it has as we will see below.Also, “San Diego, another of the top-ranking US cities for sunshine, gets more sun in winter than in summer”. (Reference link)We have seen the effects of temperature and PV installation capacity on the energy demand of San Diego city. The energy demand is significantly higher in the summer months than in winter, due to higher temperatures. And the installed solar panels capacity also seems to have a slight (reducing) effect on energy consumption over the years. And, the energy consumption has multiple seasonal patterns.In Part 2 of this post, we will start from building simple short term forecasting models using traditional forecasting algorithms like SARIMAX and also linear regression. We will then also try our hands at long term forecasting using non-linear approaches like Random forest, XGBoost as well as an ensemble model (XGBoost + FB Prophet).Thanks for reading. See you in Part 2.Disclaimer: I chose San Diego out of all the utilities because, i. it serves a smaller region than other utilities so we can use temperature data from a single source without expecting much variation (actually San Diego has a wide variation in weather patterns, but we will neglect that for this project) and ii. I am an alumnus of UCSD.",python,https://towardsdatascience.com/part-1-time-series-analysis-predicting-hourly-energy-consumption-of-san-diego-short-term-long-3a1dd1a589c9?source=tag_archive---------21-----------------------
Python named tuple — boost your code readability,"Code readability has become an integral part of the software development and writing good readable, reusable and consistent code has been considered as an art of programming , every programmer not only should master, but also follow it religiously.Why it is so important to write codes which is not only readable by computers , but also by humans. Have you ever imagined how many times a computer program might been read in comparison with the number of times it has been written or modified.Its an obvious fact that a program would be read more than it has been written and that is the very reason, code readability has been the heart of the design for many of the upcoming programming languages and Python due to its versatility, better readability and natural language like syntax has been one of the primary reason to flourish among the data science community.Good readable code will not only be easy to understand , but also will be fairly easier to maintain for this very reason many software companies has nurtured community cultures to adapt coding standards, reference architecture and best practice to have a high degree of consistency across organizations.With enough of detailing the need for good readability code and making everyone wonder what is this article is all about. we will be specifically reviewing about python named tuple objects and how we can use it to boost our code readability.The inspiration for this article is from a colleague of mine who demonstrated how easy to improve readability of code just by using a named tuple in place of a tuple object.Python tuple is an order list of immutable objects that come in handy when we want to create a read only list which shouldn't be modified during the execution of the program.Lets consider a simple example of a helper function which takes the following request parameter as input arguments and depending on the HTTP Method appropriate actions will be performed, in our case we will be printing the Request URL and the respective HTTP method.In the below example attribute_names is the tuple which contains there required request parameters for api_helper function.There can be other way to pass the request parameters but we choose to pass it as a tuple for its immutable feature.The Output of the above code is illustrated below.If you carefully examine the code, the readability has not been that effective due to use the indexes params[0], params[2] to access the tuple attribute_names. Over a period of time it would be a nightmare to developer in identifying what the params[0] or [2] means.We could potentially uses a dict() to define the parameters, but we could really lose the immutability feature.That leaves us to the topic of this article, Named tuple which are nothing but a factory function for tuple and can be referenced similar to that of a dict(), there by providing immutability feature and enhanced readability by assigning name to each position of the tuple and by enabling access to attributes using their assigned name.The below examples provides implementation of the same helper function using a named tuple .We have defined an attributes as a named tuple and attributes are assigned the following names.And we can reference the tuple object using their name as shown belowAnd we could see for our self that by adding names to the tuples and referencing these names params.method , params.url to access the request parameters has greatly enhances readability. The names acts as a self documentation for the code providing greater readability to humans. The output of the above code is provided below.And to summarize, we could see the greater good in using named tuples, that can be implemented quite easily and could improve the readability of the code.The light weight nature of named tuple ensure that the memory footprint is no more that being used by the regular tuple object.With that being said, named tuple cannot be used as a replacement for classic tuples. Sometime using a tuple make more sense without complicating things. We will leave that better judgment to the all the great developer out there.Until next time … Happy reading..https://docs.python.org/3/library/collections.html#collections.namedtuplehttps://stackoverflow.com/questions/2970608/what-are-named-tuples-in-pythonhttps://gist.github.com/parithy86/15141c2e3f3caa56f91077abd88d1698Did you know that we have launched a YouTube channel? Every video we make will aim to teach you something new. Check us out by clicking here, and be sure to subscribe to the channel 😎",python,https://python.plainenglish.io/python-named-tuple-boost-your-code-readability-70b2039d393?source=tag_archive---------41-----------------------
50 Python Interview Questions and Answers,"This article will cover interview questions on the Python programming language. While the list isn’t exhaustive, it should give you a good idea of the types of questions you can expect.A tuple should be used whenever the user is aware of what is inserted in the tuple. Suppose a college stores the information of its students in a data structure — in order for this information to remain immutable it should be stored in a tuple.Since lists provide users with easier accessibility, they should be used whenever similar types of objects need to be stored. For instance, if a grocery needs to store all the dairy products in one field, it should use a list.Output: 50, ‘Eighty’, 9All we have to do is create a tuple with three elements. The first element of the tuple is the first element of the list, which can be found using my_list[0].The second element of the tuple is the last element in the list. my_list[len(my_list) - 1] will give us this element. We could also have used the pop() method, but that would alter the list.When programming, there will be instances when you will need to convert existing lists to arrays in order to perform certain operations on them (arrays enable mathematical operations to be performed on them in ways that lists do not).Here we’ll be using numpy.array(). This function of the numpy library takes a list as an argument and returns an array that contains all the elements of the list. See the example below:Output: [ 2 4 6 8 10]In Python, the term monkey patch refers to dynamic modifications of a class or module at run-time.A lambda function is a small anonymous function, which returns an object. The object returned by lambda is usually assigned to a variable or used as a part of other bigger functions.Instead of the conventional def keyword used for creating functions, a lambda function is defined by using the lambda keyword. The structure of lambda can be seen below:A lambda is much more readable than a full function since it can be written in-line. So it’s a good practice to use lambdas when the function expression is small.The beauty of lambda functions is in the fact that they return function objects. This makes them helpful when used with functions like map or filter, which require function objects as arguments.Lambdas aren’t useful when the expression exceeds a single line.Pickle module accepts any Python object and converts it into a string representation and dumps it into a file by using the dump function. This process is called pickling, while the process of retrieving original Python objects from the stored string representation is called unpickling.Inheritance allows one class to gain all the members (say attributes and methods) of another class. Inheritance provides code reusability, makes it easier to create and maintain an application. The class from which we’re inheriting is called super-class and the class that’s inherited is called a derived/child class.There are different types of inheritance supported by Python:Polymorphism means the ability to take multiple forms. For instance, if the parent class has a method named ABC, the child class can also have a method with the same name ABC having its own parameters and variables. Python allows for polymorphism.For the most part, xrange and range have exactly the same functionality. They both generate a list of integers for you to use. The only difference is that range returns a Python list object and xrange returns an xrange object.This means that xrange doesn’t actually generate a static list at run-time, as range does. It creates the values as you need them with a special technique called yielding. This technique is used with a type of object known as generators.Django is a Python web framework that offers an open-source, high-level framework that “encourages rapid development and clean, pragmatic design.” It’s fast, secure, and scalable. Django offers strong community support and detailed documentation.The framework is an inclusive package, in which you get an admin panel, database interfaces, and directory structure right when you create the app. Furthermore, it includes many features, so you don’t have to add separate libraries and dependencies. Features it offers include user authentication, templating engine, routing, database schema migration, and much more.The Django framework, in which you can work with MVPs to larger companies, is incredibly flexible. For some perspective, some of the largest companies that use Django are Instagram, Dropbox, Pinterest, and Spotify.Flask is considered a microframework, which is a minimalistic web framework. It’s not “batteries-included,” meaning that it lacks a lot of features and functionality that full-stack frameworks like Django offer, such as a web template engine, account authorization, and authentication.Flask is minimalistic and lightweight, meaning that you add extensions and libraries that you need as you code without automatically being provided with it by the framework. The philosophy behind Flask is that it gives only the components you need to build an app so that you have the flexibility and control. In other words, it’s un-opinionated. Some features it offers are a build-int dev server, Restful request dispatching, HTTP request handling, and much more.It’s an environment variable, which is used when a module is imported. Whenever a module is imported, PYTHONPATH is also looked up to check for the presence of the imported modules in various directories. The interpreter uses it to determine which module to load.PEP stands for Python Enhancement Proposal. It is a set of rules that specify how to format Python code for maximum readability.A decorator is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate.__init__ is a method or constructor in Python. This method is automatically called to allocate memory when a new object/instance of a class is created. All classes have the __init__ method.The ternary operator is a way of writing conditional statements in Python. As the name ternary suggests, this Python operator consists of three operands.Note: The ternary operator can be thought of as a simplified, one-line version of the if-else statement to test a condition.The three operands in a ternary operator include:The variable var on the left-hand side of the = (assignment) operator will be assigned:Output: Even EvenThe above code is using the ternary operator to find if a number is even or odd.Global variables: Variables declared outside a function or in global space are called global variables. These variables can be accessed by any function in the program.Local variables: Any variable declared inside a function is known as a local variable. This variable is present in the local space and not in the global space.The @property is a decorator. In Python, decorators enable users to use the class in the same way (irrespective of the changes made to its attributes or methods). The @property decorator allows a function to be accessed like an attribute.An exception is an error that occurs while the program is executing. When this error occurs, the program will stop and generate an exception which then gets handled in order to prevent the program from crashing.The exceptions generated by a program are caught in the try block and handled in the except block.Python 2 is entrenched in the software landscape to the point that co-dependency between several pieces of software makes it almost impossible to make the shift.The join method in Python takes elements of an iterable data structure and connects them together using a particular string connector value.The join method in Python is a string method, which connects elements of a string iterable structure, which also contains strings or characters (array, list, etc.) by using a particular string as the connector.This will join the elements in an array using an empty string between each element.Output:HELLODictionary comprehension is one way to create a dictionary in Python. It creates a dictionary by merging two sets of data which are in the form of either lists or arrays.The data of one of the two lists/arrays will act as the keys of the dictionary while the data of the second list/array will act as the values. Each key acts as a unique identifier for each value, hence the size of both lists/arrays should be the same.Here we’ll look at simple merging. Simple merging is merging or combining two lists without any restrictions. In other words, it’s the unconditional merging.The general syntax is as follows:The following example runs for the college’s database and uses simple merging. Imagine a college database storing lots of data — for example, students' addresses, grades, sections, fees, roll numbers, and so on. Now we need to identify each student uniquely and create a new dictionary that stores all students only. Our decision simply depends on two questions:Here we will choose roll numbers as key and names as the value because roll numbers are unique and names could be repetitive. So, Alex’s roll number is 122 so the tuple will look like 122: Alex. This will be better explained once you try the code attached below.Output: {456: ‘don’, 233: ‘bob’, 122: ‘alex’, 353: ‘can’}A deep copy refers to cloning an object. When we use the = operator, we’re not cloning the object — instead, we reference our variable to the same object (a.k.a. shallow copy).This means that changing one variable’s value affects the other variable’s value because they are referring (or pointing) to the same object. This difference between a shallow and a deep copy is only applicable to objects that contain other objects, like lists and instances of a class.To make a deep copy (or clone) of an object, we import the built-in copy module in Python. This module has the deepcopy() method which simplifies our task.This function takes the object we want to clone as its only argument and returns the clone.Output:Shallow copy: [5, 15, 3]Deep copy: [10, 20, 30]It’s a safe practice to check whether or not the key exists in the dictionary prior to extracting the value of that key. For that purpose, Python offers two built-in functions:The has_key method returns true if a given key is available in the dictionary, otherwise, it returns false.Output: Key existsThis approach uses the if-in statement to check whether or not a given key exists in the dictionary.Output: Key existsConsider this computationally expensive code:Memoization can be achieved through Python decorators. Here’s the full implementation.Output: 4.9035000301955733e-051.374000021314714e-061.2790005712304264e-06We can sort this type of data by either the key or the value and this is done by using the sorted() function.First, we need to know how to retrieve data from a dictionary to be passed on to this function. There are two basic ways to get data from a dictionary:Dictionary.keys(): Returns only the keys in an arbitrary order.Dictionary.values(): Returns a list of values.Dictionary.items(): Returns all of the data as a list of key-value pairs.This method takes one mandatory and two optional arguments:keys()values()items()any() is a function that takes in an iterable (such as a list, tuple, set, etc.) and returns True if any of the elements evaluate to True, but it returns False if all elements evaluate to False.Passing an iterable to any() to check if any of the elements are Truecan be done like this:Output:TrueFalseThe first print statement prints True because one of the elements in one_truth is True.On the other hand, the second print statement prints False because none of the elements are True; i.e. all elements are False.Use any() when you need to check a long series of or conditions.all() is another Python function that takes in an iterable and returns True if all of the elements evaluate to True, but returns False if otherwise.Similar to any(), all() takes in a list, tuple, set, or any iterable, ​like so:Output:True FalseFalseThe first function call returned True because all_true was filled with truthy values. Passing one_true to all() returned False because the list contained one or more falsy values. Finally, passing all_false to all() returns False because it also contained one or more falsy values.Use all() when you need to check a long series of andconditions.The Python docstrings provide a suitable way of associating documentation with:It’s a specified document for the written code. Unlike conventional code comments, the doctoring should describe what a function does, not how it works.The docstring can be accessed usingOutput:The value is 9.The product of x and y is 8.The function Examplefunc takes a variable str as a parameter and prints this value. Since it only prints the value there’s no need for a return command.The function Multiply takes two parameters, x and y, as parameters. It then computes the product and uses the return statement to return the answer.An iterator in Python serves as a holder for objects so that they can be iterated over. A generator facilitates the creation of a custom iterator.Apart from the obvious syntactic differences, there are some other noteworthy differences.GeneratorIteratorImplemented is using a function. It’s implemented using a class. It uses the yield keyword. Usage results in a concise code. Usage results in a relatively less concise code. All the local variables before the yield statements are stored. No local variables are used.Output:012345Output:012345The Python dictionary, dict, contains words and meanings as well as key-value pairs of any data type. The defaultdict is another subdivision of the built-in dict class.The defaultdict is a subdivision of the dict class. Its importance lies in the fact that it allows each new key to be given a default value based on the type of dictionary being created.A defaultdict can be created by giving its declaration, an argument that can have three values: list, set or int. The dictionary is created according to the specified data type. When any key that doesn’t exist in the defaultdict is added or accessed, it’s assigned a default value rather than giving a KeyError.The code snippet below shows a simple dictionary giving an error when a key that does not exist in the dict is accessed:Let’s introduce a defaultdict and see what happens.Output: 0In this case, we have passed int as the datatype to the defaultdict. Hence, any key that does not already exist in defaultdict_demo will be assigned a value of 0, unless a value is defined for it.Note: You can also have set or list as the parametersA Python module is a Python file containing a set of functions and variables to be used in an application. The variables can be of any type — arrays, dictionaries, objects, etc.Modules can be either built-in or user-defined.There are a couple of benefits of creating and using a module in Python:In this section, we’ll take a look at common coding interview questions that pertain to lists, linked lists, graphs, trees, multithreading/concurrency, and more.Let’s reverse the string Python using the slicing method. To reverse a string, we simply create a slice that starts with the length of the string and ends at index 0.To reverse a string using slicing, write the following:To reverse a string without specifying the length of the string, write:The slice statement means start at string length, end at position 0, and move with the step -1 (or one step backward).Output: nohtyPThis is just one method to reverse a string in Python. Two other notable methods are Loop and Use Join.There are a couple ways to do this. In this article, we’ll look at the find method.The find method checks if the string contains a substring. If it does, the method returns the starting index of a substring within the string, otherwise, it returns -1.The general syntax is as follows:Output:Check if Python Programming contains Programming: 7Check if Python Programming contains Language: -1The other two notable methods for checking if a string contains another string are to use in operator or use the count method.Consider the​ graph implemented in the code below:Output: A B C D E FLines 3–10: The illustrated graph is represented using an adjacency list. An easy way to do this in Python is with a dictionary data structure, where each vertex has a stored list of its adjacent nodes.Line 12: visited is a list that is used to keep track of visited nodes.Line 13: queue is a list that is used to keep track of nodes currently in the queue.Line 29: The arguments of the bfs function are the visited list, the graph in the form of a dictionary, and the starting node A.Lines 15–26: bfs follows the algorithm described above:Since all of ​the nodes and vertices are visited, the time complexity for BFS on a graph is O(V + E), where V is the number of vertices and E is the number of edges.Consider this graph, implemented in the code below:Output: A B D E F CLines 2–9: The illustrated graph is represented using an adjacency list — an easy way to do it in Python is to use a dictionary data structure. Each vertex has a list of its adjacent nodes stored.Line 11: visited is a set that is used to keep track of visited nodes.Line 21: The dfs function is called and is passed the visited set, the graph in the form of a dictionary and A, which is the starting node.Lines 13–18: dfs follows the algorithm described above:Since all the nodes and vertices are visited, the average time complexity for DFS on a graph is O(V + E), where V is the number of vertices and E is the number of edges. In the case of DFS on a tree, the time complexity is O(V), where V is the number of nodes.In Python, you can implement wildcards using the regex (regular expressions) library.The dot . character is used in place of the question mark ? symbol. So,​ to search for all words matching the color pattern, the code would look something like this:Output: colorHere’s the code for merge sort in Python:Output: [17, 20, 26, 31, 44, 54, 55, 77, 93]This is the recursive approach for implementing merge sort. These are the steps that are taken:The algorithm works in O(n.logn). This is because the list is being split in log(n) calls and the merging process takes linear time in each call.Basic algorithm:The implementation:OutputThe shortest distance of a from the source vertex a is: 0The shortest distance of b from the source vertex a is: 3The shortest distance of c from the source vertex a is: 3.5The shortest distance of d from the source vertex a is: 4.5Output: [-2, -1, 0, 4, 5, 6, 7]This is a more intuitive way to solve this problem.The time complexity for this algorithm is O(n+m)O(n+m) where nn and mm are the lengths of the lists. This is because both lists are iterated over at least once.Note that this problem can also be solved by merging in place.Output: None[1,4]You can solve this problem by first sorting the list. Then, for each element in the list, use a binary search to look for the difference between that element and the intended sum. In other words, if the intended sum is k and the first element of the sorted list is a0, we will do a binary search for a0. The search is repeated until one is found. You can implement the binarySearch() function however you like, recursively or iteratively.Since most optimal comparison-based sorting functions take O(nlogn), let’s assume that the Python .sort() function takes the same. Moreover, since binary search takes O(logn) time for finding a single element, therefore a binary search for all n elements will take O(nlogn) time.Here you can use a Python dictionary to keep count of repetitions.Sample input:Output: 2The keys in the counts dictionary are the elements of the given list and the values are the number of times each element appears in the list. We return the element that appears at most once in the list, on line 23. We need to maintain the order of update for each key in a tuple value.Since the list is only iterated over only twice and the countsdictionary is initialized with linear time complexity, therefore the time complexity of this solution is linear, i.e. O(n).The code solution to this problem involves multiple .py files To see the code in action, go to the original post.Here you can use two pointers which will work simultaneously:With this algorithm, you can make the process faster because the calculation of the length and the traversal until the middle are happening side-by-side.You’re traversing the linked list at twice the speed, so it’s certainly faster. However, the bottleneck complexity is still O(n).The code solution to this problem involves multiple .py files. To see the code solution in action, go to the original post.2. Our function reverseK(queue, k) takes queue as an input parameter. k represents the number of elements we want to reverse. The available queue functions are:3. Moving on to the actual logic — dequeue the first kelements from the front of the queue and push them in the stack we created earlier using stack.push(queue.dequeue())in line 8.4. Once all the k values have been pushed to the stack, start popping them, and enqueueing them to the back of the queue sequentially. We will do this using queue.enqueue(stack.pop()) in line 12. At the end of this step, we’ll be left with an empty stack and the k reversed elements will be appended to the back of the queue.5. We move these reversed elements to the front of the queue. To do this, we used queue.enqueue(queue.dequeue()) in line 16. Each element is first dequeued from the back.Here you can use recursion to find the heights of the left and right sub-trees. The code solution to this problem involves multiple .py files. To see the code solution in action, visit the original post.Here, we return -1 if the given node is None. Then, we call the findHeight() function on the left and right subtrees and return the one that has a greater value plus 1. We will not return 0 if the given node is None as the leaf node will have a height of 0.The time complexity of the code is O(n)O(n) as all the nodes of the entire tree have to be traversed.Here we’ll minHeapify all parent nodes:Output: [-2, 1, 5, 9, 4, 6, 7]Remember, we can consider the given maxHeap to be a regular list of elements and reorder it so that it represents a minHeap accurately — which is exactyl what we do in this solution. The convertMax()function restores the heap property on all the nodes from the lowest parent node by calling the minHeapify() function on each.The minHeapify() function is called for half of the nodes in the heap. The minHeapify() function takes O(log(n)) time and its called on n/2 nodes so this solution takes O(nlog(n)) time.The code solution to this problem involves multiple .py files. To see the code solution in action, visit the original post.Iterate over the whole linked list and add each visited node to a visited_nodes set. At every node, we check whether it has been visited or not. By principle, if a node is revisited, a cycle exists!We iterate the list once. On average, lookup in a set takes O(1) time. Hence, the average runtime of this algorithm is O(n). However, in the worst case, lookup can increase up to O(n), which would cause the algorithm to work in O(n²).Your learning and preparation have only just begun. Take your confidence to a whole new level by practicing the most frequently asked questions in a Python interview. The best way to prepare is to study the overarching patterns of coding interviews. The goal isn’t just to memorize these problems but to understand the underlying patterns that drive them. That way, you’re empowered to tackle any question that comes your way. Some of the main patterns are:",python,https://betterprogramming.pub/50-python-interview-questions-and-answers-f8e80d031bd3?source=tag_archive---------9-----------------------
Predicting Inpatient Length of Stay at Hospitals Using Python + Big Data,"Vishal TienJun 1, 2020·19 min readPatient length of stay is a critical indicator of the efficiency of hospital management. Hospitals have limited resources, requiring efficient use of beds and clinician time. With the recent onset of the COVID-19 pandemic, this notion has been exemplified. Now more than ever, we can see that it is in the best interest of patients, hospitals, and public health to limit hospital stays to no longer than necessary and to have an idea of how long a given inpatient may need to stay.The ability to predict how long a patient will stay, only with information available as soon as they enter the hospital and are diagnosed, can therefore have many positive effects for a hospital and its efficiency. A model that can predict patient length of stay could allow hospitals to better analyze the factors that influence length of stay the most. Such analysis could pave the path for reductions in the length of inpatient stay, which could, in turn, have the effect of decreased risk of infection and medication side effects, improvement in the quality of treatment, and increased hospital profit with more efficient bed management. Furthermore, predicting patient’s length of stay also greatly benefits the patients and patient’s families as they can have an idea of how long they can expect to stay upon being admitted. The goal of this project is to create a model that can predict length of stay for patients upon admission to a hospital, and the steps to accomplish such a task are detailed here.The entire code used throughout this analysis can be found here.To conduct this analysis, I used the publicly available “NY Hospital Inpatient Discharges in 2015” dataset, which can be found on the New York State Government health data website. This dataset contains 2.3+ million rows of patient data, including information such as patient demographics, diagnoses, treatments, services, costs, and charges. Patient data has been de-identified according to HIPAA regulations. To analyze a dataset of this size, I utilized various big data analytic tools within a Python interface such as Spark, AWS clusters, SQL query optimization, and dimensionality reduction techniques. However, the bulk of the code shown in this post uses Pandas and scikit learn. Let’s first take a look at all the features and associated datatypes in the dataset.Next, let’s take a look at how our numerical features are correlated with each other. We can achieve this through the use of a correlation heat map.From this initial look at a correlation map of our data, it is evident that a couple of features are strongly correlated with each other, and perhaps more importantly, with length of stay. Although the correlation matrix does not make sense to look at for all of our columns (such as the license numbers), we can see from it that APR Severity of Illness Code has a strong positive correlation with length of stay, along with total charges and total costs. CCS Diagnosis Code also seems to have a slight positive correlation with length of stay. We can see other positive correlations in the dataset between features such as CCS Diagnosis codes and APR DRG codes. This plot gives us an idea about which features might be especially important to look at to predict length of stay, in addition to letting us know if there is multicollinearity present in the data.Next, let’s explore and visualize underlying relationships in the data. For the following analysis, length of stay is kept as the primary variable along the y-axis of the plots I create since it is the predictor variable for this project. First, the univariate distribution of length of stay is visualized.As we can see, length of stay values range from 1 to 120+ days (120 has been aggregated to include all lengths of stay 120 days or longer). In addition, the distribution is very skewed, with most of the patients in the data having lengths of stay between ~1 to 5 days. This is an important aspect of the data to keep in mind. We’ll put a pin in this for now and revisit it later. Next, let’s take a look at how different age groups vary in their length of stay distributions.Does length of stay vary with age of the patient?From this plot, the trend of longer lengths of stay distributions on average as we go up in age groups is easily apparent.Which diagnoses have the longest average length of stay?On average, patients with diagnoses related to birth complications have the longest length of stay on average, followed by those with respiratory diseases. Let’s also look at how some features, which we might not expect to have much influence, vary with length of stay.Does length of stay vary with race?We can see that as one might suspects, a patient’s race does not result in largely different distributions of length of stay. However, it is interesting to note that Black/African Americans have the largest spread in length of stay values by ~2 days from the group with the next largest distribution (white).How does length of stay vary with patient payment types?From this plot, it is evident that different health insurance forms of payment tend to have different length of stay distributions. Specifically, we can see that Medicare patients, for example, tend to have some of the longest lengths of stay on average. However, health insurance programs are strongly correlated with other factors, such as income and age. Thus, it may be that Medicare patients tend to have longer lengths of stay because they also are in an older age bracket. Let’s dig deeper into this idea:What is the age distribution of patients with Medicare as their primary payment typology?This idea is supported by the plot above and gives us an idea of how different variables can interact to affect the length of stay.Let’s move on to some of the most important features present in the dataset: the severity of illness and the type of patient admission into the hospital.How much does length of stay vary with the severity of the illness?We can see a large amount of variance within the APR Severity of Illness feature, indicating it will most likely be an important feature to include in machine learning (ML) models.How does length of stay vary with type of admission?How do diagnosis descriptions vary for each illness severity type?Here, I perform text parsing analysis on the diagnosis descriptions column of the dataset, followed by the creation of wordclouds to visualize these results. Wordclouds were created using the diagnosis descriptions for each patient stratified by their illness severity category.From this analysis, we can see some similarities but also some large differences in the diagnosis descriptions for each illness severity type.In the brief discussion of health insurance programs above, I also mentioned patient income as potentially playing a role, and ultimately being a feature that might be correlated with length of stay. One example of how this might be the case is that for some low-income patients, the hospital provides better living conditions than their situation outside of the hospital. Thus, these patients are motivated to do everything in their ability to lengthen their stay. Unfortunately, our dataset does not include any information about patient income. However, it does include the 3 digit zipcode of the patient. As we prepare to begin the modeling section of this project, is important to consider the fact that zipcode is not a good feature for most ML models. Although its information is encoded by numbers, it is not a numerical feature, as its numbers do not mathematically mean anything. In turn, this makes zipcodes hard to understand and create patterns from for prediction by ML models. As a result, the next step in my analysis will be replacing this raw 3 digit zipcode data with a rough measure of patient income, while will prove a much more useful feature.To achieve this, I webscraped average income data by zipcode from this website, which contains data from 2006–2010. Below is a plot of the median income for each 3 digit zipcode present in the dataset, achieved by inner joining (through SQL in python) the webscraped data onto our dataframe.We can use this new income feature to visualize how other features vary with the new constructed feature.We are now ready to begin preparing for the modeling portion of this analysis.To begin this section, I first will drop all columns that will not be useful for predictive modeling. This includes zipcodes, diagnosis descriptions, operating certificate numbers, etc., in addition to features that would not be present at the time of patient admittance, such as total costs and total charges, thereby preventing data leakage.Next, I performed feature encoding of all categorical columns containing strings. Categories that had corresponding numerical codes, such as APR Severity of Illness Code vs. APR Severity of Illness Description, were used as opposed to their string counterparts in order to prevent the need for category binarization and thus potentially large increases in dimensionality.In addition, I will utilize string indexing for a particular subset of the features. For both Age Group and APR Risk of Mortality, string indexing is performed due to the fact that there is an inherent ordinality to the categories within these features. For example, age group ‘0 to 17’ is a younger age group than ’70 or Older’. As such, it makes sense to encode the youngest age group ‘0 to 17’ with a 1, and larger age groups with increasingly larger integers. Similarly, a 1 is given to the ‘Minor’ category in APR Risk of Mortality, whereas a 4 is given to ‘Extreme’. String indexing on particular features such as this can be extremely beneficial as it prevents increasing the dimensionality of the dataset and allows for models to learn the ordinality present in the feature categories. The code to manually accomplish this is shown below. However, it is important to be wary of using string indexing on features that do not have an inherent ordinality, such as ‘Health Service Area’. Thus, one-hot encoding of the remaining categorical features is performed.Now, let’s take a look at our predictor feature, length of stay. Length of stay ranges from 1 to 120 and only takes on integer values. I decide to treat predicting length of stay as a multi-class classification problem (instead of regression). Multi-class classification yields one large advantage in comparison to regression that I utilize: the ability to manually define class bins to manually control prediction specificity. Instead of treating length of stay as having 120 different classes, we can group these values into bins that make more sense for predictions without a significant loss in specificity of predictions. Following the exploration of multiple options for these bins, which involved trading off usefulness of the model (if bins are too large, the predictions will no longer be useful) and model accuracy, I ultimately settled on the following bin format:1–5 days, 6–10 days, 11–20 days, 21–30 days, 31–50 days, and 50–120+ daysFrom this visualization, a large class imbalance is apparent. Upon revisiting the univariate distribution of the raw length of stay data that was plotted at the beginning of this analysis, the same trend is apparent. This imbalance in the data must be handled carefully, as it can lead to misleading accuracy scores. Class imbalance can lead to a model over-predicting the class that occurs the most often, as simply predicting this class for most of the data instances, regardless of their features, will lead to the highest overall accuracy score. Thus, a model aiming to optimize accuracy score may fall into this trap. For example, even if the model were to always predict an inpatient’s stay would be 1 to 5 days (the most common length of stay in this dataset), it would achieve an accuracy of ~1.6e6/2.3e6 = 69%. In comparison to the baseline accuracy of the model (the accuracy the model would achieve if it randomly guessed), which is 1/6 or ~17%, this is significantly better model performance at first look.Under-sampling the most occurring class and assigning penalties for over-predicting classes are two of the most common methods for dealing with class imbalance, both of which were explored on this dataset. Ultimately, I found that assigning penalties was the most effective method, resulting in higher model accuracies than the under sampling method, while equally preventing the associated risks with class imbalance as when under sampling was performed. In order to assign these penalties to classes, I made use of the ‘class weight’ parameter in scikit learn’s ML functions, and set this parameter to ‘balanced’. This effectively assigns a weight to each class inversely proportional to the frequency with which it appears.PCA:Prior to training any models, I perform PCA on the data after using the StandardScaler() function to normalize the train and test data sets. PCA is a powerful tool that allows one to reduce the dimensionality of a dataset, which can be extremely beneficial for large datasets such as the one we are dealing with here. In addition, it removes any multicollinearity in the data. Although PCA is not intended for use on datasets with a mix of numerical and categorical features (that have been one-hot encoded/string indexed), performance was found to not be harmed significantly with the use of PCA (more can be read here about when to use PCA vs. MFA). It is important always normalize the data before applying PCA because PCA projects the raw data onto directions which maximize variance by calculating relative distances between points within a feature. Thus, when features are on different scales, PCA can be thrown off.Below, I plot the % explained variance against the number of components from PCA that are used. From this graph, we can decide how many components to use for the rest of the analysis in order to retain a certain percentage of explained variance in the data. Here, I choose to keep 29 components as this is the minimum number of components required to explain 95% of the variance in the data.Logistic Regression:Next, let’s train a one-vs-rest logistic regression classifier. Logistic regression classifiers are strong models for classification problems. Let’s train a model after applying PCA to the standardized data, as shown below.Train accuracy: 0.7315117403879348Test accuracy: 0.7318516452316673We get very good results given the baseline accuracy and nature of this dataset. However, this model was trained on the data without balancing any class weights. It is good practice to create a confusion matrix after making predictions to allow for closer examination of model performance, as an accuracy score alone can be misleading.The confusion matrix showcases the dangers of ignoring class imbalance in a dataset. Although we get a very high accuracy score, the model is significantly over-predicting prediction label 5 as it is the label with the highest frequency. On the other hand, the model never predicted labels 30 and 50. Both these trends are evident in the darker colors within the heat map for the first column in contrast to the lightest colors in the 30 and 50 columns (also denoted by 0's). Now, let’s see if we can deal with this issue.Logistic Regression with balanced class weight parameter:Train accuracy: 0.5892498813701775Test accuracy: 0.5889674625391544As we can see, defining the class weight parameter had huge improvements on the performance of our model, preventing the classifier from simply over-predicting the highest frequency classes. Although the overall accuracy drops, our model will now be a better predictor for any given instance during future “real-world” testing. Furthermore, depicted by the darker colors along the diagonal of the heat map, even when the model does predict the wrong label, it is usually off by one label to the right or left.Let’s print out some more classification metrics to get an even closer look at how our model is performing:Let’s use the f1-score to compare the logistic regression classifier to the rest of the models we will create. F1-score is a measure of a model’s accuracy that is calculated by taking the harmonic mean of precision and recall, thus combining both precision and recall, into one metric. If you are unfamiliar with the concepts of precision and recall, refer to this article to learn more. In essence, f1-score provides a quantitative measure for assessing the tendency to over-predict certain class labels and the ability for a model to separate classes.Decision Tree:For our next model, let’s try training a decision tree classifier on our data. Decision trees tend to perform very well in multi-class classification problems and are intuitive to understand. However, before we create the model, let’s optimize the hyperparameters for the decision tree classifier first. To do this, we can run a randomized search across a range of hyperparameter value combinations defined by a dictionary. From this search, we can then return the optimal values that are found. This hyperparameter search is enabled through the scikit RandomizedSearchCV function. Here, I choose to optimize the max_depth and max_leaf_nodes parameters, as they are key parameters in preventing overfitting, a property that decision trees are especially prone to. 3-fold cross validation is utilized.Output: {‘max_depth’: 50, ‘max_leaf_nodes’: 1000}I also plot a validation curve while utilizing 3-fold cross validation in order to visualize the effect of hyperparameter optimization and model complexity against model accuracy. The validation curve for the max_depth parameter is shown below.The validation curve highlights tendency for decision trees to overfit as the max_depth parameter becomes less stringent (allowing deeper trees). However, it is interesting to note that although there is a large gap between training and validation accuracy, validation accuracy does not go down between this max depth range.Using the hyperparameters found above, let’s train a decision tree classifier and use it to create predictions. Below is the code I used to train the model. Let’s set the ‘class_weight’ parameter to ‘balanced’ just as before. Note that I train the model on the raw training data, and not the PCA scaled + transformed data, as decision trees are scale-invariant. Thus, they do not require the training data to be scaled. In addition, lower accuracy was achieved when the decision tree was trained on the PCA transformed data.Train Accuracy: 0.6170775710955541Test Accuracy: 0.6134612839367724Ultimately, we can see that accuracy has gone up with the decision tree classifier! F1 scores for each class label has also increased, indicating an increase in model performance. Let’s see if we can do better with a decision tree ensemble, otherwise known as a random forest model.Random Forest:We can improve upon the decision tree classifier by bagging the trees into an ensemble and utilizing the power of majority voting between all of the trees built. This is called a random forest model. Below is the code and model performance for this model. Note that model parameters were chosen by trial and error (observing f1 and accuracy scores simultaneously) as a grid search/randomized search was very computationally expensive. However, parameters could be further optimized using said techniques.Train Accuracy: 0.6729519125323106Test Accuracy: 0.6436519717021008We have improved on the accuracy of the decision tree, as well as on the f1 scores for each class. Next, let’s take advantage of one of the functions in scikit to generate a plot of feature importances in the random forest model. Here, feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node in the tree.From this plot, we can see that the APR DRG Code (a classification system that classifies patients according to their reason of admission, severity of illness and risk of mortality) and the APR Severity of Illness Code are the two most important features in predicting a patient’s length of stay. Interestingly, the payment typology medicare subgroup has a relatively high importance in predicting length of stay in comparison to the other payment typology groups. Average income of the patient also played a significant role.Boosted Decision Trees:Lastly, let’s see what performance we can achieve with another type of ensemble method: boosting. Using the AdaBoostClassifier in scikit learn, we can create a boosted decision tree classifier. This model works by creating n short decision tree stumps, where each successive tree stump that is built aims to improve on the classification error of the one before it. The majority vote of these trees is then combined to produce final predictions. The code used to achieve this is shown below, where hyperparameters were optmized using equivalent methods to those used for the random forest model.Train Accuracy: 0.9397049441494455Test Accuracy: 0.7065437824744865Here, we can see boosted decision trees achieved the highest model accuracy, beating out both the logistic regression classifier and the random forest ensemble. However, most of the f1 scores for the minority classes (less frequently occurring), are slightly lower. Thus, changes in how severe a false positive or false negative may be on a patient to patient basis may warrant switching between the use of the random forest model or the boosted decision tree classifier as the optimal model.Spark Cluster Environment:Due to the size of this data, some modeling aspects such as hyperparameter optimization took a long time to run. Thus, I wanted to include a note on the benefits of Apache Spark. We can achieve significant code speedup if we take advantage of Apache Spark and AWS clusters by parallelizing many computationally intensive tasks. Check out the full code I have linked at the beginning of this blog post for scripts that can be used to accomplish this. This code includes how to set Spark up in a Python environment, connect an AWS cluster to the notebook, retrieve the dataset from an S3 bucket, and create models. One of the drawbacks I found of using Spark for this project was the lack of the class weight parameter in the Spark ML model functions. In the full code, I also show a method for manually assigning class weights. This code was adapted from a blog post that can be viewed here.Ultimately, through the steps highlighted in this blog post, I was able to predict a patient’s length of stay using data only available from the moment they enter the hospital and are diagnosed with an accuracy of ~70%. Such a model has the ability to profoundly improve hospital management and patient well-being. Future directions could look to a develop a second model that would allow for a sequence of model predictions to be made that would ultimately predict how much a given patient will cost to treat over the course of their stay. This could be accomplished through first predicting length of stay through models such as the one created here. Next, a second model could be created to predict “Total Costs” from length of stay.Before putting a model such as the one created here into use, important ethical considerations need to be taken into account as well, since features such as patient income and race were used to predict length of stay. More on this topic can be read about here.Takeaways:References:[1]. Baek, H., Cho, M., Kim, S., Hwang, H., Song, M., & Yoo, S. (2018). Analysis of length of hospital stay using electronic health records: A statistical and data mining approach. PloS one, 13(4), e0195901. https://doi.org/10.1371/journal.pone.0195901[2]. Scikit-Learn. (n.d.). Retrieved from https://scikit-learn.org/stable/The work in this article expanded upon my final project and leveraged concepts I learned in CIS 545: Big Data Analytics at the University of Pennsylvania",python,https://towardsdatascience.com/predicting-inpatient-length-of-stay-at-hospitals-using-python-big-data-304e79d8c008?source=tag_archive---------11-----------------------
Accessing Shared Mailbox Using Exchangelib — Python,"SMJun 1, 2020·4 min readI wanted to share this small trick to access a shared mailbox in Office365 using python.The challenge came up while building a data pipeline which would need to interface with an email that arrives daily with an embedded HTML file having a link and one needs to click on the link to download a “csv” file which would then be loaded to our data warehouse.While solutions are available in the internet, they are scattered and not updated, as I set out to solve this problem, I came across really cool snippets in Stackoverflow which I took the liberty of re-using and I want to credit the source please even though I do not remember who it was actually. What I want to do here is to summarize the solution so that this post can be a one-stop-shop for those facing same or similar problem.The 2 aspects of my problem was: 1. Interfacing with Outlook 2. Parsing HTMLI want to speak about the first aspect. Little bit of google search and going through stack overflow revealed Python libraries like: PyOutlook, exchangelib etc. Among all the libraries, what worked for me was exchangelib. Basically trying to connect and play with outlook programmatically, means that we need to use “EWS API” (Exchange Web Services API). Also, when a shared mailbox is set up in the enterprise, individual users are granted access to this mailbox in a way that they can access this mail box using their outlook credentials.Lets say my outlook email address is: Firstname.Lastname@someenterprise.com, the admins have set up a shared mailbox like: shared_mail_box_name@someenterprise.com and granted me access. The python code to access the shared mailbox would be as follows:The snippet above connects to the shared mail box using your credentials and lists out first 100 ( notice the [:100] in the snippet ) emails in the order in which they were received. Now that we are able to list emails, lets put a filter in the snippet to look for emails with Specific subject. In my case I was looking for a specific email with subject which said something like: “Data is ready for so and so date…” and also the latest one. So, the snippet for doing that is:The snippet above gives me the latest email. Once I can identify the latest email, my next step is to download its attachment, which is an HTML file. I use a PC so next set of code snippets are basically opening a file at a certain directory in my PC and writing to it by reading from the attachment file.Now that the file is downloaded, my next step is to parse the HTML file using libraries like BeautifulSoup and extract the hyperlinks. Next, go to that hyperlink to download the file. Once the file is downloaded, I can process it further and load it to any database. The full code looks like this(I just excluded the database loading part of the code as it is not relevant to this post):I hope this code is helpful to anyone trying to solve similar problem, as well as a good read for those trying to enrich their skills.Link 1: https://pypi.org/project/exchangelib/Link 2: https://social.technet.microsoft.com/Forums/en-US/044b3e47-2e31-49f1-9ef5-f3b68a218008/use-python-to-monitor-exchange-shared-mailbox?forum=exchangesvrgenerallegacy",python,https://medium.com/@theamazingexposure/accessing-shared-mailbox-using-exchangelib-python-f020e71a96ab?source=tag_archive---------29-----------------------
"Cheat Sheets #3: Python Essentials, Object Oriented, Data Structures, Complexities, Flask","Speedster SaurabhJun 1, 2020·10 min readLearning any new concept is difficult for newbies. Are you finding difficult in remembering all the syntax that you need to work with Python for Data Science? Guys, don’t worry if you are a beginner and have no idea about how Python works, this Python cheat sheet will give you a quick reference of the basics that you must know to get started. I am creating this series with cheat sheets which I collected from different sources.Over the past few months, totally redesigned the cheat sheets. The goal was to make them easy to read and beautiful so you will want to look at them, print them and share them.Do read this and contribute cheat sheets if you have any. If you like this post, give it a ❤️. Here we go:Python is a high-level dynamic programming language which is very easy to learn. It comes with powerful typing and the codes are written in very ‘natural’ style, thats the reason, it is easy to read and understand. Python programming language can run on any platform, from Windows to Linux to Macintosh, Solaris etc.In Python, every value has a datatype. in Python programming, everything is an object, data types are classes and variables are instance that means object of these classes.There are various data types in Python. Some of the important types are listed below.In python, Operators are only the constructs which can manipulate the value of operands. For example, in the expression 5 + 10 = 15. Here, 5 and 10 are operands and + is operator.Python provides some of the built-in operations on various data types.Python programming language provides various looping and control statement that allow for more complicated execution paths.A loop statement allows us to execute a statement or group of statements multiple times.if price>=700: print(“Buy.”) else: print(“Don’t buy.”)a=“New Text” count=0 for i in a: if i==‘e’: count=count+1 print(count)a=0 i=1 while i<10: a=a*2 i=i+1 print(a)Loop Control: Break, Pass and continueA function is a block of code which only runs when it is called. We can pass data (parameters) into a function and after executing a function, it will return data as a result.A lambda function is a small anonymous function. It can take any number of arguments but can only have one expression.In python, we have a huge a list of Python built-in functions. Some of them are:In python, we have several functions for creating, reading, updating, and deleting files. Theopen() function takes two parameters — filename, and mode.There are four different methods (modes) for opening a file:f= open(“File Name”,“opening mode”)(Opening modes: r: read, w: write, a: append, r+: both read and write)The try block allow us to test a block of code for errors.The except block allow us to handle the error.Python is an object-oriented programming language. In Python, almost everything is an object and has its own properties and methods. Here a class is like an object constructor, or a “blueprint” for creating objects.Surce:- https://intellipaat.com/blog/tutorial/python-tutorial/data-structures-with-python-cheat-sheet/Integer: It is used to represent numeric data, more specifically whole numbers from negative infinity to infinity.Example: 4, 5, -1 etcFloat: It stands for floating point numberExample: 1.1,2.3,9.3 etc.String: It is a collection of Alphabets, words or other characters. In python it can be created by using a pair of single or double quotes for the sequenceExample:x = ‘Cake’y = ‘’Cookie’’Certain operations can be performed on a string:Example: x*2Example: CokeExample:Example:Example:Syntax of writing an array in python:Example:The list can be classified into linear and non-linear data structuresLinear data structures contain Stacks and queuesNon-linear data structures contain Graphs and TreesExample:Ordered sequence of values indexed by integer numbers. Tuples are immutableSyntax:Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:It is an unordered collection with no duplicate elements. It supports mathematical operations like union, inters­ection, difference and symmetric differ­ence.Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:It is an unordered set of key value pairsSyntax:Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:Syntax:AlgorithmBest caseAverage caseWorst caseRemarksSelection sort½ n 2½ n 2½ n 2n exchanges,quadratic is the best caseInsertion sortn¼ n 2½ n 2Used for small or partial-sorted arraysBubble sortn½ n 2½ n 2Rarely useful,Insertion sort can be used insteadShell sortn log3 nunknownc n 3/2Tight code,Sub quadraticMerge sort½ n lg nn lg nn lg nn log n guarantee;stableQuick sortn lg n2 n ln n½ n 2n log n probabilistic guarantee;fastest in practiceHeap sortn †2 n lg n2 n lg nn log n guarantee;in placeReference:If you like this post, give it a 👏 and ❤️. And Many Thanks for your genuine Support, it matters.Till then- Keep Learning, keep Sharing, keep Growing.",python,https://medium.com/@brahmankarsaurabh/cheat-sheets-3-python-essentials-object-oriented-data-structures-complexities-flask-6ddbec0ae848?source=tag_archive---------45-----------------------
5 Advanced Operations Using Dictionaries in Python,"One manifestation of Python’s flexibility is the availability of various data structures, including those for storing single values (e.g. integers and strings) and those for storing multiple values (e.g. lists and tuples). For the latter, we can call them container data types. Among them, dictionaries are one of my favorites.In essence, dictionaries are collections of key-value pairs. For any given dictionary, the keys have to be distinct and hashable, and the hashability of these keys allows the searching and updating to have O(1) time complexity. In addition, there are no particular requirements in terms of what data types can be stored as dictionary values. The basic usage of dictionaries can be summarized below:Most of us should be very familiar with these basic operations for dictionaries. Expanding upon these basics, I would like to introduce some intermediate or advanced operations for dictionaries in this article.Most of the time, we are used to creating a dict object with the curly braces to enclose key-value pairs, like the example shown in the section above. The use of curly braces for creating dictionaries is known as the literals method. However, Python provides a very powerful constructor method for creating dictionaries under different scenarios.In addition to the creation of dictionaries using the dict() constructor method, the other handy way to create a dict object is using the dictionary comprehension technique.Many of you have probably heard of list comprehension, but did you know that we could also use the comprehension technique for the creation of dictionaries? It’s very straightforward. Let’s see an example:As shown above, we create a dict object using the dictionary comprehension, which has the following basic syntax: {key_expr: value_expr for item in iterable}. Isn’t it nice and easy?We can have a slightly more complicated use case by implementing some conditionals if we need to filter some elements from the iterable, as shown in the example below:You may laugh when you see the title of this section. “Are you kidding me? I know how to retrieve values — just as you showed at the beginning.” You’re definitely right about the use of square brackets to retrieve a particular key’s value. However, have you ever experienced something that can cause your program to crash?I understand that using the square brackets to get a dictionary’s value is the most conventional way in many programming languages, but we have options in Python. To prevent the possible KeyError, one tedious way is to first check if the dictionary object has the key or not before we decide to retrieve its value or not. Something like this:We have other options without using this ternary expression. We can actually use the get() method that allows us to set a default value if the dictionary doesn’t have the specified key, such that we could prevent ourselves from running into the KeyError problem. Let’s see its usage below:Dictionaries are iterables, which means that we can go over the elements in a dictionary to execute particular operations. However, dictionaries store key-value pairs and thus give us more ways to iterate. Let’s see these different ways. For the purpose of simplicity, the following examples all use the same dictionary:One thing to note in the iterations above is that with the evolution of Python, dictionaries have become ordered. Thus, when the order of elements matters, you can still use dictionaries. However, the catch is that after you delete a key-value pair, if you’re to add it back, the new insertion will be at the end and the order will change.Sometimes, we start with multiple dictionaries and want to merge them into a single dictionary. Certainly, the most tedious but straightforward way is to create a new dictionary, iterate over the existing dictionaries, and add these key-value pairs to the new dictionary.However, there are better ways to merge dictionaries in Python. For the purpose of demonstration, I’ll just use two dictionaries and merge them into one dictionary. The examples are given below.In this article, we reviewed five intermediate or advanced operations with dictionaries. These tricks can be very handy in your coding where dictionaries are involved.Thanks for reading this article. Here are a few other articles that are relevant to the present discussion for your reference.medium.commedium.com",python,https://betterprogramming.pub/5-advanced-operations-using-dictionaries-in-python-5f8edb4719fa?source=tag_archive---------4-----------------------
Web App Development in Python,"Roman PaolucciJun 1, 2020·3 min readFlask is a lightweight Python library that helps us develop web applications. I’ve used Flask in personal projects to build trading dashboards, and in a professional setting to deploy AI workflow management systems for hospitals. In this article, I want to introduce Flask and demonstrate how easy it is to get started building your own web applications.The best way to understand how Flask works is by working with it hands-on. After that, you will get a sense of how to build out each section of your web application. First, let’s import and create an instance of Flask.When we eventually run our Flask instance and connect to it in our browser it will have a default host (127.0.0.1 or localhost) and a default port (5000). To connect to our web application in a browser we would type in the URL and port (localhost:5000) in tandem with a rule to help us retrieve the appropriate content. So let’s create a new URL rule, this will essentially be equivalent to a home page. When we type localhost:5000 into the browser it will retrieve the data returned by this function.Now we need something to display.A template is HTML that will be rendered and returned to the user by the function that accompanies the URL rule. In this case, the home function will be responsible for rending and returning the home page template. All templates should be stored in a folder called templates in the same folder as the script persisting the instance of Flask. The HTML can be whatever you’d like, for this example, I am going to create a simple grid layout similar to a layout I used to build my trading dashboard.To have the home function render and return this template we can add the following code to our home function.Whenever we arrive at the home page (localhost:5000), the home function will render and return the home HTML template.The entire purpose of having a Python back-end is to display relevant data to the user on the front-end. To do this we can use Jinja. Jinja allows us to easily reference Python variables and write Python code manipulating how we choose to display those variables in HTML. To start, I’m going to create two variables in Python: first_name and last_name. Then I’m going to pass them to the template via render_template in our home function. I will be passing the Python variables to fn and ln — this is how we will reference first_name and last_name in the HTML.I want to display the first_name variable in the first column and the last_name variable in the second column. To do this, I am going to wrap the references to those variables, fn and ln, with curly brackets ({{ fn }}, {{ ln }}) indicating to HTML that they are being passed to the template from Python.This will render the home HTML template with the Python variables.To launch our web application all we have to do is run the instance of Flask.After running the Python script, head on over to your browser and in the URL type localhost:5000.There you have it, your first web application built in Python with Flask.A pretty typical question arises: how do we deploy a Python web application online? There are a lot of ways to accomplish this. However, if you are looking for a specific answer, I would suggest checking out Amazon Web Service’s Elastic Beanstalk. That is how I have deployed all of my web applications.aws.amazon.com",python,https://towardsdatascience.com/web-app-development-in-python-469e1cf2116b?source=tag_archive---------6-----------------------
Building a Dynamic Weather Download App,"David HurleyJun 1, 2020·9 min readThis article describes the framework I used to build a dynamic dashboard web app for downloading and visualizing historical Canadian weather data…FAST!!!Check Out the App: WeatherHistoryCanda.comGet the Code: GitHub RepoThere are more than 8000 active and inactive government weather stations in Canada all with open-source data. Yet, downloading the data is not easy.Presently, historical Canadian weather data is accessed and downloaded through a government-run web portal maintained by Environment and Climate Change Canada (ECCC). While ECCC provides excellent open-source data services the download portal leaves something to be desired. The portal has no map to visually search and display weather station locations, downloads are limited to small subsets of data, and there is no way to graph data once downloaded.Build a dashboard web app to dynamically search, filter, and download historical Canadian weather data.A Few Requirements:Initially, I developed a loose concept of how I wanted the app to be structured, such as which microservice framework to use, and after some trial and error, I settled on the architecture shown below. It’s a standard structure consisting of front-end processes, background processes, and database storage.For small dashboard applications, microservice is the way to go. It accelerates development and is easy to deploy. I chose to use the Python microservice framework Plotly Dash to build my application. Dash is an AWESOME tool, especially for anyone not fluent in JavaScript, because it allows you to build complex interactive dashboards in pure Python!The main point of my app is to allow users to download historical weather data. Since the download task takes seconds to minutes to execute it can’t be run on the front-end efficiently (i.e. it would introduce latency and prevent the app from responding quickly to incoming requests). A better solution is to execute long-running tasks as background jobs and feed the results to the front-end.To do this I used Celery, an asynchronous task queue, with CloudAMQPs RabbitMQ message broker and Heroku Redis result store. There are many great and detailed tutorials on how to set up an asynchronous task queue (i.e. Miguel Grinberg’s blog) so I won’t go into that here. Instead, I provide a summary of the Celery background job architecture.I HIGHLY recommend not using Heroku Redis as both the result store and message queue. I tried this at first and constantly reached the maximum number of concurrent connections allowed. By allocating the message queue to RabbitMQ and result store to Heroku Redis there are more concurrent connections available.Originally, my app was structured to download data directly from the ECCC server (3rd party). This turned out to be slow and not very robust. Instead, I downloaded all the ECCC data (~37 Gb) to an AWS S3 bucket. I did this by running multiple batch download jobs using AWS Lambda and SQS message queue. Once I had the data in an S3 bucket I used AWS Lambda and CloudWatch Events to update active weather station data every day at midnight. The result was a flexible database with no dependency on the ECCC server.The best part of hosting the data in an S3 bucket is that data can be queried in the app directly from CSV files in S3 using S3 SELECT (i.e. simple SQL expressions). I had considered using traditional SQL databases such as MySQL but S3 was cheaper in my use case and great for persistent storage. 💪I chose to use Heroku cloud services to deploy my app. I don’t have a great reason for using a cloud service such as Heroku over DigitalOcean, Linode, or EC2 other than it’s extremely easy to deploy on. Heroku apps run on dynos (virtual Linux containers) and come in two types, a web dyno that handles web processes (i.e. HTTP traffic) and a worker dyno that handles background jobs (i.e. Celery tasks). I ended up deploying the app on Heroku free tier with one free web and worker dyno.There are a few downsides of Heroku for my use case:In the end, it was pretty easy to get around both of these problems by using AWS S3 for database storage.I wanted a simple interface that displayed weather stations on an interactive map so a user could visually filter weather stations based on things like station location and data availability. The Dash library includes easy to implement high-level components such as graphs, dropdowns, and input boxes. The structure of a Dash app is the app layout, consisting of HTML and Dash core components, followed by app callbacks which create chained interactivity.The background map is from the Mapbox API and weather station metadata, such as station location, is loaded on app startup from AWS S3. Callbacks in Dash are easy to implement and include input components (i.e. province or station name) and output components (i.e. map graph).After clicking a weather station on the map a user can view station metadata in the table and select the desired station to download data from. Then a user can set the download interval (i.e. how often data was recorded at the station) and download dates. Once this has been completed a message is displayed to let the user know what they will be downloading. This message also makes the “Generate Data” button active.When the “Generate Data” button is clicked a task is sent to the RabbitMQ message queue where a Celery worker receives it. The Celery task uses the selected table data (i.e. station name, province, etc.), download dates, and data interval to query a subset of the associated CSV in S3. While Celery is executing the task, the front-end uses the task.id to query task.state (i.e. pending, progress, success, or failed). This happens every 250ms and updates the visibility of the progress spinner.If I wasn’t using Heroku I could save the queried data to disk, or if my data was smaller I could cache in browser. Instead, I push the queried S3 data back up to S3 to a temporary folder in the same bucket. On the front-end, an S3 pre-signed URL pointing to the temporary data is created and linked to the “Download Data” button using Flask routing. When a user clicks the “Download Data” button it redirects to the temporary data and downloads the file in browser.The awesome thing about pre-signed URLs, even if my download structure is a bit slower and less efficient (i.e. multiple S3 GET and PUSH commands), is the client downloads directly from S3 which avoids having data saved to the server or cached in browser.In my opinion, being able to visualize downloaded data in near real-time is the most important feature for a user as it allows them to do exploratory data analysis on the spot.I chose to present three basic graphs to convey different data statistics (i.e. box plots for seasonal trends and histograms for data distribution) using Dash graph components to render figures. The line and histogram graphs render relatively quickly with even large datasets (>500000 data points) but the box plot can be slow. To keep rendering to a few seconds I use a subsample of the data for the boxplot graph. This is done with the Pandas DataFrame.sample function and keeps the distribution the same as the original.As mentioned, because I am using Heroku, saving to disk was not an option, and since the datasets can be >50mb, using Heroku Redis to store Celery task results was impractical (i.e. only 25mb of memory on the free tier). Additionally, sharing data between Dash callbacks and Dash pages can be tricky. The standard solution is to use the dcc.Store component (i.e. in browser storage) but this only works for data less than a few megabytes.After some testing, the solution that I found best maximized cost and speed was using S3 SELECT to query data in S3 and graph. The biggest downside to this is longer load time in the graphs (i.e. querying data from a cloud database instead of retrieving locally) and the need to query S3 on each figure render.Dash (i.e. Flask) apps natively only process a single HTTP request at a time, so to handle multiple requests Dash should be deployed to Heroku using the Gunicorn WSGI web server. The Gunicorn web server allows you to run the app concurrently. Heroku has several recommendations for Gunicorn worker configuration, such as limiting the number of Gunicorn worker processes to 2–3 on a free tier.Getting the Procfile, RabbitMQ, Heroku Redis, and Celery configurations set correctly was pretty challenging. CloudAMQP recommends configurations for the RabbitMQ message queue when using Celery. The most important aspect when using the CloudAMQP free tier is to reduce the “background” message rate (i.e. configuring the Celery worker to run without gossip, mingle, and heartbeat).If your hobby app costs you an arm and a leg you won’t be able to host it very long! My app currently costs me ~$1–2 a month but this will change with scaling.There are a few things I want to add and change in future versions of the app.Overall, this was a really fun project and I learned a lot about full stack development!",python,https://towardsdatascience.com/building-a-dynamic-weather-download-app-1ce64a6c3e61?source=tag_archive---------31-----------------------
JavaScript to Java,"matthritzJun 1, 2020·3 min readI have recently decided to teach myself Java. Coming from a JavaScript background, I figured it can’t be too different. Just have to lose the script, right? All joking aside there are a lot of similarities between the two, but there are major differences as well.For starters, Java was created to run on any machine. When its script compiles, the outcome can be read by Windows, Linux, and OS based systems. JavaScript does not have support for Windows systems and therefore has to use a Linux based virtual machine in order to read the code.Java is based on Object Oriented Programming, which is available in JavaScript, but with Java it is the only way to run code. This means every command run in a Java program will be accessed through an object. JavaScript allows functional programming, which allows functions to be created and called anywhere in our code script.Declaring variables is similar, however, instead of using var, let or const, we need to tell Java what type of datatype we are declaring. Below is how we declare a string datatype in JavaScript as compared to Java. Notice that Java has a String and a char datatype. We can think of this as “char” being a simple datatype and String is an array of chars.First the JavaScript string:And the Java equivalents:Datatypes are pretty similar between the two. We can compare JavaScript’s number to Java’s int. There are actually a few number datatypes in Java and can be used in certain situations to save memory, but int is its most basic form of a number. Java also has number datatypes for non-whole numbers and numbers within a certain limit, but for simplicity sake we won’t get into them. For more information you can look here.With complex datatypes it gets a little different. When we look into arrays between the two classes, we can see that JavaScript’s arrays are their own datatype while Java’s array is more of a modified primitive type. Again, we must declare what type of array we are declaring followed by an array literal to tell the program that the following data must be stored as an array. Or Java arrays do share a similarity with our JavaScript arrays in that they are zero-indexed and can be accessed the same way as if we would access a JavaScript index.JavaScript Array:Java Array:The similarities are close enough that we can look at Java code and be able to tell what most of it is doing if we just refer back to our JavaScript knowledge. Look out for my next blog where I will dive deeper into Java operators and manipulating datatypes.",java,https://medium.com/@atmatthritz/javascript-to-java-14237312e7e9?source=tag_archive---------25-----------------------
AEM: Extend Core Component models using resource type association and delegation,"AEM Core Components are an amazingly useful resource, and with their inclusion in the AEM Maven Archetype and in the WKND tutorial, they have become ubiquitous in AEM Sites development. If you aren’t using them already I highly recommend that you do.Whenever I decide to create a new component, I use the Core Components as a point of reference to see if they offer a solution for my requirement. The answer is:Authors want to place some very high-resolution images on the website, performance be damned! They’ve asked for an option that would allow them to use an image asset’s original rendition rather than fetching a web-friendly rendition that is adapted to the image size on the page.In this case, the Core Component (CC) Image is almost perfect, it just needs a bit of tweaking. So how can we satisfy this requirement with minimum effort? That’s the topic for this tutorial.Here is the banner image again (I just saved you three mouse-wheel-scrolls, you’re welcome):We are going to leverage the power of Sling model association to implement a Sling model which overrides only the bare minimum of behavior from the CC Image model. This powerful technique is unfortunately very briefly mentioned in the Sling documentation on the subject, so this tutorial will serve as a practical example.As you can see in the diagram, all we need is a proxied Image component (which you should already have if you used the AEM Maven archetype to bootstrap your project) and a super-short model (30 lines, imports excluded) that we will create below. We will let CC and Sling do the heavy lifting!By doing this, we can:The CC Image component relies on a class called ImageImpl to implement it’s model.“Okay, let’s extend the model” you might be thinking. Not quite. Instead we will be using the delegation pattern to override the CC model’s behavior and rely on the CC Image interface to provide the methods we need, take a look:I’ve gone overboard with the comments so you have a better line-by-line idea of how the model works, but there may be some things you aren’t familiar with:If you now create place an Image component on a page in your site you will see… nothing, because at the moment are not supplying a useOriginal property value. But behind the scenes, Sling is instantiating our CustomImage model.This happens because we haven’t changed the component’s HTL script, which contains the following line:So when it runs, Sling will receive a request to create an instance of com.adobe.cq.wcm.core.components.models.Image, except now it has two choices:Faced with this dilemma, Sling will check the sling:resourceType of the component making the request, which in our case is demo/components/content/image. This matches our CustomImage resourceType, but not that of ImageImpl, so our class will be instantiated instead.Now lets supply a useOriginal boolean to our model. Override the cq:dialog node in the proxied Image component to add a checkbox. Here is the XML:Save and reload your page. Edit the Image component and you should see the checkbox appear:Select a high-resolution image, check the box and save. You should now see that the image is rendered in its original resolution.Here is the src attribute of the image when useOriginal is false:And here it is when useOriginal is true:I’m not providing a GitHub link this time because there’s so little code to actually share (which is kind of the point 😉). I hope this has helped to show how easy it is to use CCs behind the scenes. If you have any questions or just want to reach out, feel free to find me on LinkedIn!",java,https://levelup.gitconnected.com/aem-extend-core-component-models-using-resource-type-association-and-delegation-b8855ed281e2?source=tag_archive---------3-----------------------
"Introducing the DeepCode PlugIn for IntelliJ, PyCharm, and WebStorm","The JetBrains IDE extension provided by DeepCode.ai finds bugs and critical vulnerabilities in your code. We support Java, C/C++, Python, JavaScript and TypeScript. You can find it listed at JetBrain’s marketplace here. Below, you can find a description of how to install and use the plugin. Have fun and let us know if we could do better.Note: We support IntelliJ, PyCharm, and WebStorm. While it works in CLion, it is not officially supported. Feedback is always welcome, though.Through the extension, you can quickly start using DeepCode’s code review and analysis within your development workflow. The extension will automatically alert you about critical vulnerabilities you need to solve in your code the moment when you hit Save in your IDE. With DeepCode’s superior code review, you save time finding and fixing bugs before they go to production.DeepCode uses symbolic AI to process hundreds of millions of commits in open source software projects and learns how to find serious coding issues. Because the platform determines the intent of the code — and not only the syntax mistakes — DeepCode identifies 10x more critical bugs and security vulnerabilities than other tools.In order to show a detailed explanation of a potential bug, we introduced a new AI technique called Ontology. With Ontology, we’ve integrated the capability to present logical argumentation used by the DeepCode engine.JavaScript, TypeScript, Java, C/C++, and Python are currently supported. IntelliJ, WebStorm, and PyCharm as a platform, officially minimal supported version: 2019.2.4. CLion support is experimental.Open Settings Menu then PlugIns, make sure you are searching in Marketplace. Type DeepCode in the search bar. From here, you can install the plugin easily.DeepCode wants you to accept the general terms and conditions, and authenticate. Login will redirect you to your default browser to login with either GitHub, GitLab, or Bitbucket and generate a token that is saved in the configuration.When you open a new project folder, you are asked if you want this project to be included in the scan.If you accept, DeepCode will bundle the sources, analyze them, and show you the result. A summary, you can see in the status bar entry on the footer of the workspace. The status bar entry shows also how many errors, warnings and info suggestions have been found.Note: DeepCode will update the scan results whenever you change the file content.You can open the DeepCode Tool Window by either clicking on the Statusbar entry or by View, Tool Windows, DeepCode.The tool window shows the overview of the suggestions in a tree-view over the files. You can see how many issues were found overall, in how many files, and drill down into each file and finally each suggestion. The toolbar on the outmost left provides the following options (Note: If you cannot see the toolbar, make sure it is enabled in the menu accessible via the wheel icon):In the editor, DeepCode decorates the code elements where it found a suggestion.You find two functions in the context menu that you can reach by either hovering over the suggestion or by clicking on the yellow light bulb symbol.The disabling functions are interesting if you want to flag testing code. DeepCode has heuristics to separate testing and production code but this gives you the flexibility to flag your intent.If you want to ignore certain files/folders (like node_modules for example), create a .dcignore file. You can create it in any folder on any level starting from the directory where your project resides. The file syntax is identical to .gitignore. See also DeepCode Documentation on .dcignore.In Settings, you can find DeepCode Settings.",java,https://medium.com/deepcode-ai/introducing-the-deepcode-plugin-for-intellij-pycharm-and-webstorm-b4de2abb31fb?source=tag_archive---------5-----------------------
Kotlin ‘either’ keyword,"Aleksei JegorovJun 2, 2020·2 min readAs many developers know, Kotlin does not have ternary operator as ASP.NET C#, Javascript has:var result = (boolean condition) ? true : false;Python variant of ternary operation:age = 25print(“Eligible”) if age>20 else print(“Not Eligible”)Kotlin implements it in own way:val result: Boolean = (boolean condition) true else falseBut we can go ahead in case of use Arrow Kt library and work with exceptions in functional style.In standard impreative way we can use try catch:Unfortunately Kotlin does not has standard keyword to provide such service like Either<Int, Boolean>. Idea is to use Pair or Tuple: value and (possible) error.But it is not a problem, we can use arrow libraryBy example, we want to convert String to Int and catch exception in functional way:or more detailed when we work with errorsBy example: validate phone number, either show error.In addition we can also use Kotlin functional way of show error with help of runCatching. It catch everything.It implemented as inline funcion this way:And example of runCatching:Take a look at the docs for Result:kotlinlang.org",java,https://medium.com/@alekseijegorov/kotlin-either-keyword-e225e267dad9?source=tag_archive---------7-----------------------
How to Create Dark theme in Android,"Most people prefer a dark theme over a light theme in any application that they use. So, in this article I am going to walk you through the process of creating a dark theme for your android application.So, Let’s get started!We would like to make it easy for the users to switch between light and dark modes. So, we will create the call to action for this in the settings activity.Firstly open/create your activity_settings.xml to style your activity.Now we need to set the styling theme for dark modes in the styles.xml as shown below:Also, refer to the colors.xml file’s snippet given below that are used for creating the dark theme:Now, when the switch “Dark mode” is clicked it should toggle between dark mode and light mode. In order to that let’s head onto SettingsActivity.java. Also, the user’s preferences need to be reflected each time the app is opened again. So, we need to create SharedPreferences. Create a class called “SharedPrefs” as shown below:Now, in the SettingsActivity.java file we implement our toggle switch as shown below:To apply the theme for each activity, go to that activity and type the below code in the onCreate() method before calling the setContentView() :Firstly create an object of the class SharedPrefs :Then inside the onCreate() method, we need to check if the dark mode is set in the user’s preferences.So, in this way you can implement dark theme for your android application.If you have any queries please post in the comment section below. Connect with me on LinkedIn . Also, if you want to look at my amazing collection of apps developed, don’t forget to check Google Play Store.Know more about me here.With that being said, thanks for reading my article and Happy Coding!",java,https://medium.com/nerd-for-tech/how-to-create-dark-theme-in-android-55a84c9a3caa?source=tag_archive---------15-----------------------
Background Services running forever in Android (Part 1),"Lakshya PunhaniJun 2, 2020·4 min read(Tested and verified in Android version 7,8,9,10)With all the background execution limits starting from Android 8.0, it seems impossible to make an app running forever but still there are many ways. So without wasting any more time let’s explore.Part 2medium.comPart 3medium.comWorkManager is a library used to enqueue deferrable work that is guaranteed to execute sometime after its Constraints are met. WorkManager allows observation of work status and the ability to create complex chains of work.There are two types of work supported by WorkManager: OneTimeWorkRequest and PeriodicWorkRequest. You can enqueue requests using WorkManager as follows:Here UploadLocationWorker is a ListenableWorker class. All background work is given a maximum of ten minutes to finish its execution. After this time has expired, the worker will be signalled to stop. For more details please refer https://developer.android.com/reference/kotlin/androidx/work/WorkManagerSo these are the basics of what WorkManager is but we are here to talk about running app in background forever so PeriodicWorkRequest is what we are looking to implement.This way you are initialiasing PeriodicWorkRequest which will run every 15 minutesNow you will have questions likeCongratulations WorkManager works perfectly in every state.Here are some test results done on various devices. I logged in time in worker threadOppo F11 ProSamsung S10Mi A3Vivo Z1 ProAlarms (based on the AlarmManager class) give you a way to perform time-based operations outside the lifetime of your application. For more details please refer https://developer.android.com/training/scheduling/alarms and https://developer.android.com/reference/android/app/AlarmManager.Now we know here what Alarm manager is and how this works but the complications are still there. Now the question arises how to make it work forever? Answers can be setRepeating() or setInexactRepeating() but NO they don’t work anymore.Alarms do not fire when the device is idle in Doze mode. Any scheduled alarms will be deferred until the device exits Doze. You can use setAndAllowWhileIdle() or setExactAndAllowWhileIdle() to guarantee that the alarms will execute.This will schedule alarm now for receive after 295 seconds (Approx 5 minutes) and now in AlarmReceiver onReceive() method schedule same alarm again every time which will make it repeating alarm (Code below)Now, there are still some ways left to cover. Also if you will notice logs of vivo device you will see a lot of anomalies which we will cover in next blog why is this happening? Also we will study about the best practices for using workmanager",java,https://medium.com/@Lakshya_Punhani/background-services-running-forever-in-android-100261df88ee?source=tag_archive---------3-----------------------
Best Languages To Write Games In,"Chukwunenye MosesJun 2, 2020·3 min readMillions of active android users play games every day on their phone or PC, either online or offline. Most of the games played are fun, challenging, interactive and a great way to spend your time while at home or free from work. Such amount of people taking interest in one game or the other has paved way for the development of more games. The development or building of games involves writing several lines of code, otherwise known as programming. In order for you to build a game, you don’t just need to be efficient in coding. You also need to be proficient in a particular set of programming languages that are most suitable for writing games.There are a host of programming languages out there that are used in developing and building websites, software and apps/games. Among these programming languages are Python, JavaScript, Ruby, Perl, phP, JAVA, C, C++, C#, COBOL, BASIC, PASCAL, etc. Programming requires a lot of creativity and critical thinking, and all these programming languages are built upon different structures and syntaxes. The syntax of a programming language could be more suitable in an application than that of another programming language. It is to that effect that JavaScript is considered more suitable in building web-based applications than C#. Narrowing down to the niche of game development, there’s a subset of the aforementioned languages that are widely accepted as the best languages to write games in. The languages in this subset are C, C++, C# and JAVA.C++ will give you more speed, you will be able to produce codes that are faster which gives it an edge over others. C++ is another object-oriented programming language that expands on C to execute higher-level computer tasks. Released in 1983 by Bjarne Stroustrup, C++ organizes and stores info in bundles for more complex programs. It is an easy language to learn. It’s also the language used to build most big console and windows games. Game programmers also commonly use Java because Java supports multithreading and sockets. Multithreading uses less memory and makes the most of available CPU, without blocking the user out when heavy processes are running in the background. Sockets help in building multiplayer games. Plus, Java runs on a virtual machine, so your game will be easier to distribute. If you want to develop on android, you might want to consider Java. Java is the best language for Android games. Some people code in C++, but then there’s the hassle of dealing with cross-device compatibility. Java runs on virtual machines, and it’s compatible across different devices.Conclusively, C++ and JAVA are beyond every iota of doubt, the most suitable programming languages to write games in, both having their fantabulous idiosyncrasies.",java,https://medium.com/@chunkums088/best-languages-to-write-games-in-9164a42ff967?source=tag_archive---------9-----------------------
UI Automation for mortals: simple yet elegant Page Objects using Java and Selenide,"Alexander PushkarevJun 1, 2020·4 min readOne of the most discussed topics in UI Test Automation is Page Object Model. Page Object Model is a de-facto industry-standard pattern. This pattern was introduced by Martin Fowler in 2013 [1] and is an adaptation of Facade and Adapter patterns to the UI Test Automation.Page Object Model helps to decouple business logic from implementation, so instead of brittle and unreadablewe would write something shorter and much easier to comprehend:Traditionally, due to Selenium Web Driver low-level API writing Page Objects was not an easy task. However, it may be done in a fast and elegant way using Selenide, which is the best Selenium Web Driver wrapper for Java that I am aware of at the moment. Selenide offers:Using Selenide, you will no longer have to worry about shutting down browser, handling timeouts or StaleElement Exceptions. You will just concentrate on testing your application business logic without unnecessary technical troubles.As an application under test let’s choose this simple and neat “TODO list” application:This application conveniently consists of only one page, so we will need to implement only one Page Object for it.Disregards of what tool you use, the one of the most important things is Page Object API design.Many people would start from looking at the page internals (i.e. HTML code), but I would argue that is is a wrong place to start. Instead, we should better to think how we going to interact with the page. By doing this we’re making sure that the Page Object will be convenient and easy to use.In Java, for example, we could start by creating an interface:If you have played with the application a bit you can immediately notice that I have decided to design a fairly low-level API for a Page Object. I did this so I could follow the three-layer framework architectural pattern. However, it is not necessarily has to be this way, and one can design a little bit more high-level API Page Object, especially with the help of Selenide.Why Interface?Interface may be useful if we consider possibility of reusing the same test scenarios for different flavours of the same application, for example for Web-version (driven via Selenium/Selenide) and mobile application (driven by Appium)On the other hand, if we don’t consider such possibility, it may be absolutely OK to not to use interlace.I used interface mostly because it helps me to think of API and not to think about implementation before it is absolutely necessary.Anyway, by starting with interface definition we do two things at once:The cornerstone of the Selenide are two simple methods (which is way more than two methods under the hood, but it does not matter):By using those simple method, we can, for example, interact with elements this way:Putting it simpler — by using those two magic methods one can locate and interact with elements without all the Selenium boilerplate! And even more, from the Selenide documentation:The majority of operations on elements, acquired by the $ and $$ commands, have built-in implicit waits depending on a context. This allows in most cases to be not distracted by handling explicitly the waiting for loading of elements while automating testing of dynamic web applications [2].Putting boring things aside (like selecting a proper locator for elements) we may end up with a Page Object like this:Notice how many things we have out of the box without any hassle:In addition, Selenide can:And of course it is not a whole list of nice things Selenide has.With the Selenide available there’s no real reason to use plain Selenium in Test Automation Frameworks. With the current rate of development I consider Selenide not a Selenium wrapper, but pretty much an alternative.References:[1] https://www.martinfowler.com/bliki/PageObject.html[2] https://selenide.org/documentation.html",java,https://medium.com/@alexspush/ui-automation-for-mortal-elegant-page-objects-with-java-and-selenide-3122b17dc473?source=tag_archive---------12-----------------------
Building a Blockchain: Key Value Stores,"Angel Java LopezJun 2, 2020·4 min readIn other posts, I described the idea, the entities, the serialization implementation of my personal blockchain project (source code repository in Java). I really enjoy writing this code: it pushes me to really understand the involved use case, in the simplest way possible.Today I want to describe how to store the information needed to run the blockchain, even after the restart of the node.First, I want to keep the blocks and the world state. And the list of blocks that makes the best chain: maybe the node received more than one block per height, that is, a block could have siblings. And for some other uses case, the node needs also the result of each transaction execution (called usually the transaction receipt).All this information does not need a full fledged relational database. Simple key value stores are enough. So, I have key value stores, where a key is associated with a value:At the end, every key and value is a byte array: original keys and values are serialized to byte arrays. Ie, I have a key value store where the key is a hash (serialized to bytes), and the value is the corresponding block entity (serialized to bytes using RLP (Run Length Prefix)). The accounts and the contract storage are saved as tries, each node trie having as key its own hash.This is the interface for KeyValueStore.java:When running some code tests, I don’t need a disk-based key value store, a simple map is good enough, HashMapStore.java:And when the node starts, the services (message communication with other nodes; attending JSON RPC request from external applications like wallets; etc) need a list of stores, I have KeyValueStores.java:Notice that the transaction receipts store is not yet used nor implemented. Using TDD, I will add it when I wrote a use case and tests that need it. Don’t cross the bridge before you come to it.Not only I have an in-memory implementation, I also have a file-based implementation, KeyValueDb.java:It’s oriented to have values that are not likely to change a lot. I should improve to support some few use case of many writings of different values for the same key.Exploring some use cases, related to synchronized a node with a network that is already running, I discovered the usefulness of having access to the key value stores of peer nodes. So, now I have a RemoteKeyValueStore.java:Its getValue method broadcast a query to peer nodes, and when the response message arrived, the future is resolved (code not shown):And the key value store that a node runs could access the remote ones, using the DualKeyValueStore.java:Its getValue method checks one store, but if the value is missing, the other store is used:All this code allows the execution of a new block, without having ALL the world state available, so I could implement a quick synchronization (in Ethereum jargon, it is called a beam sync). But this use case deserves a separated post. It could be also used in a light client implementation.Angel “Java” Lopez",java,https://medium.com/@angeljavalopez/building-a-blockchain-key-value-stores-5103f1052dc4?source=tag_archive---------16-----------------------
"NSA, Ghidra, and Unicorns","This time, the PVS-Studio team’s attention was attracted by Ghidra, a big bad reverse-engineering framework allowing developers to analyze binary files and do horrible things to them. The most remarkable fact about it is not even that it’s free and easily extensible with plugins but that it was developed and uploaded to GitHub for public access by NSA. On the one hand, you bet NSA has enough resources for keeping their code base clean. On the other hand, new contributors, who are not well familiar with it, may have accidentally introduced bugs that could stay unnoticed. So, we decided to feed the project to our static analyzer and see if it has any code issues.PVS-Studio issued a total of 651 high-level, 904 medium-level, and 909 low-level warnings on the Java part of Ghidra (release 9.1.2, commit 687ce7f). About half of the high-level and medium-level ones were “V6022 Parameter is not used inside method’s body” warnings, which typically show up after refactoring, when some parameter becomes no longer needed or some feature is temporarily commented out. A quick look through these warnings (there are too many of them to closely examine each as a third-party reviewer) didn’t reveal anything outright suspicious. I guess it would be OK to turn off this diagnostic for the project in the analyzer’s settings at least temporarily so that it doesn’t distract developers. In general practice, though, you don’t want to neglect it because setters’ or constructors’ parameters often have typos in their names. I’m sure most of you have seen an annoying pattern like this at least once:More than half of the low-level warnings were produced by the “V6008 Potential null dereference of ‘variable’” diagnostic — for example, the value returned by File.getParentFile() is often used without a prior null check. If the file object the method is called on was constructed without the absolute path, the method will return null, which may cause the application to crash if the check is missing.As usual, I’ll be discussing only high-level and medium-level warnings since they usually reveal the major portion of actual bugs found in the project. When you deal with analysis reports, we recommend always going through the warnings in descending order of their certainty.Below I’ll share several analyzer-reported code snippets, which I found suspicious or interesting. The large size of Ghidra’s code base makes it almost impossible to find these defects manually.PVS-Studio warning: V6001 There are identical sub-expressions ‘newDataType.getLength()’ to the left and to the right of the ‘>’ operator. DataTypeSelectionEditor.java:366This class provides a UI component for selecting data types with autocompletion support. The developer can specify the maximum size of the selectable data type (using the maxSize field) or make it unlimited by specifying a negative value. Should the limit be exceeded, the check of input data is supposed to throw an exception, which will be caught further up in the call stack, with the user getting an error message accordingly.It seems the author of this code got distracted right in the middle of writing that check — or maybe they just started meditating upon the purpose of life and all. Whatever it was, they ended up with broken validation: the condition checks if the value can be greater than itself, and since it never can, the check is ignored. And that means the component could provide invalid data.Another similar mistake was found in two other classes: GuidUtil and NewGuid.PVS-Studio warning: V6007 Expression ‘data[i] != 0xFFFFFFFFL’ is always true. GuidUtil.java:200The condition in the for loop of the isOK method checks if a value is not equal to two different values at the same time. If it’s true, the GUID will be acknowledged as valid right away. Thus, the GUID will be acknowledged as invalid only when the data array is empty, which will never be the case because the variable in question gets its value only once — at the beginning of the parseLine method.The isOK method has the same body in both classes, which makes me think it’s just another case of cloning incorrect code using copy-paste. I’m not sure what exactly the author wanted to check, but I’d say this method should be fixed as follows:PVS-Studio warning: V6006 The object was created but it is not being used. The ‘throw’ keyword could be missing: ‘new MemoryAccessException(“Cyclic Access”)’. BitMappedSubMemoryBlock.java:99As is widely known, exception objects don’t do anything by themselves (at least they shouldn’t). New instances are almost always thrown using the throw statement, and in certain rare cases they are passed somewhere or stored in a collection.The class the method above belongs to is a wrapper over a memory block allowing the developer to read and write data. Since the method doesn’t throw exceptions, the restriction imposed on accessing the current memory block using the ioPending flag may get violated, and, furthermore, AddressOverflowException is ignored. That’s how the data may get spoiled and you’ll never know about it; instead of getting an explicit error message in a particular spot, you’ll get strange artifacts that you’ll have to debug.There was a total of eight missing exceptions like that:Curiously, those files also contain methods that look very much like the method above but do have throw in them. I suspect the programmer made a mistake in the original method, similar to the one discussed, cloned it a few times, and eventually discovered the mistake and fixed every clone of the method that they could remember of.PVS-Studio warning: V6008 Null dereference of ‘selectedNode’ in function ‘setViewPanel’. OptionsPanel.java:266The analyzer isn’t entirely correct about this one: the call of the processSelection method won’t currently lead to NullPointerException as this method is called only twice, with an explicit null check of selectedNode before the call. That’s not a good thing to do because another developer may assume that since the method explicitly handles the selectedNode == null case, this value should be valid and use it, potentially ending up with a crash. This is particularly true for open-source projects because contributing developers may not be well familiar with the code base.Actually, the entire processSelection method looks strange. This might well be a copy-paste error because you can also find two if blocks with identical bodies but different conditions further down in this same method. However, selectedNode won’t be null by that time and the entire call chain setViewPanel-setHelpLocation won’t lead to a NullPointerException.PVS-Studio warning: V6053 The collection is modified while the iteration is in progress. ConcurrentModificationException may occur. DWARFExpressionOpCodes.java:205Again, the analyzer is not completely right: the exception will not be thrown because the UNSUPPORTED_OPCODES collection is always empty and the loop just won’t execute at all. Besides, this collection happens to be a set, which means adding an already existing element won’t affect it. The programmer must have entered the collection name in the for-each loop using autocompletion, which suggested the wrong field, with the programmer never noticing that. A collection can’t be modified while it is being iterated, but if you are lucky enough — as in this case — the application won’t crash. In this snippet, the typo affects the program indirectly: the DWARF file parser relies on the collection to stop analysis when encountering unsupported opcodes.Starting with Java 9, it’s better to use factory methods of the standard library to create constant collections: for example, Set.of(T… elements) is not only much more convenient to use but also makes the newly created collection immutable from the start, thus enhancing reliability.PVS-Studio warnings:Something distracted the developer, and they accidentally implemented the indexOf method in such a way that it returns 0, i.e. the index of the first element of the paths collection, instead of -1 for a non-existent value. This will happen even if the collection is empty. Or maybe they generated the method but forgot to change the default return value. Anyway, the setValueAt method will refuse any offered value and show the message “Name already exists” even if there’s not a single name in the collection.By the way, the indexOf method is not used anywhere else, and its value is actually needed only to determine if the sought element exists. Rather than writing a separate method and playing around with indexes, it would probably be better to write a for-each loop right in the setValueAt method and have it return when encountering the matching element.Note: I didn’t manage to reproduce the assumed bug. Perhaps the setValueAt method is no longer used or is called only under certain conditions.PVS-Studio warning: V6033 An item with the same key ‘@’ has already been added. FilterOptions.java:45Ghidra supports data filtering in different contexts. For example, you can filter project files by name. Filtering by several keywords at a time is also supported — specifying ‘.java,.c’ with ‘OR’ mode enabled will display all files whose names contain either ‘.java’ or ‘.c’. Theoretically, you can use any special character as a separator (it’s selected in the filter settings), but in reality the exclamation point is not available.It’s very easy to make a typo in long initialization lists like that because they are often created using copy-paste, and your attention weakens quickly when reviewing such code manually. If the duplicate lines are not adjacent, there’s almost no chance of noticing the typo through manual review.PVS-Studio warnings:The hex byte viewer allows selecting the length of groups to be displayed: for example, you can set the data output format as ‘ffff ffff’ or ‘ff ff ff ff’. The setFactorys method is responsible for arranging these groups in the UI. While both the customization and representation are fine here, the loop in this method doesn’t look right at all: the remainder of division by one is always 0, which means the x coordinate will be increased at each iteration. The fact that the dataModel parameter has a groupSize property makes it even more suspicious.Are these some refactoring leftovers? Or maybe some calculations of the defaultGroupSizeSpace variable are missing? In any case, attempting to replace its value with dataModel.getGroupSize() results in the broken layout, and only the author of this code could tell us what it’s all about.PVS-Studio warning: V6007 Expression ‘zeroLengthArray’ is always false. PdbDataTypeParser.java:278This method parses the dimensions of a multi-dimensional array and returns either the remaining text or null for invalid data. The comment next to one of the validity checks says only the last dimension read may be 0. The parsing runs from right to left, which means ‘[0][1][2]’ is valid input text and ‘[2][1][0]’ is not.But, unfortunately, the loop has no condition to check if the read dimension is 0, so the parser consumes invalid data without any questions. I think it should be fixed by modifying the try block as follows:Sure, this validity criteria may have eventually turned out to be irrelevant or the programmer meant something else by their comment and it’s the first read dimension that should be checked. In any case, data validation is a critical spot of any application, and you must take it seriously. Mistakes in it may cause both relatively harmless crashes and severe security holes, data leaks, data corruption, or data loss (overlooking an SQL injection during request validation is one such example).You may have noticed that while there were many warnings issued, I’ve actually shared just a few of them. The cloc utility, which I didn’t bother to fine tune, counted about 1.25 million lines of Java code (blank lines and comments excluded). It’s just that almost all of these warnings look very much the same: some deal with missing null checks, others with legacy code no longer needed but still present. I don’t want to bore you by showing you the same types of bugs over and over again, and I did mention some of them in the beginning.To show you one last example, let’s talk about half a hundred “V6009 Function receives an odd argument” warnings in the context of unsafe usage of the substring method (CParserUtils.java:280, ComplexName.java:48, etc.) to get the part of a string following a separator. Developers often hope for the separator to be present in the string and forget that indexOf will return -1 otherwise, which is an incorrect value for substring. Of course, if the data has been validated or doesn’t come from “the outside”, the program will be much less likely to crash. But generally, these defects are potentially unsafe and we want to help eliminate them.The overall quality of Ghidra is pretty high — there are no outright screwups, which is cool. The code is mostly well formatted and the style is quite consistent: most of the variables, methods, and other entities have intelligible names, certain details are cleared up through comments, and the number of tests is huge.Of course, there were some issues, including the following:Please don’t disregard developer tools. Static analysis, just like seat belts, is not a cure-all, but it helps prevent some of the accidents before the release. After all, no one likes using bug-teeming software, do they?Check our blog for other posts about projects we have checked. We also offer a trial license and a handful of options of using the analyzer without paying for it.",java,https://medium.com/pvs-studio/nsa-ghidra-and-unicorns-b7506da52777?source=tag_archive---------23-----------------------
8 Best Popular Projects on Java,"Today, according to reliable publicly available sources, Java remains one of the most popular programming languages. Java developers are always in demand. Various large corporations use this programming language to develop and support their products known around the world.In this article, I’m going to discuss those companies and their products. Perhaps this will help you to determine the language for your next project or decide whether you want to learn Java.Java is a general-purpose programming language. It can be used to create applications in various fields: from websites to computer games, from mobile applications to full-fledged programs for corporations and scientific purposes.The creators have implemented the WORA principle: write once, run anywhere. This means that an application written in Java can be run on almost any platform, which is a great advantage.Java is also used to work with Big Data, to develop programs for scientific purposes, for example, processing natural languages, devices programming — from household devices to industrial installations.Therefore, this language traditionally has strong positions in industrial programming, in the enterprise segment.NASA World Wind belongs to the type of geographic information systems. This is a fully three-dimensional interactive virtual globe that uses NASA satellite imagery and aerial photography of the US Geological Survey to build 3D models of the Earth, Moon, Mars, Venus, and Jupiter.NASA World Wind is an open-source proprietary software. The program is written in Java and runs on all operating systems the OpenGL stack is implemented for.Initially, the program contains low-resolution maps. When zooming in some areas on the map, high-resolution images are downloaded from NASA servers.The program allows you to select the scale, direction and viewing angle, visible layers, as well as to search by geographical names. The application also has the ability to display the names of geographical objects and political boundaries.NASA World Wind has an extensible architecture. There are GPS plugins for displaying clouds, earthquakes, hurricanes in near real-time, and a number of others.Google uses Java for many of its products. Mainly for backend programming but also for user interface development.For example, Google Docs — a document management application that is very similar to Microsoft Word. But all changes here are saved automatically, and if you are authorized with Google on your devices, you can always open and work with a document everywhere without having to download it.And of course, we cannot but mention the Android OS — after all, it originates from the Java Virtual Machine. Therefore, this language also can be used for mobile application development.By the way, regarding the use of Java in Android, the Oracle company, which owns Java, has had a serious conflict with Google. You can read more about this here.Despite the popularity of Java, many Android developers are already actively switching to Kotlin — another programming language (developed by JetBrains) that runs on top of the JVM and has a number of advantages specifically for mobile development — it is much more concise and less resource-intensive.This company and its platform do not need a long introduction. Netflix, one of the largest US entertainment companies, is a provider of movies and TV shows based on streaming multimedia.The company uses Java for the back end of most of its applications, along with Python. Android and Android TV applications are almost entirely built in Java, with a slight admixture of C ++.Spotify is one of the most popular online streaming audio services in the world, allowing users legal and free listening to over 50 million songs, audiobooks, and podcasts. There are web applications, desktop applications, applications for smartphones, and many other smart devices, as well as for car media systems.Basically, Spotify uses Java and Python to implement some functionality of its web application, for example, to log and stabilize data transfer.And of course, Java is the main technology for the Spotify application on Android OS.Today LinkedIn is one of the largest social networks for searching and establishing business contacts. LinkedIn has approximately 600 million registered users representing 150+ business sectors from 200 countries.Java is one of the main languages of the server-side of the platform. It mainly helps to implement the logging functionality, messaging, and also recognize and convert links in texts.The official LinkedIn mobile app for Android OS is also developed with Java.For the Uber application, Java is one of the most important technologies along with Python, Node.js, and Go. According to the developers, they chose Java precisely because of its high performance, which is one of the main characteristics of this language. Java also has the benefits of an open-source ecosystem and integrates with external technologies such as Hadoop and other analytic tools.And of course, as in most cases presented earlier, Uber used Java as the main language in the development of its mobile application for the Android operating system.Most people know this giant for its huge online e-commerce platform. However, this is far from all that they can offer. In addition to electronic commerce, the main areas of the company include the development and delivery of cloud and streaming solutions, and they also actively develop the AI direction.Amazon is recognized as one of the largest technology companies in the world along with Google, Apple, and Facebook.Mostly, Amazon uses Java for its web services (AWS). They also prefer to write automation scripts in Java, as they are well supported, have many internal tools and environments for developing helper tools.Yes, this world-famous sandbox indie computer game is also written in Java. According to the developers, “Minecraft launcher comes with Java version 1.8.0_51, which is used by default.” However, they recommend upgrading to improve performance.Unlike many other languages, Java does not run on hardware, but directly on the JVM. In the game, Java is used primarily for building game logic, rendering, and messaging.And, if you are learning Python, you can also see 8 Python Projects to learn Python better.medium.comAll of the above proves the popularity and effectiveness of Java. Although this language has begun to slightly lose its popularity over the past couple of years, the fact that major corporations continue to use it underlines its reliability and good support.I tried to pick up completely different examples to show you how great Java capabilities are. Hope you enjoyed. If you have some more interesting examples of using Java, comment below.",java,https://medium.com/javarevisited/8-best-popular-projects-on-java-e1a663ab3cc1?source=tag_archive---------0-----------------------
Java Concepts in light of interviews,"This article will explain Java concepts for those questions which were asked in my interviews.The answer is no. In order to explain this we must view the internal working of Java data structures. Every object in java inherits two methods.“equals()” method is used to compare two objects. For e.g. to compare two strings “str1” and “str2”, equals can be used in Java like str1.equals(str2). “hashcode()” method returns hash code which is simply an integer. Data structures like HashMap and HashSet use hash code to identify the bucket. For example, by default initially in an HashSet there are 16 buckets. So a HashSet will take a modulus of hash code by total number of buckets (hash code % 16) to identify the bucket number. If hash code of an object is 20 then HashSet will place this object in bucket number 4 (20 % 16 = 4). If two objects having same hash code are placed inside the HashSet by using “add” method, then the HashSet will place both of them in the same bucket and create a Linked List among them. Similarly “get” method will also identify the bucket through the above procedure but in order to retrieve the object from the linked list, it will iterate through the linked list and call equals on each object. If equals method returns true, it will return that object.So if two equal object have different hash code then the HashSet might place them in different buckets, which will lead to erroneous behavior of HashSet.There are also other variations of this question.Can two unequal objects have same hash code? The answer is yes.Is it must that two equal objects have same hash code? The answer is yes. Its a virtual contract in Java which should not be broken otherwise insertion and retrieval(get) will not work for HashSet and HashMap.As you can see, there are various variations of this question, one must always think about the working of HashSet or HashMap before answering this question. When an interviewer ask this question, pause a moment and go through the above explained logic and you will get your answer.When a question related to Data structure is asked, think in terms of insert, retrieval (get) and delete operations.Insertion in ArrayList: In ArrayList, insertion complexity is O(n) time for worst case scenario and for average case is O(1) time. By default the size of ArrayList is 10 in Java. If the ArrayList reaches its full capacity and wants to insert an additional 11th element, it will create a new ArrayList with a larger size and copy all the elements from the original list to the new list. This operation has time complexity of O(n). But on average insertion takes O(1) time since ArrayList keeps a size variable which stores the number of inserted items in the list and when an additional item needs to be inserted into the ArrayList, it just needs to insert at the cell with an index equal to the size of the array.Retrieval in ArrayList: Getting an object with an index in an ArrayList takes O(1) time.Deletion in ArrayList: Deleting from an ArrayList takes O(n) time as shifting of elements needs to take place.Insertion in LinkedList: Insertion in LinkedList takes O(1) time since only the pointer pointing to the next element needs to be updated.Retrieval in LinkedList: Getting an object with an index from LinkedList takes O(n) time since it has to traverse the LinkedList in order to reach a certain index.Deletion in LinkedList: Deletion in LinkedList also takes O(n) time since it has to traverse the LinkedList in order to reach a certain index. In addition LinkedList has removeFirst and removeLast methods which takes O(1) time since only the first and last pointer needs to be adjusted and no shifting operation needs to take place like ArrayList.If there are more inserts then reads, then we should use LinkedList otherwise we should use ArrayList.There are mainly two types of ExceptionsIOException or ClassNotFound Exception are examples of Checked Exceptions because they can be checked at compile time. IndexOutOfBoundException or NullpointerException are examples of Unchecked Exceptions because they can only be detected at Runtime.Suppose you have a collectionThe above code will sort the list. Now suppose you have a list of students and you want to sort them according to their roll number.There can be following approaches to solve this problem:Approach 1: Class Student implements comparable interface and then Collections.sort(lstStudent) can be used to sort the list.Approach 2: An anonymous class can be used to give the implementation of the Comparator interface in the sort method of Collections API. Through anonymous class we can provide an implementation of an interface, abstract class or extend a class with a single expression at the point of use.Approach 3: Provide a lambda expression in the sort method of Collections API. It is extremely simple to write. I will recommend to memorize it because it is frequently used in day to day coding problems.As you can see, the collection is sorted using only one line of code. Since the comparator interface is a functional interface, so lambda expression can be used to provide its implementation. A functional interface is simply an interface with only one abstract method.Lambda Expression Explaination: (x,y)->x.getRollNumber() — y.getRollNumber()X and y are two parameters of compare function and the implementation of this compare function is x.getRollNumber() - y.getRollNumber().This program will throw a Nullpointer Exception at line(Car car=new Car()). Since we know that at first the parent constructor Engine() will be called by default inside the Car() constructor before carNumber is set. Inside Engine constructor “getCarInitials()” is called which is in fact overridden in the child class. Since it is overridden, Java will call the overridden method and try to get the substring of the null “carNumber” String. This will result in the Nullpointer exception.This program will print “Parameter is shape”. Java uses static or compile time binding for overloaded functions. So here the type of parameters will be checked on compile time. On the other hand, dynamic or runtime binding is used for overridden functions. Groovy language as opposed to Java, uses dynamic binding for overloaded functions.Java lists:LinkedList vs ArrayListLoad factor in hashmapCheck and unchecked exceptions in java with examplesAnonymous Classes in JavaJava Functional interfaces",java,https://medium.com/javarevisited/java-concepts-in-light-of-interviews-1b3e41c20ac2?source=tag_archive---------15-----------------------
Consumer Driven Contract Testing for gRPC + Pact.io,"Ivan Garcia Sainz-AjaJun 1, 2020·6 min readMicroservices come with great benefits: they can be self contained, loosely coupled, more maintainable and testable, independently provisioned and deployed, with distributed development and continuous delivery…But they also come with some important challenges of its own: API Integration Testing and API Evolution Management being two of them.API Integration tests is about how service integrations are tested:API Evolution & Breaking changes:These two questions are directly related with the technology we are using to implement and define our APIs as well as how we are testing them.When using gRPC (and Protobuf) we can rely on the parser and code generation to maintain backward/forward compatibility between different versions of the same API but that is only half the problem.For instance with Protobuf v3 all fields are optional, introducing a new field on the provider, outdated clients and server can still communicate but the common understanding or business logic may be definitely broken. It’s easy to see that updating the API definition on the clients will not be enough if business implementation it’s not also updated accordingly.Contract Testing is not only about API compatibility and detecting breaking changes, it is also about independent but coordinated integration testing for both consumers and provider.Contract testing with Pact.io allows you to define that shared understanding on a common document (“contract”) that can be used to test, independently but coordinated, that both provider and consumers are compatible.From Pact.io website we can read:Contract testing is a technique for testing an integration point by checking each application in isolation to ensure the messages it sends or receives conform to a shared understanding that is documented in a “contract”.With Pact.io consumers can define in a simple DSL how they plan to interact with a given service, defining their requests and the shape of responses they expect to receive.Pact.io will generate a “pact.json” file that will be used to:Pact.io is great to test REST APIs (or HTTP+JSON in general) but currently it does not have out-of-the-box support for technologies that use other protocols and serialization like gRPC that uses HTTP/2 and Protocol Buffers. For bridging that gap we have implemented an easy solution using only Java and Spring-Boot to make Pact.io+gRPC work together.While gRPC is commonly used with Protobuf over HTTP2, Pact.io works great to test HTTP + JSON APIs so we will need to proxy and transform gRPC requests in order to use Pact.io.We have successfully implemented a simple solution to use Pact + gRPC in javaland with Spring-Boot, with no extra software or dependencies required, no extra open ports and absolutely no fuss.Technology stack:The following instructions are based on this stack but you can probably leverage this ideas to port it to your platform of choice.For Pact and gRPC to work together we are going to need a way to transform Protobuf to JSON and connect gRPC to HTTP/1.1 back and forth.And this will be all we need on consumer side.If your spring-boot application already listens on port 8080 this means no extra open port will be required for this to work.In the following section you can find all implementation details you need to use Pact+gRPC for consumer driven contract testing.These will be all java classes (in bold) your need to implement this solution along with some sample classes to illustrate this example:We are going to use the following proto definition. This is just minimum example:GRPC methods will be mapped to HTTP:The following class is just a standard pact-jvm JUnit test:The only peculiarity on this class is that we are testing a gRPC stub (line 47) using a custom io.grpc.Channel (line 48) that will connect to pact http mocking server.We will connect gRPC calls to Pact mocking server to test our consumer.Inside Channel.newCall(…) (L53) we will receive all information we need to connect to pact mocking server:This is just an standard SpringRestPactRunner there is nothing really special here, just notice how you can pass authentication headers to service (line 51), that can later be forwarded to gRPC process.This is where magic happens on the provider side:We are just implementing an SpringMVC controller that can be placed in src/test/java so it’s only loaded for tests.All information we need to map http to gRPC is collected on @EventListener in line 161 where we read all MethodDescriptor to be used later when handling incoming http requests.Notice how we forward JWT credentials downstream to gRPC service.While gRPC and Protobuf can define an “strongly typed” API “contract” and we can also rely on the parser and code generation to maintain backward/forward compatibility between different versions of the same API that is only half the problem.Contract Testing can help you test your API shared meaning independently and coordinated while avoiding expensive end-to-end setups.Contract Testing brings reliability at low cost in your service integration tests.Contract Tests can replace service integration tests that ensure your provider and consumers communicate correctly while maintaining a common understanding. But they don’t replace functional tests that ensure that the core business logic of your services is working.Now, with the ideas shared on this post you can leverage Pact.io capabilities and Contract Testing benefits also for your gRPC services.",java,https://medium.com/@ivangsa/consumer-driven-contract-testing-for-grpc-pact-io-d60155d21c4c?source=tag_archive---------4-----------------------
Managing Spring boot Micro services with Kong— Part 2 (Create Springboot API),"Suraj BatuwanaJun 1, 2020·3 min readCreate a simple spring boot application and add REST interfaces to it, codeRest Controller was created with @RequestParam and @PathVariablePlease fellow the instructions given in the README.mdCheck the REST ServiceIf REST Service is working add it to KongWe can verify the connected containers to the kong-net docker network:Create the serviceService refer to the upstream APIs and microservices that Kong manages.172.18.0.2 is the address of the component container. In this case temperature-converter container.Create the routeOne route can be associated with one service, and this later can have one to many route to an upstream.Kong expose the created API at the port 8000. In this case we can visit the created route by:Above example we used exact Service URL from the Spring boot/v1/temperature/convert-to-fahrenheit/102, but in reality value 102 should be dynamic like it can be /v1/temperature/convert-to-fahrenheit/200 or even /v1/temperature/convert-to-fahrenheit/2000, so how can we handle this kind of Dynamic parameters inside KongCreate Service for Dynamic Path/Request parametersCreate Route for Dynamic Path parametersCheck Service with PathVariableCheck Service with RequestParamThere are different kind of combinations you can add when creating route with Kong like methods like GET, POST, hosts , protocol Etc..You can delete the added services and routes as fellows, When deleting make sure you delete the route firstNext step is adding security to API, which comes in my next blog",java,https://medium.com/@suraj-batuwana/managing-spring-boot-micro-services-with-kong-part-2-create-springboot-api-ef8af12e6522?source=tag_archive---------9-----------------------
Top 5 Spring Professional Certification Exam Resources for Java Developers,"Ever since Pivotal has announced that Spring training is not mandatory for becoming a Pivotal Certified Spring developer, more and more experienced Java programmers who have been using Spring for years are preparing for this prestigious certification.This is obvious because the expensive Spring training, which cost around 32000 USD in the USA and most of the western nations and about 50,000 INR in India, was the biggest hurdle in getting Spring certified.Even though the training is the best way to prepare for certification because it is totally focused on the certification exams and you will cover all topics there, it is not affordable for most Java developers around the world. Now, the cost of Spring certification has just reduced to 200 USD from the whopping 3200 USD it cost earlier. The move to allow Java developers to appear in Spring certification without mandatory training has also encouraged the popular self-study model, which is quite popular because even traditional Oracle Java certifications are based upon that.It allows programmers to prepare at their own pace and it's also not very expensive. All you need is a study guide and a Spring mock exam simulator.www.certification-questions.comEven though you can use Spring framework books to prepare for certifications, but the study guide is particularly essential. The study guide has many benefits over general books like they are focused on the certification exam. They fully cover all topics and have topic-wise assessment questions. They also do an excellent job of highlighting key concepts from the exam point of view. If you want to do well on Spring certification, it is a must that you choose a study guide and a course for your preparation, and you will find a log of excellent resources in this article.Unfortunately, there are not many resources available to prepare for Spring certifications because it is still not that popular as compared to Oracle’s Java certification. One reason for that was a few developers opting for Spring certification due to expensive mandatory training. Now that restriction is lifted, you can expect more study guides, books, and mock exams will come to help Spring developers preparing by themselves for Spring certifications. Btw, If you are an experienced Java developer and using the Spring framework for a couple of years and you think that you can pass Spring certification without much preparation, I suggest you first go through David Mayer’s Online Spring Mock test, It’s a free test, and if you can score over 80% then you are ready for the real exam. Anyway, here are a couple of useful study guides and online training courses for Spring certification currently available in the market. This list includes study guides for Spring core, Spring web, and Spring Enterprise Integration certifications.This is an excellent course on Udemy, which is specially designed for Spring certification and currently. This is a multi-part course, and each part covers one module, there are 8 modules to prepare for Spring certification. This course explains and answers all questions from the Spring Professional Certification Study Guide. Each answer to exam the topic is explained in detail with a video tutorial, description, and a good example for you to get a deep understanding of the discussed topic.With this course, you will get all the required knowledge and skills to pass the Spring Professional Certification Exam.Here is the link to join this course — Spring Professional Certification Exam TutorialThe course is divided into 8 modules:This is the first module of the entire course, and inside, you can find answers to all topics from Spring Professional Certification Study Guide's first section, which is Container, Dependency, and IoC.This is a free study guide for the Spring Core Professional Certification version 5 from Pivotal itself. It contains some frequently asked questions and information about the exam itself.It also includes puzzles that test whether you know the essential concepts or not. These Spring questions are also an excellent resource for an interview.If you are using Spring for a couple of years, then you already know the answers to most of them. They are also useful to quickly check if you are ready for Spring certification or not because the passing percentage is quite high for Spring certification. You need to get 38 out of 50 questions to pass the Spring certification. In general, If you can answer those questions along with David Mayer’s online Spring Professional Dumps, then you are ready for the real exam.This is a proper study guide for Spring core certification. It covers all topics and essential concepts. It also has topic-wise questions for Spring certification to prepare well. One book, which you can combine with Spring in Action to prepare really well for the exam.This is another free study guide from Pivotal, which contains some must-know information about Spring certification and topic-wise questions to check your understanding.It just contains topic-wise distribution and how much weigtage each topic hold in the real exam. It also contains some useful information for the exam itself like a number of questions, format, and passing score.And you can also use them for preparing Spring interviews.There are not many courses out there that cover Spring 5, which is very important if you are preparing for Spring Professional v5.0 certification.This course not only covers the Spring framework in-depth but also explains all the new features of Spring 5.0 like the Reactive Programming model and support for Java 8 and JUnit 5.Here is the link to join this course — Spring Framework 5: Beginner to GuruThat’s all about some of the best resources to prepare for Spring certifications. We have looked at some of the best study guides available for the Spring core, Spring web developer exam, and Spring Enterprise Integration Specialist exam.I have also shared a couple of books to learn the Spring framework in general before reading a study guide to focus more on exam topics. The study guide offers several benefits over the conventional publications like they cover all exam topics and also have topic-wise quizzes, multiple-choice questions to prepare better, but the courses on Udemy are also great, and you can use them in conjunction with a study guide and book to prepare better.I have also created a Spring certification practice test with 250+ questions from all exam topics which you can buy on Udemy.Other Spring Certification Resources For Java DevelopersThanks for reading this article, if you find these resources useful then please share them with your friends and colleagues. If you have any other resources which help prepare for Spring certification, then feel free to share them with us. P.S. — If you are an experienced Java developer and using the Spring framework for 3 to 5 years and want to get certified for your skill, I suggest you go through David Mayer’s Online Spring Mock test; it’s a free test. If you can score over 80%, then you are ready for the real exam.www.certification-questions.com",java,https://medium.com/javarevisited/top-5-spring-professional-certification-exam-resources-for-java-developers-3ef9fa42fe13?source=tag_archive---------0-----------------------
A Handy Guide to Optimizing Your Java Applications,"Alibaba CloudJun 1, 2020·41 min readPerformance problems are different from bugs. It’s pretty easy, relatively speaking, to analyze and solve bugs. Often, you can directly find the root cause of a bug from the application log, or to be a bit more technical, a log of a node in a distributed service. However, in contrast to all of this, the troubleshooting methods required for performance problems is a far bit more complicated.Application performance optimization can be described as a systemic project or process that requires engineers to possess a great deal of technical knowledge. A simple application not only contains the application code, but it also involves the container, or virtual machine, as well as the operating system, storage, network, and file system. Therefore, when an online application has performance problems, we need to consider many different factors and complications.At the same time, besides some performance problems caused by low-level code issues, many performance problems also lurk deep in the application and are difficult to troubleshoot. To address them, we need to have a working knowledge of the sub-modules, frameworks, and components used by the application as well as some common performance optimization tools.In this article, I will summarize some of the tools and techniques often used for performance optimization, and through doing so, I will also try to show how performance optimization works. This article will be divided into four parts. The first part will provide an overview about the idea behind performance optimization. The second part will introduce the general process involved with performance optimization and some common misconceptions. Next, the third part will discuss some worthwhile performance troubleshooting tools and common performance bottlenecks you should be aware of. Last, the fourth part will bring together the tools introduced previously to describe some common optimization methods that are focused on improving CPU, memory, network, and service performance.Note that, unless specified otherwise, thread, heap, garbage collection, and other terms mentioned in this article refer to their related concepts in Java applications.Application performance problems are different from application bugs. Most bugs are due to code quality problems, which can lead to missing or risky application functions. Once discovered, they can be promptly fixed. However, performance problems can be caused by many factors, including mediocre code quality, rapid business growth, and unreasonable application architecture designs. These problems generally require a series of time-consuming and complicated analysis processes, and many people are simply unwilling to do this work. Therefore, they are often covered up by temporary fixes. For example, if the system usage is high or the thread pool queue on a single host overflows, developers address this situation simply by adding machines to scale out the cluster. If the memory usage is high or out-of-memory (OOM) occurs during peak hours, they tackle this simply by restarting the system.Temporary fixes are like burying landmines in the application and only partially solve the initial problem. To give one example, in many scenarios, adding hosts alone cannot solve the performance problems of applications. For instance, some latency-sensitive applications require extreme performance optimization for each host. At the same time, adding hosts is prone to resource waste and is not beneficial in the long run. Reasonable application performance optimization provides significant benefits in application stability and cost accounting.This is what makes performance optimization necessary.Consider the following situation. Your application has clear performance problems, including a high CPU utilization rate. You are ready to start working out how you can optimize the system, but you may not be well prepared for the potential difficulties involved in optimizing your application. Below are difficulties you will probably face:1. The performance optimization process is unclear. If we detect a performance bottleneck and set to work to fix it immediately, we may later find that, after all our work, the issue we resolved was only one of the side effects of a greater, underlying cause.2. There is no clear process that we can use to analyze performance bottlenecks. With the CPU, network, memory, and so many other performance indicators showing problems, we don’t know which areas should we focus on and where should we start.3. Developers often do not know what sort of performance optimization tools are out there. After a problem occurs, the developer team might not know which tool they should use or what the metrics obtained from the tool actually mean.So far, there is no strictly defined process in the field of performance optimization. However, for most optimization scenarios, the process can be abstracted into the following four steps:1. Preparation: Here, the main task is to conduct performance tests to understand the general situation of the application, the general location of the bottlenecks and the identification of optimization objectives.2. Analysis: Use tools or techniques to provisionally locate performance bottlenecks.3. Tuning: Perform performance tuning based on the identified bottlenecks.4. Testing: Perform performance testing on the tuned application and compare the metrics you obtained with the metrics in the preparation phase. If the bottleneck has not been eliminated or the performance metrics do not meet expectations, repeat steps 2 and 3.These steps are illustrated in the following diagram:Among the four steps in this process, we will focus on steps 2 and 3 in the next two sections. First, let’s take a look at what we need to do during the preparation and testing phases.Preparation PhaseThe preparation phase is a critical step and cannot be omitted.First, for this phase, you need to have a detailed understanding of the optimization objects. As the saying goes, the sure way to victory is to know your own strength and that of your enemy.a. Make a rough assessment of the performance problem: Filter out performance problems caused by the related low-level business logic. For example, if the log level of an online application is inappropriate, the CPU and disk load may be high in the case of heavy traffic. In this case, you simply need to adjust the log level.b. Understand the overall architecture of the application: For example, what are the external dependencies and core interfaces of the application, which components and frameworks are used, which interfaces and modules have a high level of usage, and what are the upstream and downstream data links?c. Understand the server information of the application: For example, you must be familiar with the cluster information of the server, the CPU and memory information of the server, the operating system installed on the server, whether the server is a container or virtual machine, and whether the current application will be disturbed if the hosts are deployed in a hybrid manner.Second, you need to obtain the benchmark data. You can only tell if you have achieved your final performance optimization goals based on benchmark data and current business indicators.Testing PhaseWhen we enter this stage, we have already provisionally determined the performance bottlenecks of the application and have completed the initial tuning processes. To check whether the tuning is effective, we must perform stress testing on the application under simulated conditions.Note that Java involves the just-in-time (JIT) compilation process, and therefore warm-up may be required during stress testing.If the stress test results meet the expected optimization goals or represent a significant improvement compared with the benchmark data, we can continue to use tools to locate more bottlenecks. Otherwise, we need to provisionally eliminate the current bottleneck and continue to look for the next variable.During performance optimization, taking note of the following precautions can reduce the number of undesired wrong turns.a. Performance bottlenecks generally present an 80/20 distribution. This means that 80% of performance problems are usually caused by 20% of the performance bottlenecks. The 80/20 principle also indicates that not all performance problems are worth optimizing.b. Performance optimization is a gradual and iterative process that needs to be carried out step by step and in a dynamic manner. After recording benchmark values, each time a variable is changed, multiple variables are introduced, causing interference in observations and the optimization process.c. Do not place excessive emphasis on the single-host performance of applications. If the performance of a single host is good, consider it from the perspective of the system architecture. Do not pursue the extreme optimization in a single area, for example, by optimizing the CPU performance and ignoring the memory bottleneck.d. Selecting appropriate performance optimization tools can give you twice the results with half the effort.e. Optimize the entire application. The application needs to be isolated from the online system. A downgrade solution should be provided when new code is launched.Performance optimization seeks to discover the performance bottlenecks of applications and then to apply optimization methods to mitigate the problems causing these bottlenecks. It is difficult to find the exact locations of performance bottlenecks. To quickly and directly locate bottlenecks, we need to possess two things:So, to do a good job, we must first find the appropriate tools. But, how do we choose the right tools? And, which tools should be used in different optimization scenarios?Well, first of all, let’s take a look at the “Linux Performance Tools” diagram. Many engineers are probably familiar with this diagram created by Brendan Gregg, a system performance expert. Starting from the subsystems of the Linux kernel, this diagram lists the tools we can use to conduct performance analysis on each subsystem. It covers all aspects of performance optimization, such as monitoring, analysis, and tuning. In addition to this panoramic view, Brendan Gregg also provides separate diagrams for benchmark tools, specifically Linux Performance Benchmark Tools, and performance monitoring tools, Linux Performance Observability Tools. For more information, refer to the descriptions provided on Brendan Gregg’s website.Image source: http://www.brendangregg.com/linuxperf.html?spm=ata.13261165.0.0.34646b44KX9rGcThe preceding figure is a very informative reference for performance optimization. However, when we use it in practice, we may find that it is not the most appropriate reference for two reasons:1. It assumes a high level of analysis experience. The preceding figure shows performance metrics from the perspective of Linux system resources. This requires the developer to be familiar with the functions and principles of various Linux subsystems. For example, when we encounter a performance problem, we cannot try all the tools under each subsystem. In most cases, we suspect that a subsystem has a problem and then use the tools listed in this diagram to confirm our suspicions. This calls for a good deal of performance optimization experience.2. The applicability and completeness of the diagram are not great. When you analyze performance problems, bottom-up analysis from the underlying system layer is inefficient. Most of the time, analysis on the application layer is more effective. The Linux Performance Tools diagram only provides a toolkit from the perspective of the system layer. However, we need to know which tools we can use at the application layer and what points to look at first.To address these difficulties, a more practical performance optimization tool map is provided below. This map applies to both the system layer and application layer, including the component layer. It also lists the metrics that we need to pay attention to first when analyzing performance problems. The map also highlights the points most likely to produce performance bottlenecks. Note that less-used indicators and tools are not listed in the graph, such as CPU interruption, index node usage, and I/O event tracking. Troubleshooting these uncommon problems is complicated. So, in this article, we will only focus on the most common problems.Compared with the preceding Linux Performance Tools diagram, the following figure has several advantages. By combining specific tools with performance metrics, the distribution of performance bottlenecks is described from different levels simultaneously, which makes the map more practical and operational. The tools at the system layer are divided into four parts: CPU, memory, disk, including the file system, and the network. The toolkit is basically the same as that in the Linux Performance Tools diagram. The tools in the component layer and application layer are made up of tools provided by JDK, trace tools, “dump” analysis tools, and profiling tools.Here, we will not describe the specific usage of these tools in detail. You can use the man command to obtain detailed instructions for using the tools. In addition, you can also use info to query the command manual. info can be understood as a detailed version of man. If the output of man is not easy to understand, you can refer to the info document. Here involves many commands, which actually do not need to be remembered.How to Use This Graph of Performance Optimization ToolsFirst, although the distribution of bottlenecks is described from the perspectives of systems, components, and applications, these three elements complement each other and affect each other in actual operations. The system provides a runtime environment for applications. A performance problem essentially means that system resources have reached their upper limits. This is reflected in the application layer, where the metrics of applications or components begin to decline. Improper use or design of applications and components can also accelerate the consumption of system resources. Therefore, when analyzing the bottleneck point, we need to combine the analysis results from different perspectives, find their commonalities, and reach a final conclusion.Second, we recommend that you start from the application layer and analyze the most commonly used indicators marked in the map to find the points that are most important, most suspicious, and most likely to impact performance. After reaching preliminary conclusions, go to the system layer for verification. The advantage of this approach is due to the fact that many performance bottlenecks are reflected in the system layer and presented in multiple variables. For example, if the GC metric at the application layer is abnormal, it can be easily observed by the tools provided by JDK. However, it is reflected in the system layer through abnormal current CPU utilization and memory indicators. This can lead to confusion in our analysis.Finally, if the bottleneck points present as a multi-variable distribution at both the application layer and the system layer, we recommend that you use tools such as ZProfiler and JProfiler to profile the application. This allows you to obtain the overall performance information of an application. Note that, here, profiling refers to the use of event-based statistics, statistical sampling, or byte-code instrumentation during application runtime. After profiling, we can then collect the application runtime information to perform a dynamic analysis of application behavior. For example, we can sample CPU statistics and bring in various symbol table information to see the code hotspots in an application over a period of time.Now, let’s go on to discuss the core performance metrics that we need to pay attention to at different analysis levels, and how to preliminarily determine whether the system or application has performance bottlenecks based on these metrics. Then, we will talk about how to confirm bottleneck points, the cause of the bottleneck points, and the relevant optimization methods.CPU-related metrics are CPU utilization, load average, and context switch. Common tools include top, ps, uptime, vmstat, and pidstat.The first line shows the current time, system running time, and the number of currently logged-on users. The three numbers following “load average” represent the average load over the last 1 minute, 5 minutes, and 15 minutes in sequence. Load average refers to the average number of processes in the running state (that is, processes using CPUs or waiting for CPUs in the R state) and non-interrupted state (or D state) per unit time. This is actually the average number of active processes. It’s important to recognize that the CPU load average is not directly related to the CPU utilization.The third row indicates the CPU utilization metric. You can use man to view the meaning of each column. The CPU utilization shows the CPU usage statistics per unit time and is displayed as a percentage. The calculation method for this is: CPU utilization = 1 — (CPU idle time)/Total CPU time. Note that the CPU utilization obtained from performance analysis tools is the average CPU usage during the sampling period. Note also that the CPU utilization displayed by the top tool adds up the utilization rates of all CPU cores. This means that the CPU utilization of 8 cores can reach 800%. You can also use htop or other new tools for the same purpose.Use the vmstat command to view the ""context switching times"" metric. As shown in the following table, a set of data is output every 1 second:The context switch (cs) metric in the above table indicates the number of context switches per second. Based on different scenarios, CPU context switches can be divided into interrupted context switches, thread context switches, and process context switches. However, in all of these cases, excessive context switching consumes CPU resources in the storage and recovery of data such as registers, kernel stacks, and virtual memory. As such, the actual running time of the process is shortened, resulting in a significant reduction in the overall performance of the system. In the vmstat output, us and sy, which represent the user-mode and kernel-mode CPU utilization, are important references.The vmstat output only reflects the overall context switching situation of the system. To view the context switching details of each process, such as voluntary and involuntary switching, you need to use pidstat. This command can also be used to view the user-mode and kernel-mode CPU utilization of a process.Causes of Abnormal CPU-related Metrics1. CPU utilization: If we find that the CPU utilization of the system or application process remains very high, such as more than 80% for a single core, for a certain period of time, we need to be on guard. We can use the jstack command multiple times to dump the application thread stack in order to view the hotspot code. For non-Java applications, we can directly use perf for CPU sampling and then perform the offline analysis of the sampled data to obtain CPU execution hotspots. Java applications require stack information mapping for symbol tables, and therefore we cannot directly use perf to obtain the result.2. CPU load average: When the average load is higher than 70% for the number of CPU cores, this means that bottlenecks occur in the system. There are many possible causes of high CPU loads. We will not go into detail here. You must note that monitoring the trends of load average changes makes it easier to locate problems. Sometimes, large files are loaded, which leads to an instantaneous increase in the load average. If the load averages for 1 minute, 5 minutes, and 15 minutes do not differ much, the system load is stable and we do not need to look for short-term causes. This indicates that the load is increasing gradually. Therefore, we need to then switch our attention to the overall performance.3. CPU context switching: There is no specific empirical value recommended for context switching. This metric can be in the thousands or tens of thousands. The specific metric value depends on the CPU performance of the system and the current health of the application. However, if the number of system or application context switches increases by an order of magnitude, then a performance problem is very likely. One possible issue could be an involuntary increase in upstream and downstream switches. This sort of problem would indicate that there are too many threads, and they are competing for CPUs.These three metrics are closely related. Frequent CPU context switching may lead to an increased average load. The next section describes how to perform application tuning based on the relationships among the three indicators.Some CPU changes can also be detected from the thread perspective, but we must remember that thread problems are not only related to CPUs. The main thread-related metrics are as follows, and all of them can be obtained through the jstack tool that comes with JDK:Important Thread ExceptionsTo understand threat exception, you’ll need to do the following:1. Check whether the total number of threads is too high. Too many threads cause frequent context switching in the CPU. At the same time, these threads consume a great deal of memory. The appropriate number of threads is determined by the application itself and the host configurations.2. Check whether the thread status is abnormal. Check whether there are too many waiting or blocked threads. This occurs when too many threads are set or the competition for locks is fierce. In addition, perform comprehensive analysis based on the application’s internal lock usage.3. Check whether some threads consume a large amount of CPU resources based on the CPU utilization.Memory-related metrics are listed below, and common analysis tools include top, free, vmstat, pidstat, and JDK tools.1. System memory usage indicators, including the free memory, used memory, and available memory and cache or buffer.2. Virtual memory, resident memory, and shared memory for processes, including Java processes.3. The number of page faults of the process, including primary page faults and secondary page faults.4. Memory size and Swap parameter configurations for swap-in and swap-out.5. JVM heap allocation and JVM startup parameters.6. JVM heap recycling and Garbage Collection conditions.free allows you to view the system memory usage and Swap partition usage. The top tool can be used for each process. For example, you can use the top tool to view the resident memory size (RES) of Java processes. Together, both tools can cover most memory metrics. The following is the output of the free command:We’re not going to describe what the above column results indicate here. They should be relatively easy to understand.Next, let’s take a look at the swap and buff/cache metrics.Swap uses a local file or disk space as the memory and includes two processes: swap-out and swap-in. Swap needs to read data from and write data to disks, and therefore its performance is not very high. In fact, we recommend that you disable Swap for most Java applications, such as ElasticSearch and Hadoop, because memory costs have been decreasing. This is also related to the JVM garbage collection process. The JVM traverses all the heap memory used during garbage collection. If this memory is swapped out, disk I/O occurs during traversal. Increases in the Swap partition are generally strongly correlated to the disk usage. During specific analysis, we need to comprehensively analyze the cache usage, the “swappiness” threshold, and the activity of anonymous pages and file pages.Buff/cache indicates the sizes of the cache and buffer. A cache is a space for temporarily stored data generated when a file is read from or written to a disk. It is a file-oriented storage object. You can use cachestat to view the read and write cache hits for the entire system and use cachetop to view the cache hits of each process. A buffer is a temporary data storage for data written to a disk or directly read from a disk. It is intended for block devices. The free command outputs the total of these two metrics. You can use the vmstat command to distinguish between the cache and the buffer and see the size of the Swap partitions swapped in and out.Now that we understand the common memory metrics, here are some common memory problems to look out for:a. Insufficient free or available system memory. This means that a process occupies too much memory, the system itself does not have enough memory, or that there is memory overflowb. Memory reclamation exceptions. This may be memory leakage, which is the process where memory usage continues to rise over a period of time, and an abnormal garbage collection frequency.c. Excessive cache usage during reading or writing of large files, and a low cache hit ratio.d. Too many page faults. This would be probably due to frequent I/O reads.e. Swap partition usage exceptions. This would be likely due to high consumption.Analysis of Memory AbnormalitiesNow, we need to analyze the memory abnormities:Consider this. If the cache or buffer usage is low, after the impact of the cache or buffer on memory is excluded, run vmstat or sar to observe the memory usage trend of each process. Then, if you find that the memory usage of a process continues to rise, for a Java application, you can use jmap, VisualVM, heap dump analysis, or other tools to observe the memory allocation of objects. Alternatively, you can run jstat to observe the memory changes of applications after garbage collection. Next, considering the business scenario, locate the problem, such as a memory leakage, invalid garbage collection parameter configuration, or abnormal service code.When analyzing disk-related issues, we can generally assume these issues have something to do with the file system. Therefore, we do not distinguish between disk-related and file system related issues. The following metrics to be listed below are related to disks and file systems. iostat and pidstat are common monitoring tools. The former applies to the entire system, whereas the latter can monitor the I/O of specific processes.a. Disk I/O utilization: indicates the percentage of time a disk takes to process I/O.b. Disk throughput: indicates the amount of I/O requests per second, in KB.c. I/O response time: indicates the interval between sending an I/O request and receiving the first response to this request, including the waiting time in the queue and the actual processing time.d. IOPS (Input and Output per Second): indicates the number of I/O requests per second.e. I/O wait queue size: indicates the average I/O queue length. Generally, the shorter the better.When you use iostat, the output is as follows:In the preceding output, %util indicates the disk I/O utilization, which may exceed 100% as the CPU utilization may in the case of parallel I/O. rkB/s and wkB/s respectively indicate the amount of data read from and written to the disk per second, namely, the throughput, in KB. The metrics for the disk I/O processing time are r_await and w_await, which respectively indicate the response time after a read or a write request is processed. svctm indicates the average time required to process I/O, but this metric has been deprecated and has no practical significance. r/s and w/s are the IOPS metrics, indicating the number of read requests and write requests sent to the disk per second, respectively. aqu-sz indicates the length of the wait queue.Most of the output of pidstat is similar to that of iostat, the only difference being that we can view the I/O status of each process in real time.Determining Whether Disk Metrics Are AbnormalNext are the steps for determining whether disk metrics are abnormal or not.When the disk I/O utilization exceeds 80% for a sustained period of time or the response time is too long. For SSDs, the response time ranges from 0.0x to 1.x milliseconds, and 5 to 10 milliseconds for mechanical disks, which generally indicates a performance bottleneck in disk I/O.If %util is large while rkB/s and wkB/s are small, a large number of random disk reads and writes exist. In this case, we recommend that you optimize random reads and writes to sequential reads and writes. You can use strace or blktrace to check the I/O continuity and determine whether read and write requests are sequential. For random reads and writes, pay attention to the IOPS metric. For sequential reads and writes, pay attention to the throughput metric.Last, if the avgqu-sz value is large, it indicates that many I/O requests are waiting in the queue. Generally, if the queue length of a disk exceeds 2, this disk is considered to have I/O performance problems.The term “network” is a broad term and may refer to several different things, and as a consequence also involves several different metrics to measure performance at the application, transport, and network layer, as well as the network interface layer. Here, we use “network” to refer to the network part at the application layer. The common network indicators are as follows:a. Network bandwidth: This refers to the maximum transmission rate of the link.b. Network throughput: This refers to the volume of data successfully transferred per unit time.c. Network latency: This refers to the amount of time from sending a request to receiving the first remote response.d. The number of network connections and errors.Generally, the following network bottlenecks can be found at the application layer:a. The network bandwidth of the cluster or data center is saturated. This causes an increase in the application QPS or TPS.b. The network throughput is abnormal. If a large amount of data is transmitted through the interface, the bandwidth usage is high.c. An exception or error occurred in the network connection.d. A network partition occurred.Bandwidth and network throughput are metrics that apply to the entire application and can be directly obtained by monitoring the system. If these metrics increase significantly over a period of time, a network performance bottleneck will occur. For a single host, sar can be used to obtain the network throughput of network interfaces and processes.We can use ping or hping3 to check for network partitioning and specific network latency. For applications, we pay more attention to end-to-end latency. We can obtain the latency information of each process through the trace log output after middleware tracking.We can use netstat, ss, and sar to obtain the number of network connections or network errors. Excessive network connections result in significant overhead because they occupy both file descriptors and the cache. Therefore, the system can only support a limited number of network connections.We can see that several tools, such as top, vmstat, and pidstat, are commonly used to analyze the CPU, memory, and disk performance metrics. Some common tools are given below:Most of these above tools are used to view system-layer metrics. At the application layer, in addition to a series of tools provided by JDK, some commercial products are available, such as gceasy.io for analyzing garbage collector logs and fastthread.io for analyzing thread dump logs.To troubleshoot online exceptions for Java applications or analyze application code bottlenecks, we can also use Alibaba’s open-source Arthas tool. So, let’s take a closer look at this powerful tool.Alibaba Cloud’s Arthas is mainly used for the real-time diagnosis of online applications and can be used to solve problems such as online application exceptions that need to be analyzed and located online. At the same time, some tracing tools for method calls provided by Arthas are also very useful for troubleshooting problems such as slow queries. Arthas has the following features:You must note that performance tools are only a means to solve performance problems. Therefore, you only need to understand the general usage of common tools. Therefore, it is advised to not put too much effort into learning about tools that you do not actually need.After detecting abnormal metrics through tools and initially locating the bottlenecks, you need to confirm and address the problem. Here, we provide some performance tuning routines that you can learn from. We will look at how to determine the most important performance metrics, locate performance bottlenecks, and finally perform performance tuning. The following sections are divided into code, CPU, memory, network, and disk performance optimization practices. For each optimization point, we will present a system routine to help you move from theory to practice.In the case that you encounter performance problems, the first thing we’ll need to do is to check whether the problem is related to business code. We may not be able to solve the problem by reading the code, but we can eliminate some low-level errors related to the business code by examining logs or the code. The best ground for performance optimization is within the application.For example, we can check the business log to see whether there are a large number of errors in the log content. Most performance problems at the application layer and the framework layer can be found in the log. If the log level is not properly set, this can result in chaotic online logging. In addition, we must check the main logic of the code, such as the unreasonable use of for loops, NPE, regular expressions, and mathematical calculations. In these cases, you can fix the problem simply by modifying the code.It’s important to know that you should not directly associate performance optimization with caching, asynchronization, and JVM optimization without analysis. Complex problems sometimes have simple solutions. The 80/20 principle is still valid in the field of performance optimization. Understanding some basic common code mistakes can accelerate the problem analysis process. When working to address bottlenecks in the CPUs, memory, or JVM, you may be able to find a solution in the code.Next, we will look at some common coding mistakes that can cause performance problems.These are merely some of the potential code optimizations. We can also extract some common optimization ideas from these code optimizations:a. Change space into time: Use memory or disks instead of the more valuable CPU or network resources, for example, by using a cache.b. Change time into space: Conserve memory or network resources at the expense of some CPU resources. For example, we can switch a large network transmission into multiple smaller transmissions.c. Adopt other methods, such as parallelization, asynchronization, and pooling.We should also pay more attention to the CPU load. High CPU utilization alone is generally not a problem. However, CPU load is one key factor for determining the health of system computing resources.High CPU Utilization and High Load AverageThis situation is common in CPU-intensive applications. This means that a large number of threads are in the ready state and I/O is low. Common situations that consume a large amount of CPU resources include:Common troubleshooting approaches for high CPU usage: If we use jstack to print thread stacks more than five times, we can usually locate the thread stacks with high CPU consumption. Alternatively, we can use profiling, specifically event-based sampling or tracking, to obtain an on-CPU flame graph for the application over a period of time, allowing us to quickly locate the problem.Another possible situation is that frequent garbage collection, including Young garbage collection, Old garbage collection, and Full garbage collection, occurs in the application, which in turns increases the CPU utilization and load. For troubleshooting purposes, use jstat-gcutil to continuously output the garbage collection count and time statistics for the current application. Frequent garbage collection increases the CPU load and generally results in insufficient available memory. So, for this, run the free or top command to view the available memory of the current host.If CPU utilization is too high, a bottleneck may occur in the CPU itself. In addition, we can use vmstat to view detailed CPU utilization statistics. A high user-mode CPU utilization (us) indicates that user-mode processes occupy a large amount of the CPU. If this value is greater than 50% for a long time, look for performance problems in the application itself. High kernel-mode CPU utilization (sy) indicates that the kernel-mode consumes a large amount of CPU resources. If this is the case, then we recommend that you check the performance of kernel threads or system calls. If the value of us + sy is greater than 80%, more CPUs are required.Low CPU Utilization and High Load AverageIf CPU utilization is not high, it means that our application is not busy with computing but running other tasks. Low CPU utilization and high load average is common in I/O-intensive processes. After all, the load average is the sum of processes in the R state and D state. If R-state processes are removed, we are left with only D-state processes. The D-state is usually caused by waiting for I/O, such as disk I/O and network I/O.Troubleshooting and verification methods: Use vmstat 1 to regularly output system resource usage. Observe the value of the %wa(iowait) column, which identifies the percentage of the disk I/O wait time in the CPU time slice. If the value is greater than 30%, the degree of disk I/O wait is severe. This may be caused by a large volume of random disk access or direct disk access, such as when no system cache is used. The disk may also have a bottleneck. This can be verified by using the iostat or dstat output. For example, when %wa(iowait) rises and there is a large number of disk read requests, the problem may be caused by disk reads.In addition, time-consuming network requests, network I/O, can also increase the average CPU load, such as slow MySQL queries and the use of RPC interfaces to obtain interface data. To troubleshoot this problem, we need to comprehensively analyze the upstream and downstream dependencies of the application and the trace logs produced by middleware tracking.Increase in CPU Context SwitchesUse vmstat to check the number of system context switches and then use pidstat to check the voluntary context switching (cswch) and involuntary context switching (nvcswch) situation of the process. Voluntary context switching is caused by the conversion of the internal thread status of the application, such as calling sleep(), join(), wait(), or using a lock or synchronized lock structure. Involuntary context switching is triggered when the thread uses up the allocated time slices or is scheduled by the scheduler due to the execution priorities.If the number of voluntary context switches is high, then this means that the CPU is waiting for resource acquisition, which can be due to insufficient I/O, memory, or other system resource insufficiencies. And, if the number of involuntary context switches is high, the cause may be an excessive number of threads in the application, which leads to fierce competition for CPU time slices and frequent forcible scheduling by the system. Such a situation can be evidenced by jstack's thread count and thread status distribution.Memory is divided into system memory and process memory, of which process memory for Java application processes is included. Generally, most memory problems occur in the process memory, and relatively few bottlenecks are caused by system resources. For Java processes, the built-in memory management capability automatically solves two problems: how to allocate memory to objects and how to reclaim the memory allocated to objects. The core of memory management is the garbage collection mechanism.Although garbage collection can effectively prevent memory leakage and ensure the effective use of memory, it is not a universal solution. Unreasonable parameter configurations and code logic still cause a series of memory problems. In addition, early garbage collectors had poor functionality and collection efficiency. Too many garbage collection parameter settings were highly dependent on the tuning experience of developers. For example, improper setting of the maximum heap memory may cause a series of problems, such as heap overflow and heap shock.Let’s look at some common approaches to solving memory problems.Insufficient System MemoryJava applications generally monitor the memory level of a single host or cluster. If the memory usage on a single host is greater than 95% or the cluster memory usage is greater than 80%, this might indicate a memory problem. Note that, here, the memory level is for the system memory.Except for some extreme cases, high memory usage results from insufficient system memory, and the problem is usually caused by Java applications. When we run the top command, we can see the actual memory usage of the Java application process. Here, RES indicates the resident memory usage of the process, and VIRT indicates the virtual memory usage of the process. The memory consumption relationship is as follows: VIRT > RES > Heap size that is actually used by the Java application. In addition to heap memory, the memory usage of the Java process as a whole also includes the method area, metaspace and JIT cache. The main components include: memory usage of Java applications = Heap + Code cache + Metaspace + Symbol tables + Thread stacks + Direct buffers + JVM structures + Mapped files + Native libraries + so on.You can run the jstat-gc command to view the memory usage of the Java process. The output metrics can display the usage of each partition and metaspace of the current heap memory. The statistics and usage of direct buffers can be obtained by using Native Memory Tracking (NMT, introduced by HotSpot VM Java8). The memory space used by the thread stack can be easily ignored. Although the thread stack memory adopts lazy loading, the memory is not allocated directly by using the +Xss size. However, too many threads may cause unnecessary memory consumption. As such, we can use the jstackmem script to count the overall thread usage.Troubleshooting methods for insufficient system memory:Java Memory OverflowA memory overflow occurs when an application creates an object instance and the required memory space is greater than the available memory space of the heap. There are many types of memory overflows, which usually are indicated by the OutOfMemoryError keyword in the error log. Below are the common memory overflow types and their corresponding analysis methods:1. java.lang.OutOfMemoryError: Java heap space. The cause of this error is that objects cannot be allocated in the heap (the young generation and the old generation), the references of some objects have not been released for a long time and the garbage collector cannot reclaim these objects, or a large number of Finalizer objects are used and these objects are not in the garbage collection reclamation period. Generally, heap overflow is caused by memory leakage. If you confirm that no memory leak occurs, the solution is to increase the heap memory.2. java.lang.OutOfMemoryError: GC overhead limit exceeded. The cause for this error is that the garbage collector uses more than 98% of the GC time, but less than 2% of the heap memory is collected. This is generally due to memory leakage or insufficient heap space.3. java.lang.OutOfMemoryError: Metaspace or java.lang.OutOfMemoryError: PermGen space. For this type of error, troubleshooting can be done as follows. Check whether dynamic classes are loaded but not promptly uninstalled, whether there is a large volume of string constant pooling, and whether the set permanent generation or metaspace is too small.4. java.lang.OutOfMemoryError: unable to create new native Thread. The cause of this error is that the virtual machine cannot request sufficient memory space when expanding the stack space. You can reduce the size of each thread stack and the total number of threads for the application. In addition, the total number of processes or threads created in the system is limited by the system's idle memory and operating system, so we need to carefully inspect the situation. Note that this stack overflow is different from StackOverflowError. The latter occurs because the method calling layer is too deep and the allocated stack memory is insufficient to create a new stack frame.Other OutOfMemoryError types, such as Swap partition overflow, local method stack overflow, and array allocation overflow, are not described here because they are less common.Java Memory LeakageJava memory leakage is a nightmare for developers. Memory leakage is different from memory overflow, which is simple and easy to detect. Memory leakage occurs when the memory usage increases and the response speed decreases after an application runs for a period of time. This process eventually causes the process to freeze.Java memory leaks may result in insufficient system memory, suspended processes, and OOM. For this, there are only two troubleshooting methods:In addition, as the heap memory continues to grow, we recommend that you dump a snapshot of the heap memory for subsequent analysis. Although snapshots are instantaneous values, they do have a certain significance.Garbage CollectionGarbage collection metrics are important for measuring the health of Java process memory usage. Core garbage collection metrics include the following: the frequency and occurrences of GC Pauses, including MinorGC and MajorGC, and the memory details for each collection. The former can be directly obtained by using the jstat tool, but the latter requires an analysis of garbage collection logs. Note that FGC/FGCT in the jstat output column indicates the number of GC Pauses (Stop-the-World) during an old-generation GC. For example, for the CMS garbage collector, the value is increased by two each time old-generation garbage is reclaimed. For the initial marking and re-marking stages of Stop-the-World, this statistical value is 2.So, when is garbage collection tuning required? The answer to this depends on the specific situation of the application, such as its response time requirements, throughput requirements, and system resource restrictions. Based on our experience, if the garbage collection frequency and time greatly increase, the average GC pause time exceeds 500 milliseconds, and the Full GC execution frequency is less than one minute, this indicates that garbage collection tuning is required.Due to the wide variety of garbage collectors, garbage collection tuning policies vary depending on the application. Therefore, the following only describes several common garbage collection tuning policies.1. Select an appropriate garbage collector. You’ll need to select an appropriate garbage collector based on the latency and throughput requirements of the application and the characteristics of different garbage collectors. We recommend that you use G1 to replace the CMS garbage collector. The performance of G1 is being gradually improved. On hosts with 8 GB memory or less, the performance of G1 is about the same or even a bit higher than that of CMS. Parameter adjustment is convenient for G1, while the CMS garbage collector parameters are too complex and may cause space fragmentation and high CPU consumption. This can cause the CMS garbage collector to cease operation. The ZGC garbage collector introduced in Java 11 can be used for concurrent marking and collection in all stages. It looks like a promising tool.2. Set a proper heap memory size. The heap size cannot be too large. To avoid system memory exhaustion, we recommend that the heap size not exceed 75% of the system memory. The maximum heap size must be consistent with the initialized heap size to avoid heap shock. The size setting of the young generation is critical. When we adjust the garbage collection frequency and time, we are usually adjusting the size of the young generation, including the proportions of the young and old generations and the proportions of the Eden and Survivor areas in the young generation. When setting these ratios, we must also consider the ages of objects for promotion in each generation. Therefore, a lot of things need to be considered throughout the process. If the G1 garbage collector is used, it is much easier to decide on the young generation size because the adaptive policy determines the collection set (CSet) each time. The adjustment of the young generation is the core of garbage collection tuning and is highly dependent on experience. However, in general, if the young garbage collection frequency is high, this means that the young generation is too small or the configurations of the Eden and Survivor areas are unreasonable. Alternatively, if the young garbage collection is time-consuming, this means that the young generation is too large.3. Reduce the frequency of Full GC. If frequent Full GC or old-generation GC occurs, a memory leak is likely to have occurred, causing objects to be held for a long time. This problem can be located quickly by dumping a memory snapshot for analysis. In addition, if the ratio of the young generation to the old generation is inappropriate, objects are frequently assigned to the old generation and Full GC may also occur. If this is the case, it is necessary to analyze both the business code and memory snapshot.Also, configuring garbage collection parameters can help us obtain a lot of key information required for garbage collection tuning. This includes configuring -XX:+PrintGCApplicationStoppedTime, -XX:+PrintSafepointStatistics, and -XX:+PrintTenuringDistribution. These parameters allow us to obtain the distribution of GC pauses, the time consumption of safepoints, and the distribution of ages of objects for promotion, respectively. In addition, -XX:+PrintFlagsFinal can be used to help us understand the final garbage collection parameters.Below are some steps you can take in Disk I/O Troubleshooting:An increase in %wa(iowait) does not necessarily indicate a bottleneck in the disk I/O. This value indicates the proportion of the time used for I/O operations on the CPU. A high value is normal if the main activity of the application process is I/O during a given period.Possible Reasons for Network I/O Bottlenecksa. Too many objects are transmitted at a time, which may cause slow request response times and frequent garbage collection.b. Improper network I/O model selection leads to low overall QPS and long response times for the application.c. The thread pool setting for RPC calls is inappropriate. We can use jstack to compile statistics on thread distribution. If there are many threads in the TIMED_WAITING or WAITING state, we need to look into the situation carefully. For example, if the database connection pool is insufficient, this will affect the thread stack because many threads will compete for a connection pool lock.d. The set RPC call timeout is inappropriate, resulting in many request failures.For Java applications, the thread stack snapshot is very useful. In addition to its use to troubleshoot thread pool configuration problems, snapshots allow us to begin troubleshooting from the thread stack in other scenarios, such as high CPU usage and slow application response.This section provides several commands we can use to quickly locate performance problems.1. View the system’s current network connections.2. View the top 50 objects in a heap to locate memory leaks.3. List the top 10 processes by CPU or memory usage.4. Display the overall CPU utilization and idle rate of the system.5. Count the number of threads by thread state (in the enhanced version).6. View the 10 machine stacks that consume the most CPU resources.We recommend that you use the show-busy-java-threads script, which allows you to quickly troubleshoot Java CPU performance problems when the top us value is too high. This script automatically identifies the threads that consume a large amount of CPU resources in the running Java process and prints out their thread stacks. This allows us to identify the method calls that cause performance problems. This script is already used in Alibaba's online O&M environment. You can find the script here.7. Generate a flame graph, which requires you to install perf, perf-map-agent, and flushgraph:8. List the top 10 processes by the usage of Swap partitions9. Compile JVM memory usage and GC status statistics10. Other common commandsPerformance optimization is a big field, which would take dozens of articles to explain. In addition to the content discussed here, application performance optimization also involves frontend optimization, architecture optimization, distributed and cache usage, data storage optimization, and code optimization, such as design mode optimization. Due to space limitations, we could not discuss all these topics here. However, I hope this article will inspire you to learn more. This article is based on my personal experience and knowledge, and surely it is not perfect. Any corrections or additions are welcome.Performance optimization is a comprehensive task. We need to constantly practice it, make use of relevant tools and experience, and continuously improve our approach to form an optimization methodology that works for us.In addition, although performance optimization is very important, do not put too much effort into optimization too early. Of course, sound architecture design and code are necessary. But, premature optimization is the root of all negatives. On the one hand, premature optimization may not be suitable for rapidly changing business needs and even obstruct the implementation of new requirements and new functions. On the other hand, premature optimization increases the complexity of applications, reducing application maintainability. The appropriate time and extent of optimization are questions that must be carefully considered by the many parties involved.www.alibabacloud.com",java,https://medium.com/@alibaba-cloud/a-handy-guide-to-optimizing-your-java-applications-7e254110f583?source=tag_archive---------17-----------------------
ElasticSearch On Steroids With Avro Schemas,"Felix KuestahlerJun 2, 2020·16 min readThe following article explains an approach to use modern data storage and serialization technologies to decouple components and stop the explosion of service interface versions inherent in large-scale data consumer applications of an enterprise.Being responsible in a large financial institute for a heavily used data provider system, I’m always confronted with endless deployment iterations of new service versions and the inability of multiple consumers to move to the new version in a decent timeframe. This results over time in a landscape of a multitude of parallel versions running for a service.A word of caution: The approach is based on a NoSQL data storage infrastructure and, I’m fully aware that such data storage may not be suited for all kinds of business applications. Nevertheless, there are more than enough use cases which would fit nicely in such type of architecture.Our tutorial covers the following scenario:We have a Service Component which processes a user input provided via a React-based browser app to persist its data in an ElasticSearch Cluster-based persistency store.A key feature of the Avro Message is that it is self-describing via it’s associated Avro Schema. The Avro Schema is a JSON-based definition of the message structure.Refer to the below simple Avro Schema.The example outlines already some specifics of the Avro Schema definition language.Optional values are always expressed as unions, and to be prepared for seamless Schema Evolution (more on that later), you should always define a default value of optional attributes.The Schema is used by the Avro Serializer and Deserializer to parse an Avro binary message (or an Avro JSON message depending on your configuration chosen) into Java Data Access Objects (DAO’s). These DAO’s may be generic or pre-compiled strong typed schema Java classes.I.e., the Maven goal generate-sourcesgenerates a class Personwho has three attributes personId, lastName and firstname The person object can then be instantiated out of a binary message of this type by using the Avro parser.To be self-describing, an Avro Message may be complemented with the Schema itself or a Schema Fingerprint.Provisioning of the full schema JSON is usually done in file-based approaches, which bundles a large amount of Avro Messages. In our interaction journey — a request-response style with a single Avro Messages — such an approach would be too much of an overhead. For this scenario, the Schema fingerprint is the right approach.The Avro Schema Fingerprint is a globally unique identifier for referencing the correct schema in a central Schema Registry.The below abstract SchemaRegistry class describes the methods required by such a Registry.That’s it, having a global Schema Registry established which can be used by readers or writers of Avro messages to exchange a schema fingerprint for the associated JSON schema you are ready to exploit the full power of Avro.The tutorial provides two implementations of a Schema Registry:The below example shows the ElasticSearch registered schema under its fingerprint 8354639983941950121 in the index avroschema. Any writer or reader of messages will use Avro fingerprints for referencing its underlying schema used in its processing.Using ElasticSearch, you will get out of the box a central repository of the schema definitions, which can be easily enhanced with lookup- and query capabilities used by business analysts, designers, or developers during the system development time.You are wrong if you think there is a lot of coding necessary to implement a Schema Registry.Below you see the full functioning ElasticSearchSchemaRegistry java class.Pretty straightforward and compact. You could argue now the ESPersistencyManager class hides the complexity (which is used and implemented as part of the tutorial).Well not really, the class is an ultra-light layer around Jest, an HTTP Java client for Elasticsearch. While Elasticsearch provides its own native Java client, Jest provides a more fluent API and a more natural interface for working. Interested in JEST, check out Blaedung’s tutorial article.Our ESPersisencyManager just shields our tutorial classes from direct Jest exposures. An encapsulation technique used to be capable of replacing the persistency manager in a future version.Again very compact to get an Avro Schema persisted. It’s worthwhile to mention that you can provide ElasticSearch your own primary key (in ES called the _id). In our tutorial, we use the global unique fingerprint as the primary identifier in ES, which makes the lookup straightforward (see screenshot above). It’s a key feature to provide your primary key, especially when using ElasticSearch in a data replication scenario, where ES is only the slave of another master system, which already has primary keys generated.As you can imagine retrieving a JSON object back is even simpler Jest does the job for us.We should now have a pretty good understanding of what must be prepared and configured by the Backend component using our approach.The diagram below outlines the most important steps:Let’s check out what the Service Component has to prepare.The last step -resolving a schema based on the “pre-configured static” fingerprint-must be understood in detail.The Schema fingerprint represents the message version number, which either the ServiceComponent or BackendComponent is capable of processing.In our simple Hello world example, the Schema fingerprint is identical for the Backend- and ServiceComponet. But in reality, the BackendComponent as well one or multiple ServiceComponent(s) may evolve with a different change speed over time.This scenario is illustrated in the below diagram.There we are in real life. In case you are the owner of a BackendComponent, which offers data sets that are important for a lot of independent data consumers (ServiceROComponent2 and ServiceROComponent3), you will be confronted with a need to support multiple interface versions. This version support requirement may grow dramatically over time and impacts your time to deliver, the flexibility, and the increasing costs of your components.As a data provider to third party components (which are not under your control), you may have only limited power to enforce them to upgrade to newer release interface version numbers.Service Versioning and its management over the whole lifecycle of your component play a vital (and sometimes cumbersome) role for a lot of data providers.Running SOAP-Webservices backed by WSDL interface definition language or JSON-Services supported by Open API Specification 3 (i.e., Swagger) — two very popular request-response interaction technologies — will expose you to constant version change, in case you enhance your published interfaces.The version change is explicit and must be adopted by your consumers, which may be out of reach for a single coordinated version upgrade allowing the removal of the older version from your productive component.And here Avro shines with its Schema Evolution support out of the box.Avro supports schema evolution, which means that you can have producers and consumers of Avro messages with different versions of the schema at the same time. It all continues to work (as long as the schemas are compatible). Schema Evolution is a crucial feature required in large production systems to decouple components and allowing them to run system updates with interface changes at different times.The Avro algorithm implementation requires that the reader of an Avro message has the same version of the schema as the writer of the message. So how does Avro support then schema evolution? In case the reader is on another schema version, the reader may pass into the Avro parser two different schemas. The parser then uses its resolution rules to translate data from the writer schema into the reader schema.Let’s check that out on our BackendComponent.convertAvroBinarToJSON method. It’s one of the tutorial’s main methods, which converts an Avro Binary Message into an Avro JSON Message format, which is persisted afterward in ES.Single Avro objects are encoded as follows: A two-byte marker, C3 01, to show that the message is Avro and uses this single-record format (version 1). The 8-byte little-endian CRC-64-AVRO fingerprint of the object’s schema. The Avro object encoded using Avro’s binary encoding. (Link)The methodBackendComponent.persist is responsible for this task, which is straightforward.In a real-life scenario, the persist method would be exposed by the BackendComponent as a JSON-, Webservice, or any other remote interfaces.Our persisted Avro JSON document is stored as part of the _source attribute.Just think about what we have now achieved.An essential aspect of data management is schema evolution. After the initial schema is defined, applications may need to evolve, as shown in the above diagram. When this happens, the downstream consumers must be able to handle data encoded with both the old and the new schema seamlessly.We differentiate two compatibility types:If a schema evolution is backward-, as well as forward compatible is dependent if the change complies to Avro Schema Resolution constraintsHere three essential rules out of the resolution spec:This implies that adding or removing optional fields to your data message keeps the message fully compatible, as long as a default value was or will be defined for the removed or added attribute. So when you start defining your Schema, you should consider that from the beginning.Adding or removing optional data attributes in a data message is a scenario, which happens all the time in production systems, which evolve to fulfill new business requirements.Let’s see how Avro will help us to reduce the versioning complexity and component coupling in a simple example, by comparing a classic application setup, as well our enhanced one using ElasticSearch and Avro.Assume the following component setup in a productive environment.Now let’s check the scenario with our new Bounded Context Application. which is using in its BackendComponentan Avro Message Interface supporting Schema Evolution, as well as a NoSQL data store.As a return, the service would give back the unique identifier of the Avro Object, as well as the fingerprint of the BackendComponent writer. This will allow the interface consumer to detect if he is working on the same version.This ultimately results in a faster delivery cycle, and fewer change costs for introducing new features in your component.So it’s time for you to check out the tutorial, which can be found on Github: https://github.com/talfco/tutorial-apache-avro (a Maven-based tutorial).Have fun!",java,https://towardsdatascience.com/elasticsearch-on-steroids-with-avro-schemas-3bfc483e3b30?source=tag_archive---------1-----------------------
Spring Boot with Flyway Placeholders,"Suchit GuptaJun 2, 2020·1 min readPlay around with multiple schemas in your flyway scripts by using Flyway placeholders.RequirementIn each environment, we need to support two Database schemas. The schema should not be hardcoded and take the environment name as a suffix.application.yamlOur configuration:Default Flyway Schema: This is were flyway scripts will be executed.Second Schema: This is the second schema which we will use in our flyway scripts.The default prefix is ‘${‘, which is not suitable if you want to use spring properties. So I have updated the placeholder prefix and suffix as we need to use spring properties.Flyway ScriptNow you can use the placeholder like this:",java,https://medium.com/@suchit-g/spring-boot-with-flyway-placeholders-4c3e748b9f35?source=tag_archive---------10-----------------------
"A tri-folded Intelligent System to pre-monitor and predict a tsunami, flood, and earthquake based disasters and guide safer directions prior to the occurrence","Soumyadip ChowdhuryJun 1, 2020·7 min readThis Article is the Outline of my Patent on “A tri-folded Intelligent System to pre-monitor and predicts a tsunami, flood, and earthquake based disasters and guide safer directions prior to the occurrence”. (Anatomy of the Patent Specifications)Patent File No: 201931043975Filing Date: 30/10/2019A tri-folded Intelligent System to pre-monitor and predict a tsunami, flood, and earthquake based disasters and guide safer directions prior to the occurrence2. Field of invention:Internet of Things, Machine Learning and Data Analytics3. Background of the invention with regard to the drawback associated with known art:The currently know system predicts and analyzes the weather over a certain area. Our system analyzes the data over a local area and gives the result of a local area.4. The objective of the invention:The proposed work is about disaster management where the parameters related to disaster (earthquake, flood) will be continuously monitored and displayed. The parameters include temperature, humidity, angle in analog value, air pressure, wind speed, and vibration. After gathering the data, it predicts the weather of future days and alerts if there is a chance of occurring any natural disaster.5. Statement of the invention:A standalone weather station that gathers and monitors weather information for 24 X 7. It uses humidity, air pressure, rain gauge, water level sensors to gather information and uses tools like machine learning and data analytics to predict the weather of upcoming days. It also notifies it users of any weather changes and alerts those if any natural disasters like flood, tsunami by sending a notification through SMS. Another part of this system consists of guiding the people to a safer place during floods, earthquakes using the digital elevation model.6. A summary of the invention:In the first part weather station using DHT-11, Barometer, Water Level, Vibration, and Rain gauge sends data to the client (web browser). Here, Raspberry Pi3 B+ acts as a web server plus a data processing system. For data visualization and analyzing, a web application is made which shows real-time data. A python script runs and takes data from a NoSQL database and gives prediction using machine learning and data analytics. If the system detects that the data received exceeds the threshold then it sends warning and notification to its users through SMS. A python code executes and guides the user to move to a safer place using the digital elevation model.RASPBERRY PIThe Raspberry Pi is a small single-board computer developed by the Raspberry Pi Foundation. The proposed work uses the Raspberry Pi 3 Model B+, which is the latest product in the Raspberry Pi 3 range, which has the following specifications:· Broadcom BCM2837B0, Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz· 1GB LPDDR2 SDRAM· 2.4GHz and 5GHz IEEE 802.11.b/g/n/ac wireless LAN, Bluetooth 4.2, BLE· Gigabit Ethernet over USB 2.0 (maximum throughput of 300 Mbps)· Extended 40-pin GPIO headerIt uses the NOOBS (New Out Of Box Software) operating system.2. SENSORS1. Hygrometer(DHT11)A hygrometer measures relative humidity. The relative humidity is the quantity or percentage of water vapor (water in gas form) in the air. Humidity influences environmental factors and calculations like precipitation, fog, dew point, and heat index. Humidity sensor senses, measures, and reports the relative humidity in the air. It measures both the moisture and air temperature. It measures the moisture and temperature in air and expresses relative humidity as a percentage of the ratio of the moisture in the air to the maximum amount that can be held in the air at the current temperature. Air becomes hotter, it becomes more moisture, so the relative humidity changes with the temperature.2. Barometer (BMP180)–A barometer measures atmospheric pressure. A barometer can help to forecast upcoming weather based on the changes it measures in the atmospheric pressure.FIG 03: BMP180 Barometric Pressure Sensor3. Rain Gauge — A rain gauge measures rainfall or liquid precipitation. These weather stations include rainfall alerts to notify you when a rain event has begun, or to alert you of potential flood conditions.Vibration Sensor Module — SW-420 The Vibration module based on the vibration sensor SW-420 and Comparator LM393 to detect if there is any vibration beyond the threshold. The threshold can be adjusted by the on-board potentiometer. When this no vibration, this module output logic LOW the signal indicates LED light and vice versa.7. A brief description of the accompanying drawing:8. A detailed description of the invention :At first part weather station using DHT-11 collects humidity data from the atmosphere. The BMP180 measures both pressure and temperature because temperature changes the density of gasses like air. At higher temperatures, the air is not as dense and heavy, so it applies less pressure on the sensor. At lower temperatures, the air is far denser and weighs more, so it exerts more pressure on the sensor. The sensor uses real-time temperature measurements to compensate for the pressure readings for changes in air density. Water Level is used to identify the point at which a liquid falls below a minimum or rises above a maximum level. The vibration sensor module produces logic states depending on vibration and external force applied to it. When there is no vibration, this module gives the logic LOW output. When it feels vibration then the output of this module goes to logic HIGH and Rain gauge measure the amount of rainfall over a period of time. Rainfall is generally measured in millimeters or inches. After collection, all these data are sent to Raspberry pi3. Here, Raspberry Pi3 B+ acts as a web server along with the data processing system. For data visualization and analyzing, a web application is made which shows real-time data. In the background, a python script runs and takes data from a NoSQL database and gives prediction using machine learning and data analytics. The linear regression model is used for predicting the weather for the future. All the data processing is done on a cloud-based system. After processing the data if the system detects that the data received exceeds the threshold then it sends warning and notification to its users through SMS. A python script executes and guides the user to move to a safer place using the digital elevation model.9. Claim(s):1. It is a localized system that takes data on a local zone basis rather than taking the data on an average basis over a wide zone of the area. Thus it will give the user correct data over a specific zone.2. It is a portable machine and can easily be setup. Besides, the cost of maintenance is very low as compared to existing techniques.3. Existing systems analyze data from a single source. No such model exists which collects data from different sources like humidity, barometric, vibration, rain level, and rain gauge altogether.4. No such model exists which guides the user to move to a safer zone using a digital elevation model with the help of machine learning.10. Abstract:A disaster is a serious and devastating problem that occurs in a short time, which interrupts the functioning and creates a huge loss of a community or society involving widespread human, material, and environment. To predict the occurrence of a disaster by using various technologies such as the Internet of Things (IoT), data analytics, machine learning. By early warnings, remote monitoring, real-time data analytics, and notifications can predict the occurrence of a disaster. The data are stored on a cloud-based server. The system is created using raspberry pi where the analog data are collected from the sensors and they are transmitted through the WIFI module. A web page displays the recorded data for monitoring and analyzing. When sensors exceed the threshold, an alert message will be sent.",java,https://medium.com/@iamsoumyadip/a-tri-folded-intelligent-system-to-pre-monitor-and-predict-a-tsunami-flood-and-earthquake-based-b92961094b32?source=tag_archive---------22-----------------------
Storing a HTTP Request Body Using a WireMock Extension,"The complete source code is available on GitHub.Some time ago, while creating an integration test, I was in need to capture a file from a MultipartFile HTTP request body sent to another service. That was the place, where WireMock became handy, but its default behavior did not bring exactly what I needed. Searching the internet unfortunately did not give me the answer.If you are stuck at the similar problem, please stop shedding your tears and let me introduce you to the WireMock extension!What the heck is that WireMock extension?WireMock extension is a feature enabling to introduce a specific modification to the WireMock behavior. With its help you can perform a transformation of the considered HTTP request, as well as the response.Now when I know what it is, how to create it?There are some important steps to be followed to implement the WireMock extension:2. extension registration,3. placement of the extension within a JSON mapping file.Now when we know what needs to be done, let’s start with the real code example.1. Creating the ResponseTransformer class with the extension logic3. Referring to the created extension within the considered mappingFinally, we can move to the test class, where all the magic takes place!In the example below, we firstly generate an invoice .txt file, and write it to an array of bytes.The next step is to send it via a POST request with MultipartFile body and defined key as a file identifier — that is the place where WireMock starts his job!The connection to the considered endpoint is simulated on the WireMock stub server. Do not forget about our WireMock extension role, which captures the sent attachment, and saves it locally while the POST request is being processed.As this stage we can finally move to the main part of testing to determine if the captured file content matches our test criteria.Et voila! Now you are all set to do your own fancy tests using WireMock extensions.",java,https://medium.com/edc-dev-chapter/storing-a-http-request-body-using-a-wiremock-extension-ef3f2a60abcb?source=tag_archive---------8-----------------------
Design Patterns Saga: The Cabbage,"I was born in the city, located on the western bank of the Amur River in the Russian Far East. This area is known for its humid continental climate, which is typified by large seasonal temperature differences. Huge differences! The winters there were very cold. Sometimes temperatures fell to -40℃.One of my most vivid memories of the Russian winter was the dressing up process before you step outdoors. I can describe it in one word: layers. And in a few words: a lot of layers. Countless. It was like, leggings over the cotton warm tights, and wool socks over both. Then the sweater over the shirt. Then, wool mittens, connected by a long rubber band, so they don’t get lost, and a thin knitted cap. Now, comes the outer layer, that includes a fur coat and a fur hat (that you put over the cap you wore previously), and valenki ( traditional Russian felt boots, mine were with stitched initials). And that’s not all. The coat, hat, and your face are wrapped by a wool scarf. So only your eyes were exposed. Voila, it’s about time you got ready!We in the business named this method of dressing: “Layered Clothing”. Catchy and descriptive. I take it a step forward and call it “Cabbage” (just try to imagine it). This is my real-life example of the Decorator Design Pattern. We are going to face this pattern shortly. Let’s warm-up it by a regular “What do we know about anything?” column. Today’s topic is aggregation.Cambridge dictionary says that “aggregation is a process of combining things or amounts into a single group or total”. One of the synonyms of aggregation is “heap”, and heap, according to the same source, is “an untidy pile or mass of things”. I’m pretty ok with this definition. There is just one thing that ruins it for me. If aggregation is a process and the heap (an untidy heap) is a result of the process, why are these two words synonymous?In Java Aggregation is more straightforward. It’s a relationship between classes represented by one-way association. Such as, “you show me yours” (but I’ll not show you mine 😜 ). Heap is the embodiment of aggregation. A thing can exist without a heap, but a heap without things is … nothing!Take a look. This is a Thing object. It has a type and the name. And it can exist as is.Here is a heap. It has a height (big heap / huge…) and location, but it can’t exist without things.Only when you have things, for example, clothes, can you create a heap of clothes. Peel off your layers and throw them untidy to the floor 😉!I didn’t just bring up aggregation. The Decorator Pattern uses aggregation to combine behaviors at runtime.If you didn’t read the previous Design Patterns Saga chapter, this is a good time to do so. We will use the same examples and will share the code. Briefly, I described the way to prepare season outfits for Tanja (a paper doll), using the Template Method pattern. To implement it, we created the Outfit abstract class that declared methods that describe the steps that create a daily look. Some of the steps were declared as abstract (e.g. describeTop(), describeBottom(), describeFootwear()), others had a default implementation (e.g. drawDescribedOutfit(), cutDrawnOutfit()). It also defines the actual template method createDailyLook(). This method strung the step methods in a daily outfit creation algorithm.The current challenge is to add outerwear to the outfit dynamically, without changing base-class definitions. Can you think of a solution? I whisper “layers”, but you should hear “Decorator Pattern”.The following quote was taken from Wikipedia:Decorator Pattern is a design pattern that allows behavior to be added to an individual object, dynamically, without affecting the behavior of other objects from the same class.Exactly our case! It’s a structural pattern that extends the object’s behavior by constructing a wrapper around it. Think about layered clothing. Each layer is a decorator, wrapping the previous layer.Let’s consider adding outerwear to Tanja’s outfit data-diagram.Alright, here’s what you got. The left side of the diagram remains unchanged. Abstract Outfit class includes a template method. Concrete outfit classes extend the base-class and can be instantiated. An instance of these concrete classes is an original outfit that will be decorated afterward.Now, let’s review the right side. The Outerwear Decorator is an abstract class. Just like the Autumn Outfit and Summer Outfit classes, it extends the base Outfit class. The main differences are that the decorator aggregates other types of components that will allow layers to be added. The decorator sub-class, Coat Decorator adds a new outwear layer on top of the previous outfit layers.I hope it makes a little bit of sense. Let’s see the code example, to make it clearer.You already have the Autumn Outfit implemented from the Template Method example. This is your basic outfit.Now, you need an Outwear Decorator to add more garments to your basic outfit using aggregation. You will need to use the sub-class of the abstract decorator class to add to a basic outfit. As we explored earlier, the Outwear Decorator is a sub-class of an Outfit. Therefore, a concrete decorator is also a sub-class of the Outfit.The base decorator holds new abstract behavior describeOuterwear(), that will be implemented by the concrete decorator. And, it overrides the createDailyLook() method. Sounds sketchy? It could be. Once I told you that the template method should not be overridden by the sub-classes. This is the one exception to the rule. You can override the template method by wrapping it in a decorator.The next step is to implement the abstract method. Create a concrete Coat Decorator.The decorator class behaves just like another class that implements the abstract methods. The only difference is, that it aggregates the base-class. It will allow stacking a few behaviors together.Finally, add a new method to Outfit Service, that will create an autumn daily look with a coat for Tanja.The important part is that the basic outfit must be the first one in the stack. Next, wrap it in a Coat Decorator. Then call the createDailyLook() method, to Draw and Cut the described outfit. Your abstract decorator simply delegates the daily look creation to the Outfit object that it aggregates. Check the logs.You may notice that daily look creation template steps were extended by coat decorator, without changing the base class. QED.That’s all! Tanja has her paper clothes and I’m still thinking about valenki☃️.Here is a link to the git project of this post.Design Patterns Saga chapters:",java,https://medium.com/swlh/design-patterns-saga-the-cabbage-58962749c2b3?source=tag_archive---------8-----------------------
Learning Functional Programming with Scala,"Ryan SusanaJun 2, 2020·7 min readIn this blog post I will write about my journey learning functional programming with Scala. I have a very object-oriented background in Java. To accompany me on this journey, I have decided to take this Coursera course.I feel like it’s one thing to learn the theory and another thing to apply it. For this reason, I have also decided to build a working application after I completed my course. This is further detailed in the final chapters.So my goals are:Several reasons.The Scala language reminds me a lot of Kotlin. Types are written in a format like “var: Type”. A lot of things in Scala are implicit. For instance, the last expression of a function is its return.In Scala, everything is an expression. Even if-statements! This can be shocking for a Java developer. Luckily, I have some experience writing Kotlin, so this concept is not that new to me. Here’s an example of an if expression:Functions can exist within functions with Scala. This can be used to hide parts of the surface area of a function, while still be able to re-use a private function. Here is an example of calculating the square root of a number by approximation and recursion. The ‘abs’, ‘isGood’, ‘improve’ and ‘sqrtIter’ functions are not accessible outside the scope of the functions.Scala knows a big difference between definitions(defs) and values(vals). Defs are just definitions to what a variable should be evaluated against. This means that defs follow a lazy evaluation strategy that evaluates a def as it is needed. Vals on the other hand are evaluated on the spot. It properly reflects what the value of a variable is.Functional programs embrace the idea of immutability. That means that you don’t mutate the state of data(e.g. person.setName(String)), you return new versions of data(e.g. person.copyWithName(String)). It seems inefficient, but it grants a heap of benefits.Functional programming embraces the idea that functions should be pure. That means that:Recursion is not a new concept for me. But sometimes, I find it hard to wrap my head around it. Seeing that in pure functional programming there is no notion of a loop, this will be fun. In the exercises below, you will see me do the following using recursion:Recursion can generate a lot of load on your stack trace, causing stack overflow errors. Compilers & runtimes have implemented the concept of tail recursion to tackle some of these problems. Tail-recursion recycles the originating call graph, effectively making the call a for loop in disguise. Tail-recursion can only be applied if the recursive call is the last expression in a function and no other operators are applied to the recursive call. Beware, tail recursions introduce another level of cognitive load, as the function typically becomes harder to read.Higher-order functions are functions that take other functions as parameters and/or returns other functions. Here’s a quick example. Let’s say I have these functions:If I wanted to write a function that sums up all the cubes or factorials of a number, I can write something like this:Higher-order functions enable the concept of currying. *Insert lame joke about Indian food here*. It’s basically the concept of nesting returning functions. A function can return a function that returns another function.Now let’s see if we can rewrite factorial using the curry technique:At the end of week 2, I implemented the concept of a functional set. The general notion of a functional set is that: We represent a set by its characteristic function, i.e. its `contains` predicate. This was a pretty fun assignment, as it really shows the elegance of functional programmingIn the course I learned how to use the basic object-oriented primitives in Scala. I am very familiar with these concepts. However, in the course I noticed that the instructor also applied more declarative/functional principles to his code. For instance, he implemented a Binary Tree with classes and objects. However, he did not use imperative code to do so. Instead of updating a tree’s state, he would return a new instance of the tree. This by nature is purely functional. Wow. There are no side effects. The data is immutable. But, wow. Here is my example of an immutable List.I use decomposition a lot. One of the problems I ran into while developing my own open-source project was the simplification of expressions. It turns out that the use of decomposition was not a feasible solution for what I wanted to do.Pattern matching helps you decompose and navigate data structures in a very convenient and compact syntax. It looks like a Java switch statement, but is definitely not. Switch could never! The pattern matching feature looks very beautiful to me.You can also pattern match against Lists, Strings and even RegEx patterns. Here I use pattern matching to implement the Insertion Sort. As you can see I match against the head and the tail of the list using ‘::’In week 5, I implemented (and barely understood) Huffman coding. It is essentially a way to compress and decompress Strings by assigning every character in a String a weight. This weight represents how many times a character is used within a String. The characters are put into a certain code tree, which is contains Fork nodes and Leaf nodes.Here is my solution(got an 8/10)Tuples are a concept that does not exist in the Java world. They allow you to define a data structure that allows you to return multiple values from a function. This is one of my favourite parts of the Scala language. You can then destructure or pattern match against tuples like what I did in this Merge Sort implementation.I present to you, the Map-Filter-Reduce sandwich.These three functions (along with forEach) form the basis of most functional operations.Map allows you to transform data. Filter allows you to weed out unwanted data. Reduce allows you to summarize data.These three methods are very powerful. And they form a basis to what you can do with sequences of data.In my last week picking up Scala, I decided to build an application that runs on the cloud. I decided to do so in the form of a cloud function, considering that this is functional programming 😉. I wanted to create a service that analyses the contents of Word, PDF and Text files and extracts key information about those files. The program would essentially perform sentiment analysis on the text content of the file and respond with the following contents:This service will use the Google Cloud Natural Language API to perform the analysis.I applied the following principles to my program, located here:I feel like these principles reflect my favourite parts about functional programming.I feel like I have learned a lot about functional programming. At first, I didn’t like it, because of the initial learning curve. I felt everything was hidden behind some weird notation. After I understood what some of these notations meant, I grew a lot of appreciation for functional programming. I will definitely use more of these practices in my professional career. Who knows, I might even use Scala in a professional project 🤷🏾‍♂️.I’m usually down for a spar session. Maybe even a new code homie?",java,https://medium.com/@ryansusana/learning-functional-programming-with-scala-ccc7bf68214f?source=tag_archive---------2-----------------------
Automate test Results Reporting to TestRail,"Andrey KonovkaJun 2, 2020·4 min readThis article is a continuation of the previous article on the approach “test cases as code”. In a previous article, we mapped and exported all automated tests to TestRail. Now it’s time to report test results into TestRail and observe beautiful reports.I have prepared a Junit 5 extension which takes test IDs from the test run, creates a test run in TestRail, and sets the appropriate status after test execution is done.In order to achieve that, we need to do a few simple steps.First of all, we need at least version 5.4 of Junit 5 in order to use the TestWatcher interface. What is TestWatcher? — In short, the TestWatcher interface defines the API for extensions that wish to process test results.Second, we want to interact with TestRail via API. I use testrail-api-java-client which is used to make API calls to TestRail. In order to use it just add a dependency to build.gradleTo gather the results, let’s create new class TestRailReporterExtension which will implement TestWatcher and BeforeAllCallBack interface methods. The methods in TestWatcher are called after a test has been skipped or executed. Any CloseableResource objects stored in the Store of the supplied ExtensionContext will have already been closed before methods in this API are invoked. ExtensionContext input parameter in the method provides the details of the test case and can be used to extract the @TmsLink defined using the Allure annotations.In addResult method, a Result object is created. This method will only process the test cases which have the @TmsLink annotation.Also, the beforAll method from the BeforeAllCallback interface should be implemented. In this method, the CloseableOnlyResource is registered to the global context and close method in it is only triggered after the completion of all the test runs. In this close method, the final results are reported using the reportResults method. Static started flag defined in this class will ensure that the context is registered only once per execution context and not for each and every class.There is an additional check of test cases mapping to IDs. In case if some ID does not exist in TestRail, you will get a warning in the console output:I have prepared 2 types of extensions:We can define a configuration for TestRail in the properties file(username, password, plan, milestone, etc). But the most important option is the one that gives us the ability to turn on/off test results reporting. For example, if you run tests locally and you don’t want results to be reported, just change the value of property to false and check state in extension. This is how it can be done via config.properties:There is a tricky part with TestRail statuses. If you disable some test, test results reporting will fail with an error. In order to fix that, add a new custom status to TestRail — DisabledNow, when everything is configured, we need to enable the extension. Each class with tests should be annotated with @ExtendWith(TestRailReporterExtension.class)Or, you can do as we do — we have a BaseTest class which contains general configurations and some global @Before* annotations. All the test classes are extended from this class. So simply do the following:Now start the test run, wait until it’s finished and check the TestRail. Depending on selected tests and browser configuration you will see something close to the following screenshot:Now TestRail can contain not only manual test runs, but also results from automated tests.In this article I described how to easily report results of automated tests to TestRail and make automated testing more visible and useful.The simplest use case is — enable test results reporting during the release, run the nightly builds, schedule test report creation in TestRail and grab fresh report every morning while you drink a cup of coffee :)",java,https://medium.com/@legionivo/automate-test-results-reporting-to-testrail-101b17c5780e?source=tag_archive---------13-----------------------
How to set up meaningful logging in your Spring Boot application?,"Akash ChandwaniJun 2, 2020·6 min readHave you ever wished to write your application logs so perfect, that anyone looking at those can understand what has happened in your application? Have you ever wanted to resolve all your application issues just by looking at the logs? If the answer to both of these questions is yes, you’ve landed at the right place. In this article, I am going to show you how you can set up meaningful logs which can help you achieve both of the above goals.But before we dive in, let’s analyze at the default logs of a simple spring boot application.(Source — https://github.com/akashchandwani/logging-demo/tree/default branch: default)These application logs have the following columns —These column values look good but are not sufficient. If someone would look at these logs and would want to find what’s going on in the application, they would be unsuccessful. It is basically due to the following limitation of these default logs—Now if the above information is convincing enough for you, and you are sure that these default logs won’t do us any good on itself, and we have to add more logs in our application, you are right. Let’s proceed to make these logs more meaningful. Note that we won’t be adding those manually. Instead, we would be using Filters, Aspects and configurations in the application.properties file to automate these. Let’s go through how we can incorporate each of the above limitations of the default logs of spring boot —2. Change root logging to WARN level — The default INFO logs can be easily removed by making our logging.level.root property to warn. Now you should only see the WARN level and ERROR level logs from your spring boot app. In your application.property add —3. Use a Filter to put API information — Now to print API request log and API response status log, create a filter and add those logs there. In this way, all incoming requests and their responses would be printed in your logs. You would also need to add @Component annotation to your Filter class so that it will automatically be wired in your spring boot application. We will be logging the HTTP method and URI before the execution of your API and HTTP response status after the request is completed. Both would be logged at `info` level Here is the code to do the same4. Add user and session information — If your request is authorized and you have users using your application, you can add his/her userId in the logs. You can extract the username from the UserPrincipal Object. Then to add the user information in the logs, we can use the MDC (Mapped Diagnostic Context) feature of Slf4j. Put the userId attribute in the MDC static variable and then configure your logging.pattern.level property to place if before the logging level variable, i.e., %5p. In the below example, I am adding userId, sessionId, HTTP method and Request-URI to the MDC variable, and adding them before the level pattern.5. Add Aspects for detailed logs — Before you can use aspects, you’ll need to make sure you have the following dependency pom.xml fileNow create a pointcut to snoop all the service method as we want to add a log before a method execution with the method information and arguments information, and after a method is executed, we want to add the return type and the value returned from that method. The implementation of the aspect if below —If your service method parameters are an object or the return type is a custom object, you can add their values to the logs by implementing a toString() method in them. If you are using Lombok, you can add @ToString annotation at the top of that class implementation. Now we would also need to enable TRACE logging for our detailed logs as we have configured our aspect class to print TRACE logs. We can change the logging level below to DEBUG or INFO if we don’t need those detailed logs on any environment.At this point, our application.properties should look something like this —Now let’s run our application again and make some user operations —Woah! These are much better. Now you can easily see what the user is doing in our application. The user is hitting the health API multiple times, and he is getting 200 Ok response each time.Mission Accomplished! We have now made our spring boot’s default logs to be meaningful. Now anyone looking at the logs would be able to guess what is happening in the application.Now, you might think — Are these the standard of logging? — Unfortunately, the answer is — No. You can, and should always customize your logs according to your features and requirement. The aim of customizing the logs in this way was to make the logs more meaningful. That is — to enable anyone looking at the logs to find the issues and resolve them on the fly. I don’t suggest you copy these configurations and put in your application blindly. It would be best if you thought about what would work best for you and apply those configurations only.If you would like to read more about spring logging, please refer —Thanks for reading. If you have enjoyed reading this, please give some claps to this post, It would encourage me to write more of such informative blogs/articles.P.S. —You can find the complete source code of this demo on GitHub— https://github.com/akashchandwani/logging-demo",java,https://medium.com/@akashchandwani/how-to-set-up-meaningful-logging-in-your-spring-boot-application-f8cbfd01c348?source=tag_archive---------4-----------------------
Goki and his breakup,"ABhishek DubeyJun 1, 2020·1 min readGoki recently had a breakup, so he wants to have some more friends in his life. Goki has N people who he can be friends with, so he decides to choose among them according to their skills set Y i(1<=i<=n). He wants atleast X skills in his friends. Help Goki find his friends. INPUT First line of the input contains an integer N denoting the number of people. Next line contains a single integer X — denoting the minimum skill required to be Goki’s friend. Next n lines contain one integer Y — denoting the skill of ith person. OUTPUT For each person print if he can be friend with Goki. ‘YES’ (without quotes) if he can be friends with Goki else ‘NO’ (without quotes).Solution — https://programsaddaa.blogspot.com/2020/05/goki-and-his-breakup.htmlOriginally published at https://programsaddaa.blogspot.com on June 1, 2020.",java,https://medium.com/@dubeyabhishek748/goki-and-his-breakup-569a559ff819?source=tag_archive---------24-----------------------
Building a Personalized Serializer and Deserializer using Java Gson Library,"Alex TherrienJun 2, 2020·8 min readBefore I begin, let me say that this tutorial applies to a very few. There are only some rare cases where this needs to be done. I did it because I created my own linked list and wanted to convert it into JSON. By default, Gson can automatically convert objects to their corresponding JSON format. However, the Gson library did not find a way to correctly convert my customized linked list to JSON.I used this website to help me find this solution. Towards the end of the web page, there is some information about customizing the serializer and deserializer. If you need basic examples, this is the place to visit.If you need any help with JSON syntax, you should refer to this website.I knew the conversion was not being done well, because I ended up with ‘TreeNode’ elements (or something with a similar name) in my custom linked list. This is when I started to search for a solution online. You can look here for examples, although I feel having it explained would be a lot simpler.The skeleton of any Gson Serializer resumes to this:(Code block 1.1)Where SomeObject is the object that you desire to read to put into a JSON file.The first thing you need to do is start by creating an object of type JsonObject. Then, you will need to add your values to that JsonObject instance. If the values added are primitive values, you only need to convert your values to JsonPrimitive objects. If the values added are arrays, you would need to extract all of the indexes of those arrays and add them into a JsonArray one by one. If the values added are objects, then you need to create other JsonObjects to your main JsonObject. The following sections will show how it is done.Creating the JSON ObjectTo create the JSON object that will store the object being serialized, do the following:Adding a String, Integer, or DoubleTo add any value to the JSON object (being String, Integer, or Double), you need to do the following:This code shows that there is the need to create a new JsonPrimitive object. Gson only understands values being passed as JsonPrimitive objects. Also, the string title is the key that is going to be associated to our value.Adding an Array of ValuesTo add an array of values to our JSON object, you need to do the following:Essentially, the first line of this code block is irrelevant; this is the way I get the array of information from the object. The next lines, however, are important. First, a JsonArray having the same size as the array of reference (someArray in this case) must be created. Then, iterating over the indexes of the array of reference is done. For each index, a primitive value is being added to the JsonArray using the add() method. Finally, once the array is completed, the array is added to jsonObject using the add() method. The first argument of that method is the key that will be associated with the array. The second argument is the actual JsonArray that was just created.Adding an Array of ObjectsTo create an array of objects, it would pretty much look like the previous section. the only difference would be that you would be adding objects instead of strings with the add() method from the JsonArray object. To do so, you would need to instantiate a JsonObject object and put key/value pairs in it, as shown from the section Adding a String, Integer, or Double in this article.To conclude this section, do not forget to return your main JsonObject at the end of the function as the serialize method returns a JsonElement! (refer to code block 1.1)To be able to use your custom serializer, you have to reference it somewhere in your code. To do so, you would follow the common procedure as referenced in the Gson documentation, but with an extra line of code. This line of code would identify the custom serializer as the way to translate the object to JSON. Refer to the Custom Instance Creators in GSON on this website for more information.Essentially, all you have to do is assign the builder object to a builder that considers the custom serializer that was built (look at line 2. This is the line that matters the most, only briefly mentioned in Gson documentation). Note that the class for the custom serializer that was designed in this tutorial was SomeSerializer, and that the object that was serialized was called SomeObject.The variables PATH_TO_JSON and titleOfJsonFile are variables that determine where the file is going to be stored.The variable someObjectInstance is an object of type SomeObject that we want to have serialized.Also, I am able to write the object of type SomeObject to a file in JSON using the gson.toJson() function.The skeleton of any Gson Deserializer resumes to this:(Code block 2.1)Where SomeObject is the object that you desire to create after having read a JSON file. The JsonElement object contains all the data that Gson could extract from the JSON file. This is the only parameter that is required to extract the content of a JSON file.The first step in the deserializer is to get the content of the JSON file. To do so, refer to the next section. The second step is to extract the values from the JSON file by using the keys associated with the values that you want to extract. The final step is to return the object built as a result of deserializing the JSON file. Following from code block 2.1, that would be the SomeObject instance.Extracting the JSON ContentTo extract the JSON content from a file, all you need to do is:The jsonElement object is of type JsonElement and is from the parameter list of your function deserialize (refer to Code block 2.1). You can get the content of your file from this function as the syntax of a JSON file starts with a { and ends with a }.Extracting Primitive Values from JSONTo extract primitive values from a JSON file (either integer, double, or string) can simply be done from the corresponding functions:You can extract those values from the corresponding JSON element you have chosen.Inside the jsonObject of type JsonObject, there was a field called title. The function get() returns an object of type JsonElement. This object only comes to use to us when using either the getAsString(), getAsDouble(), or getAsInt() method.In this previous code block, I extract the string value associated with the key title.In this previous code block, I extract the double value associated with the key quantity.In this previous code block, I extract the integer value associated with the key unit.Extracting the Content of a Common JSON ArrayTo extract the content of some JSON array, use the following:This example speaks for itself. From your JsonObject jsonObject, get the associated value with the key someArray. Using the function size() from the JsonArray class to know when the array ends, iterate over the JsonArray and extract all the primitive values from it. The get() function from the JsonArray class will allow you to get a value at a certain index.This code extracts the content of a common JSON array, which is some thing of the sort:Extracting the Content of a Custom JSON ArrayThe difference between a custom JSON array and a common JSON array is that the custom one contains JSON objects as indexes, and the other contains primitive values as indexes. Below is an example of how to extract data from a custom JSON array.In this example, I get the array with the key ingredients. Then, I define a for loop the same way I did when extracting data from a common JSON array (see previous section).Because indexes are now objects, I extract the content of indexes using the function getAsJsonObject(). Within that object, I get the values I want using the keys that I defined in those (nameIngredient, quantity, and unit).Last but not least, do not forget to return the object you just built from your JSON file!There is not much difference from the Referencing the Custom Serializer section of this article. The only difference is that you now reference a deserializer instead of a serializer. Of course, after the Gson object is created, things get really different. Because we are reading a JSON file, we have to use a BufferedReader object to be able to convert the content of the file to a JSON object. Using the gson.fromJson() method we accomplish this goal. Refer to the Custom Instance Creators in GSON on this website for more information.Remember that the SomeObject class is only used as an example. The value of canonicalPathToFile is the path to the file.To conclude, I would like to remind the readers to use the same key fields for serializing and deserializing the object. Otherwise, there will be errors because the values you will be looking for will not be found. What I mean by that is you need to match the key value when serializing the object (say, title) with the one you will extract when deserializing the object. If you put anything else than title, Gson will not find it in the JSON file.Happy coding!",java,https://medium.com/@alexandre.therrien3/personalized-serializer-and-deserializer-using-java-gson-library-c079de3974d4?source=tag_archive---------12-----------------------
Kotlin “use” keyword,"Aleksei JegorovJun 1, 2020·2 min readJava instatiate and initialize new object with help of “new” keyword:MyObject obj = new MyObject();but we need to release it after execution complete.Resource resource = new getResource();try { useResourceSomehow(resource);} catch(Exception ex) { ex.printStackTrace();} finally { releaseResource(resource);}Java objects hold a resource and are eligible for automatic resource management implement a specific Closeable interface.try (FileInputStream input = new FileInputStream(“file.txt”)) { … }This is the try-with-resources construct. The FileInputStream variable is declared inside the parentheses after the try keyword. Additionally, a FileInputStream is instantiated and assigned to the variable.When the try block finishes the FileInputStream will be closed automatically. This is possible because FileInputStream implements the Java interface java.lang.AutoCloseable. All classes implementing this interface can be used inside the try-with-resources construct.So, try-with-resources is the short alternative to try...catch...finallyOtherwise…ASP.NET C# has the same approach to release object after it is no longer needed with help of “using” keywordIt provides a convenient syntax that ensures the correct use of IDisposable objects. Beginning in C# 8.0, the using statement ensures the correct use of IAsyncDisposable objects.Also I remember about Python way of doing the same task:with open(“input.txt”) as f: for line in f: print(line, end=’’)Kotlin doesn’t have ‘try-with-resources’Instead, it has the use extension function.This only acts on a single Closeable resource.Syntax of use function:inline fun <T :Closeable?, R> T.use(block: (T) -> R): R (source)Or we can extends Closeable class like this:Or more practical example:",java,https://medium.com/@alekseijegorov/kotlin-use-keyword-31225f80b8c0?source=tag_archive---------11-----------------------
Image upload to Cloudinary using Android SDK,"In this article, I’m going to implement a sample android (Java) project to upload images to Cloudinary using the android SDK.Cloudinary is a cloud-based image and video management solution for web and mobile applications with the capability of image and video uploads, storage, manipulations, optimizations to delivery. Additionally, Cloudinary offers comprehensive APIs and administration capabilities, which you can easily integrate with your web and mobile apps.“ Cloudinary is the market leader in providing a comprehensive cloud-based image management solution. Cloudinary is being used by tens of thousands of web and mobile application developers all around the world, from small startups to large enterprises. We are here to cover your every image-related need.” sourceYou can set up a Cloudinary account here. The Cloudinary SDK documentation can be found here.This article covers how to pick up an image from the device gallery and upload it to the Cloudinary. Complete code of the project can be taken from here. Let’s start step by step.Step 1. Start a new Android ProjectName of the project is ImageUploader.Step 2. Add dependenciesCloudinary and Circle Image View libraries are needed to add to the project build.gradle file.Step 3. Access the permission When the app is installed to a device, it requests permission to access your sensitive data. Therefore, enable the permissions in the AndroidManifest.xml file.Step 4. Build layout activity_main.xml.Here, I added vector assets named ic_accout and ic_image in res>drawable.5. ActivityMain.java5.1 Cloudinary ConfigurationCloudinary must be configured at least your cloud_name. Additionally, it can be defined optional configuration parameters. There are configuration credentials in the dashboard of the Cloudinary management console. Here, I added cloud name, API key and API secret to a HashMap.The MediaManager.init() method is called once per application life cycle before using the Android library, preferably in onCreate(). Here configCloudinary() method is used to Cloudinary configurations.5.2 Pick up an image from the Gallery. Permission of the external storage is a dangerous permission. Therefore, when you click the imageAdd, device needs to request permission to access the external storage.ContextCompat.checkSelfPermission() in the requestPermission() method, checks whether the app already has granted the permission.After the permission is granted,accessTheGallery() is called. Then the image you selected from the gallery, set up as a bitmap image. The method getRealPathFromUri() takes the imageUri and returns file path of the image(location). Then it is assigned to filePath. Now, thefilePath has the location of the image.5.3 Upload image to the Cloudinary After click the upload button, uploadToCloudinary(filePath) is called.MediaManager’s callback method provides an easy way to hook into the upload’s progress and provide specific code to run at each stage of the upload.complete code MainActivity.java.The complete code of the project can be taken from here.Referencecloudinary.comdeveloper.android.com",java,https://levelup.gitconnected.com/image-upload-to-cloudinary-using-android-sdk-7bbe60204b44?source=tag_archive---------7-----------------------
Shared Preferences,"Nikitha GullapalliJun 1, 2020·7 min readTopics covered: Data Persistence implementation, javaProfiles:Linkedin: https://www.linkedin.com/in/nikitha-gullapalli/Github: https://github.com/nikitha2/SunshineApp.gitSharedPreferences- This class gives you an object which points to a local file that will be used to store the data through the API. These files are usually stored inside the App Data subdirectory of your application’s installation location.SharedPreferences are intended to save Key-Value pairs in storage and Android doesn’t recommend saving confidential information in shared preferences without proper security.Android leverages the functionalities of XML to save these key-value pairs easily in these files. SharedPreferences files can be either private or shared among other applications.In this article, we will look into how to implement data persistence in android app development using java. The video below shows the settings screen we are trying to implement.XML is used to create a layout for the preferences. We can use this XML to place different preferences in the desired location on-screen and provide a key for each of them.we can have different preferences based on the need like SwitchPreference, ListPreference, CheckBoxPreference, etc.We can future have a hierarchy of preferences. Check on the link for more information:https://developer.android.com/reference/android/preference/PreferenceScreenNote we are using an array to set entries and entry values as shown below.2. Create a fragment class and inflate the XMLThe code below will inflate the settingspreferrences XML file on creating the fragment. Now we need to place the fragment on the activity so that it can be displayed on the UI.( as we know an Activity is an application component that provides a screen with UI)3. Create an activity say SettingsActivity. I will have all my preferences settings in this activity. So we need to place the fragment we created on this activity.There are 2 ways to place the fragment onto this activity. I can do it programmatically or through the layout.a. activity_settings.xml- if through the layout.and SettingsActivity.java just call set the layout onCreate.b. SettingsActivity.java — if through the programmatically.activity_settings.xmlAwesome now at end of step 4 we have an activity with all the preference. The screen should look like the figure below.4. Now we have an activity with the preference screen. If SettingsActivity is the main activity it will be loaded on starting the app, but usually, settings screen is not the main screen of the app. So we need to create a way for us to navigate to SettingsActivity.Here we have a MainActivity that is loaded on starting the app. We created a settings menu item. On clicking on setting in the menu we want to navigate to SettingsActivity.Menu implementation is out of scope for this article. I will create a new article for it.Clicking on settings in the menu in MainActivity.java SettingsActivity is called using intents using the code below. Here we use explicit intent as we want to call the SettingsActivity every time the settings button is called and it is class inside the call.Great, now we should be able to navigate to settings activity on clicking settings menu in the main screen5. What we need now is to be able to reflect the options selected in the settings in our app.To achieve this we need to know when a preference is changed, what preference is changed and what is the changed value.Android provided us with pre implemented class to make our lives simple. So simply implements SharedPreferences.OnSharedPreferenceChangeListener. in the activity, you want to listen to the changes to as shown below. (in our case we want to listen to the changes in the preferences in the mainActivity so that I can make changes)Note: make sure you register your OnSharedPreferenceChangeListener when activity is created so that it knows it needs to hear for preference changes in the SharedPreference file and unregister on the destroying of the activity for better use of memory.On implementing SharedPreferences.OnSharedPreferenceChangeListener we are prompted to implement the method onSharedPreferenceChanged. This method is called every time there is a change in the SharedPreferences file. (This is the file where all our preferences are saved in key-value pairs). You can use sharedpreferences and key values to make specific changes in the app. For the purpose of explanation, I am setting my variable preferencesChanged to true and every time my activity starts- if preferencesChanged is true I am loading my data from the web using loaders.Now the app is expected to look something like below. Once the preferences are updated we see that mainActivity screen reloads the data as implemented.Now the app is expected to look something like thisIf you observe now when we make changes to the preferences in the settings activity and navigate back to mainActivity data is reloaded but in the settings, the screen summary does not reflect the option selected for each screenPreference.Note: Summary is the tag we have for each screenPreference so that we know what is the value currently for the preference. if we make some change to the preference we expect the summary to reflect the latest option.6. Summary updating is to be done in the fragment because every time we call this fragment in any activity we want this functionality to exist.Implement OnSharedPreferenceChangeListener to know if preferences are changed. This will prompt you to implement a method onSharedPreferenceChanged. Here we are basically checking if preference changed is not null and if its an instance of Listview, get the value and find its index. Now we want to set the label to the summary. So we use the index to get the label in the index and set it to the summary.Note:-Do not forget to register and unregister the listener.-Here we used getPreferenceScreen to get the preferences-We are changing only listview summary as SwitchPreference summary is already taken care of in the layout-OnCreate we go through each preference on the screen and set the summaryAwsome now we a functioning settings screen that is updating the summary and changes made in the settings are being seen in the app. Thank you for reading. Please let me know any flaws in the article and I would love to fix them.https://zocada.com/using-shared-preferences-to-save-data-locally-in-android/https://www.tutorialspoint.com/android/android_sqlite_database.htmhttps://www.futurelearn.com/courses/secure-android-app-development/0/steps/21599https://www.udacity.com/",java,https://medium.com/@gullapalli-nikitha/sharedpreferences-c32497c90441?source=tag_archive---------13-----------------------
"PHP PDO Connection and Authentication(Registration, Login and Logout)","SujithJun 1, 2020·2 min readPlease add this to config.php<?phpsession_start();// Define databasedefine(‘dbhost’, ‘localhost’);define(‘dbuser’, ‘root’);define(‘dbpass’, ‘’);define(‘dbname’, ‘db_name’);// Connecting databasetry { $connect = new PDO(“mysql:host=”.dbhost.”; dbname=”.dbname, dbuser, dbpass); $connect->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);}catch(PDOException $e) { echo $e->getMessage();}?><?phprequire ‘config.php’;if (isset($_POST[‘register’])) { $errMsg = ‘’;// Get data from FROM $parentname = $_POST[‘parentname’]; $mobileno = $_POST[‘mobileno’]; $password = $_POST[‘password’]; $email = $_POST[‘email’];try { $stmtuser = $connect->prepare(‘SELECT * FROM tbl_users WHERE user_mobile = :mobile’); $stmtuser->execute(array( ‘:mobile’ => $mobileno, )); $datauser = $stmtuser->fetch(PDO::FETCH_ASSOC); //exit; if (empty($datauser)) { $stmt = $connect->prepare(‘INSERT INTO tbl_users (user_name, user_pass, user_mobile, user_email) VALUES (:user_name, :user_pass, :user_mobile, :user_email)’); $stmt->execute(array( ‘:user_name’ => $parentname, ‘:user_pass’ => $password, ‘:user_mobile’ => $mobileno, ‘:user_email’ => $email, ));date_default_timezone_set(‘Etc/UTC’); $stmtuser = $connect->prepare(‘SELECT * FROM tbl_users WHERE user_mobile = :mobile’); $stmtuser->execute(array( ‘:mobile’ => $mobileno, )); $datauser = $stmtuser->fetch(PDO::FETCH_ASSOC);$_SESSION[‘user_name’] = $datauser[‘user_name’]; $_SESSION[‘user_id’] = $datauser[‘id’]; $_SESSION[‘user_mobile’] = $datauser[‘user_mobile’]; $_SESSION[‘role’] = $datauser[‘role’]; $_SESSION[‘status’] = $datauser[‘status’]; echo ‘<script>window.location.replace(“index.php”)</script>’;  exit; } else { $stmt = $connect->prepare(‘UPDATE tbl_users set user_name = :user_name, user_pass = :user_pass, user_email = :user_email where user_mobile = :user_mobile’); $stmt->execute(array( ‘:user_name’ => $parentname, ‘:user_pass’ => $password, ‘:user_mobile’ => $mobileno, ‘:user_email’ => $email, )); $stmtuser = $connect->prepare(‘SELECT * FROM tbl_users WHERE user_mobile = :mobile’); $stmtuser->execute(array( ‘:mobile’ => $mobileno, )); $datauser = $stmtuser->fetch(PDO::FETCH_ASSOC);$_SESSION[‘user_name’] = $datauser[‘user_name’]; $_SESSION[‘user_id’] = $datauser[‘id’]; $_SESSION[‘user_mobile’] = $datauser[‘user_mobile’]; $_SESSION[‘role’] = $datauser[‘role’]; $_SESSION[‘status’] = $datauser[‘status’]; echo ‘<script>window.location.replace(“index.php”)</script>’; exit; } } catch (PDOException $e) { echo $e->getMessage(); }}if (isset($_GET[‘action’]) && $_GET[‘action’] == ‘joined’) { $successMsg = ‘Registration successful Now you can <a href=”/”>login</a>’;}?><?phprequire ‘config.php’;if (!empty($_SESSION[‘user_id’])){ header(‘Location: index.php’); } if(isset($_POST[‘login’])) { $errMsg = ‘’;// Get data from FORM $mobile = $_POST[‘mobile’]; $password = $_POST[‘password’];if($mobile == ‘’) $errMsg = ‘Enter mobile no’; if($password == ‘’) $errMsg = ‘Enter password’;if($errMsg == ‘’) { try { $stmt = $connect->prepare(‘SELECT * FROM tbl_users WHERE user_mobile = :mobile && user_pass = :pass && otp_verification = :verification’); $stmt->execute(array( ‘:mobile’ => $mobile, ‘:pass’ => $password, ‘:verification’ => 1, )); $data = $stmt->fetch(PDO::FETCH_ASSOC);if($data == false){ $errMsg = “User $mobile not found.”; } else { if($password == $data[‘user_pass’]) { $_SESSION[‘user_name’] = $data[‘user_name’]; $_SESSION[‘user_id’] = $data[‘id’]; $_SESSION[‘user_mobile’] = $data[‘user_mobile’]; $_SESSION[‘role’] = $data[‘role’]; $_SESSION[‘status’] = $data[‘status’]; header(‘Location: index.php’); exit; } else{ $errMsg = ‘Password not match.’; } } } catch(PDOException $e) { $errMsg = $e->getMessage(); } }else{ $errMsg = ‘Error in Logging In.’;} }?><?phprequire ‘config.php’;session_destroy();header(‘Location: index.php’);?>",php,https://medium.com/@sujith1396/php-pdo-connection-and-authentication-registration-login-and-logout-909e950b10cd?source=tag_archive---------2-----------------------
What’s new in PHP 8 !,"Zache AbdelatifJun 5, 2020·10 min readPHP 8 will be released on December 3, 2020. It’s a new major version, which means that it will introduce some breaking changes, as well as lots of new features and performance improvements. PHP 8 is in very active development right now, with its first alpha expected on June 18, 2020.Because of the breaking changes, there’s a higher chance you’ll need to make some changes in your code to get it running on PHP 8. If you’ve kept up to date with the latest releases though, the upgrade shouldn’t be too hard, since most breaking changes were deprecated before in the 7.* versions. And don’t worry, all these deprecations are listed in this post.Besides breaking changes, PHP 8 also brings a nice set of new features such as the JIT compiler, union types, attributes, and more.Starting with new features, remember that PHP 8 is still in active development, so this list will grow over time.Given the dynamically typed nature of PHP, there are lots of cases where union types can be useful. Union types are a collection of two or more types which indicate that either one of those can be used.Note that void can never be part of a union type, since it indicates ""no return value at all"". Furthermore, nullable unions can be written using |null, or by using the existing ? notation:The JIT — just in time — compiler promises significant performance improvements, albeit not always within the context of web requests. There haven’t been any accurate benchmarks done at this point, but they sure will come.If you want to know more about what the JIT can do for PHP, you can read another post I wrote about it here.Attributes, commonly known as annotations in other languages, offers a way to add meta data to classes, without having to parse docblocks.As for a quick look, here’s an example of what attributes look like, from the RFC:If you want to take a deep dive in how attributes work, and how you can build your own; you can read about attributes in depth on this blog.This RFC adds syntactic sugar to create value objects or data transfer objects. Instead of specifying class properties and a constructor for them, PHP can now combine them into one.Instead of doing this:You can now do this:There are lots of variations of this syntax, head over to the RFC to read about them.While it was already possible to return self, static wasn't a valid return type until PHP 8. Given PHP's dynamically typed nature, it's a feature that will be useful to many developers.Some might call it a necessary evil: the mixed type causes many to have mixed feelings. There's a very good argument to make for it though: a missing type can mean lots of things in PHP:Because of the reasons above, it’s a good thing the mixed type is added. mixed itself means one of these types:Note that mixed can also be used as a parameter or property type, not just as a return type.Also note that since mixed already includes null, it's not allowed to make it nullable. The following will trigger an error:This RFC changes throw from being a statement to being an expression, which makes it possible to throw exception in many new places:Built upon the weakrefs RFC that was added in PHP 7.4, a WeakMap implementation is added in PHP 8. WeakMaps hold references to objects, which don't prevent those objects from being garbage collected.Take the example of ORMs, they often implement caches which hold references to entity classes to improve the performance of relations between entities. These entity objects can not be garbage collected, as long as this cache has a reference to them, even if the cache is the only thing referencing them.If this caching layer uses weak references and maps instead, PHP will garbage collect these objects when nothing else references them anymore. Especially in the case of ORMs, which can manage several hundreds, if not thousands of entities within a request; weak maps can offer a better, more resource friendly way of dealing with these objects.Here’s what weak maps look like, an example from the RFC:A small, yet useful, new feature: it’s now possible to use ::class on objects, instead of having to use get_class() on them. It works the same way as get_class().Whenever you wanted to catch an exception before PHP 8, you had to store it in a variable, regardless whether you used that variable or not. With non-capturing catches, you can omit the variable, so instead of this:You can now do this:Note that it’s required to always specify the type, you’re not allowed to have an empty catch. If you want to catch all exceptions and errors, you can use Throwable as the catching type.Already possible when calling a function, trailing comma support was still lacking in parameter lists. It’s now allowed in PHP 8, meaning you can do the following:You can already create a DateTime object from a DateTimeImmutable object using DateTime::createFromImmutable($immutableDateTime), but the other way around was tricky. By adding DateTime::createFromInterface() and DatetimeImmutable::createFromInterface() there's now a generalised way to convert DateTime and DateTimeImmutable objects to each other.The Stringable interface can be used to type hint anything that is a string or implements __toString(). Furthermore, whenever a class implements __toString(), it automatically implements the interface behind the scenes and there's no need to manually implement it.Some might say it’s long overdue, but we finally don’t have to rely on strpos anymore to know whether a string contains another string.Instead of doing this:You can now do thisTwo other ones long overdue, these two functions are now added in the core.The new fdiv() function does something similar as the fmod() and intdiv() functions, which allows for division by 0. Instead of errors you'll get INF, -INF or NAN, depending on the case.get_debug_type() returns the type of a variable. Sounds like something gettype() would do? get_debug_type() returns more useful output for arrays, strings, anonymous classes and objects.For example, calling gettype() on a class \Foo\Bar would return object. Using get_debug_type() will return the class name.A full list of differences between get_debug_type() and gettype() can be found in the RFC.Resources are special variables in PHP, referring to external resources. One example is a MySQL connection, another one a file handle.Each one of those resources gets assigned an ID, though previously the only way to know that id was to cast the resource to int:PHP 8 adds the get_resource_id() functions, making this operation more obvious and type-safe:Traits can specify abstract methods which must be implemented by the classes using them. There’s a caveat though: before PHP 8 the signature of these method implementations weren’t validated. The following was valid:PHP 8 will perform proper method signature validation when using a trait and implementing its abstract methods. This means you’ll need to write this instead:The token_get_all() function returns an array of values. This RFC adds a PhpToken class with a PhpToken::getAll() method. This implementation works with objects instead of plain values. It consumes less memory and is easier to read.From the RFC: “the Uniform Variable Syntax RFC resolved a number of inconsistencies in PHP’s variable syntax. This RFC intends to address a small handful of cases that were overlooked.”Lots of people pitched in to add proper type annotations to all internal functions. This was a long standing issue, and finally solvable with all the changes made to PHP in previous versions. This means that internal functions and methods will have complete type information in reflection.Previously it was possible to compile PHP without the JSON extension enabled, this is not possible anymore. Since JSON is so widely used, it’s best developers can always rely on it being there, instead of having to ensure the extension exist first.As mentioned before: this is a major update and thus there will be breaking changes. The best thing to do is take a look at the full list of breaking changes over at the UPGRADING document.Many of these breaking changes have been deprecated in previous 7.* versions though, so if you’ve been staying up-to-date over the years, it shouldn’t be all that hard to upgrade to PHP 8.User-defined functions in PHP will already throw TypeErrors, but internal functions did not, they rather emitted warnings and returned null. As of PHP 8 the behaviour of internal functions have been made consistent.Lots of errors that previously only triggered warnings or notices, have been converted to proper errors. The following warnings were changed.It’s possible that this change might reveal errors that again were hidden before PHP 8. Make sure to set display_errors=Off on your production servers!It’s now E_ALL instead of everything but E_NOTICE and E_DEPRECATED. This means that many errors might pop up which were previously silently ignored, though probably already existent before PHP 8.From the RFC: The current default error mode for PDO is silent. This means that when an SQL error occurs, no errors or warnings may be emitted and no exceptions thrown unless the developer implements their own explicit error handling.This RFC changes the default error will change to PDO::ERRMODE_EXCEPTION in PHP 8.While already deprecated in PHP 7.4, this change is now taken into effect. If you’d write something like this:PHP would previously interpret it like this:PHP 8 will make it so that it’s interpreted like this:Before PHP 8, it was possible to apply arithmetic or bitwise operators on arrays, resources or objects. This isn’t possible anymore, and will throw a TypeError:Three method signatures of reflection classes have been changed:Have now become:The upgrading guide specifies that if you extend these classes, and still want to support both PHP 7 and PHP 8, the following signatures are allowed:From the RFC: Inheritance errors due to incompatible method signatures currently either throw a fatal error or a warning depending on the cause of the error and the inheritance hierarchy.During the PHP 7.* development, several deprecations were added that are now finalised in PHP 8.",php,https://medium.com/@a-zache/whats-new-in-php-8-4a6258efbd77?source=tag_archive---------0-----------------------
MemoryLess the new ServerLess,"Brian AkeyJun 2, 2020·3 min readJust as we have gone from servers to instances, instances to containers, and containers to functions. The idea of memoryless to move low usage services to nonrunning services. With a few small servers, you can provide many apps without the need for many resources. Using memoryless and containers with intelligent load balances services can be ramped up during the day and turned on to memoryless systems at night to save money. It also reduces an application's carbon footprint. Simplicity and demand-based services.gitlab.comHaving worked at many startups I have seen many Internet services that get more traffic from the monitoring service than from actual users.With many companies needing to find ways to save money this way is an easy solution. Only run things when used.Monitoring can still be done by using a check on the service's primary script. Check the port and the main script without having to light up any real services and wasting resources.Code can be added at any time and is available on the next request. It can support many types of languages. Anything that can be run once and handle stdin and stdout. Using alpine the base memory footprint is 40 megabytes. It can be run in an instance, a container, or even an on-demand ECS cluster.It can also be a great tool for microservices that only need to run infrequently. Many sites see low usage until a company big. Smaller microservices that don’t see a lot of usage are a great example of what can go MemoryLess. We can’t all be Facebook and Google.Even though the service is simple and uses low resources it can still be scaled up to enable large environments. With current services like react apps we keep the application running in memory even when it is not needed. A huge waste of resources. If the site is seeing thousands of connections per second normal methods work great. MemoryLess is a great idea for anything smaller keeping it on disk is better.Currently, it takes hundreds if not thousands of gigabytes of ram to get all the site code loaded into memory. With the MemoryLess system, 40 meg is all that is needed to start the service and only enough memory to run each piece of code so one instance or container can run the whole site.Used in combination with current methods you can balance cost and speed. One example, you can run MemoryLess during the nights and use container pods during the day for speed. Everything works all the time without the need for so much hardware.What about on-demand or functions? They still have a bring up a whole container and the subsystems like pm2 for node applications and this takes time. With MemoryLess you run the code when you need it. Nothing more and nothing less. Much quicker than starting up a full container. It also goes hand in hand with microservices because code can be simplified into a smaller application. A directory full of files is much easier to administer than building a Kubernetes cluster full of pods using tons of resources.MemoryLess: simple, and just this side of free.",php,https://medium.com/@bakey/memoryless-the-new-serverless-f2e8b111dc27?source=tag_archive---------7-----------------------
Laravel: Registering your Blade directives the cool way,"Italo Baeza CabreraJun 1, 2020·2 min readCustom Blade directives in Laravel are very handy to allow some quick logic around an “expression” without having to pollute our views with verbose code, and allows to have them reachable in any view.The problem of these is that registering them has been always a pain in the ass. What a directive must return is a string of inert PHP code but with a live the expression variable. The Laravel documentation points this is the way:This can become very cumbersome when you need to make logic that is more than just one line. Most IDE and Code Editors don’t help when you are using this kind of code, so we can’t optimally get code assistance when using this kind of strings.Alternatively, we can register a directive by just reading a PHP file. That’s it. Yep. You heard that right.We can create a directive in a PHP file anywhere and then retrieve its contents from the function itself. For that, we can use the File facade.Since we’re retrieving the PHP file as a string, the expression variable will become a string unless we replace it with the expression value itself. After that, we will ensure the contents finish with a PHP closing tag.And what contains our directive? Well, it’s just plain PHP code:This will allow to use this directive in our Blade views easily:The above will be compiled toand will be rendered as:This code is available in my Laratraits package for Laravel, with a lot of other useful tools. Check it out if you don’t want to reinvent the wheel.github.com",php,https://medium.com/@darkghosthunter/laravel-registering-your-blade-directives-the-cool-way-ac6ebc3e5c90?source=tag_archive---------3-----------------------
Laravel: Throttling any method class elegantly,"Italo Baeza CabreraJun 4, 2020·2 min readOne of the thing the new Rate Limiter will introduce in Laravel 8 is the brain-dead easy way to throttle certain routes. It’s not documented at time of writing, but I fully expect it will be what developers need to throttle anything before hitting the controller logic without hacking their way in.While this will become very handy for almost everybody, sometimes you won’t want to throttle the whole request, but rather, a single piece of it.If you want to throttle a piece of code, you must use the Rate Limiter manually. Then, something that was just one line of code becomes four.Instead of this doing this, I decided to create a very handy throttler for a class that could wrap this logic in one line.Yes, you can. For this, we can use a very simple concept of wrapping the object into another that will check the throttling, and call the method of the target object if it’s not throttled.You can perfectly do something like throttling the next object method to call this way:For this to work, we can create a method on a trait that will create an object that uses the Rate Limiter, checks the method to throttle, and then executes the method if the limit has not been set.First, we will make a convenient trait we can add to any object we want.What is that Throttler class? Well, it’s just a simple class that encapsulates the object we are using. It will pass any method to it underlying target after checking the rate limiting.The magic happens in the _call() method. We will simply check for the class name and method name combination, and check for the attempts number. If the it’s permitted, the method will be executed, otherwise it won’t. In both cases, we return the target object.With this, we can call the throttler, check automatically if we have called the same method too many times, and if not, call it for real. We can even chain methods with this throttle:Of course you can expand on this, like using a custom key, and allow to clear the throttle key or reset it’s condition, and finally, use a Closure to execute alternate logic if the action is throttled.You can find this trait and other helpers in my Laratraits package. Give it a chance if you find something useful to use in your project.github.com",php,https://medium.com/@darkghosthunter/laravel-throttling-any-method-class-elegantly-81f12168b163?source=tag_archive---------1-----------------------
Laravel package ecosystem,"Packages play a big role in the Laravel scene. How big and which packages? To find it out we took the data from the Packagist.org API and filtered out the Laravel packages.How did we identify the Laravel packages? By looking at composer.json. We classified a package as belonging to the Laravel ecosystem if name, description, keywords or type fields contained the string laravel. We also considered it to be a Laravel package if it has laravel/framework or any of the illuminate/* subtree splits as a dependancy. And finally — the laravel key in extra field is also a trigger.While these criteria do not guarantee a package to be Laravel-only, they do show that the package was made with Laravel in mind.By naively looking at the list of the most downloaded packages, we see quite a bit of unexciting results — core packages and packages with a plenty of uses outside Laravel projects.Perhaps a more interesting toplist would be one with these “false positives” removed. We manually filtered out the top 30 packages that are made for Laravel projects and are installed deliberately. We also loosely classified the purposes of those packages.Explanation of purposes:The development tools are most prevalent on this list followed by integration.We expeceted to see more of the Spatie packages at the very top, but it turns out that maatwebsite/excel and the tools by Barry vd. Heuvel are more universally used.In total there were 47337 Laravel packages identified from 23167 different developers. The distribution is quite even with lots and lots of small players.Half of packages (23668) were made by the top 3927 developers which is far more than would be if Price’s law was obeyed here. 15042 of the developers have only one package published.If we sort package developers by number of packages, we get to see some quite unheard names. Here are the most prolific package developers along with the download numbers summed over their packages.Apparently there are quite a few developers who make a lot of components and put them up without much advertisement. To see more of the better known names and how they compare, let us sort the list by total downloads.If one sorts this list by monthly dowloads instead, the picture is slightly different.These distributions are extremely top heavy. Regardless if you order by daily, monthly or total downloads, more than half of all downloads go to top 6 package developers. For example, out of 5.65 million daily package downloads, 2.85 million are split among laravel, illuminate, nesbot, spatie, fideloper and barryvdh.We can use something a kin to Hirsch index to assess the depth of the scene. There are only 317 developers that get 317 or more daily downloads. 1118 vendors get 1118 or more monthly downloads and the same number is 3003 for total numbers.Our licensing overview is not very precise as various strings are used for the same license, for example gpl-3.0, gpl 3.0, gpl3, gplv3, even gnu general public license version 3 and many more for the same license.Luckily, the main conclusions are clear as the string mit is the license of 37502 Laravel packages. 6304 packages are unlicensed, More than 500 packages are licensed under each of Apache 2.0 and GPL 3.0. And there are also few hundred packages with BSD 2, BSD 3 and GPL 2.0.79% of Laravel packages are MIT-licensed, 14% are unlicensed and the other licenses are each used in used in 2% of projects or less. This is probably in part influenced by Laravel which itself is MIT licensed. Among all of the packages the prevalence of the MIT license is considerably smaller — 60%.The number of packages for different frameworks shows the involvment of the community in the development of the framework tools and how much they are willing to share their code with others. We also counted the abandoned packages and their percentage to see which communities are thriving and which are limping.On the other hand, the total download numbers are reflective of how much the packages are important in the development of everyday projects.Please keep in mind that these download numbers represent all of the packages related to a framework not just the core. It’s just like the number of packages listed above. The criteria for classifying a package as belonging to certain framework are here. One package can belong to ecosystems of multiple frameworks.It should be noted that the top position doesn’t mean Symfony projects download the most packages as some of the Symfony core is used by Laravel and many of the Symfony packages can also be used in Laravel projects.It is also noteworthy that Zend packages have almost as many monthly downloads as Laravel even though the project is no longer maintained.As a farewell we shall leave you with the list of most downloaded PHP packages overall.The data displayed in this article was compiled mainly by Gatis Šūpulnieks in April and May of 2020 as a part of his thesis “Laravel package development and usage”.Relevant sources:",php,https://itnext.io/laravel-package-ecosystem-4afd53fad192?source=tag_archive---------0-----------------------
PHP on Localhost (SIMPLE),"Armin ProductionJun 5, 2020·1 min readInstall PHP 7.3Install PHP 7.2Install PHP 7.1Install PHP 5.6 — Running with OSX 10.11 El Capitan or lower versions.for windows user … oh J christ … follow these instructions on “site point” it is too long to be fit inside this small and quick tutorialhttps://www.sitepoint.com/how-to-install-php-on-windows/Windows : C:/folder/index.phpMac :Navigate to that folder : cd [folder]type this command :that 4000 is the port and localhost is where u can run the local serverany code inside index.php in ur folder with php codes will be shown inside localhost:4000 … just open localhost:4000 in ur browser",php,https://medium.com/@arminproduction/php-on-localhost-simple-edf9577a662?source=tag_archive---------4-----------------------
Introduction to Laravel,"Laravel is a free, open-source PHP web framework, created by Taylor Otwell. It is a powerful Model-View-Controller (MVC) PHP framework, designed for developers who need a simple and elegant toolkit to create full-featured web applications.Laravel offers a rich set of functionalities which incorporates the basic features of PHP frameworks.It saves a lot of time if you are planning to develop a website from scratch.When you design a web application based on Laravel it offers you the following advantages.Provide 20 built-in libraries and modules which help in the enhancement of the application.Laravel includes features and help in testing through various test cases. This helps in maintaining the code as per the requirements.Laravel provides a flexible approach to the user to define routes in the web application. It helps to scale the application in a better way and increases performance.A web application designed in Laravel can be run in different environments. Laravel provides a consistent approach to handle the configuration in an efficient way.Laravel incorporates a query builder that helps in querying databases using various simple chain methods.Schema Builder maintains the database definitions and schema in PHP code and maintains a track of changes with respect to database migrations.Laravel uses the Blade template engine, a lightweight template language used to design hierarchical blocks and layouts with predefined blocks that include dynamic content.Laravel includes a mail class that helps in sending mail with rich content and attachments from the web application.Laravel eases designing authentication as it includes features such as register, forgot password, and send password reminders.Laravel uses Redis to connect to an existing session and general-purpose cache.These queues help in completing tasks in an easier manner without waiting for the previous task to be completed.Laravel 5.1 includes Command Bus which helps in executing commands and dispatch events in a simple way. The commands in the Laravel act as per the application’s lifecycle.Thank you for reading. ❤Stay tuned for more about Laravel.🤞",php,https://medium.com/linkit-intecs/laravel-e2c36a9bda13?source=tag_archive---------3-----------------------
The PHP Microsite Boilerplate,"Jens KuerschnerJun 3, 2020·4 min readThere are cool frameworks for Frontend and Backend. Many cool languages and best practice. Still, they all focus on larger projects.What if you just want to build a high performance, highly secure microsite, which can be easily moved to basically any cloud or webhoster (even shared hosting)?Have a look at my PHP Microsite Boilerplate (Framework)!→ Demo page.→ GitHub project page.As with many others, my web development career started with PHP. And I still love it — not saying it is a good choice for everything.Over the last years, I have built hundreds of digital products and website. From small marketing pages to full-scale Software-as-a-Service solutions.While, in the latter case, you usually put some work into finding the right architecture, the right languages, and the right hosting environment; the first case is different.If you need to build a simple website quickly, you either have no server-side functionality, or a huge overhead (of code you don’t need), or critical dependencies, or low performance and bad security, or it definitely does not run on cheap shared hosting.However, it is a common case to build a website, fast, with almost no resources, and no money for basically any infrastructure.That’s where this PHP Microsite Boilerplate Framework steps in!It includes all the basic setup, which would take you days, makes sure you benefit from great performance and security, while also providing you with some kick-ass features.PHP is a good way to go, since it should run on basically every hosting environment.It does not include any complex modules, crazy structures, or other overhead code. This makes it extremely easy to read and enables you to use it on every device in every situation (no need to spend hours setting up your IDE).Still, it includes a lot of handy features. From easy PWA and AMP support out of the box to many SEO functionalities (like automatically generated sitemaps).The maybe most important feature is its Directus CMS support. Directus is a modern, open source, headless content management system, which can be fully customized without the huge overhead of WordPress. This microsite boilerplate is already prepared to display the content from Directus. So, even if you need to add a fancy CMS for your marketing team, this package got your back (in combination with Directus).Bottom line: If you are not planning to create the next multi-billion-dollar AI application, but rather need to get something out there fast, without a lot of resources, but high expectations, give it a try.Didn’t you also always dream about reaching full score at Google’s Lighthouse report? Spoiler Alert: You get some confetti with this framework.Such a project is never finished. Still, it is absolutely ready for production use.Talking about limitations, the maybe only big one is the size of your project.Theoretically, you can build almost anything on top of it. But you shouldn’t!One important feature is getting rid of code structure complexity (e.g. clean MVC, dependency injections, …). This complexity, however, is necessary when scaling development — like handing it over to other developers (or a client), working on it as a team, etc.So, if you are using it for microsites (as the name implies) or a first MVP of your product, it is really awesome! But it is definitely not thought to be the base of the next Facebook.Discover the demo page and get the source code from GitHub.I would love to hear your feedback. Any kind of contribution (it’s Open Source) is, of course, appreciated.Enjoy!→ Demo page.→ GitHub project page.",php,https://medium.com/@jenskuerschner/php-microsite-boilerplate-ee7caea7b7d3?source=tag_archive---------0-----------------------
Upgrading a laravel backend over 9 releases — Part 1,"It was July 2014. One of our earliest US customers required an iOS mobile app. We were building the app from scratch and end to end. While iOS was new to us, for backend, we wanted to stick to a familiar programming language. At that time, most of our developers were php developers. Their previous experience was with codeignitor or yii framework. Around this time, laravel framework was becoming a popular choice in php world. We decided to build the app backend in laravel (which was in version 4.2).We found the framework was easy to pick up and had excellent documentation. Our backend was pretty much a collection of API/web services, except for a couple of Admin web pages. Given this, we had decided early on that we will write unit tests. Unit test was new to our developers, but again, laravel had excellent support and documentation for it.The unit tests were useful also to refine our APIs — request parameters, response data and format, and error handling.Our application went live in Nov 2014.In a few months, our customer was back with v2.0 requirements for the app. Our iOS mobile app was in ObjectiveC and it did not require much changes to support newer iOS version. Our focus was on adding new features.However, laravel had released v5.0 and was on v5.1 (LTS) by then. We now had a dilemma. To upgrade or be on the older (v4.2) version. One of the factors which tilted things towards upgrading was that v5.1 was an LTS version with support/fixes upto 2018. Another factor was, we were adding a bunch of new features in v2.0. Given that we were going to write a lot of code, we may as well write it on the newer version.Upgrading from 4.2 to 5.1 (we by-passed 5.0) pretty much involved, as documentedFresh Install, Then MigrateThe folder structures had changed, so had many of the constructs in code and tests. It was a bit painful, but our unit tests helped in ensuring we were not going wrong! We also added unit tests for the newer features thereby having a safety net for future refactoring/changes!While our app went on to v3.0 and v4.0 in the next two years, our backend continued to remain in laravel v5.1.The app was finally retired a year or so back.As part of our continuous learning in front-end technologies, we have our junior developers build apps in various frameworks as part of their learning. These typically need a backend to talk to, and we thought, why not revive the backend that we had built?A lot of changes had taken place by this time in the development environment. php 5.x had reached end of life. Mac OS Catalina had php 7.3.x by default. Our servers running Ubuntu 18.04 had php 7.2. What about laravel? Laravel v7 was being released in March 2020. And there had been 8 releases in between — v5.2, v5.3, v5.4, v5.5 (LTS), v5.6, v5.7, v5.8 and v6.0(LTS)!Do we try to run the v5.1 backend in the latest php and try to troubleshoot issues? Or do we try to upgrade the app to the latest laravel version?While we were grappling with the dilemma came Covid-19 and the shutdown!Our application was fairly simple and only used a few additional dependencies (push notifications, mandrill, to name a few). And we had our unit tests! So why not give a shot upgrading?Each release of laravel brought a number of new features, as well as changes. The recommended upgrade strategy is to upgrade one version at a time — in other words, from v5.1 to v5.2 and then from v5.2 to v5.3 and so on. Laravel has excellent and very detailed Upgrade Guide for each version!After doing a pilot upgrade with php 7.4, it was clear that a lot more things were broken in our laravel project, when run with php 7.4. So we decided to use php 7.3 to do our project’s laravel upgrade.As documented, the first step in the upgrade was to update composer.json.laravel/framework version has to be updated to 5.2.*The additional changes for v5.2 was to add ""symfony/dom-crawler"": ""~3.0"" and ""symfony/css-selector"": ""~3.0"" to require-dev section.We also had to update config/auth.php with the contents specified in the upgrade guide.Changing composer.json and running composer update installed the updated dependencies in our development system. There were a few warnings as well. We ignored the one below.Our unit tests ran with out any error with php 7.3. In php 7.4, we observed a couple of issues:Mockery library error. Our project used v0.9.3 of the library,phpunit errorRight, so we had our tasks cut out, but with php 7.3, we had now upgraded to laravel v5.2 and our tests had all passed. The application was running fine!Following the upgrade steps, the first step was to update the version of laravel/framework in composer.json to 5.3.*.The next was to update the versions of symfony/css-selector and symfony/dom-crawler dependencies to 3.1.*Once composer update was done, the next step was to do the following:You may remove the arguments from the boot method on the EventServiceProvider, RouteServiceProvider, and AuthServiceProvider classes. Any calls to the given arguments may be converted to use the equivalent facade instead.For instancetoOnce these were done, the tests ran fine, and we were now in laravel v5.3The following dependencies had to be updated as per upgrade guide.laravel/framework to 5.4.* and phpunit/phpunit to ~5.7We also did view:clear and route:clear which flush the various caches.Now, the testing layer had been re-written in laravel v5.4. We could have continued to use the v5.3 testing layer by installing the laravel/browser-kit-testing or Laravel Dusk. However, since our goal was to move forward, we didn’t.Interestingly only one of our test broke. This was the basic test which hit the landing page.A bit of research led to the relevant discussion in stackoverflow. We ended up using get instead of visit and assertStatus instead of see, since our application was a web services/API app.We encountered one more problem.Ah, our code had this and it led us to this discussion, which suggested the fix.We still had the old app\Http\Controllers\Auth\AuthController and not the separate app\Http\Controllers\Auth\RegisterController and app\Http\Controllers\Auth\LoginController that came in a fresh install of laravel v5.4.We ended up replacingwithWe also needed to tweak our login and logout methods a bit.We were now on laravel v5.4!In the second and concluding part, we will look at the changes we did from v5.4 to reach our end goal — v7.x!",php,https://medium.com/innoventes/upgrading-a-laravel-backend-over-9-releases-part-1-157be5749d5d?source=tag_archive---------2-----------------------
7 reasons to use Laravel,"Laravel is the open-source PHP framework for advanced backend development. It helps to create wonderful, scalable applications in a short term and provides high reliability. Laravel makes software development a creative, enjoyable and truly fulfilling experience and includes a lot out of the box solutions. It reuses the existing components of different frameworks. That helps you to ease your routine tasks, used in most web projects such as routing, authentication, sessions, and caching. Apart from being accessible, it also provides powerful tools needed for massive, robust applications for custom software development services.Here are 7 key features that make Laravel such an amazing framework to use:Today, every business is looking for the most rapid implementation of new technologies to stay afloat and successfully compete in IT market. Using Laravel will help you to significantly speed up the production process by using the best practices and reusing ready-made software solutions.Now you don’t have lunch breaks at home, you have to create your personal routine and be rigid about it. For example: get up, get ready, make breakfast, and start. And don’t forget break times scheduled in advance. The best way of the routine organization will stick to the same schedule as it was in the office.2. Increased productivity. CachingAnother good reason to choose Laravel is an excellent web application performance. This is achieved through many tools that increase the speed of web pages generating.Basically, these are caching tools. By default, file system level caching is enabled, but you can change this behavior and use more powerful tools like non-SQL databases such as REDIS, Memcache or APC. These databases are famous for storing data in the form of key-value pairs and do this in the server’s RAM. Due to this, the data access time is sharply reduced and it lets developers to cache entire pages, or parts of them, as well as arrays of objects — the results of database queries, thereby reducing their number and speeding page loading. In this case, the main thing for the developer is to invalidate the cache and delete obsolete data in time when they change.3. Mail and NotificationsLaravel provides a clean and simple API on top of the popular SwiftMailer library with drivers for SMTP, Mailgun, SparkPost, Amazon SES to send mail through local or cloud services. Moreover, Laravel provides support for sending notifications through many other delivery channels, including SMS and Slack.4. Database MigrationThis technology helps the programmer easily and conveniently deploy finished database structure without direct interaction with the database management system, that means: you don’t need to drag database dumps.5. MVC architectureThanks to following the MVC architecture, a clear separation is achieved between 3 abstract layers of the application:They become independent from each other and can be used separately.6. Deferred task systemLaravel has a queuing system for long-running processes. Also an API for many different backends for managing these queues. This allows you to postpone the execution of processes that take a long time, so as not to slow down the delivery of pages to the user. For example, delayed sending of letters to users.7. Friendly codeWhen developing libraries for distribution and reuse, all developers strive to write code in such a way that it not only works as it should but understandable to other developers. Therefore, working with the framework brings not only material but also moral dividends.Original article at webspaceteam.com",php,https://medium.com/webspace-team/7-reasons-to-use-laravel-882261726a7c?source=tag_archive---------8-----------------------
"Configurar Apache, PHP y MySQL en entorno de desarrollo local","Esta configuración está basada en Debian 9.8Una vez tengamos instalada nuestra máquina virtual con Debian 9.8 (Si no tienes conocimiento de cómo instalarla te dejo el siguiente video: ¿Como instalar debian (terminal) en VirtualBox?)# Instalamos los módulos de apacheapt-get updateapt-get upgradeapt-get -y install dirmngr apache2-utils# Habilitamos los siguientes módulos de Apachea2enmod rewritea2enmod headersa2enmod authz_hosta2enmod dira2enmod expiresa2enmod mimea2enmod alias#Reiniciamos apache/etc/init.d/apache2 restart# Instala PHPapt-get install php#Instalamos las librerías adicionales para PHPapt-get -y php7.0-mysql php7.0-bcmath php7.0-curl php7.0-gd php7.0-imap php7.0-json php7.0-mbstring php7.0-mcrypt php7.0-simplexml php7.0-zip# Actualizamos variable php.inised -i “s/;fastcgi.logging = .*/fastcgi.logging = 0/” /etc/php/7.0/apache2/php.inised -i “s/;date.timezone =.*/date.timezone = America\/Mexico_City/” /etc/php/7.0/apache2/php.inised -i “s/max_execution_time = 30.*/max_execution_time = 120/” /etc/php/7.0/apache2/php.inised -i “s/max_input_time = 60.*/max_input_time = 120/” /etc/php/7.0/apache2/php.inised -i “s/;mbstring.func_overload.*/mbstring.func_overload = 0/” /etc/php/7.0/apache2/php.inised -i “s/memory_limit.*/memory_limit = 512M/” /etc/php/7.0/apache2/php.inised -i “s/post_max_size =.*/post_max_size = 100M/” /etc/php/7.0/apache2/php.inised -i “s/session.cookie_httponly =.*/session.cookie_httponly = 1/” /etc/php/7.0/apache2/php.inised -i “s/upload_max_filesize =.*/upload_max_filesize = 100M/” /etc/php/7.0/apache2/php.ini# Actualizamos el DocumentRoot para habilitar el servidor apacheecho “<VirtualHost *:80>” > /etc/apache2/sites-available/000-default.confecho “ <Directory /var/www/html/>” >> /etc/apache2/sites-available/000-default.confecho “ Options Indexes FollowSymLinks” >> /etc/apache2/sites-available/000-default.confecho “ AllowOverride All” >> /etc/apache2/sites-available/000-default.confecho “ Require all granted” >> /etc/apache2/sites-available/000-default.confecho “ </Directory>” >> /etc/apache2/sites-available/000-default.confecho “ ServerAdmin webmaster@localhost” >> /etc/apache2/sites-available/000-default.confecho “ DocumentRoot /var/www/html” >> /etc/apache2/sites-available/000-default.confecho “ ErrorLog ${APACHE_LOG_DIR}/error.log” >> /etc/apache2/sites-available/000-default.confecho “ CustomLog ${APACHE_LOG_DIR}/access.log combined” >> /etc/apache2/sites-available/000-default.confecho “</VirtualHost>” >> /etc/apache2/sites-available/000-default.conf# Instalamos MySQLapt-key del A4A9406876FCBD3C456770C88C718D3B5072E1F5export GNUPGHOME=$(mktemp -d)gpg — keyserver ha.pool.sks-keyservers.net — recv-keys A4A9406876FCBD3C456770C88C718D3B5072E1F5gpg — export A4A9406876FCBD3C456770C88C718D3B5072E1F5 > /etc/apt/trusted.gpg.d/mysql.gpgapt-key listapt-get updatewget https://dev.mysql.com/get/mysql-apt-config_0.8.8-1_all.debdpkg -i mysql-apt-config_0.8.8–1_all.debapt-get updateapt -y install mysql-server mysql-clientsystemctl enable mysql && systemctl start mysqlecho ‘sql_mode = “STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION”’ >> /etc/mysql/mysql.conf.d/mysqld.cnfrm mysql-apt-config_0.8.8–1_all.debEste bloque contiene contenido inesperado o no válido.ResuelveConvertir a HTML/etc/init.d/mysql restart/etc/init.d/apache2 restartexit 0;Para conocer más acerca de un entorno de desarrollo local, te invito a ver mis vídeos en mi canal de Youtube: R4nbryQue sigas pasando un excelente día.¡Hasta la próxima!",php,https://medium.com/configurar-apache-php-y-mysql-en-entorno-de/esta-configuraci%C3%B3n-est%C3%A1-basada-en-debian-9-8-d617ab85fedb?source=tag_archive---------6-----------------------
What is CSS? | Top 4 Advantages of CSS (cascading style sheet),"iTriangle TechnolabsJun 3, 2020·3 min readCSS stands for Cascading Style Sheets. HTML and CSS are behind the technique of creating a webpage. The use of HTML gives the webpage a shape and the use of CSS gives the webpage a new and attractive look. HTML and CSS are always used together. Without CSS, we can use html but without html, css cannot be used.HTML and CSS is a computer language that is very simple and can be easily taught. To write the code of html and css we need a text editor such as Notepad. After writing these codes, a web browser is required to view it through internet.Many tags are used in HTML such as header tag <h1>, font tag <font>, table tag <table>, image tag <img> etc. All these tags are also used together with css to show them more well in the browser.Using CSS, we can show the text of the webpage in good color, control the space between the styles and paragraphs of fonts, the images in the background and the use of what color in the background will give the webpage a good look. Css is used to set things. The css html document gives a completely new look which attracts more users.We will not have to write css code repeatedly to create a different web page. And by using the css code written only once, we can create as many web pages as we want, in which we save a lot of time.If we use css, we do not need to write the attributes of the html tag again and again. Just once according to the rule of css, by writing the attributes of the tag and applying it in the web page, that tag will appear correctly everywhere. Therefore, the tag will not have to write the same code again and again to appear in different places on the web page, and if there is less code, then the web page will load quickly in the browser.To change the style of web page completely, just changing the style code of css once will automatically change all the elements used in html at once and change all elements one by one will not be necessaryplatform independent means that we can use css in any platform like windows, linux, macintosh etc. And it also supports all the latest browsers.This was CSS and some related information that is used to create a web page. Hope you have helped a lot in this article about What is CSS. If you want more information related to this, then you can comment below.iTriangle Technolabs is top-ranked and expertise web development company, which follows the current and popular trends in the it industry to deliver clients a website with all trending features.If you require a CSS Development with all the trending features in the market, our team is happily available for you 24 by 7.Originally published at http://itriangletechnolabs.com on June 3, 2020.",php,https://medium.com/@itriangletechnolabs/what-is-css-top-4-advantages-of-css-cascading-style-sheet-8bb15ad3dfdf?source=tag_archive---------11-----------------------
".htaccess: Invalid command ‘Header’, perhaps misspelled or defined by a module not included in the server configuration","Aluya MatthewJun 1, 2020·1 min readYou have ever come across this error message ‘.htaccess: Invalid command ‘Header’, perhaps misspelled or defined by a module not included in the server configuration’ ? It usually logged in var/apache2/error.log file.The effect on the browser would be 500 internal error. Its truly an internal error.The error is due to htaccess configuration. Its header related error. Caused because mod_headers is not enabled. Anyways here is how you can fix it. You would need root access to the server. Login as root and run the command below on a terminalThat should fix the error. Happy coding",php,https://medium.com/@discretematt/htaccess-invalid-command-header-perhaps-misspelled-or-defined-by-a-module-not-included-in-the-8f9cc9278f4f?source=tag_archive---------5-----------------------
Docker for PHP: The Abstract,"DigitalactJun 3, 2020·4 min readBundles like XAMPP and WAMP have always been the straightforward choice for developing PHP applications. One-click installation and almost zero configuration makes it easy to bootstrap a development environment across different operating systems. However, like almost every magic, it has own trade-offs.Using bundles or local installation of php and web server were okay at first. Things could go relatively painless with small number of non-complex dependencies and a handful of developers. A single page installation guide could cut it. As the dependencies grows, it becomes harder and sometimes imposible to replicate same environment across the operating systems. Real show begins when the number of developers starts to increase in the project. Just multiply the burden with the number of developers and you can see the picture. JThere are several ways available to overcome this operational cost. One of the most popular ways is utilizing linux containers and it’s defacto orchestrator docker. At Digitalact, we don’t install any server specific tools in our workstations. Everything is dockerized. In this post, we will talk about what we do and why when dockerizing a php application.When you consider dockerize an application the very first thing you must decide is base image. We are using debian-slim flavour as our base image. There is one big reason behind this decision: Debian isn’t a derivative distribution. It is the root. This is the where the journey begins. Rock solid packages with professional maintainers. Plus, debian-slim is one of the most “clean” base image out there.What is inside our Dockerfile?Upgrade PackagesJust after putting base image definition right at the top, we do a package upgrade immediately. This is pretty straightforward. Start with up to date image!Install Pre-requisitesWe install most basic packages like unzip, git, curl, cron etc. Just because we have choosen a bare-minimal image to build with, these most basic tools don’t exist in the image.Install PHPOfficial debian repositories don’t have up to date PHP versions due to their strict package maintain procedures. However, there is a guy named Ondrej Sury. As being a debian developer since 2000, he is doing the heavy lifting for us to bring up to date php versions to debian and derivatives. (which he points out importance of his work in deb.sury.org “Who am I?” section).So, we are relying on Ondrej’s work and adding his repository to our sources.list file. Then we install PHP as usual. (we prefer php-fpm) Offcourse, starting from most basic packages like pdo, zip, gd. We don’t want to bloat our beloved docker image!Install NginxYes, instead of using sepereate image, we do install nginx alongside with PHP inside our image. Number one reason to do this, there are some huge performance penalties in OSX with docker bind-mounts. Things can get crazy pretty quick if you run different bind-mounted images with frameworks that needs high number of i/o operations to disk in development mode. (like symfony/magento). We don’t want to fry eggs with our macbooks. So we allow one high i/o image in our docker setups. No more.We don’t really need Apache. Plus, nginx is lighter and faster.Create Custom User & ConfigureWe like to put everything under one non-admin unprivileged standart system user. PHP, Nginx and other services will utilize this user to run. There are two main reasons:Users home directroy is also created to hosts everything, including logs for php and nginx. This alone eliminates most of the permission issues in our docker image.CleanupWe do a cleanup before finishing a docker image. Examples below:SummaryIn this article we talked about Digitialact way of dockerization of php applications. We explained main concepts and why we choose to do that way. There is no code examples in this article, because this is the first article regarding dockerization in our blog and we didn’t want to overwhelm you with details. However you can find a link below to your github repo which contains concrete implementation of what we talked about in abstract.Check out: https://github.com/digital-act/dockerized-php-fpm-nginxTake care~by Tuncay Üner",php,https://medium.com/@digitalact/docker-for-php-the-abstract-29c28d33de5f?source=tag_archive---------7-----------------------
Laravel Installation,"In this article, you can get to know how to install Laravel to your system within 5 steps.Before that, you want to know about two main things we will find while installing Laravel.1.ComposerA composer is a tool that includes all the dependencies and libraries. It allows a user to create a project with respect to the mentioned framework. All the dependencies are noted in the composer.json file which is placed in the source folder.2. ArtisanThe command-line interface used in Laravel is Artisan. It provides a number of helpful commands for your use while developing your application.Now let us see the steps of installation.Go to the following URL and download the composer to install it on your system.https://getcomposer.org/Go to command prompt and check the installation by typing the Composer command as shown in the following screenshot.By typing the following command in your command prompt you can install the complete framework.composer create-project laravel/laravel test dev-developThe output of the command is as shown below-Now you can start Laravel service by executing the following command.php artisan serveCopy the URL underlined in gray in the above screenshot and open it in the browser. If you see the following screen, it implies Laravel has been installed successfully.Now you can develop your web application as you need.Thank you for reading.❤Stay tuned for more about Laravel.🤞",php,https://medium.com/linkit-intecs/laravel-46f9a4c72505?source=tag_archive---------3-----------------------
5 Compelling Benefits of PHP Technology for Your Business Solutions,"Hemang RindaniJun 3, 2020·4 min readThe impact of COVID-19 is causing market disruption. As a result, the businesses are experiencing drastic decline in sales and profitability. During this uncertain situation, it is imperative for every business leaders to shift their focus on providing digital services to their customers.Enterprises will have to find opportunities to deliver hyper personal experience to their clients and customers from a distance. To do that, you need a dynamic and design-rich website.Developing or improving your website can become your long-term business asset. Your business can gain profitable results by increasing search visibility, bring more visitors and in a long run bring potential sales leads and more success.PHP is the most compelling scripting language specially designed for robust and proficient website development. It is a server-side scripting language incorporating command-line interface capability to develop client-side GUI applications. Moreover, it is best suitable language for responsive and dynamic websites.According to W3Techs’ data, PHP is used by 78.9% of all websites with a known server-side programming language. So almost 8 out of every 10 websites that you visit on the Internet are using PHP in some way- W3TechsPHP can develop websites that are cross-browser compatible. Developers and designers can implement cutting edge features with advanced frameworks, components and tools that make the website interactive and high performing with reduced load time and memory.There are several benefits of PHP technology that businesses can leverage for software development.By virtue of being a versatile open-source scripting language, PHP is popular among businesses across every scale and size. The architecture supports databases, such as Oracle, MySQL, Sybase, and is platform and design independent. Having the ability to develop website at faster turnaround time and enhanced security, businesses can reap benefits from robust and Scalable website architecture.The latest update of PHP has created an adaptive outreach in CMS customization and PHP has become an effective competitive need for businesses. It makes websites adaptable, allowing the website owners and developers to customize every content as per the need.One of the leading UAE-based ISV (Independent Software Vendor) provider wanted to create virtual training programs for their global clientele. As a result they wanted to develop Learning Management System. Cygnet’s expert team of specialized designers and project leaders offered a web-based LMS (learning Management system) using PHP technology. The offering increased business revenue, brand value and the business efficiency by 40%.PHP is best for Unit testing as it provides a renowned programmer-oriented testing framework PHPUnit. This framework will increase team’s test speed, testing efficiency, improve test accuracy and reduce test maintenance costs. PHP framework ensures test-driven development with the intent to identify errors at the early stage of web development.The key differentiating factor of a good programming language is the extent of its standard library. PHP gives multiple GUI options available in GUI Libraries. These libraries play a crucial role in simplifying and optimizing the data processing ability of the language. In addition it facilitates you to use varied impactful designs and graphical icons to keep users engaged within the website. PHP offers extensive support for URL parsing, HTTP fetching, or database driven activities for businesses to develop a unique and exceptional design-led website.Businesses using PHP in website development has the potency to improve speed of development as much as it improves speed of execution. Since PHP operates on a multiple standard web stack, it allows developers to create customized, high performing applications.Apart from the aesthetic point of view, there are certainly other aspects that robust website architecture encompasses such as improves SEO ranking, enable social sharing buttons for higher lead conversions which finally helps to build an impactful brand image. Websites developed using PHP is compatible on all operating system such as Windows, UNIX and so forth.Develop Future-Ready Web Solutions with Cygnet’s PHP Development ServicesCygnet’s team of zealous and creative PHP developers hold an experience of more than two decades and has a knack of delivering high-performance, scalable and future-ready web solutions that drive the business forward with evolving technology requirements. Get in touch with our PHP experts now, at inquiry@cygnetinfotech.com or call us on +1–609–245–0971This article first appeared at Cygnet Infotech.",php,https://medium.com/@hemang_rindani/5-compelling-benefits-of-php-technology-for-your-business-solutions-913c3d24f946?source=tag_archive---------9-----------------------
Upgrading a laravel backend over 9 releases — Part 2,"In the previous Chapter, we saw the sequence of steps that we took to take our laravel v5.1 application to v5.4. We still had a few more miles to go before we (could) sleep (to quote Robert Frost).Laravel v5.5 was the first version to not work with php 5.x. It needed php 7.x.As before, based on the upgrade guide, we changedWe also made a few other changes, especially for the new package discovery feature introduced in this version and it was time to run composer update.Our update went through fine and the unit tests passed as well.In another project, where we had done a similar exercise, we encountered a problem where we had used withCount in some of our queries.We hadPreviously this resulted in a column named upVotes_count (with the _count suffixed). This was no longer the case in v 5.5. This meant, wherever we used upVotes_count in frontend and backend, we had to change them to upVotes.In the other project, we also had a couple of tests breaking due to change in the Exception format. The default format for JSON validation errors had changed.For instance, where the earlier the error wasit was nowThis meant, some of our tests (as well as our frontend code) needed some update.Earlier it wasIt was nowWe moved past these and were now in laravel v5.5Laravel v5.6 needed php ≥ 7.1.3.Following the upgrade guide, we changed the versions of laravel/framework to 5.6.* and phpunit/phpunit to ^7.0. In addition, a few other dependencies had to be updated as well.v5.6 also had changes to Trusted Proxies middleware. We ended up removing our old configuration in config/trustedproxy.php and creating a new configurationapp/Http/Middleware/TrustedProxies.phpIt was also time to remove the deprecated php artisan optimize artisan command from the scripts in our composer.json.This upgrade was quite straightforward and only involved updating the dependencies for laravel/framework and phpunit/phpunit in composer.json and running composer update.This upgrade also went through with only composer.json changes to laravel/frameworkLaravel v6.0 required php 7.2 and above.Besides updating laravel/framework to ^6.0, one of the changes we had to make was to handle the removal of the Input facade. Where we hadwe now need to useand remove the following line from config/app.php'Input' => Illuminate\Support\Facades\Input::class,We also had to remove the following line from our phpunit.xml which had been deprecated.So far we were using php 7.3 that was the system default version in macOS Catalina. We had a dependency on Laravel Excel. Now this dependency needed to be upgraded to 3.0 to work with laravel v6.0. But composer install failed with the following errorThe default php installation on Mac does not include all the php extensions. ext-zip is one of them, xdebug is another (which is needed to get code coverage details).brew install php allowed us to install a version of php which had all the extensions that we wanted. But wait, the default brew php version was now php 7.4!It was time to see if our, code which we were now trying to run on laravel v6.0 worked with php 7.4.composer install went through fine. We need to upgrade mockery/mockery to ^1.3 for php 7.4 support.In v6.0, all str_ and array_ helpers had been moved to a new laravel/helpers package. We had used str_random in a couple of places and they were now breaking. The fix was toA few minor tweaks and we were on the latest laravel LTS on date!In another laravel project, we had a web application using AdminLTE. We had used a laravel admin lte plugin for it. This plugin did not work beyond laravel v5.x and for our project to work in laravel v6.0, we had to switch to another plugin, which supported a newer version of AdminLTE (v3, that used bootstrap 4). This involved a fair bit of UI code change as well as a few test case changes (perhaps a topic for another write-up!)Having reached this state, we didn’t want to stop and decided to see what it would take to move to the latest laravel version.Laravel v7.0 needs php 7.2.5 or above. Both our development and production environments had php versions above this. So this was not an issue.As the upgrade guide specified, we had to update the dependencies ofLaravel v7.0 is built on Symfony 5.x. One of the changes due to this was to use Throwable interface in place of Exception interface.This was in app/Exceptions/Hander.phpOther than this, the biggest change in laravel v7.0 was the integration of CORS support into the framework.Previously, we used the third-party dependency for laravel-cors, which was barryvdh/laravel-cors. This dependency was now changed to fruitcake/laravel-cors and came by default in new laravel v7.0 projects.The configuration file config/cors.php was the same and updated as per our project needs.We were not using the authentication scaffolding of laravel. Else laravel v7.0 moved this to a separate laravel/ui repository.Our tests started failing in laravel v7, with the errorWhat was happening? Our test cases were still legacy code and while it had run well so far, it now broke with phpunit 8.x.An internet search revealed many reasons and corresponding workarounds. The one that worked for us was adding namespace for our tests as discussed here. We added the following line at the beginning of each of our tests including tests/Testcase.phpWe also need to make the following change in composer.json.FromtoIn the course of investigating this, we discovered that we had missed this crucial information in laravel v5.4 upgrade guide. New projects created from laravel v5.4 already came with namespaced tests!Our tests were passing and we were now on the latest version of laravel!The exercise of upgrading our app revealed the following:Hope this write-up helps those with legacy laravel applications to take the plunge to upgrade them. (Of course, if they don’t have unit tests, they should be prepared for a round of testing after each version upgrade)",php,https://medium.com/innoventes/upgrading-a-laravel-backend-over-9-releases-part-2-f3f53c93bdfb?source=tag_archive---------3-----------------------
Laravel — The Greatest PHP Framework for Web Development,"Zoe WillamJun 2, 2020·3 min readPHP or Hypertext Preprocessor is the most famous server-side programming language. The emergence of various PHP frameworks is one of the primary reasons for its popularity.Among all the PHP frameworks such as CakePHP and CodeIgniter, Laravel is the most widely utilized framework. This extensive framework comes with a robust code foundation, feature-rich toolkit, and smooth maintainability.Since its launch, the Laravel framework has enabled developers to seamlessly develop feature-rich web apps with elegant tools.Laravel supports MVC architecture for web development and it also provides modular packing, dedicated dependency manager, routing services, separation of codes, and multiple methods for fetching relational databases which further spreads the popularity of Laravel.8 Amazing Features of Laravel PHP Framework:MVC ArchitectureOne of the most significant factors in making Laravel the best PHP framework is its extraordinary support for MVC architecture. Most frameworks don’t provide MVC support, and that is the reason Laravel sticks out.Easy Database MigrationThe most troublesome thing to oversee for PHP specialists is the database relocation part. Keeping databases in a state of harmony between advancement frameworks is probably the hardest test. The Laravel migration framework assists developers with defeating that challenge.In-Built Project EnvironmentLaravel Artisan is an in-built project environment that makes Laravel simpler for developers. It is essentially an order line interface that offers different accommodating orders to create code.Template EngineLaravel is exceptionally recognized for its Blade templating engine which is incredibly intuitive and effectively versatile with HTML/PHP. It assists engineers with building amazing formats utilizing dynamic content seeding.Authorization TechniqueThe Laravel framework disentangles the usage of authorization strategies. Practically all the things in Laravel are superiorly arranged. This remarkable framework is ideal for developers who need a basic method to sort out authorization logic and control access to assets.Premium SecurityIn spite of the fact that there is no single framework that is totally shielded from online dangers, Laravel deals with all security-related things inside its structure. With hashed and salted passwords, passwords are not spared as only text inside its database. Laravel’s defensive devices like CSRF tokens take care of the security of the undertaking by checking every single solicitation.Expressive SyntaxA popular book called “Clean Code” by Robert C. Martin is the one on which Laravel’s principles are based. This book discloses straightforward approaches to compose and look after code. As the originator of Laravel followed this book, the language structure in Laravel is entirely expressive and natural.Better DocumentationTaylor Otwell, the originator of Laravel, curated Laravel’s documentation. As he needed to make composing code simpler and clear, he wrote everything in extraordinary detail. Accordingly, Laravel has exceptional documentation which makes it easy to learn Laravel development.After going through all the premium features of the Laravel framework, anyone who is familiar with web design and development will definitely acknowledge that all the hype behind this framework is real. Laravel framework’s advanced and powerful features make it the most popular and trusted PHP framework.",php,https://medium.com/@zoewillam/laravel-the-greatest-php-framework-for-web-development-73322ae26b7a?source=tag_archive---------12-----------------------
Simple Contact form with PHP,"James AubleJun 3, 2020·3 min readAs I slowly and steadily build up my library of web dev snippets, I occasionally stop and admire a snippet that I end up coming back much more than I had originally thought I would.If you wanna quickly review the snippet before we dive in, here it is:And here it is in action:snippets.jamesauble.comAlright so what the hell is going on here?First let’s look at our markup (HTML). Essentially, we have a <form> tag that wraps our <table> that contains rows, <td> and inputs for the values we want (i.e. first name, last name, etc…).We could easily decouple the markup and the PHP code and have them in separate files— no problem — but for simplicity’s sake, we’ll leave them in the same file. In order to do this we must leave the action="""" empty inAlso, we need to add a name=""submit"" to our submit button input if there isn’t one already:Our final markup looks like this:Pretty basic right? Yeah, that’s why this is called a “simple form”!Okay now to the logic (PHP).Feel free to customize the PHP to your hearts content, but the only 2 variables you need to configure are:You’ll see that even before that, the entire block of PHP is wrapped in an if-statement that checks if anything has been posted by looking for $_POST['email'] in the headers of the http request.If it evaluates to true (or 1 in this case) it’s like “sweet, let’s process this form and get this data on its way!”.Next we define a function to print an error message and kill the process if need be:Next up — a big fat if-statement to ensure that we were passed all of the values we are expecting (tweak to your liking).Hey wait — we don’t wanna keep typing out $_POST all the damn time do we? No, that’s what I thought. Let’s assign them to our own variables.This is followed up with a bunch of server-side input validation. You could just as easily remove it and include your own — and/or add client-side validation on top of it — or even do away with validation all together! I don’t recommend it of course, but it’s up to you and your specific use case.Finally we run our values through clean_string() {} to help prevent PHP injection.Well that about does it!Again, check out the demo and say hello to me through my form if you’d like!https://snippets.jamesauble.com/simplephpform/Code on and thanks for reading.",php,https://medium.com/@jamesauble/simple-contact-form-with-php-82d4edfad075?source=tag_archive---------4-----------------------
Understanding your Symfony app with Prometheus,"You can try all of the examples by downloading this repository https://github.com/rk4xxicom/symfony-prometheus-example (you will need docker-compose to run it though).If you are already familiar with Prometheus / OpenMetrics format, just skip the introduction and move on to the Symfony part.In short, almost any web application needs some sort of observability in production.As developers we need to understand how many resources it actually consumes relative to the traffic it serves. But not just any kind of traffic, we (ideally!) would want to distinguish between different kinds of traffic. For example, buying something on an e-commerce website often requires more resources than just displaying a product page.Of course, as project stakeholders we are also interested in how many items have been sold this month and not just in the total number of hits.There are many tools that can solve this sort of problem in your project, but at 4xxi we have found that Prometheus suits our needs — at least for now. It can be easily integrated with both legacy and modern web apps in PHP and Go, has plenty of different integrations for web servers, databases, Linux, message queues and it fits well in a container-based environment.Eventually, the Prometheus metrics format will become an open standard for metrics collection — OpenMetrics. So even if you find that Prometheus does not work for you, keep in mind that major cloud providers support Prometheus format (DataDog, NewRelic).Prometheus works using the “pull” model over HTTP protocol — give it a URL and it will start scraping it, parsing the metrics, and saving them into internal time-series database. Normally, if you want to start monitoring a system or a product, you run a small utility or an extension that would start an HTTP server and translate the internal state of the product into OpenMetrics format.Metrics consist of types, labels, and values (either integer or float). Here is a simple metric that was taken from a Linux-level exporter:The important parts here are:However, for the system monitoring tools there are generally some charts that you can install to you Grafana or another dashboard. Here is the one that we used https://grafana.com/grafana/dashboards/1860, and here is how it interprets the values above (in conjunction with another metric, total_bytes):Now, let us move onto Symfony integration and application metrics.If you’re strapped for time or don’t really want to dive into the old code, here is a quick solution that you could trySuppose you have a number of accounts in different stages and you want to see how that number changes over time — when do you get an influx of new accounts, how it is correlated with CPU load etc. (this example is loosely based on a real project).Let’s create the Entity class describing our account:Now we need to get the data from the database:And finally, we need to translate the data into OpenMetrics format for Prometheus to consume:Now, if you launch your application, you’ll see something like thisNow we need to tell Prometheus about this.I won't talk about installing Prometheus here, if you do not have it installed, check out this documentation page or again, try all of the examples for the article in the repository.What I will talk about, however, is how to add it to a Prometheus configuration file:Here we specify the host, the port, and the path from which metrics can be scraped by Prometheus. If you use k8s, there is a better way to tell Prometheus about your services and pods, but we are trying to keep it simple here.After restarting the Prometheus instance, you should be able to access the web interface by going to http://localhost:9090 and you can check if your changes were applied properly:Caveats and downsidesThe downside of this method is that you cannot easily keep the state between different requests, for example, to store the total number of requests or logins to the application.If you need to monitor such metrics, but do not want to rewrite the application code because of it, you can try to extract the data from the logs.Google created a tool called mtail that parses log files and exposes extracted metrics via HTTP endpoint. And yes, it handles log rotation.Let us try it on a Symfony app:Here each time a user is created the app puts its id in the logs.mtail has a simple DSL to translate log lines into metrics:Put the program in /etc/mtail .Download a “mtail” binary from Github or build a docker image, and run it specifying logs folder and “progs” folder.This should launch a web server on localhost:3903Triggering a new log entryThe user total has increased!Caveats and downsidesDespite the issues described above, it’s a useful tool in many cases, when working with legacy apps or apps that are not written by you (e.g. web server — there are some readymade dashboards that display average response time etc. based on logs https://grafana.com/grafana/dashboards/9516)If you use Symfony 4/5, there is a great Symfony bundle for Prometheus.Metrics are stored either in apcu or in Redis.The README has everything you need to set it up, I’ll just provide a simple example.After enabling the bundle via flex or using README, add collector registry as a dependency.In the same place where we added logging before, increment a counter manually.“getOrRegisterCounter” adds a new counter, with relevant parameters being the “name” of the counter, “help”, and the last one — the list of labels.After that, we increase the counter twice — first with type “all” and then with type “enabled” (because users are enabled by default).If we were to disable a user, we would increment the same counter with type “disabled”.Let us see how it works! Create a user one more time:and go to http://localhost/metrics/prometheusMetrics are stored in Redis as hashes. You can see the exact data that is being stored by connecting to Redis:Since it is working now, we once again need to tell Prometheus what to scrape (the full config can be found here).I created 26 new users by calling the endpoint at random periods of time and here is the resultSame data represented as “new users per minute”:You can find more info on how to work with counters and rates here and here.The bundle also gives you some application metrics, such as the number 500x and 200x responses, percentiles of response times for different routes, etc.Caveats and downsidesObservability of application and business metrics is often underestimated compared to technical metrics such as “number of hits“, “cpu load” and others.To better understand your application and how it can grow, you need a way to track some higher-level metrics.One such solution is Prometheus which is quickly becoming a standard in the industry (although there are many other great tools).If you have a Symfony application, either a legacy or an actively developed one, there is an easy way to start monitoring and learning.Here are some useful links if you’re interested in monitoring:",php,https://blog.4xxi.com/understanding-your-symfony-app-with-prometheus-fdb4b0124d99?source=tag_archive---------1-----------------------
Hello World! Antidot Framework,"Antidot framework(@antidotframewo1) was born as an exercise to learn and understand the inner workings of the different components that are common in professional development tools.Let me put you in context. I’m self-taught, my first contact with a PHP framework was in 2013 with Codeigniter, Then, Symfony2, also at work we had to deal with Zend Framework 1, who’s been with me for several years, Laravel, Slim, and finally my favorite, Zend Expressive.Having seen several generations of frameworks, I will explain why Zend Expressive(now Mezzio) is my favorite framework. It’s the only framework that allows us to select among the implementations of the key pieces that are most convenient or best suited to our project, as it conforms to the PSR standards accepted by most of the PHP community. It defines a set of rules that makes all the code framework agnostic. I mean that we can run the same code in any other PHP Framework with a minimum of effort.The only issue I see with it is its learning curve since it involves selecting different implementations for components that a junior programmer does not have to understand at first contact.So, my idea was to craft a framework following the same paradigm as Mezzio without the need of a too pronounced learning curve.From this premise, the Antidot Framework was born with the idea of applying the same paradigm as Mezzio, adding a standard installation. To simplify the configuration and the injection of dependencies, trying to take advantage of the things I like best from the different frameworks I know, and above all learn how the pieces of the most used professional tools work.Well, no more boring stories;-D. I’m going to introduce you to their features.The standard configuration is very light, only needs 33 packages in production mode, and is designed to develop long-time distributed applications, API or CLI style, and is easy to deploy as a monolith or microservices, even as a serverless application.The main characteristics of the framework are the following:It adopts the standards of coding and self-loading classes, PSR-1, PSR-2, and PSR-4, it comes with Grumphp configured, with Phpunit and Phpstan to ensure optimal code quality before each commit.Dependency injection with auto wiring, which makes it very easy to start knowing and taking advantage of concepts like dependency inversion,or just not caring too much about how to build the classes.For HTTP applications, adopt the PSR-15 standard using the Http Request Handler Runner component, with a pipeline routing implementation inspired by the Mezzio Framework and Slim.I focus the configuration design on simplicity and flexibility, friendly to integrate third-party libraries, and lets to the choice between PHP or YAML in the style of Symfony.It comes configured with a PSR-14 Event Dispatcher implementation that can easily use to distribute domain or application events, and a PSR-3 log system with Monolog and the integration library wshafer/psr11-monolog. Finally, it comes with an implementation of the Symfony console tool and the Antidot Dev Tools component that contains several useful commands for generating code templates, enabling development mode, config caching, and so on.In my opinion, this project is a collection of concepts that I have been learning over the years using the popular frameworks day by day. I made it with all my love, and currently, it has only just been born. The next steps are: to continue completing documentation, create and build different adapters for the most used libraries, and of course, keep meeting people, learning, and enjoying my work.A special mention to the creators of Mezzio(formerly Zend Expressive), because they have inspired this project. Another salute to co-workers at Uvinum, twitter friends, Larry Garfield(@crell) for your constructive criticism on the implementation of the Antidot Event Dispatcher component, and especially to Marc Morera(@mmoreram), a private conversation with you made this project public ;-).I want to add that I’d love to get your feedback, PRs, advice, and so on, I will be happy to listen and discuss both on Github and Twitter.Thank you very much for reading this post and sharing Antidot Framework. The story just started.See the docs if you want to start using Antidot Framework.Happy Coding!",php,https://medium.com/php-fad/hello-world-antidot-framework-92f613212008?source=tag_archive---------6-----------------------
What is PHP? | How does PHP work and how to learn PHP?,"iTriangle TechnolabsJun 2, 2020·4 min readToday we will learn about this topic, what is PHP? How does it work and how to learn it? So let’s know about PHP. If you run the internet then you will be well acquainted with the word PHP and if not then we will tell you.PHP stands for Hypertext Preprocessor (earlier called, Personal Home Page). It is a free scripting language that is used in web development. This language was created in 1994 by Rasmus Lerdorf. He created this language for the purpose of web development.But seeing its specialty, later programmers also used it to create desktop applications. We can make a website page dynamic by using PHP. This language is also used by big websites like Facebook, Google, Amazon, Flipkart.Also read: — What is Bootstrap? Advantages of Bootstrap FrameworkPHP language is a very easy language that anyone can learn easily. And its biggest advantage is that it can be easily integrated with any HTML language or with any popular database language. Another advantage of this is that this language can Encrypt and Decrypt your data. And like HTML and javascript, your source code is not visible in any browser, which also makes it very secure language. Another advantage of its language is that you don’t have to pay much attention to its syntax. If you know C language and javascript then it will be even easier for you to learn PHP language.By the way, there are many features but will talk about a few.You can do all this and I have only told the main feature, apart from this you can do many things with PHP.Let us now know what are static websites and dynamic websites, which you can create from PHP.You must have understood by the name itself that this is the page where all the pages remain fixed. Which a normal user cannot change. These static web pages are the same for every new and old user. Which you cannot change. Whenever you open a website, you have seen those content never changes. Those pages look the same for everyone. But there are some sites like Facebook.com whose contents of the page keep changing all the time and there are different webpages for different users.Here are some examples of some static web pages. EX- About us page and Contact page whose content never changes. Hopefully, now you will have become easier to understand the static page. Now know dynamic webpage, with this you will understand PHP very well.If you have understood the static web page then you will understand it very easily. Because Dynamic web page content always changes. Here every user means that the page you open will be something else for me. Like when I login to FB, my page will be quite different from your FB page.Dynamic means variable or changeable, which changes the page frequently. Likewise, take an example, the pages of shopping sites also keep changing for every user. Because you must be doing some search but I can OPEN the page to do some more page shopping. Both of these are very good examples of the dynamic webpage. Do you want to make your website the design of your choice, then you will have to teach PHP, let’s talk about it now.Before learning PHP, you must have basic computer knowledge. Apart from this, you have to be logical, that is, why is anything happening and how can it happen? There should be such a visualization. Because you have to understand a computer through a language. So you have to be logical. You should learn a little HTML before learning this. Even if you do not know the rest of the language, you will be easily coding PHP.Hope this article must have been liked, how did you feel, please comment below. If you want to ask any questions now, then, please. If you want to give any other suggestions, then please give it so that we can do something new for you.iTriangle Technolabs is a top-ranked and expertise web development company, which follows the current and popular trends in the it industry to deliver clients a website with all trending features.If you require a PHP Development with all the trending features in the market, our team is happily available for you 24 by 7.",php,https://medium.com/@itriangletechnolabs/what-is-php-how-does-php-work-and-how-to-learn-php-9c95f81040c8?source=tag_archive---------10-----------------------
DIY Local Pipeline,"Thiago MariniJun 5, 2020·3 min readIf you’re working alone on a personal project and for any reason can’t bother with setting up CI/CD pipeline this post is for you.Deploying directly from your machine without automation is tedious and dangerous. Amongst other things you might forget to run tests or correctly prepare the env for deployment. Thus some level of automation is needed do do it right.This is supposed to be a small post so I’ll go straight to the point. That’s the stack I’m using on my local env:I’m assuming you are fluent on your local env thus I won’t get into the nitty-gritty of mine. I’ll use make for my local pipeline.Place the Makefile in your project root and open the terminal there. To list the available commands on make just run make, you should see:To deploy to dev run make deploy-dev, for production make deploy-prodAnd that’s it, you can now deploy from your local machine running tests first and correctly preparing the target env.",php,https://medium.com/@marinithiago/diy-local-pipeline-d1a054902295?source=tag_archive---------2-----------------------
PHP Frameworks in 2020: Laravel or Symfony,"WDP TechnologiesJun 5, 2020·3 min readThis blog post showcase the difference between two powerful PHP frameworks, i.e., Laravel and Symphony, also help you to make an informed decision on which one is right for web application development?When developers set out to decide upon a PHP framework for their development project, they look for a solution that is not only highly organized but also fares exceptionally well when it comes to code that is maintainable and reusable in equal measure. And rightly so, because such qualities are crucial for any web app development project where the goal is to build a robust product without spending more time and effort than is necessary. And given how much they are in demand all over the globe, it should come as no surprise that the market is brimming with a plethora of PHP frameworks to choose from.And, despite the abundance of options, the choice almost always comes down to just two names: Laravel and Symfony. Owing to their top-rated abilities in the context of their usability, highly productive features, and so much more.Unfortunately, it can prove to be harder than one would imagine choosing which one of the two is better suited for your requirements because of their many similarities. It includes a whole list of comparable features, such as scaffolding, full-text search support, and more. Then there’s also the fact that both Symfony and Laravel come equipped with features to enable cross-platform app development, making it even harder to pick between the two.So, now it is time we see how they are different to help you make an informed decision for your PHP Development project.Speed: In this context, Symfony makes use of a variety of strategic approaches to handle, sustain, and maintain the app’s speed. It enables developers to determine the rate of the entire app or just a single feature by bypassing features that are not essential to the app’s core function.Unfortunately, Laravel doesn’t get any such special provisions to help developers take care of the speed challenge, though it does deliver relevant version control feature that allows migrating the app in the future.Database support: For data access, both Laravel as well as Symfony provide Object Relation Mapping (ORM). However, in the former, Eloquent is used to tend to the ORM, whereas in Symfony, Doctrine takes care of the ORM. First things first, ORM facilitates easier data manipulation with Eloquent and Doctrine.Now, the differences in this regard — Symfony may enable automatic database migration, but it needs simple definitions for every single field in the model. Laravel, on the other hand, may not offer automated migration, but it doesn’t necessitate defining areas in the database model either.Forms and validators: This is the aspect where the difference between Laravel and Symfony becomes clear and evident. Symfony needs a validator component to ratify objects against a specific group of rules. While Laravel enables the validation of certain inputs against a restricted rule.The above discussion may translate into you wanting to look for the Laravel web application development company article Search, the fact remains that you must first carefully evaluate your requirements against the provisions of each one of the two PHP frameworks before making any decision.",php,https://medium.com/@wdpnew/php-frameworks-in-2020-laravel-or-symfony-a0c188ff8380?source=tag_archive---------1-----------------------
How use PHP 5.6 in Dreamhost / compiling and enabling,"VitorGGAJun 1, 2020·2 min readA few months ago i have a problem in Dreamhost account, theirs desactived all PHP 5.6 accounts, and upgrade to PHP 7.2. But all websites i have in Dreamhost is a old accounts, created in PHP 5.6 and using mysql_connect functions.Migrate this old websites not is viable, i think migrate all this accounts but, i pay for Dreamhost to use 5.6 accounts, i don’t like migrate, and now i found this solution. Compile and use my PHP 5.6 in dreamhost shell.To start is need enable ssh in account. After enable ssh need conect in this shell account.To use PHP 5.6, need download and compile all, see commands i used.After download and unzip, you can configure and make install.https://help.dreamhost.com/hc/en-us/articles/215074947-How-to-install-a-custom-version-of-PHPhelp.dreamhost.comPS.: Sorry my bad english, i will revise this.",php,https://medium.com/@vitorgga/how-use-php-5-6-in-dreamhost-compiling-and-enabling-f4cb5edd1e04?source=tag_archive---------6-----------------------
Four Steps to Achieve Dynamic Transitions with Motion Layout on Android,"Animations and transitions are essential ingredients for developing any mobile application. Think about how rarely we come across apps that do not have at least one animated component. It helps in keeping the app as user friendly as possible and becomes a very effective marketing tool.At the same time, developing complex animations and transitions could be quite challenging, and not every developer looks forward to working on such tasks. While the user experience is highly increased with animations, we can’t say the same about development experience.To address these issues, MotionLayout was introduced as a subclass of ConstraintLayout in v2.0We will not be talking about the basics and fundamentals of MotionLayout, but rather take a deep dive on how to implement dynamic transitions with MotionLayout.At the end of this article, we will be developing the following sample application which demonstrates a bottom navigation bar with some animations:Let’s go through this implementation in a step-by-step process:Here is a link to the sample repository in case you want to take a look at it firstLet’s start by adding the dependency constraint layout v2.0 in Gradle:Let us begin developing the UI with MotionLayout without adding MotionScene to the app:layoutDescription . The XML file would look something like this:ImageFilterView is a ConstraintLayout util widget which can be used for crossfading. We provide the two images in android:src and app:altSrcapp:overlay is set to false so that both the images do not overlap each otherWe have used constraint chains to keep the views evenly distributed. There are multiple chaining styles available, you can choose the one that suits your application, best. For this sample app, we have used spread chaining styleNotice that we have set all TextView visibility to ‘gone’. This is because, as per our requirement, only the text of the active tab should be visible. Hence, we are trying to achieve a base state where no tabs are selected at the app launch.Your app UI after Step 2 should look something like this:Now, before creating a motion scene file, let's try to understand what kind of constraint sets and transitions we are trying to achieveStates 1–4 are achieved when the respective images are clicked.Since the view represents a bottom navigation bar, these states are not achieved sequentially. It is not necessary that the state transition will occur only in the following way:Base State → State 1 → State 2 → State 3 → State 4In reality, there can be multiple permutations while reaching any state. Hence, to achieve this scenario lets create a constraint set for every State that we discussed until now. The ConstraintSet for base state would look like this:Here, we make sure that the default constraints are set to copy the base state. CustomAttribute is used to crossfade the image within the two images we provided earlier. app:customFloatValue has a range of 0 to 1The ConstraintSet for State 1 would look like this:Here, we modify the visibility of the TextView and toggle the image by setting app:customFloatValue to 1Notice that we have derived constraints from our base state by adding this line of code — app:deriveConstraintFrom=”@id/base""This is the most important piece of code since we need to make sure that we reset all the other states while transitioning to a user-selected state.All the constraints from ‘base’ ConstraintSet would be implemented and the constraints that we have further defined in this ConstraintSet are the overridden constraints.ConstraintSet for other states would be similar to State 1. Here is the final motion_scene.xml file:Don’t forget to link motion_scene.xml to your motion layout in the following way:Now that we have MotionLayout and MotionScene in place, all that's left to do is to add transitions to the ConstraintSets we implemented in the previous step.If you are wondering why didn’t we implement transitions in the same MotionScene file, it is because these transitions are not expected to occur in a sequential static flow. As discussed earlier, depending on what the user selects, constraintSetStart and constraintSetEnd are derived.Let’s consider the user selection is in the following manner:Now, for this transition to work, we would need to defineconstraintSetStart and constraintSetEnd while defining the Transition in motion_scene.xml for both the state changes i.e from State 1 to 3 and State 3 to 2. This doesn’t seem possible to achieve for a dynamic transition.Hence, to achieve the required transition effect, we would need to define the current state in constraintSetStart during runtime and the ConstraintSet of the user-selected image in constraintSetEnd:Depending upon the default state that the user would like to show when app launches, we can set the required transition stateHere, motion_layout.currentState gives us the current state during runtime. R.id.home_expand is the ID of the view we want to highlight.We have implemented click listeners to identify which view was clicked and set the respective transition accordingly.By setting transitions in the activity, it becomes convenient to set transitions during runtime in motion layout.That’s all for this article, let me know your thoughts on this approach. Since I am still exploring MotionLayout, if you know any better way to achieve this functionality, I would love to discuss it.You can find the sample app on GitHub:github.comFeel free to Fork it/contribute to it.Happy Coding!",android,https://proandroiddev.com/four-steps-to-achieve-dynamic-transitions-with-motion-layout-on-android-2c7405d32185?source=tag_archive---------0-----------------------
Adobe Premiere Rush apk full unlocked download latest version,"ToddwillsnorJun 1, 2020·2 min readFeed your channels a steady stream of awesome with Adobe Premiere Rush, the all-in-one, cross-device video editor. Create and share professional-looking videos with video effects, such as speed and filters — quickly and easily. Plus, work across all your devices — phone, tablet, and desktop.The professional in-app camera lets you capture high-quality content and start editing immediately. Add music and titles to videos for free, use auto-ducking, do voiceovers, and apply video effects to clips within your multitrack timeline with the video editor recommended by influencers, vloggers, and pros. Crop the video screen size to customize and share to your favorite social sites, including YouTube, Facebook, Instagram and Behance, right from the app. Use for free with three exports, or upgrade for unlimited sharing.PRO-QUALITY VIDEO EDITORBuilt-in professional camera functionality lets you capture high-quality content right from the app and start video editing with no watermarks immediately.EASY EDITINGArrange video, audio, graphics, and photos by dragging and dropping. Intuitive tools let you trim and crop videos, adjust audio, enhance color, and add video effects, titles, transitions, voiceovers, and more.MULTITRACK TIMELINEEdit videos using four video and three audio tracks for tons of creative flexibility.CUSTOMIZE TITLESAccess built-in templates (some animated) and change the color, size, font, and more to make them uniquely yours. Browse 100s more free motion graphics templates on Adobe Stock and add to videos with no watermark.GREAT SOUNDAdd music to video, record voiceovers, and use advanced tools powered by Adobe Sensei artificial intelligence for sound balancing, noise reduction, and auto-ducking.MADE FOR SHARINGCrop videos for social, including Instagram, and YouTube. Easily resize videos from landscape to portrait to square for different channels. Rush also allows you to select a thumbnail and publish to multiple social sites right from the app.AUTOMATICALLY SYNCED TO THE CLOUD Your latest videos are always at your fingertips, no matter where you are or what device you’re using — phone, tablet, or desktop.TAKE YOUR WORK FURTHERWhen you want to do more, open your Rush projects in Adobe Premiere Pro, the industry-leading software for professional film and video editing.",android,https://medium.com/@toddwillsnor/adobe-premiere-rush-apk-full-unlocked-download-latest-version-2ed2836b52d4?source=tag_archive---------30-----------------------
Android Apps Using MVVM with Clean Architecture,"In the event that you don’t pick the correct design for your Android venture, you will make some hard memories keeping up it as your codebase develops and your group extends.This isn’t only an Android MVVM instructional exercise. In this article, we are going to consolidate MVVM (Model-View-ViewModel or now and then adapted “the ViewModel design”) with Clean Architecture. We are going to perceive how this engineering can be utilized to compose decoupled, testable, and viable code.MVVM isolates your view (for example Exercises and Fragments) from your business rationale. MVVM is sufficient for little undertakings, however when your codebase gets enormous, your ViewModels begin swelling. Isolating obligations turns out to be hard.MVVM with Clean Architecture is quite acceptable in such cases. It goes above and beyond in isolating the obligations of your code base. It obviously abstracts the rationale of the activities that can be acted in your application.Note: You can consolidate Clean Architecture with the model-see moderator (MVP) engineering also. Be that as it may, since Android Architecture Components as of now gives an inherent ViewModel class, we are going with MVVM over MVP — no MVVM structure required!Our information stream will resemble this:The code is divided into three separate layers:Our information stream will resemble this:Indeed, even inside the Android application architecture, we’re utilizing, there are numerous approaches to structure your file/folder chain of command. I like to bunch venture documents dependent on highlights. I think that its perfect and compact. You are allowed to pick whatever venture structure suits you.This includes our Activitys, Fragments, and ViewModels. An Activity should be as dumb as possible. Never put your business logic in Activitys.An Activity will talk to a ViewModel and a ViewModel will talk to the domain layer to perform actions. A ViewModel never talks to the data layer directly.Here we are passing a UseCaseHandler and two UseCases to our ViewModel. We’ll get into that in more detail soon, but in this architecture, a UseCase is an action that defines how a ViewModel interacts with the data layer.The domain layer contains all the use cases of your application. In this example, we have UseCase, an abstract class. All our UseCases will extend this class.And UseCaseHandler handles execution of a UseCase. We should never block the UI when we fetch data from the database or our remote server. This is the place where we decide to execute our UseCase on a background thread and receive the response on the main thread.As its name implies, the GetPosts UseCase is responsible for getting all posts of a user.The purpose of the UseCases is to be a mediator between your ViewModels and Repositories.Let’s say in the future you decide to add an “edit post” feature. All you have to do is add a new EditPost UseCase and all its code will be completely separate and decoupled from other UseCases. We’ve all seen it many times: New features are introduced and they inadvertently break something in preexisting code. Creating a separate UseCase helps immensely in avoiding that.Of course, you can’t eliminate that possibility 100 percent, but you sure can minimize it. This is what separates Clean Architecture from other patterns: The code is so decoupled that you can treat every layer as a black box.This has all the repositories which the domain layer can use. This layer exposes a data source API to outside classes:PostDataRepository implements PostDataSource. It decides whether we fetch data from a local database or a remote server.The code is mostly self-explanatory. This class has two variables, localDataSource and remoteDataSource. Their type is PostDataSource, so we don’t care how they are actually implemented under the hood.In my personal experience, this architecture has proved to be invaluable. In one of my apps, I started with Firebase on the back end which is great for quickly building your app. I knew eventually I’d have to shift to my own server.When I did, all I had to do was change the implementation in RemoteDataSource. I didn’t have to touch any other class even after such a huge change. That is the advantage of decoupled code. Changing any given class shouldn’t affect other parts of your code.Some of the extra classes we have are:UseCaseThreadPoolScheduler is responsible for executing tasks asynchronously using ThreadPoolExecuter.This is our ViewModelFactory. You have to create this to pass arguments in your ViewModel constructor.Our motivation with this venture was to comprehend MVVM with Clean Architecture, so we skirted a couple of things that you can attempt to improve it further:Use LiveData or RxJava to evacuate callbacks and make it somewhat neater.Use states to speak to your UI. (For that, look at this astounding talk by Jake Wharton.)Use Dagger 2 to infuse conditions.This is truly outstanding and most adaptable designs for Android applications. I trust you appreciated this article, and I anticipate hearing how you’ve utilized this methodology in your own applications!",android,https://medium.com/onebyte-llc/android-apps-using-mvvm-with-clean-architecture-607a05da1aa9?source=tag_archive---------20-----------------------
3D EARTH PRO apk free download latest version,"DanielandofiJun 1, 2020·2 min readMeet the wonderful 3D Earth. The most beautiful app, ever!This unique application combining: weather forecast, clocks, widgets and a beautiful view from the space to our Earth.Accurate weather conditions and forecast is the main feature of this app. The unique weather forecast algorithm has access to thousands of weather stations. It allows making a mostly accurate forecast for each location around the world!Current weather condition has a detailed description, air temperature, wind direction and force, humidity, dew point, pressure, visibility, as well as comfort index with reference to current weather conditions.Weather forecast information has a detailed description, day and night air temperature, wind direction and force, humidity, UV index, air quality, ozone, geomagnetic storms, the probability of precipitation, and comfort index with reference to current weather conditions. FEATURES:- Real-time 3D rendered Earth - Rain Radar provided by Rain Viewer- The world clock for each location- Weather condition for more than 150 000 locations around the world with 15 days and hourly forecasts- Beautiful charts for 15 days and 48 hours forecast- Sunrise and Sunset time- Accurate 15 day weather forecast- Weather widgets with weather forecast- Weather notifications and weather alerts- Weather forecast for multiple locations all around the world- Fabulous animated wallpaper with 3D Earth- Local time in 12 or 24-hour format- “Feels Like” temperature and weather bug forecast too- Humidity and precipitation forecast information, pressure in inches, mm or mbar- Fahrenheit or Celsius and Miles or Kilometers- Easy navigation between cities- Barometer and pressure forecast for you- UV index and forecast- Precipitation probability forecast- Visibility on the roads- Space weather forecast- Dew point in forecast weather channel - Share forecast, alerts, by e-mail, Twitter, Facebook, WhatsApp, Telegram, SMS and other ways- Live Weather notification indicators in the status bar",android,https://medium.com/@danielandofi/3d-earth-pro-apk-free-download-latest-version-9d08115568a7?source=tag_archive---------32-----------------------
Alarm clock Pro apk free download latest version,"DanielandofiJun 1, 2020·1 min readCaynax Alarm Clock is more than just alarm clock.It is perfect solution for time management, todo list, tasks list or tasks reminder.Alarm categories:Quick — make a note/alarm with two simple clicksEveryday — runs everydayWork days — runs on selected work daysCyclic — runs every x-th day, every x-th week or every x-th month (PRO only)Timer — simple timer (countdown) alarmAny — select any days from calendarAnnual — birthday, anniversary (PRO only)Main features:- independent settings for each alarm like alarm length, ringtone, volume level, snooze and more- dismiss option: standard buttons, math problem or arrange/complete quotation- next alarms widget- gentle alarm — increasing ringtone mode- Android Wear notifications supportFree alarm clock version contains in-app ads and has some limitations.What’s more in PRO version:- no ads- annual/Birthday/Anniversary alarm- cyclic alarm repeat up to 90 days- cyclic alarm can be repeated every x-th month- sunrise and sunset alarms (with dawn and dusk support)- shake to snooze- snooze count up to 100- full alarm profiles support",android,https://medium.com/@danielandofi/alarm-clock-pro-apk-free-download-latest-version-683b06a094d?source=tag_archive---------36-----------------------
Audiomack Premium apk free download latest version 2020,"JakefernotJun 1, 2020·2 min readStream and download the best new songs, albums, and playlists for free and play them offline data-free! Browse music from top categories like Hip-Hop, Rap, R&B, EDM, Afropop, and Reggae on the Audiomack free music app. Download songs and even listen offline, without using your data!Audiomack provides streaming and free download access to the newest and hottest tracks, right at your fingertips. Our free music download feature allows you to play your favorite songs offline, and our proprietary “Trending” section shows you the best albums and songs that are hot right now.KEY FEATURES• Find & stream music and mixtapes that are new or trending. Be the first of your friends to be up on what’s next.• Download songs and albums for offline, data-free listening.• Unlimited listening & streaming — play as much Hip-Hop, Afrobeat, Electronic, Reggae & dancehall music as you want.• Favorite tracks, albums, and playlists and easily search, browse, and shuffle your favorites collection.• Browse expertly curated playlists by mood, genre, and much more.• Create unlimited playlists of your own.• Follow your favorite artists, producers, and tastemakers including 21 Savage, Young Thug, Future, and more.• Go ad-free for just $4.99 per month, or stay free foreverSTREAM AND DOWNLOAD SONGS FROM TOP ARTISTS• Hip hop, rap, and trap from Migos, 21 Savage, Young Thug, Gunna, Juice WRLD, Chance the Rapper, 6IX9INE, Future, 2 Chainz, Kodak Black, Lil Baby, Ynw Melly, Lil Durk, Moneybagg Yo, Famous Dex, A Boogie Wit Da Hoodie, Hoodrich Pablo Juan, Flipp Dinero, Youngeen Ace and more• Afrobeats, Afro-Caribbean, Reggae, dancehall, and soca from Shatta Wale, Ade Gold, Chronixx, DJ Frass, Kranium and more• Latin, urbano, latin trap, musica urbana, reggaeton, and dembow from Quimico Ultra Mega, La Insuperable, Melymel, Musicologo, Don Miguelo, Juhn, Mark B, Bad Bunny• R&B and soul from August Alsina, PnB Rock, Brent Faiyaz, SiR, Alina Baraz, Lloyd, and more• Electronic, EDM, house, and bass from Diplo, Mad Decent, Whethan, NGHTMRE, Poppy, Cheat Codes, Krewellla, Snails, Lowly Palace, Dim Mak, Fool’s Gold, Trap Nation and more",android,https://medium.com/@jakefernot/audiomack-premium-apk-free-download-latest-version-2020-24b9d4698d0?source=tag_archive---------46-----------------------
Apple Watch Series 6 Not Coming This Year Leaker Suggests,"sonoJun 1, 2020·1 min readThe Apple Watch Series 6 will feature an OLED screen like previous models, according to a leaker of upcoming Apple products, suggesting the company isn’t quite ready to use its in-house MicroLED display technology in consumer products.Apple reportedly has a secret manufacturing facility in Santa Clara, California where it is designing and producing test samples of displays that use MicroLED, a technology that will follow OLED. MicroLED screens can result in devices that are slimmer, brighter, and use less power.The technology isn’t expected to reach an iPhone for another year or so, but there is precedent for new screen technologies showing up in the ‌Apple Watch‌ first. When it was introduced in 2014, the ‌Apple Watch‌ had an OLED screen. The technology then migrated to the ‌iPhone‌ X three years later.Read MoreOriginally published at https://bbcstoriesnews.blogspot.com.",android,https://medium.com/@sonoagarwall/apple-watch-series-6-not-coming-this-year-leaker-suggests-3292d2ade907?source=tag_archive---------47-----------------------
Snapchat is not as secure as you think it is(Saving Snaps without notifying sender).,"Disclaimer: First thing, I am pretty stupid. Whatever I write are my views at present. They might change at the time you are reading this. So, don’t blame me for not sticking to my words. I can give my justification why I thought like that anytime though. I am bad at English writing too, So please bear with me.As you might have read the title, we are going to perform the same thing. But even if you are not interested in doing that giving a read to this not gonna do any harm. It can teach you, how easy it is to prove these “privacy-focused” apps wrong about their claims.If you have an iPhone, go buy an android. If you have an android let’s proceed.You will need:Working explained: Whenever a snap(Pic or Video) is received it’s saved even before you open the app. So we are just gonna pick those files from the foldersHow to :Option 1:cp -r /data/data/com.snapchat.android/files/file_manager/chat_snap/ /data/data/com.snapchat.android/files/file_manager/snap /sdcard/Option 2:Automate flow: Snapchat Save.floAfter any of the above step, there will be two files in your Sdcard chat_snap and snapOpen chat_snap folder in Sdcard, there will be files like below. These are the snaps but have extensions changes.Just use any files manager and do open as :Try Image and Video. It should either one of them.That’s all. These are “Auto deleting snaps” you received but haven’t even opened.Automate flow is just a 30-sec fix might not always work. You are welcome to improve it.This is for educational purpose only. If you don’t want your pics or videos to be saved or go public just remember one rule “Don’t capture anything that you are too embarrassed to share with the world”. One more thing, nothing is secure.",android,https://medium.com/ujjwal-thakur/snapchat-is-not-as-secure-as-you-think-it-is-saving-snaps-without-notifying-sender-2e1c5151a6d?source=tag_archive---------17-----------------------
Points To Note Before Hard Reset or Factory Reset A Mobile Phone,"Hard ResetJun 1, 2020·2 min readBefore you’re going to Hard Reset or Factory Reset your mobile phone whether it is to Hard Reset iPhone or Android phone, you should note some important points to save your phone from bricking.If you don’t know about these major points, then you may lose your data and can brick/dead your handset and the Hard Reset or Factory Reset process will not be done properly.So please read carefully the following points:If you don’t charge your mobile to the following percentage or below 75% then the risk is that the mobile phone may switch off or become dead during the Hard Reset or Factory Reset process. and you have to perform all the processes again from starting.When you perform a hard reset or factory reset of your mobile, it will delete all data, file and installed applications from the mobile’s memory and will reboot as a new phone comes out of the box.Your mobile will not have any data or call logs or anything, so it’s better to backup your mobile phone data onto a different memory card or on PC before hard reset.Hard Reset or Factory Reset method will delete all of your data including data in your internal memory, sim cards and external memory cards.So it will be good to remove the external SD card and Sim cards before Hard Reset or format.Originally published at https://www.howtohardreset.in.",android,https://medium.com/@hardresetguide1234/points-to-note-before-hard-reset-or-factory-reset-a-mobile-phone-26fd56c9504?source=tag_archive---------31-----------------------
3D Flip Clock & Weather ad free apk free download latest version,"PatrickeastonJun 1, 2020·1 min read3D Flip clock & world weather widget is a full featured, size 4x2 or 5x2 fully customizable digital clock and weather forecast widgetThe widget features the following:- 2 widget sizes; 4x2 and 5x2- Several widget skins to choose from- Different weather icon skins- Display the next alarm and week number on the widget (optional)- Several widget hotspots (most of them can be user defined to launch specific applications)- Automatic location (from cell/wifi or GPS) or manual- Automatic weather update interval (30 min, 1, 2, 4, 6, 8 hours) or manual- Hour change notification option- Detailed current weather forecast display that includes the following:- Local time (for current location)- Sunrise and sunset time for current location- Humidity, pressure, chance of rain, dew point, visibility, UV index and wind conditions- Current condition, temperature, low and high temperatures- Last weather update time- Background according to the weather condition and day/night- Optional display of the current moon phase- 24 hourly weather forecast- 24 hourly wind forecast- 7 day future forecast- Future forecast details (sunrise, sunset, moon phase, wind, humidity and chance of rain- World weather: Choose to display weather information for up to 10 different locations worldwide",android,https://medium.com/@patrickeaston_15994/3d-flip-clock-weather-ad-free-apk-free-download-latest-version-c3de5ab27531?source=tag_archive---------41-----------------------
May 2020,"Another busy month in isolation, with lots of minor updates to the extensions. Hope you are all keeping well in these unprecedented times.If you have been having issues releasing new applications due to the UIWebView deprecation on iOS, make sure you have updated all your extensions. We have had a few of our extensions brought to our attention that we didn’t realise contained UIWebView usages. So over the last month we have been through them all and migrated any using UIWebView to WKWebView.Please let us know if you are still encountering this issue. Note that you should be using the latest release of AIR as well, as the AIR SDK itself contained UIWebView usages.We have updated our extensions utilising older iOS Google libraries. This includes Firebase, Adverts (AdMob), Google Identity, Google Analytics and Push Notifications. Additionally we have released some point updates to Google Play Services on Android.We have been holding these back due to conflicts with AIR (see below), however we made the decision to push forward due to UIWebView usages in the underlying SDKs. So you may find some issues with the latest releases until the issues below in AIR are fixed.We are asking all our customers to put your voice behind these couple of issues in AIR. They are currently either causing issues or blocking further releases to our ANEs. In particular the iOS version of Firebase and related Google services are being affected.Firstly there is a conflict between the version of sqlite used in AIR and the version in iOS since iOS 12. This has become an issue due to the functionality in the newer version of sqlite being used in core Google libraries. When an application is packaged including a version of sqlite this version takes priority over the system version, which means newer api calls are failing.github.comThe second is an issue with the crashing at launch when we use ANEs containing certain types of iOS libs.github.comAll you need to do is to add a reaction to the initial issue report. Hopefully this will push it up the list of goals for Harman.Stay safe out there!",android,https://medium.com/airnativeextensions/may-2020-f21a646d894c?source=tag_archive---------18-----------------------
Transferring data between activities using dagger,"Rajarajan KJun 1, 2020·5 min readIt’s not always easy to transfer data between the activities especially if the model object is not simple filled with normal datatypes. There will be cases where you’ll be having observable properties, interface implementations, etc. Can this be achievable through serializable, parcelable implementations? We’ll look at how these are used currently, their downsides and what could be possible solution for this. We’ll also be discussing a hint of dagger here. Taking iOS for example, data can be passed between activities through constructors like a normal class. So, can we do the same in android? Let’s find out at the end of this post.When it comes to passing objects between activities, having our class to implement the serializable interface would be the easiest choice. Nowadays, parcelable is preferred over serializable because it is faster. Is speed the only reason why parcelable is chosen ahead? The answer is definitely no. There are two other important reasons from my point of view.If you don’t have Any() or observables in your class, have a simple model class and doesn’t want to complicated code then you can use serializable.Now let’s move on to parcelable interface.Now when you want performance improvement, parcelable is chosen. Also, you can achieve here that is not achievable using serializable. Yes, you can have observables and properties initialized with Any(). But parcelable comes with disadvantages too. To say the least, it has more. They are:There is one another problem. It is not possible to transfer a callback alone using parcelable but can be done using serializable but with some restrictions. For example,We have an interface which implements Serializable:And its implementation as:But there is a problem here. All the code written within the someFunction() block needs to be serializable. In case if you return any data from the second activity and you want to set that to a text view, you cannot do it.So what shall we do now? Dagger comes to our rescue. Dagger is used for many purposes to have a good architecture in an app, provide dependencies, etc. So we can use it to solve our problem too, that is we can use dagger to provide us our required dependencies and inject other activities or fragments with necessary dependencies.So what shall we do now? Dagger comes to our rescue. Dagger is used for many purposes to have a good architecture in an app, provide dependencies, etc. So we can use it to solve our problem too, that is we can use dagger to provide us our required dependencies and inject other activities or fragments with necessary dependencies.After configuring dagger in your project, we need to create an app component and a base application class that extends DaggerApplication class.We have a user class like with mandatory properties like id, name and observable property hobbies:AppComponent interface that implements the AndroidInjector<BaseApplication> interface:Provide the necessary modules to the @Component annotation. Those modules will be bound with the instance of Application so as to get context and also these modules provides the necessary functions/properties for injection.BaseApplication class that extends the DaggerApplication class:Now coming to our MainActivity, it should be annotated with @Module and also it should extend DaggerAppCompatActivity instead of AppCompatActivity like this:In the Activity2, we can get the injected listener and object (eg: User) like:To set a received value to a text view inside the callback:This works perfectly fine. But what if there are more listeners? We need to put into a set or a map, provides unique keys to them and identify them at the target point using these keys and use them.In Activity2, this can be retrieved as:The same can be achieved using Set, with Class as key types. Users can use them accordingly to their usage. The @JvmSuppressWildcards must be used for error free usage when Kotlin is used.It is left to the developer’s convenience when to use where. If it’s a simple model object to be transferred which has no observables or a simple callback, serializable can be used. If there is no worry on mandatory properties or non optional properties or simple callbacks, parcelable can be used. If you are having a quite complex model object that has a lot of observables, mandatory or non-optional properties or event listeners where you set data to view, etc dagger can be used. Although there is some additional work on configurations and creating modules/components, it will greatly reduce the boilerplates if used for the entire app for dependency injections. Nevertheless, using dagger we can pass data between activities in android too like iOS, though it is not through the constructor but through properties that makes us feel activities are like any other class.",android,https://medium.com/@rajarajankulothungan/transferring-data-between-activities-cefc7036da32?source=tag_archive---------23-----------------------
AIO Launcher PRO apk free download latest version,"DanselpronJun 1, 2020·1 min readAIO Launcher is not ordinary home screen. It does not have colorful icons, many animation effects and a variety of themes. Instead, AIO Launcher uses a screen space to show you the most important information.AIO Launcher can display the following information on the screen:* Weather — current weather and forecast for 10 days;* Notifications — standard android notifications;* Player — when you turn on the music, playback control buttons appear;* Frequent apps — frequently used applications buttons;* Your apps — the icons of the selected applications;* Contacts — quick contacts;* Dialer — numpad for quick calls;* Timer — timer start buttons;* Mail — list of received emails;* Notes — list of your notes;* Tasks — list of tasks;* Telegram — last messages (paid);* RSS — latest news;* Twitter — all tweets or a single user tweets;* Calendar — upcoming events in the calendar;* Exchange rates — currency exchange rates;* Bitcoin — the bitcoin price;* Finance — stocks, precious metals, cryptocurrencies etc (paid);* Calculator — simple calculator;* Audio Recorder — record, play and share audio;* System monitor — RAM and NAND usage, percentage of battery power;* Control panel — toggles for WiFi/BT/GPS etc;* Traffic — shows current download/upload rates and connection type;* Android widget — standard app widgets (paid).",android,https://medium.com/@danselpron/aio-launcher-pro-apk-free-download-latest-version-30ce9a99fbcd?source=tag_archive---------50-----------------------
528 Player Pro apk free download latest version,"DanselpronJun 1, 2020·1 min readHiFi Lossless 528hz Music Player that can play all of your music while pitching the frequency in real time to the 528hz.Supports all regular music formats and all Lossless formats (ape, flac, alac, wav, m4a and more)FREE DOWNLOAD:The 528 Player will pitch shift your music in real time to 528hz (if the song is in 440hz it will be pitched to the 444hz frequency) when the sign “Pitch to 528hz” is visible to the right of the song name.So you don’t need to press anything, the player automatically converts all of your songs(only temporary) to 528hz by default.​The 528hz frequency is referred as the tune of LOVE — just a clear sound with a different and positive experience :)​​Some of the Features:-20,000 + Live Radio Stations from all over the world which can be pitch shifted to 432hz and 528hz in realtime!-Album Art Auto Search-​ID3 tags Editing/Displaying​-Custom Playlists Creation, Integrate with other music players.​-Choose songs or complete folders to play-​444hz/440hz per track listening option-​Advanced filtering and search options-​Bluetooth Support-Customization of design by the user-Full Custom Themes coming up soon..-​and much more..​​Additional Pro Features:-Ad Free-432hz / 528hz Switch-Music Equalizer-Orientation Change (Landscape/Portrait)",android,https://medium.com/@danselpron/528-player-pro-apk-free-download-latest-version-f4f9cbee5188?source=tag_archive---------49-----------------------
Hacking Android Using PhoneSploit,"n00bieJun 1, 2020·4 min readHey guys, In this blog, we are going to see about hacking an android device using Phonesploit. It uses the same thing as before, the misconfiguration in ADB, but this eases most of the work for you.Phonesploit: Phonesploit is a framework using which we can exploit android devices, It uses the ADB port 5555 to connect to a device and run commands on it. Some commands are pre-built in this framework, so you don’t need to memorize or look back the ADB commands each time, which I hope make things easy for you.Lets get started!The tool runs on python2, if you don’t have it please install it.The beauty is this is platform independent because, it uses only python2.Having your python ready. We need a package called colorama,Install it as,pip install coloramaNow getting on to the tool, just clone the repo as, git clone https://www.github.com/Zucccs/PhoneSploit.gitHaving cloned the repository, lets move on to the directory, to check out what it has for us.cd PhoneSploit lsLets jump into the fun part!if you are on a linux system, run the main_linux.py file, if you are using Windows, run the main.py file as,python main_linux.pyWhen you run it, it confirms whether you have installed ADB or not, If you have seen my previous blog, you would have installed it by now. Though its not a problem, if you haven’t installed, the tool installs it automatically. Since I had it installed, It skipped the step.Then it would greet us with a wonderful screen like this,As you can see, you could do a lot with it and automate most of the that should be performed manually. And also, as I said before, If you are familiar with Metasploit, you could play around these tools easily. By the way, I hope, you will get to know about Metasploit, if you don’t know about it. Go on it would be fun.We should use the number as the means to interact with the program.To get connected to a device, type in 3 And to connect to a remote device we need its IP address. So we can remotely gain access to an android device.The error is displayed because, first the program searches for Android emulators locally on the device. After which it asks for the IP address to connect. If there is already a emulator, it would connect to it automatically.This is running adb connect command as you would know from my previous blog, where you did it manually.After this you may start playing around with this device as you wish.Lets get some information about the connected devices using number 1.Lets do an interesting thing, We could see if we can pull a screenshot from the device. Again you don’t need to memorize or look back at any commands for this, the program takes care of this.It confirms the device IP to take screenshot of, and asks for the location to save the screenshot to. Crazy isn’t it. It really pulled a screenshot of the device, it was just a black screen.Hope you are happy that you have an easy way to exploit an android device, Ghost Framework is also similar to this, consider seeing it also.With this you can’t exploit any random android device out there, you can only gain access to those that have their ADB port 5555 open without any security.Thank you for reading. Hope you learned something.If you have any problems, please watch this videohttps://www.youtube.com/watch?v=qr2L27GFeiA",android,https://medium.com/@n00bie/hacking-android-using-phonesploit-ffbb2a899e6?source=tag_archive---------2-----------------------
Audify Notifications Reader PRO apk free download latest version,"ThomaskalterJun 1, 2020·1 min readAudify helps you to be informed about your notifications without any effort. It speaks out your notifications through your phone speaker or headset or via any connected Bluetooth device and intelligently ignores the notifications that might annoy you.ACTIVATES AND DEACTIVATES AUTOMATICALLYOnce Audify is enabled, It starts to work automatically as soon as you connect a wired or wireless headset/speaker and stops itself as soon as you disconnect your headset/speaker.❤ Drive safely and let Audify take care of your notifications.❤ Enjoy music without getting anxious about annoying notifications. Ignore the unwanted and only act when you receive an important notification.❤ Cast your notifications on your big screen with google cast support.❤ Enable Audify OnSpeaker wherever you want and be informed about your notifications.❤ Tons of options to customise as you like.❤ Fully accessible via TalkBackKEY FEATURES★ Mute annoying apps and enable only those apps you like.★ Add blacklisted words and mute any specific notification from any app.★ Enable/Disable Bluetooth device or wired headsets.★ Privacy mode to skip notification content.★ Automatically save your vehicle location and easily navigate with Audify.★ Intelligently avoids consecutive notifications from the same app to avoid an annoying burst of notifications from a specific app.★ Enable Audify on phone speaker.★ Supports Google cast (Chromecast devices)★ 250 free audifications. Buy Audify Premium once to get unlimited audifications forever across all your devices.",android,https://medium.com/@thomaskalter/audify-notifications-reader-pro-apk-free-download-latest-version-3ebee89098ba?source=tag_archive---------27-----------------------
"Android, CMake and FFmpeg. Part Two: Building FFmpeg","Ilia KosynkinJun 1, 2020·14 min readOriginally published here.This series of three articles (series content below) aims to help Android developers without much of experience in the native development to integrate external native libraries (in this case FFmpeg and libx264, but it should be relatively easy to extend to other projects) and streamline the whole process by using CMake instead of previously used ndk-build.It’s important to highlight that it is not really an article or a tutorial on either CMake nor its specifics on the Android platform. As the name, “Cookbook”, suggests it’s supposed to be used as a reference to be returned to multiple times and it’s mixed both from “beginners” tips and more advanced stuff (so if it seems that a section talks about something you already know, feel free to skip it).It’s also worth noting that while it’s part of a series, mainly because it is a necessary foundation to make sense of what is happening in next parts, I do hope it will be useful to everybody, who is working or plans to work in CMake!I think the best way to describe CMake is “more elaborate version of configure-based approach”. Essentially, it configures projects for specific build systems (Make, Ninja, Visual Studio, XCode etc.) using a specially designed scripting language (note that Android Studio uses Ninja, the build system designed for fast, iterative builds).How a project is generated is defined by a file called CMakeLists.txt, which you can think of as a project file (or script, rather) of sorts. If you used the Native C++ template in the New Project Wizard in Android Studio it should’ve already generated the bare bones CMake project for you.If you have an existing project you wish to integrate native code to, you can follow this guide to navigate you through.The next section will present basic syntax and some commands that will be used later on.CMake has a relatively complex system for variables. I suggest referring to set documentation for additional details.Let’s start with the basics.set(<name>, <value>) – sets value to the variable, where <value> can be string, number or list.To set a list simply call set(<name>, <value_0> <value_1> … <value_n>). Note that lists are handled as strings by CMake internally, with elements separated by “;”. So calls set(list, a b c) and set(list, “a;b;c”) are the same.To get a variable you just need to put its name in an expansion operator like this: ${<name>}.Important note: people who worked with shell scripts before might tend to use “${<name>}” to make sure value is expanded correctly without really thinking if they need a string here or not. I strongly advise from doing this: CMake has a lot of internal magic, which splits arguments for commands and saves them into the cache and there is a good chance you’re going to break it by making it treat some parts of command as unsplittable values. A good rule of thumb would be: don’t use quotes unless it doesn’t work without them. While you’re probably not going to run into issues in simple cases, in more complex scenarios (for example making custom targets) it might create all kinds of issues and cause hours of painstaking debugging to fix the issue.To set environment variables in code use: set(ENV{<name>} <value>).To get environment variable use: $ENV{<name>}.Often you would need to invoke CMake script with certain flags or values. You can do so with -D switch. For example:${CMAKE_COMMAND} -DNAME:STRING=value -P script.cmakeWill invoke script.cmake with variable NAME set to “value”.To set environment variables for the script use “-E env” (I suggest looking at Command-Line Tool Mode for CMake for more details) switch which allows you to modify the environment for the invocation:${CMAKE_COMMAND} -E env PATH=some_path:$ENV{PATH} <COMMAND>Controlling program flow (if, for and while) is more in line with what you would expect from a programming language, but there are also a few differences to the syntax you would usually see.Note that in order to break loops you can use break() and continue().Controlling flow with if:Possible conditions are:This is most definitely not a complete list of operators, but most useful ones, in my opinion. Note that <variable> here is variable name, not expansion (i.e. name, not ${name}).To iterate over list use following syntax:To iterate with index you have following options:or with specific start, stop and optional step:This is pretty straightforward, you can use it in following way:I think it’s important to note that everything described above (including set) are also commands. But I think it’s easier mentally to distinguish syntax constructions and other commands, especially when you’re just starting out and trying to apply knowledge from other languages to CMake.File is a very useful command that allows you to manipulate local and remote file systems. I’ll give general outline of available subcommands that I find the most useful:In order to find files under a certain folder with GLOB you can use expressions like “path/expression”, i.e. just prefix your expression with path, otherwise it will run in the current directory. However, I should note that if you want to do this to add sources to the target — it’s not advised to do so by CMake creators. The main reason is that CMake won’t pick up if you have changed the target sources and will not prompt you to re-run configuration. If you work with co-workers, after they download changes it won’t prompt them either to re-run configuration, so it might create some weird issues with the project not building properly. It doesn’t necessarily mean you absolutely should not use it ever, just be aware of that possibility and make a conscious decision.Well those two are pretty straightforward: they find a library (to link against) or program (to use).You can run them with: find_(library|program)(<variable> <name>) and it will run search through PATH and CMake search locations and if the library/program is found its path will be put into <variable>.A super useful command to deal with paths/urls. Basic usage is:get_filename_component(<variable> <path/url> <mode>)It will put part of the path/url specified by mode into <variable>.Mode is one of the following:Includes CMake file or module into the project. For example: include(your_file.cmake) or include(CMakeModule) (a little bit more about built-in modules later on).Important note: it preserves the scope of the parent project, which effectively means all variables and functions declared before include() invocation are also available in the included script.Allows you to evaluate mathematical expression. Example usage:math(EXPR <variable> “100 * 2”)will put 200 into <variable>.Note that the result must be representable as a 64-bit signed integer.List is a helper command to work with lists which has a lot of useful sub-commands.Very useful command to operate on strings.Allows you to specify dependencies between targets. Basic usage looks like:add_dependecies(<target> [<dependent_target_1> … <dependent_target_n>])All targets in <dependent_target> list will be brought up to date before <target> is built.Creates a target for building an executable. Basic usage is:add_executable(<name> [sources])Creates a target for building a library. Basic usage is:add_library(<name> STATIC|SHARED|MODULE [sources])Adds subdirectory to the project. Subdirectory must have its own CMakeLists.txt in it. Effectively it allows you to create sub-modules of the project. However note that in contrast with include() add_subirectory creates its own scope, instead of inheriting parent’s. Basic usage is:add_subdirectory(<path>)Path can be absolute or relative.Adds directories to include search paths for the current CMakeLists. Note that include_directories will affect all targets declared after its invocation. Keep it in mind to avoid getting include errors. Basic usage:include_directories([path_1 … path_n])Command supports relative paths.Adds directories to library search paths for the current CMakeLists. Note that link_directories will affect all targets declared after its invocation. Keep it in mind to avoid linker errors. Basic usage is:link_directories([path_1 … path_n])Command supports relative paths.Allows you to link specific library to the target. Note that generally command is quite complex, but in most basic and common form it looks like:target_link_libraries(<target_name> [lib_name_1 … lib_name_n])Library name might be either name or full path to the library.When working with CMake it’s important to remember that it works in two phases and be aware of which phase you’re targeting.The first phase is the configuration phase itself; it defines how the project is going to be built and usually takes up most of the space of a script(s). However, there is one caveat: CMake script can be (and also often is) executed in the build phase.So what is the build phase? The build phase is the process of building targets, defined in the configuration process.What are targets? Targets essentially are commands executed by a build system during the build process. It may be a little bit cryptic, so let’s consider how you define target in CMake. Usual ways to define targets would be commands: add_library, add_executable and add_custom_target. I think the first two are self-explanatory and pretty much explain what targets are — it’s a sequence of commands defined to assemble a certain piece of software (library or executable).What about add_custom_target then? You can think about it as about a built-in way to extend the build process with custom commands. Where add_library or add_executable will strictly build a library or executable, add_custom_target allows you to specify a custom command to be run during build time. One example of such a command would be a clean target, which usually deletes files left after previous builds.The last piece in the targets’ puzzle is the fact that you can define relationships between targets with add_dependecies function. That way you can make sure targets are executed in a specific order. All together this system provides you with a strong and flexible suite to build pretty much anything the way you want.A module is essentially a collection of commands (or library if you will), which simplifies certain tasks. You can check the reference for a full list of available modules.Pretty much a “must-have” module if you’re integrating any external project into yours, especially if the integrated project is not CMake-based. While default implementation actually expects CMake project, by tweaking a few parameters you can easily (well, I guess easier is a better word) integrate configure-based projects. Essentially, you can think about ExternalProject as about system for managing dependencies (like we have Gradle in Android): it allows you to download, unpack, configure, build and install any project you can get sources of.Let’s start with really basic examples.First of all make sure you have included the module like this:include(ExternalProject)Next let’s define project we want to add:That’s a really barebones example which probably won’t really occur in the real life, but let’s go over some basic properties we used here:Quite often you might want to use a git repository with specific version to build:In Android environment, which usually builds few architectures you probably would like to avoid additional overhead of downloading and unpacking sources for every architecture:In this case we supply a path to the local folder with sources. DOWNLOAD_NO_EXTRACT makes ExternalProject skip the extract step and just copy sources folder to the prefix source folder.While it might not be a great idea for many projects (mainly because there are usually some left-overs which are left by build and they might conflict with future build, while usually you want to make clean builds), in some cases it might be worth it to at least try it out (for example, if you’re really trying to squeeze out every little bit of performance from build process).As you can see we don’t supply PREFIX here, instead we set SOURCE_DIR directly. We also override DOWNLOAD_COMMAND to empty string, so we don’t try to download anything. Setting BUILD_IN_SOURCE prevents ExternalProject from creating additional build folder and builds in SOURCE_DIR directly instead.The built-in steps that ExternalProject executes: DOWNLOAD, UPDATE, CONFIGURE, BUILD and INSTALL (in this order). You can override any of those command with according *_COMMAND parameter and log any of those with LOG_* 1 parameter.If you using default behavior which builds project as CMake project you can modify behavior with custom arguments like following:CMAKE_ARGS -DSHARED:BOOL=ONOr alternatively you might use CMAKE_CACHE_ARGS to force set cache variables.CMake allows to integrate custom platforms with so called toolchain: file which specifies some behavior and flags specific to platform. On Android this file is located ${NDK_LOCATION}/build/cmake/android.toolchain.cmake or you can find the latest version online here.If you do anything in CMake in Android I highly recommend to have this file opened all the time, since you probably going to constantly look up variables and other stuff.Another Android specific thing would be building different architectures. In your module folder there should be hidden folder called “cxx”. To get to actual build folder navigate to cxx/cmake/${variantName}/${abi}. This folder is quite important because:Another important directory you might want to check out is ${CMAKE_LIBRARY_OUTPUT_DIRECTORY} which by default is located at ${MODULE_PATH}/build/intermediates/cmake/${varianName}/obj/${abi}. This is directory where you want to output your shared libs so build system will automatically pick them up for distribution. I also prefer to put “include” folder here as well since it seems more intuitive for me, but you definitely can you use any other directory you want.Android toolchain file defines many useful variables. Some of them are listed below:This section accumulates some additional advices that hopefully will help out other developers to don’t spend half of a day trying to understand what is going on:",android,https://medium.com/@ilja.kosynkin/android-cmake-and-ffmpeg-part-one-cmake-in-android-cookbook-b9f27f9937b?source=tag_archive---------10-----------------------
"Redmi Note 8, Redmi 8 and Redmi 8A Dual Price Hiked in India | GarimaShares","Redmi has increased the price tag of some of its best-selling smartphones in India. They are Redmi Note 8, Redmi 8 and Redmi 8A Dual.Their price tags have been increased by Rs. 500. This increase applies to purchases made from all online sources as well as offline buyers.The Chinese company had increased the prices of these three devices. This happened after the government increased the GST rate on smartphones in India from 12 percent to 18 percent.Specifications of Redmi Note 8With the new price hike in effect, the Redmi Note 8 will now be available at Rs 11,999 for the 4GB RAM + 64GB storage and while the 6GB RAM + 128GB storage will now be available for Rs 14,499 — as opposed to their previous pricing of Rs 11,499 and Rs 13,999, respectively.It comes in four color options — Neptune Blue, Moon Light White, Space Black, and Cosmic Purple.Checkout Redmi Note 8 on AmazonAlso Read: Google is Helping People Deal With Anxiety DisorderThe Redmi 8 now costs Rs 9,499 for the 4GB + 64GB variant, while the Redmi 8A Dual (2GB + 32GB), will be up for Rs 7,499 across online and offline stores in India.The reason for the price hike was not disclosed by the company, but it could be due to supply chain or production issues during this coronavirus lockdown.Also Read: Locusts in North-West India and Cicadas in the USOriginally published at https://garimashares.com on June 1, 2020.",android,https://medium.com/garimashares/redmi-note-8-redmi-8-and-redmi-8a-dual-price-hiked-in-india-garimashares-dae78aa95df7?source=tag_archive---------38-----------------------
"Apphi: Schedule Posts for Instagram, FB, Twitter unlocked apk free download","ThomaskalterJun 1, 2020·1 min readAutomatically post your scheduled Instagram photos, videos, Story, IGTV.Trusted & used by thousands of Instagram influencers.Schedule for Instagram, Facebook, Twitter at the same time.Managing Instagram account requires a lot of effort. Apphi lets you schedule photo, video, Story, IGTV and automatically post them on your Instagram.Schedule & Automatically Post:- Post Photo, Video, Story, IGTV- Tag People- Tag Location- When to post. Best time to post- Suggested hashtags- Add First Comment- Tag Products- Add Story URL- Tag People on Story- Add Hashtag on Story- Schedule and post IGTV- Facebook Page- Schedule Profile Website- Schedule Profile to Public or Private- Set post delete time & Screenshot. Apphi will automatically delete the post and save a screenshot for you. - Automatic story posting- Check likes, comments of your post.- Followers Gain & Loss analytics- Preview and design your grid with drag & drop- Manage unlimited Instagram and Facebook accounts- Schedule Album with multiple photos and videos in one post- Schedule Profile Website URL- Search & Repost- How to repost best feed- Bulk upload and schedule- Receive email and push notification when post or delete.- Auto post on Instagram- Plan and program posts in advance- Support Dropbox & Google PhotoBest IG Tool, scheduler, scheduling app. Plan and manage Instagram. Schedule & grow your Instagram followers organically!",android,https://medium.com/@thomaskalter/apphi-schedule-posts-for-instagram-fb-twitter-unlocked-apk-free-download-2aa3a60f7be5?source=tag_archive---------28-----------------------
4K Wallpapers HD & QHD Backgrounds PRO apk free download latest version,"AlexandersekelJun 1, 2020·2 min readHDW Team proudly presenting you “Wallpaper Expert”, the most amazing app ever made for HD, QHD and 4K wallpapers with advanced Auto wallpaper changer . All wallpapers are hand-picked by professional designers and photographers, to make each and every background looks beautiful and stunning.Quality is the only aspect what make us the great & best wallpapers app for Android phones.★ Features:Our simple & user-friendly interface offers following features… Recent > This is where you see latest updated wallpapersTrending > Popular wallpapers sorted based on user downloadsFeatured > Best Wallpapers hand-picked by our designers teamRandom > Wallpapers randomly shown from whole collection with hourly updates.• Download high quality Premium wallpapers for free• Daily updates includes exclusive content for special occasions like Festivals, New year, Christmas, Valentine’s day and much more…• Super-fast & lightweight app• All wallpapers are suitable for HD, QHD and 4K resolution mobile phones• All backgrounds are available in “Portrait” mode only for perfect fit• Option to clear cache to make app loading faster and save memory & battery• Option to Share/Send wallpapers through various apps like Whatsapp, Mail, Skype and much more..• Save your favorite wallpapers and access them through “Favorites”With Remove ads (PRO) option:- No more Ads (Valid for life-time)- 2x faster UI experience- Priority updatesHow Auto Wallpaper Changer (AWC) works:- Download few wallpapers to our default folder “WallpaperExpert”- Navigate to Settings choose “Auto wallpaper changer”- Navigate to settings (⚙️) inside AWC and select any Interval from the options - Make sure to choose “Apply or Set Wallpaper” after selecting the options.- Now wallpaper will be changed automatically after that selected Interval time continuously with a Random wallpaper from the chosen folder.Available categories: 3D, Abstract, Animals, Cars, Bikes, Festivals, Christmas, New Year, Easter, Halloween, Mothers Day, Valentines Day, Creative, Cute, Fantasy, Flowers, Greetings, Love, Music, Nature, Landscapes, Beach, People, Autumn, Summer, Winter, Sports, Superheroes, Technology, Travel & World, Food, Military, Photography, Space and we have the best collection of Dark/Black and Minimal wallpapers. Don’t miss them",android,https://medium.com/@alexandersekel/4k-wallpapers-hd-qhd-backgrounds-pro-apk-free-download-latest-version-45226c5df89d?source=tag_archive---------43-----------------------
Auto Move To SD Card PRO apk free download latest version,"JakefernotJun 1, 2020·2 min readNew Feature for users without SD Card1. File Manager — User can see all the internal storage directory and it’s sub directory.2. File Manual Transfer — from Internal to Internal3. Schedule Transfer — from Internal to InternalNew Feature for users with SD Card1. File Manager — User can see all the internal storage directory and it’s sub directory.2. File Manual Transfer — 1) From Internal to Internal and SD Card and 2) From SD Card to Internal and SD Card 3. Default Selection view with preview options too4. Added Tutorial Screen — Show the Introduction of application features with descriptionUpdates : 1. Support multiple languages.2. Schedule Transfer to custom path : with this feature you can set a particular date and time along with custom path to transfer files. 3. Muliple auto transfer Now select multiple folders for auto transfer to SD Card.Worrying about low internal memory on your phone? If you are using a SD Card (Memory Card) on your phone then this app is very useful to you. With this app you can automatically transfer files from internal memory to your SD Card memory.Auto Transfer from Internal to External memory (SD Card) :With this feature, auto transfer helps to transfer files from internal storage to external storage automatically and avoiding your internal memory to exhausted. This feature supports all image, video, audio, document, apk and other type of files. Auto transfer will without opening this app, will transfer files from selected folders whenever a new file is added to these selected folders.You can specifically select folder from which auto transfer of files should function.",android,https://medium.com/@jakefernot/auto-move-to-sd-card-pro-apk-free-download-latest-version-6d951100ca7b?source=tag_archive---------51-----------------------
34 Best and Free Android Libraries of 2020 to Make Extraordinary Apps,"Kushagra ChaturvediJun 1, 2020·1 min readWhen it’s about developing a high-quality custom Android app in the least time possible, you need the right set of Android libraries to accelerate your app development process. Saving you from reinventing the wheel, Android libraries not only save you a lot of time and effort but also help you quickly develop an app that is entirely bugs-free.So, without further ado, let’s jump right into the list of the best and most useful Android libraries of 2020, which will help you make the development of your next Android app hassle-free and fun!Click here to read the full article on iPraxa’s official blog.",android,https://medium.com/@kushagra-ipraxa/34-best-and-free-android-libraries-of-2020-to-make-extraordinary-apps-e5e2a27b3c23?source=tag_archive---------33-----------------------
AudioLab — Audio Editor Recorder & Ringtone Maker PRO apk full free download latest version,"JakefernotJun 1, 2020·3 min readAudioLab is Most Advance, Modern, Fast, Professional Audio Editor which have all the features you would ever want.Features like Audio Trimming ( cut / crop / copy / paste / add silence / fade / undo / redo ), Audio Mixing ( Mix Four Audio, Change Speed & Pitch, have all DJ effect options and Filters ), detailed Tag Editing, Audio Merging, Audio Recorder, Audio Converter and many more features… with Instant Preview to monitor every step of your modification make AudioLab different from other➜ Want an Audio Cutter to Cut the best part of your audio and save it as your Ringtone / Alarm / Music File / Notification Tone?➜ Want to Mix Audio like Professional Desktop Mixing?➜ Want to Record quality audio?➜ Fed up With wrong Album art, Song art, and inaccurate audio tag?With AudioLab audio editor you can do all these and much more…♪ Audio Cutter / Audio Trimmer / MP3 CutterSimple, Modern and Easy to use User Interface to precisely cut the best part of Your Audio (up to 100 ms) Song for using in Music File, Mobile Ringtone, Notification, AlarmTone.Add Silence, Delete a Portion of Audio, Copy Paste any part of Audio with Edit Stack to easily navigate you through your editing and help you to easily undo or redo your work.Supports every file format you can imagine ( mp3, wav, flac, m4a, aac, ogg and more…) ♪ Audio Mixer Mix Your Audio with four different Audio track to create remixesAdd Lots of effect like Echo, Whoosh, Reverb, 3d Audio Spatializer and more… to give your Audio DJ like effectYou can also Change Tempo ( Speed ) and Pitch of Audio♪ Merge Audio Merge two or more audio and create single audio using Merge Audio. You can merge Audio files of different formats with no loss in Audio Quality♪ Tag Editor Correct wrong Album / Song art and inaccurate audio tag ( Edit all the most known tag information of Audio)♪ Audio Converter & Mp3 Converter Convert any audio files from one format to another. Choose from following Audio formats- MP3, WAV, AAC, MP4, FLAC, OGG, OPUS with different Channel, Sample & Bit Rate♪ Audio Recorder / Voice Recorder Audio Recorder for recording your voice and music with desired Source, Channel, Sample rate, Audio Format, and advance option like Noise Suppressor, Automatic Gain, and Skip Silence♪ Split Audio & Reverse Audio Split any audio file into two parts and reverse any audio file with Instant Preview♪ Text To Speech AudioLab help you to Convert Text to Speech with phone’s built-in Text to speech engine♪ Device Codec Info A simple and handy tool that allows you to detect which multimedia encoders and decoders(codecs) are available on your Android device♪ Voice Changer Change your voice/audio and have fun with your modified voiceApply Helium, Drunk, Chipmunk & many other funny effects to your voice♪ Vocal Remover / Karaoke Effect Remove vocal from your audio and use it on your Karaoke Night♪ Video To Audio Converter Extract audio of any video and save it. Select the part of Video you want to convert in AudioOther Features : -— MP3 Cutter — Noise removal in audio — Normalize Audio — Speed Changer — Karaoke Offline — Silence Removal — Audio Effects Editor — 18 band Equalizer — Channel Manipulation — Add Audio to Video — Add music to voice — Video To GIF",android,https://medium.com/@jakefernot/audiolab-audio-editor-recorder-ringtone-maker-pro-apk-full-free-download-latest-version-ad96e77cd73d?source=tag_archive---------42-----------------------
Different Modes In Android Devices.,"Hard ResetJun 1, 2020·4 min readAndroid System is a very typical operating system installed with many different modes that makes it interesting to use. Here in this article, you will come to know the different methods of Android Devices.Hard Reset is a process in which Software of Android Phones, Smartphones, Tablets or Apple Devices like iPhone and almost every other electronic gadgets restores to their Stock System Settings or to Stock Android.By performing Hard Reset, it deletes all the data, settings, passwords and applications installed in the mobile phone.Hard Reset Function is mostly used to repair a stuck or malfunctioning device or to restore the device to the Stock Android Stage.Before you perform a Hard Reset, always remember to make a copy of your data or simply backup your data, because this process is irreversible and then recovering of data will not be possible.Soft Reset is a method to restart devices such as Android Devices, Tablets or Personal Computers.This method does not delete data or device settings. It only refreshes the device by cleaning the running processes in the RAM (random access memory).Soft Reset is commonly used to clear issues like slow functioning of device or android operating system error.This is the first step everyone uses in solving the problems with their smartphone devices.Hard Reset does not cause any damage or deletes your smartphone’s data.This feature is only used to restore your smartphone device or any other electronic gadget to the factory new settings.There are more chances that after performing this operation, You will have to face a boot loop or other major problems with your smartphone.Android Recovery Mode is a hidden application installed in android’s boot system that helps you to fix problems and used to install Custom Operating Systems or Stock Android Updates and ROM by certain instructions or key combinations (volume keys, home key, power key).In most Android Devices Recovery Mode, you canThe Android Recovery Mode is a very typical process for Android device users, and you can find Recovery Mode in almost every Device.Download Mode is a feature on Android Device that is used to Flash Stock Firmware or System Upgrade or System Update.The Download Mode is widely and mostly used by Samsung Devices and is similar to Fast-boot Mode / Boot-loader Mode in HTC and Nexus Devices.An Android Smartphone is made up of several partitions in memory, in which one partition holds the Android System Files, another holds all the App Data.Boot loader is the manager for all those partitions.Boot loader instructs the System Kernel to boot the Operating System normally from one of these partitions.The bootloader is usually Locked By The Manufacturers to prevent the Android Devices from the users installing custom operating systems which are not designed for particular devices.This mode is commonly used in almost every Android Device.Fastboot is a special method which is commonly used for updating the flash file system in your device.Using the fastboot you can modify or flash the file system images using a computer via USB Cable Connection.It becomes very useful for smartphone users when you want to perform a quick Update of the Firmware or Flashing ROMs without using the Recovery Mode.You can find Fastboot Mode in every Android Device such as Samsung, Sony, Oppo, Vivo, Xiaomi, Realme, HTC, etc.Safe Mode is a stage when you power on your Android Device, iPhone, or any smartphone disabling the Third-party Apps which may be causing error or is malware suspected.With the Safe Mode On, you can uninstall any of the applications which might be causing any Conflicts, Errors or any Operating System Blips.The Hardware Key Control Mode is specially designed and is very useful when the display of your device is damaged or broken or any other issue and you have to attend the important calls.It allows you to answer the Incoming Calls, End the Calls or Switch Off the Alarms using only the Hardware Keys.This particular mode is commonly used in LG devices.Android Factory Reset Protection (FRP) is a special feature in some of the Android Devices running with Android OS 5.1 or higher.FRP prevents your Android Phone from other people from using and performing a Factory Reset without your permission.By this feature, Factory Reset is only possible with access to your Google Account Information and screen lock passwords.In this case, you should install a new firmware in your device.This process will restore the factory settings and bypass the password stage, by deleting all your data in your device.Originally published at https://www.howtohardreset.in",android,https://medium.com/@hardresetguide1234/different-modes-in-android-devices-954794cc2b7a?source=tag_archive---------16-----------------------
Let’s talk about privacy violation issues in android,"When security and privacy demand clash, privacy should usually be given a higher priority. To accomplish this and still maintain required security information, cleanse any private information before it exits the program.To enforce good privacy management, develop and strictly adhere to internal privacy guidelines. The guidelines should specifically describe how an application should handle private data. If your organization is regulated by federal or state law, ensure that your privacy guidelines are sufficiently strenuous to meet the legal requirements. Even if your organization is not regulated, you must protect private information or risk losing customer confidenceThere are several problems with this example. First of all, with the aid of default, WebView credentials are stored in undeniable text and are not hashed. So if a person has a rooted device (or makes use of an emulator), she is capable of reading saved passwords for given sites. Second, undeniable textual content credentials are broadcast to all of the registered receivers, this means that any receiver registered to listen to intents with the SEND_CREDENTIALS motion will acquire the message. The broadcast isn’t always even blanketed with permission to restrict the wide variety of recipients, despite the fact that in this case, we do not recommend the use of permissions as a fix.Example 1: The following code contains a logging statement that tracks the contents of records added to a database by storing them in a log file. Among other values that are stored, the getPassword() function returns the user-supplied plaintext password associated with the account.The code in the example above logs a plaintext password to the file system. Although many developers trust the file system as a safe storage location for data, it should not be trusted implicitly, particularly when privacy is a concern.Privacy is one of the biggest concerns in the mobile world for a couple of reasons. One of them is a much higher chance of device loss. The other has to do with inter-process communication between mobile applications. The essence of mobile platforms is applications that are downloaded from various sources and run alongside each other on the same device. The likelihood of running a piece of malware next to a banking application is high, which is why application authors need to be careful about what information they include in messages addressed to other applications running on the device. Sensitive information should never be part of inter-process communication between mobile applications.Example 2: The code below reads username and password for a given site from an Android WebView store and broadcasts them to all the registered receivers.There are several problems with this example. First of all, by default, WebView credentials are stored in plaintext and are not hashed. So if a user has a rooted device (or uses an emulator), she is able to read stored passwords for given sites. Second, plaintext credentials are broadcast to all the registered receivers, which means that any receiver registered to listen to intents with the SEND_CREDENTIALS action will receive the message. The broadcast is not even protected with a permission to limit the number of recipients, even though in this case, we do not recommend using permissions as a fix.Private data can enter a program in a variety of ways:- Directly from the user in the form of a password or personal information- Accessed from a database or other data store by the application- Indirectly from a partner or other third partyTypically, in the context of the mobile world, this private information would include (along with passwords, SSNs and other general personal information):- Location- Cell phone number- Serial numbers and device IDs- Network Operator information- Voicemail informationSometimes data that is not labelled as private can have a privacy implication in a different context. For example, student identification numbers are usually not considered private because there is no explicit and publicly-available mapping to an individual student's personal information. However, if a school generates identification numbers based on student social security numbers, then the identification numbers should be considered private.Security and privacy concerns often seem to compete with each other. From a security perspective, you should record all-important operations so that any anomalous activity can later be identified. However, when private data is involved, this practice can create risk.Although there are many ways in which private data can be handled unsafely, a common risk stems from misplaced trust. Programmers often trust the operating environment in which a program runs, and therefore believe that it is acceptable to store private information on the file system, in the registry, or in other locally-controlled resources. However, even if access to certain resources is restricted, this does not guarantee that the individuals who do have access can be trusted. For example, in 2004, an unscrupulous employee at AOL sold approximately 92 million private customer e-mail addresses to a spammer marketing an offshore gambling web site.Example 3: The following code stores user’s plaintext password to the local storage.Although many developers treat the local storage as a safe location for data, it should not be trusted implicitly, particularly when privacy is a concern.The best policy with respect to private data is to minimize its exposure. Applications, processes, and employees should not be granted access to any private data unless the access is required for the tasks that they are to perform. Just as the principle of least privilege dictates that no operation should be performed with more than the necessary privileges, access to private data should be restricted to the smallest possible group.As part of any thorough audit for privacy violations, ensure that custom rules have been written to identify all sources of private or otherwise sensitive information entering the program. Most sources of private data cannot be identified automatically. Without custom rules, your check for privacy violations is likely to be substantially incomplete.",android,https://infosecwriteups.com/lets-talk-about-privacy-violation-issues-in-android-12627c9af4d6?source=tag_archive---------11-----------------------
All-In-One Calculator PRO apk free download latest version,"DanielandofiJun 1, 2020·2 min readDesigned with simplicity in mind, it helps you solve everyday problems.From simple or complex calculations, to unit and currency conversions, percentages, proportions, areas, volumes, etc… it does it all. And it does it good!This is the PERFECT CalculatorPassionate development combined with constant feedback we receive from our users resulted in that we think is the best multi calculator on the store. Featuring over 75 FREE Calculators and Unit Converters packed in with a Simple or Scientific Calculator, it’s the only calculator you will need from now on on your device.Oh, and did we say it is completely FREE?Yes, it is free. We think everyone should enjoy this.If you are a student, teacher, engineer, handyman, contractor or just somebody who struggles with math & conversions, you really should give this one a try.• Use it for simple or complex calculations• Convert units or currencies in the same app• Enjoy easier homework or school assignments• Learn. See step by step solutions to your calculationsSo, on with the features…MAIN CALCULATOR• Simple or Scientific layout• Editable input & cursor• Copy & Paste support• Calculation history• Memory buttons• Function graphing• Dec, Hex & Binary• Floating calculator• Widget75 CALCULATORS & CONVERTERS• Algebra, Geometry, Unit Converters, Finance, Health & Misc• Currency converter with 160 currencies (available offline)• Instant results delivered as you type• Step-by-step solution & formulas• Smart search for faster navigation• Create shortcuts on home screenAlgebra• Percentage calculator• Proportion calculator• Ratio calculator• Average calculator — arithmetic, geometric and harmonic means• Equation solver — linear, quadratic and equation system• Combinations and permutations• Decimal to fraction• Fraction simplifier• Prime number checker• Greatest common factor & Lowest common multiple calculator• Random number generatorGeometry• Shape calculators for square, rectangle, parallelogram, trapezoid, rhombus, triangle, pentagon, hexagon, circle, circle arc, ellipse• Body calculators for cube, rect. prism, square pyramid, sq. pyramid frustum, cylinder, cone, conical frustum, sphere, spherical cap, spherical frustum, ellipsoidUnit converters• Acceleration converter• Angle converter• Length converter• Energy converter• Force converter• Torque converter• Area converter• Volume converter• Volumetric flow converter• Weight converter• Temperature converter• Pressure converter• Power converter• Speed converter• Mileage converter• Time converter• Digital storage converter• Data transfer speed converter• Numeric base converter• Roman numerals converter• Shoe size converter• Ring size converter• Cooking converterFinance• Currency converter with 160 currencies available offline• Unit price calculator• Sales tax calculator• Tip calculator• Loan calculator• Simple / Compound interest calculatorHealth• Body mass index — BMI• Daily calories burn• Body fat percentageMiscellaneous• Age calculator• Date calculator• Time calculator• Mileage calculator• Ohm’s law calculator — voltage, current, resistance and power",android,https://medium.com/@danielandofi/all-in-one-calculator-pro-apk-free-download-latest-version-c4b0829e1d8f?source=tag_archive---------40-----------------------
Aqua Mail PRO apk unlocked free download,"ThomaskalterJun 1, 2020·2 min readAqua Mail is the go-to app for all your email needs! Whether it’s a work or personal email, a corporate exchange server, Aqua Mail lets you easily access all your emails from one complete app with full transparency and control over your daily emails and privacy.Aqua Mail doesn’t store your passwords on alternative servers. You can use OAUTH2 to login in Gmail, Yahoo, Hotmail and Yandex without entering a password for even higher level of security.OWN YOUR PRIVACYStrong SSL encryption — SSL hardening. Use the latest encryption protocols for optimal security of your email traffic over the network.SSL certificate tracking — Aqua Mail provides extra layer of protection against “Man in the middle attacks”.DKIM and SPF validation — prevent email spoofing and make your incoming emails more trustworthy.KEY FEATURES: • Multiple email providers: Gmail, Yahoo, Hotmail, FastMail, Apple, GMX, AOL and more. • Mail accounts hosted at: Google Apps, Yahoo BizMail, Office 365, Exchange Online and others. • Use of a “more secure” login method (OAUTH2) for Yahoo, Gmail and Hotmail. • Backup and restore feature via popular cloud services (Dropbox, OneDrive, Box and Google Drive) or a file.  • Integration with popular third-party apps providing maximum control, synergy and customizability, including Light Flow, Enhanced SMS & Caller ID, Cloud Print, Apex Launcher Pro, Nova Launcher / Tesla Unread, Executive Assistant, DashClock Widget, Tasker and more. • Push mail for most mail services (except Yahoo) and self-hosted IMAP servers, also for Exchange and Office 365 (corporate mail).ADVANCED FEATURES: • Home screen widgets including message counter and message list. • Use our Smart Folder feature to easily navigate and manage your emails.  • Calendar sync for Exchange and Office 365 (use any Calendar app or widgets). • Contacts sync for Exchange and Office 365 (visible in Aqua Mail and in the Contacts app. Includes auto-complete and lookup in the corporate directory). • The rich text editor, styling choices and a large number of formatting options, including embedding images will help you create the perfect email. • With the signature support you can attach a separate signature to each mail account and include images, links and text formatting.  • Notifications and reply via voice input from an Android Wear smartwatch. • Pick from the four available Themes and further customization options to change how the app looks and operates.  • Separate network connection setting for WiFi and mobile data ensures optimum performance and cost-effectiveness.",android,https://medium.com/@thomaskalter/aqua-mail-pro-apk-unlocked-free-download-c36ecb48ce28?source=tag_archive---------29-----------------------
How to Recognize US Driver’s License on Android Mobile Apps,"Xiao LingJun 1, 2020·3 min readAccording to the American Association of Motor Vehicle Administrators (AAMVA) specification, PDF417 symbology is used for storing personal information on US driver’s license. In this article, I will use Google mobile vision APIs to recognize a driver’s license. Besides, I will create a similar Android barcode reader app to extract information from PDF417 symbology by Dynamsoft Barcode Reader SDK.Here is a forged driver’s license used for testing.Mobile Vision APIs are capable of detecting some mainstream barcode symbologies. There is a DriverLicense class that defines common fields existing on all barcode standards.To quickly verify Google’s API, I downloaded the sample code of Android Vision.Create a ResultActivity.java file for displaying the information of a driver’s license:Find the onBarcodeDetected(Barcode barcode) function in BarcodeCaptureActivity.java, and then add the following code to start the ResultActivity class when a barcode format is detected as PDF417:Configure the activity in AndroidManifest.xml:Build and launch the app.Create an Android camera project using fotoapparat:Get camera frames in the callback function process(Frame frame):Decode barcodes from the frame by calling decodeBuffer() function. The image data format is NV21:Dynamsoft Barcode Reader SDK does not provide a driver’s license class yet. We can create a custom class by referring to the AAMVA standard and Google’s Mobile Vision.Create a class named DriverLicense:Create a DBRDriverLicenseUtil class to define the common AAMVA fields:Check whether the string recognized from PDF417 complying with the AAMVA standard:Fetch the driver’s license information and save data to a HashMap:Create an instance of DriverLicense class and send it to ResultActivity :After launching the app, I could get the same results as Google’s.Since I have implemented two Android barcode reader apps, why not make a comparison?The left one is based on Google Mobile Vision SDK and the right one is based on Dynamsoft Barcode Reader SDK.https://github.com/yushulx/android-driver-licenseOriginally published at https://www.dynamsoft.com on June 1, 2020.",android,https://medium.com/@yushulx/how-to-recognize-us-drivers-license-on-android-mobile-apps-5894c05b4dff?source=tag_archive---------12-----------------------
Alight Motion apk full unlocked free download latest version,"ToddwillsnorJun 1, 2020·1 min readBe part of the movement! Alight Motion, is the first pro motion graphics app for your smartphone, bringing you professional-quality animation, motion graphics, visual effects, video editing, and video compositing.• Multiple layers of graphics, video, and audio• Vector and bitmap support (edit vector graphics right on your phone!)• Visual effects and color correction• Keyframe animation available for all settings• Animating easing for more fluid motion: Pick from presets or build your own timing curves• Velocity-based motion blur• Export MP4 video or GIF animation• Solid color and gradient fill effects• Border and shadow effects• Group layers together• Save your favorite elements for easy re-use in future projects",android,https://medium.com/@toddwillsnor/alight-motion-apk-full-unlocked-free-download-latest-version-d3106e016c55?source=tag_archive---------26-----------------------
React Native Over-the-Air(OTA) Updates with Firebase,"Soumya Ranjan SethyJun 1, 2020·13 min readI will be using my previous article’s code to continue setup for OTA update as well which is simply created using Android Studio and later React Native added manually. This is also same for existing React Native App as well.Note : Any Native side update which will impact on React Native Bundle will have to go for legacy deployment process.2. Tap on Add project to Create a new project on Firebase3. I have named this Firebase Project name as RemoteBundleUpdateSet up Remote Config to change the behavior and appearance of your app without publishing an app updateAdd dependency of Firebase Remote Config and google-services pluginAdd google-service.json under “ReactNativeSetupAndroid(root directory)->android -> app -> google-services.json”.Refer below structureI have used PR Downloader, A file downloader library for Android with pause and resume supportYou can read more about about herecode for above design main_activity.xmlModify your MainActivity.java.Create SharedPref.java and add below codeThis class will help to access Shared Preference of Android which will help to store all the details regarding React Native Bundle Name, Url & Path locally.This class will hold all Shared Preference keys, Setter/Getter Methods using SharedPref class, Firebase Remote Config keys, File Downloadable directory path and Bundle File name.Create Constants.java and add below codeThis class will hold ReactNativeHost custom configuration for packages which are not supporting auto linking yet, dynamic path for react-native bundle file loading from SharedPref class using Constant class methods.Modify your MainApplication.java and add below codeThis class is responsible to fetch latest activated appname and activated bundle name and downloadable url from Firebase everytime you start the app. This class is being initiated and used in MainActivity.javaIt uses MainActivity’s Progressbar and TextView to update process status continuously.Before downloading bundle, it compares with locally stored bundle url and Firebase’s activated bundle url. If any diff found then only it will trigger downloading new bundle and continues to save the new url, path and name of bundle in SharedPref locally for next time check.Add Utils.java and paste below codeThis Activity class will be entry point to trigger the React Native and make sure to provide correct Main Component Name. You can check the component name in index.js or Package.json name of the bundle.Modify ReactNativeActivity.java like belowNote: Run all the cmd in root directory of your project.2. To run Android App run3. If you are getting Could not connect to development server even after start metro bundler.4. Creating a release build.This will check and create android/app/src/main/assets folder if not exists and copy the default index.android.bundle file to it. This file will be the default bundle for Build (release apk file) If app runs offline or no bundle found on firebase.5. Export index.android.bundle file to android/output so that it will be easy to copy and upload to Firebase directly.2. Run the Metro builder and check the current code how it looksNote: Run all the cmd in root directory of your project.3. Create release-bundle (android/app/src/main/assets)4. You can also the run the app2. Create only react-native bundle (index.android.bundle) to upload to Firebase (android/output)3. Rename file index.android.bundle to index.android1.bundle inside ReactNativeSetupAndroid(*project root dir)/android/out/.3. Modify again index.js to create one more JS Bundle.4. Simply create again one more new react-native bundle (index.android.bundle) to android/output5. This time rename file index.android.bundle to index.android2.bundle inside “ReactNativeSetupAndroid(*project root dir)/android/out/”.6. Upto this we have successfully created two react-native bundle and ready to upload to Firebase storage.Note : active_app_name should be taken from Package.json and AppRegistry.registerComponent App name(index.js).2. Add index.android1.bundle file link copied from Firebase Storage to ru_bundle_v13. Add index.android2.bundle file link copied from Firebase Storage to rn_bundle_v2.Add bundle name which you want to activate to download and load in the App at run time. here currently activating rn_bundle_v1.Now it should look like belowNote: Make sure your local metro builder is not running or else It will always override the downloaded bundle with the local bundle from your system incase if your are in debug build. For better testing always use release build instead.I have a sample Android host project here, that you can use to test the above steps.You can also check this Pull Request on Github.Thanks for reading!Let me know what you think in the comments. I’m not the first or last person to write on this topic, but it’s something I’ve looked for, and thought it may be useful for you.Don’t give one clap 👏. Give the full +50. For all of our sakes.Feeling super happy? Buy me a coffee. 😋You can find me on LinkedinHave a nice reactive day!",android,https://medium.com/@soumyasethy/react-native-over-the-air-ota-updates-using-firebase-34fe13614101?source=tag_archive---------3-----------------------
Blend Photo Editor PRO apk free download latest version,"JeffalminJun 1, 2020·1 min readBlend Photo Editor is the best photo blender app which allows to blend two photos to create a double exposure effect! With variety of artistic photo collections and real-time blend modes, you can totally follow your passion to improvise with greater ease! Blend Photo Editor provides categorized blend effects like: Artistic, Space, Galaxy, Nature, City, Mountains, Colorful, Smoky Effect, Texture, Bokeh Light and more artful photo blend with multiple blend styles and pre-created previews to create perfect blend.Overlay two photos to produce a double exposure effect.- Get creative and mix photos together to create your own artistic graphic design.- Seamlessly merge photos with various blending modes.- A variety of real-time blend modes.- Easily adjust the blend mode for better effect.- Swipe to change the position of the background and foreground.- Erase unwanted part of image easily with erase tool.- Apply stunning filters.- Share with Instagram, Facebook, Twitter & other social media.",android,https://medium.com/@jeffalmin/blend-photo-editor-pro-apk-free-download-latest-version-5d4e2c1aabf5?source=tag_archive---------48-----------------------
"Any.do: To do list, Calendar, Planner & Reminders apk premium free download","AlexandersekelJun 1, 2020·3 min readAny.do is a To Do List, Calendar, Planner, Tasks & Reminders App That Helps Over 25M People Stay Organized and Get More Done.🥇 “It’s A MUST HAVE PLANNER & TO DO LIST APP” (NYTimes, USA TODAY, WSJ & Lifehacker).Any.do is a free to-do list, planner & calendar app for managing and organizing your daily tasks, to-do lists, notes, reminders, checklists, calendar events, grocery lists and more.📅 Organize Your Tasks & To-Do List in Seconds• ADVANCED CALENDAR & DAILY PLANNER — Keep your to-do list and calendar events always at hand with our calendar widget. Any.do to-do list & planner support daily calendar view, 3-day Calendar view, Weekly calendar view & agenda view, with built-in reminders. Review and organize your calendar events and to do list side by side.• SYNCS SEAMLESSLY — Keeps all your to do list, tasks, reminders, notes, calendar & agenda always in sync so you’ll never forget a thing. Sync your phone’s calendar, google calendar, Facebook events, outlook calendar or any other calendar so you don’t forget an important event.• SET REMINDERS — One time reminders, recurring reminders, Location reminders & voice reminders. NEW! Easily create tasks and get reminders in WhatsApp.• WORK TOGETHER — Share your to do list and assign tasks with your friends, family & colleagues from your task list to collaborate and get more done.— -ALL-IN-ONE PLANNER & CALENDAR APP FOR GETTING THINGS DONECreate and set reminders with voice to your to do list. For better task management flow we added a calendar integration to keep your agenda always up to date. For better productivity, we added recurring reminders, location reminders, one-time reminder, sub-tasks, notes & file attachments. To keep your to do list up to date, we’ve added a daily planner and focus mode.INTEGRATIONSAny.do To do list, Calendar, planner & Reminders Integrates with Google Calendar, Outlook, WhatsApp, Slack, Gmail, Google Tasks, Evernote, Trello, Wunderlist, Todoist, Zapier, Asana, Microsoft to-do, Salesforce, OneNote, Google Assistant, Amazon Alexa, Office 365, Exchange, Jira & More.TO DO LIST, CALENDAR, PLANNER & REMINDERS MADE SIMPLEDesigned to keep you on top of your to do list, tasks and calendar events with no hassle. With intuitive drag and drop of tasks, swiping to mark to-do’s as complete, and shaking your device to remove completed from your to do list — you can stay organized and enjoy every minute of it.POWERFUL TO DO LIST TASK MANAGEMENTAdd a to do list item straight from your email / Gmail / Outlook inbox by forwarding do@Any.do. Attach files from your computer, Dropbox, or Google Drive to your to- tasks.DAILY PLANNER & LIFE ORGANIZERAny.do is a to do list, a calendar, an inbox, a notepad, a checklist, task list, a board for post its or sticky notes, a task & project management tool, a reminder app, a daily planner, a family organizer, an agenda, a bill planner and overall the simplest productivity tool you will ever have.SHARE LISTS, ASSIGN & ORGANIZE TASKSTo plan & organize projects has never been easier. Now you can share lists between family members, assign tasks to each other, chat and much more. Any.do will help you and the people around you stay in-sync and get reminders so that you can focus on what matters, knowing you had a productive day and crossed off your to do list.GROCERY LIST & SHOPPING LISTAny.do task list, calendar, agenda, reminders & planner is also great for shopping lists at the grocery store. Simply create a list on Any.do, share it with your loved ones and see them adding their shopping items in real-time.",android,https://medium.com/@alexandersekel/any-do-to-do-list-calendar-planner-reminders-apk-premium-free-download-1c287253ab6d?source=tag_archive---------35-----------------------
Beginners Guide To Setup a React native project | Mac Users,"Karishma AgrawalJun 1, 2020·3 min readWelcome to your React Native journey! Are you looking for environment setup instructions? You are at the right place.Setting a React Native CLI would be easier if you have experience working with mobile development.Android studio, Xcode or visual studio is a must before setting a react server. Though I personally prefer the Visual studio Code as it is faster than the others, you can use any of the others. But Visual studio Code does have a few advantages over others. We’ll get to that later.You can install the visual studio code for mac from HERE.Visual studio code is a lighter version of Visual studio.You will need Node, Watchman, the React Native command line interface and a JDK.Step 1: Install homebrewStep 2 :  brew install node  brew install watchmanbrew cask install adoptopenjdk/openjdk/adoptopenjdk8if you already have jdk installed in your system, make sure it is JDK 8 or latest version.If you have experience in android development,You would have already Android studio installed , if so please move to ios setup.If you are not able to find .bash_profile file then create one using the following steps:To import your react Native project go inside the project folder and import the android folder from there. Now you can run it. You might need to set the proper Gradle version and SDK path.Install Xcode to support the IOS support. The best place to install Xcode is the apple app store.If it is already have installed Xcode then make sure it is the updated one.Use sudo gem install cocoapods Command in your terminal.To run ios build import ios package from complete react native project.Now try to run it on the simulator.To create a new react-native project type following commands in your terminal:npx react-native init SampleProject cd SampleProjectIf you already have a project and just want to run it, go to the project folder by using CD command in your terminal.Now you need to start react-native server by the following command:npx react-native startRun android build through the terminal after running react-native server:step 1: check connected android devices./adb devicesMake sure only one device is connected at a time. You can run on the emulator as well.step 2: Command to run the android package:npx react-native run-androidRun IOS build through the terminal after running react-native server:You can either directly run ios build through simulator using Xcode or run it from the terminal.First run simulator through Xcode.To run from terminal type following command:npx react-native run-ios It takes iPhone X by default, but if you are running any other device then type the following command in your terminal:npx react-native run-ios — simulator “iPhone 11”Now lets discuss about why Visual studio code is better choice for react native code. 1. it is lightweight2. It is easy to switch between files, when I was using Android studio to do coding i was unable to move to files by method calling. It might be they don't support react code.Still having problems with setting the development environment? Feel free to reach out to me. Always happy to help.",android,https://medium.com/@karishma-agr1996/beginners-guide-to-setup-a-react-native-project-mac-users-ae328a0c8dbd?source=tag_archive---------19-----------------------
How to start coding as a UX Designer?,"At UX Globals we believe that every UX designer needs to be able to code! Use this short guideline to find out how you can become a coding UX Designer!The first step we would advise you to take is web development. There reason for this is easy. You can begin to understand the logic of coding from HTML even though it is not real coding yet and it can be used easily for creating prototypes of any fidelity.This markup language is used to create the main structure of a page.<html>…<body><h1>Headline</h1><p>Here follows the text and a <a>link</a>.</p><p>And this is another paragraph</p></body></html>The idea behind HTML is that you have multiple elements that are beside or within each other. The main part is always the HTML-tag itself. You usually have a head-tag which includes all necessary information like fav-icons or the title of the page above the body tag. All elements you want to show need to be placed inside the body tag. In the example the H1-tag is a headline of the hierarchy one. Below that is a standard paragraph (p-tag) with a link (a-tag) inside. The first paragraph is followed by another paragraph.Why start with HTML?The answer is quite easy. There is no complex logic behind HTML. You place items below or within each other. This principle is used by other real programming languages, too, but in a more complex setup. Therefore HTML is a proper way of understanding the main idea within an easily understandable environment.If you have created an HTML element, you will have noticed that the visual appearance is not that good looking. However, if you continue with learning some Cascadian Style Sheets (CSS), you will be able to fix this as they are responsible for the visual appearance of websites.Let’s take part of the HTML code from above but edit it just slightly.<h1 class=”blue_text”>Headline</h1><p class=”blue_text”>Here follows the text and a <a>link</a>.</p><p id=”blue_text”>And this is another paragraph</p>The difference is that two tags have a class named “blue_text” now. Classes can be added to multiple elements. Whearas an ID can only be applied to a single tag like the id “blue_text” for the last paragraph.The CSS code could look like that:.blue_text, #blue_text {color: #0000FF;}#blue_text {text-decoration: underline;}This CSS code does the following. It changes the color of every element with the class (.blue_color) or ID (#blue_color) “blue_color” to blue. In addition to that the element with the ID (#blue_color) gets underlined.The third important frontend component for every website is JavaScript. This Script language can be used to manipulate the HTML. Add, remove, or change one of these: elements, classes, or anything else you have created previously with HTML or CSS.This is the moment it gets fascinating for the first time. PHP makes it possible to create database connections or include another PHP script. It can save you some valuable time when developing a website, and it is quite easy to learn. That’s why this is the language you should study next.One major benefit is that you can include other scripts to if your website’s footer is the same with all pages, you can write one script for the footer an include it in all sites of your online presence.Furthermore, you can use variables in these scripts.<?php$number = 3;echo $number;?>This PHP script would print the number 3. Beside that, you can create if conditions like that:<?php$number = 3;if( $number > 3) {echo “larger than three”;} else {echo “smaller or equals three”;}?>Or loops to execute code multiple times:<?phpfor ($i = 0; $i < 10; $i++) {echo $i;}?>This sample prints all numbers from 0 up to 9.The structured query language is also essential for web development, as you can perform multiple queries with databases. You can either add, update, remove, or fetch data from the tables in your databases.SELECT name FROM users WHERE age > 18;This sample-query will return the names from your users who are older than 18 years.Those were all the essentials you need to know as a UX designer for web development.Once you are a pro in web development, my advice is to learn to develop Android apps. As this is more complex I have decided to not include any samples in this section.The main reason is that you can implement Android apps with Java. It is one of the most used object-orientated programming languages. Therefore you will learn what classes, objects, interfaces, and many more elements are. You will find the same patterns in many more developing languages. It, therefore, is an excellent foundation to start with.Yes, I know. There is a more simple way of developing Android apps, which is using Kotlin instead of Java. However, I would recommend starting working with Java to get to know the logic behind programming.Like HTML, XML can be used to give the layouts of your app some structure and define which elements should be placed on the screen.That is my advice on how you, as a UX designer, should start coding. It will be beneficial for you in the end!",android,https://medium.com/ux-globals/how-to-start-coding-as-a-ux-designer-18d4674d49ea?source=tag_archive---------24-----------------------
Best Ecommerce App Development Company,"One Click IT SolutionJun 1, 2020·1 min readAre you planning to start a customized app for your eCommerce store? There are 182,000 active e-commerce sites and competition is going to be tougher, you have no room for error, you have to make the right choices from the start. Our Ecommerce solutions offer engaging apps that are compatible with Android, iOS, and Windows devices.Want to know how you can optimize your e-commerce website and applications checkout here 6 ways to optimize your e-commerce websiteHere is the glimpse of our work → Click HereOur development solutions are not restricted to a single niche. Over the last 8 years, we have developed custom e-commerce Android and iOS apps for a wide range of businesses across the globe. Connect with us for further inquiries. We are glad to connect with you!",android,https://medium.com/@1clickit/best-ecommerce-app-development-company-e33adc26af24?source=tag_archive---------25-----------------------
BaconReader Premium for Reddit apk free download latest version,"JeffalminJun 1, 2020·1 min readBaconReader for Reddit lets you enjoy all of the best content and features of popular social news site Reddit. Access news and entertainment content, post links, pics and comments, and upvote and downvote to your heart’s content right from your Android phone or tablet. Now featuring a brand new material design interface and a comprehensive list of features, stay informed and entertained in the tastiest app available.Tasty features include:* Beautiful Material Design Interface* Color-coded comment threads* Themes with light, dark, and black backgrounds* Multiple font sizes ranging from extra small to huge* Split-screen mode optimized for tablet use* List view, card view and slideshow mode* Full user profile support including trophy case* Moderator tools: modmail, spam, removal/approval* Set user and link Flair via the app* Upload images in comments* Format toolbar (bold, italic, etc.)* Supports i.reddituploads* Multiple widgets: rotate, scroll, subreddit* Full-featured inbox with orange envelope indicator* Discover something new with “random” subreddit feature* Comprehensive search: this subreddit, subreddits, all posts, for a subreddit, for a user* Draw mode with meme creator for creating original content or modifying images* Advanced filtering: domain, keyword, NSFW, subreddit* Spoiler tag support for standard CSS subreddits* Reddit Gold: Give gold on posts and comments, My Random, full subscribed subreddit list* Secure login via Reddit OAUTH* Multiple account support* Multireddit support* Cakeday notification",android,https://medium.com/@jeffalmin/baconreader-premium-for-reddit-apk-free-download-latest-version-448ad4e2dea6?source=tag_archive---------44-----------------------
Beelinguapp Premium apk free download latest version 2020,"JeffalminJun 1, 2020·2 min readLearn Spanish, English, German, Portuguese, Korean, French, Hindi, Russian, Turkish, Chinese, Arabic, Italian, Swedish and Japanese by reading text side by side!Learn a new language with Beelinguapp, the app that lets you read and listen to stories in different languages side by side. Read text and hear audio in the language you are learning, and read the same text in your language to use as a reference.Learn at your own pace with this fun and free language learning app. If you are familiar with language learning audio books, you will love Beelinguapp’s innovative method to learn a new language.Ditch the flashcards and pick what languages you want to learn by reading your favorite children’s stories, short stories, novels and more side by side. From Spanish to German and more, Beelinguapp teaches you through fun and familiar text.Beelinguapp is fun and easy to use for anyone that wants to learn a new language. Use your native language as a guide and start learning today!Beelinguapp Features:Language Learning Made Easy• Learn a new language by reading different stories in a language of your choosing! • Beelinguapp gives you the option to read the story in YOUR language to reference what a word or phrase means.Audio Book Reader• Spanish, German, French and more languages on easy to listen audio books.• Audio book in any language can be listened to even if your phone is sleeping.• Learn languages by following the reader of the audio book with a karaoke style animation to know exactly what they are saying.• Spanish audio books combined with English, French audio books combined with German — the choice and what language audio book you want to read is yours!Great Stories in Different Languages• Side by side readings of your favorite fairy tale stories, novels and more.• Learn languages at your own pace and choose only the stories you want to read.• Languages, genre and learning level can be sorted make learning languages easy.Learn new languages by reading different stories side by side with Beelinguapp! No memorization and no flashcards needed. Learn languages at your own pace by reading your favorite stories on Beelinguapp!",android,https://medium.com/@jeffalmin/beelinguapp-premium-apk-free-download-latest-version-2020-6f86c169d612?source=tag_archive---------45-----------------------
Implement Instant Search Using Kotlin Flow Operators,"In this blog, we are going to learn how to implement the instant search feature using Kotlin Flow operators in Android application. We will also learn about all the operators used to implement this feature.Originally published at MindOrks BlogI will be using this project for the implementation part. You can find the complete code for the implementation mentioned in this blog in the project itself.The following are the things of Kotlin Flow that we will be using to implement this search feature:Earlier this instant search feature implementation in Android was not that easy with Kotlin Coroutines, but now with Kotlin Flow Operators, it has become easy and interesting.First of all, we will create the extension function to return the StateFlow so that we can apply our required operators to that.So here, on the SearchView, we will use the setOnQueryTextListener to observe for the changes in the text, change the state of the query, and finally return the StateFlow like below:We have also simulated the data from the network with delay like below:Now, on the QueryTextChangeStateFlow, we will apply the operators like below:Now, it’s time to learn why the above operators are used and how they work when they are put together.Note: If you notice inside the flatMapLatest, if we are getting any error, we are passing the empty result. We can change this based on our requirements.This way, we are able to implement the instant search feature using Kotlin Flow Operators in an Android application.You can find the end to end implementation in this project.That’s it for now.If you are preparing for your next Android Interview, Join our Android Professional Course to learn the latest in Android and land job at top tech companies.Also, Let’s become friends on Twitter, Linkedin, Github, Quora, and Facebook.",android,https://medium.com/mindorks/implement-instant-search-using-kotlin-flow-operators-7bd658bdfc4b?source=tag_archive---------7-----------------------
Best Free Apps to Enhance your Lock-down Period,"Itta ko VittaJun 1, 2020·4 min readCronometer is a calorie tracking app that enables you to log how many calories you consume and burn throughout the day. The best part of the app is that it lets you track your micronutrient intake even in the free version. So, along with your macro intake, you can see the breakdown of your vitamin and mineral intake.As many of us are completely stuck at home during the lockdown, I think it is very important to track your diet habits. Numerous research has found a correlation between Vitamin D deficiency with an increased risk of mortality from Coronavirus. While this might just be a correlation, it doesn’t hurt to track and ensure that you are getting enough of the vitamin.Some people, me included, might not be too keen on tracking your food habits each and every day but you can just use the app intermittently to ensure that your staple diet is not deficient in anything.While the app does not have one of the most aesthetically pleasing design, it is very functional. There are a lot of options for measuring serving of food and the database is huge. A web version of the app is available as well which makes it very easy to enter new foods and recipes manually.ReWi is the companion app for the Yale University “The Science of Well-Being” course which is now available online at Coursera. The course feels especially relevant now as with a halt on our everyday life we have time to consider and ponder how we want to move forward with our life.The app aims to help the students of the course to change their behaviour and in turn, become happier. By building healthier habits, the core assumes that we can rewire our brains. The app enables users to track eight different activities including sleep, exercise, meditation, goal setting, gratitude, kindness, social connection, and savouring.Users have the option of logging in these activities at any time during the day and track their process. The app also allows users to listen to meditation tracks and get prompts for effective goal-setting.Fair warning: this game actually has nothing to do with how chemistry actually works. But this is a very fun minimalist game that you can lose yourself into.Players need to match up pairs of atoms to combine them and create a different element. The game starts with hydrogen atoms and as you continue to fuse the atoms together, new elements pop up following the sequence of elements in the periodic table.The game is very intuitive and has a minimalist user design. The accompanying music is pretty great and has an almost calm and meditative feel to it.This is the best ebook reader out there and I have tried a lot of them. If you use an android device, this is a must-have reading app.One of the best features of the app is that you can turn pages using the volume rocker. Additionally, you can dim the backlight inside the app to the brightness that is dimmer than the lowest setting of your phone’s system and that is a blessing especially if you are a night time reader. The reader is compatible with books in numerous different formats including epub, pdf, mobi, and cbz.The interface is very user-friendly and intuitive. You can adjust the settings to ensure that your reading is very smooth and clear.Question Diary is an app for self-reflection. So, basically what the app does is that you will receive a question from the app every day and you will have to answer it. The questions are usually quite introspective, thought-provoking, and insightful. So you continue answering the questions and after a year you are asked the same questions and as you answer then, you get to see whether and how you have evolved over time.The questions put forward by the app aren’t issues that we get to discuss outwardly. So, what better time to go into self-reflection and introspection rather than now right?PublishedOriginally published at http://behindbhairavsmask.wordpress.com on June 1, 2020.",android,https://medium.com/@ittakovitta/best-free-apps-to-enhance-your-lock-down-period-406dee2f4a3a?source=tag_archive---------34-----------------------
Always on AMOLED | Edge Lighting PRO apk free download latest version,"ToddwillsnorJun 1, 2020·2 min readAlways on Display | Edge Lighting will provide you with information about your notifications, clock, date, current weather, with edge lighting and much more right on your screen without having to touch your phone or tablet.• Samsung Galaxy One UI 2.0 always on display theme• Battery charging animation• Choose orientation type• Calendar view with events, and the ability to add your own notes• Root (Superuser) compatible with battery saver option• Edge Lighting with custom colors and styles• Fingerprint• Sms Reply• Tasker support• Samsung S10e, S10, S10+, and A9 Pro Notch Support• Off screen sketch pad which allows you to take notes, and draw on the go• Compatible with all screens such as amoled, oled, edge, and notched displays• Curved edge option for traditional square displays• View notifications with badge count• Glance Display which activates the always on display when you receive a notification• Time Rules which lets you set custom start and end time• Swipe-able notifications, swipe left to dismiss, swipe right to hide• Clickable notification with action buttons• Many clock watch faces to choose from, such as Digital S9, S10, & Note 9• View current weather information with minimal display• Adjust screen brightness or alpha• HD Backgrounds & Wallpaper• launcher shortcuts such as calendar, flashlight, home button, camera, sketch pad• AOD is Fully customizable with many settings which allows you to set colors, icons, styles, fonts, text size, icon size• Automatic rules which preserves battery life using predefined settings• Auto movement which avoids AMOLED burn-in• Pocket mode which uses proximity sensor to turn off screen when placed in your pocket• Memo which lets you show sticky notes right on your display• Particle animation• Keep screen on, or turn screen off with a timer or while charging / discharging• Can be used as a night clock",android,https://medium.com/@toddwillsnor/always-on-amoled-edge-lighting-pro-apk-free-download-latest-version-d5e50f7de060?source=tag_archive---------22-----------------------
React Native Android Receive Sharing Intent and IOS Share Extension,"Ajith A BJun 1, 2020·10 min readHello Guys,This is my first story in the medium. I writing this package is my challenge once upon a time. I have joined a company after b-tech. The company should give a second task for me to Implement Pdf Receiving App to build with React Native. On the Company side, I was getting more pressure to complete this task. So I was confused there is no package is available to file receive in react native. there is a package available for text and image receive to React Native Apps. First of all, I was learned java to Receive Sharing Intent then learn Objective-c IOS Share Extension. In Android and IOS, the entire Implementation is Different.Then I have completed this task within 2 weeks. I was the Written Native module in Java and Objective-c and communicate our React Native Bridge.That time I have the aim to build a package for React Native and do not suffer anyone for this feature. So I can build a package from ScratchToday I’ll be explaining how to implement sharing images, files, website links, and text from 3rd party app’s to your react native application.YarnAutomatic installation only for (React Native 0.60.xx less than versions)Note: React Native 0.60.xx or greater version not required to linkNote: Ios and Android on Debbuging time not working at sometimes while App is ClosedAndroid-Installation<Project_folder>/android/app/src/main/manifest.xmlIOS-Installation<project_folder>/ios/<project_name>/info.plist<project_folder>/ios/<project_name>/AppDelegate.m<project_folder>/ios/<your project name>/<your project name>.entitlementsIn Ios is Mandatory to Recieve file from other apps.<project_folder>/ios/<Your Share Extension Name>/info.plist<project_folder>/ios/<Your Share Extension Name>/ShareViewController.swiftNote: Important change the hostAppBundleIdentifier value to your main host app bundle identifier (example in my case: com.ajith.example ) in this ShareViewController.swift2.Create an app group for Share Extensiongithub.comThank YouAjith A B",android,https://medium.com/@ajith4ab/id-react-native-android-receive-sharing-intent-and-ios-share-extension-daddfc9fb5d6?source=tag_archive---------9-----------------------
The easiest way to add Gradle dependency in an Android studio,"Hitesh DhamshaniyaJun 1, 2020·2 min readIn this post, we are going to learn another way to add required dependency into the android studio.The traditional way to add gradle dependency into the project is to Google it and check the updated version or our prefered version copy it and go to app → build.gradle file and add it. Most of us do it in that way, I have been doing it. Now I come across with new cool short-cut way to add it.Let’s find that short-cutGo to File → Project Structure. or simply CTRL+ALR+SHIFT+S. We will have the project structure dialog, as shown in image below.Select Dependencies from the left menu, we can see the number of dependencies we already had added. Click the + icon below the label all dependencies, we will have two option for dependencies. 1. Library dependency and 2. Jar dependency, we will go with library dependency.Once we click on Library Dependencies or press 1 as a short cut, we will have below new dialog, where we have the option to search gradle.Let’s try to search recyclervew in the Search box. We can see it in the above screenshot its step 1. We get the below result when searching for recyclerview.When the search for any gradle we will have a list of related gradle and its version, one can see in the right side of the above dialog and In step 2 we can select the configuration of the gradle i.e implementation, releaseimplementation, testimplementation and so forth.So, this is the easy and cool method to add dependency gradle, please let me know your thought on it if any.Thanks for reading… Happy Coding…",android,https://medium.com/@hiteshdhamshaniya-wvmagic/the-easiest-way-to-add-gradle-dependency-in-an-android-studio-63f250badc0c?source=tag_archive---------15-----------------------
Scintillating Starts,"Devansh SharmaJun 1, 2020·4 min readHey, Devansh here!This marks the start of a new journey of blogging through which I aim to share my thoughts, reflect on the past and inspire others!A brief introduction of myself — I’m Devansh, a rising sophomore at McMaster University. My passion lies in the technology industry and I love learning about tech as much as I can while contributing and making a difference. I’m a Computer Science major while also pursuing a minor in Innovation!The year 2020 is definitely going to be the one to remember. It’s filled with many firsts and new experiences that have definitely shaped me for the better. Among many of the first’s is one of my most important ones — my first internship. At the very beginning of my post secondary career I learned about the importance of an internship and how valuable they are. But with that, I was also advised to proceed with caution because obtaining an internship as a freshman was deemed extremely unlikely. Though, this did not stop me from achieving my goal, yet it motivated me to work even harder and achieve my goal!I spent many hours networking, improving my skills, mock interviewing in the hopes of landing an internship. This process was definitely overwhelming at first. Every morning, I was waking up to notifications from numerous rejections from companies who I spent a great time researching to personalize my cover letters. In about mid-February, I learned that I would have to embrace rejections, learn from them and move forward confidently. This attitude was definitely a turning point for me. After every rejection and failed interview, I now thought about how every experience was helping me learn more about what I can improve on!I finally received a wonderful opportunity with Simulpass to work as a Software Engineering Intern for developing their platform. Simulpass is a startup looking to revolutionize data sharing using a highly secure platform. The application is built around decentralized network aiming to facilitating deeper connections, which is exactly what major social platform’s lack today!Well, I just finished the first four weeks of my virtual internship at Simulpass and it has been nothing short of amazing! I learned about a lot of new concepts and technologies that I never thought of learning before. I am sure at this point, most of the population has came across the word blockchain or bitcoin at some point. Considering the popularity of this sole word, it is definitely considered a buzzword used to grab attention. During my first two weeks, I learned tons about the usage and functionality of blockchain while being introduced to the Lightstreams network! Lightstreams provides features that can be used to implement decentralized applications which are also commonly known as Dapps. With Lightstreams, I performed transactions using my own node on their Ethereum compatible blockchain network for decentralized storage! This is an extremely exciting field and I cannot wait to learn more about it!The second week was mostly based on learning the prerequisites required for Android development with Kotlin. In the past, I have built plenty of web applications and the only native application I have built before is a Android game built on the Unity Game Engine. This was an extremely exciting week because I wanted to learn about Android development but didn’t have the time to investigate it during the school year. An interesting thing I learned during this week was the Model-View-ViewModel (aka MVVM) architecture. Software architecture is a word that I heard plenty of times but never truly understood the actual use case of it. This has truly been a game changer for me! Architectures like these assists with the principal of separation of concerns and should be considered a necessity while developing projects, especially for large-scale development! A correct software architecture can really help keep the code clean and helps with modularizing code snippets.Finally, I was able to further develop my skills with Git and GitHub. Before joining Simulpass, I mainly worked off a sole master branch and committed code directly to it. At university, I primarily used Git to a slightly more extent by making a branch off master, committed to it and then finally pushing that branch back to master once done with my project. But, there is so much more to Github than just branching, committing and pushing the branch.I recently learned about Git Flow which is a quite popular Git Workflow. I was extemely amused at how Git Flow works and this helped me better understand how software development works in the real world. GitFlow is an extremely helpful Git branching model that helps keep huge codebases organized! It provides convention for many different use cases that developers face such as adding features, testing and even making quick hotfixes! During this process, I also learned about Semantic versioning. Semantic versioning is a clean and systematic way to come up with release versions of a project.In conclusion, I have learned a lot about blockchain, Git and Android development. Reflecting on the past four weeks, I can’t imagine how much of a better developer I will become by the end of this summer! Can’t wait to see what the future holds! That’s all for this post!~Devansh",android,https://medium.com/@the.devansh/scintillating-starts-1c47a314829c?source=tag_archive---------21-----------------------
Android Espresso for Beginners,"In this article, you’ll learn how to use Espresso to find views in the layout based on properties like id, text, and more. Then you’ll learn how to check view behavior like visibility and to perform actions like click. This article also includes how to perform tests on RecyclerView.No matter what platform you’re working on, testing gets complicated as the product evolves. For that reason, it’s preferable to use testing frameworks like Espresso to make the process reliable, easy, and effective. Espresso is a testing framework from the Android team to make developers’ lives easy when implementing test cases. It acts as an abstract layer over unit tests and makes it easy to write reliable Android UI tests. Without any further delay, let’s get started.When we create an Android project, under the Java directory, you’ll find three subdirectories with application package names.These days, when you create a new project in Android Studio, Espresso is integrated by default. If you removed those libraries in the existing project or if you don’t see the Espresso implementation library under the dependency tag, add the following lines.Espresso has basically three components:Have a look at the generic usage of these components.Using this function, we can narrow down to the desired view in the view hierarchy. It can be done by using the following functions inside onview:This is used to check the behavior of the desired view, like visibility, focus, and more using matches and doesNotExist.Through perform, we can execute actions on the views. Following are some of the actions that we can execute using Espresso:Let’s say we have an application with an activity and its layout contains a view with id tv_hello. Our goal here is to find whether the view with the id tv_hello is displayed on the screen. Have a look:First, it’ll start searching a view with id tv_hello, and once it found the view using check function, it’ll verify the visibility through isDisplayed().We can also use multiple constraints to narrow down the view search, as shown below:In this test, while searching for a view, it will consider two constraints: withId (with a specific id) and withText (with a specific text in the view).Now it’s time to perform actions on the narrowed views. To execute actions, we have to invoke a perform-function on onView with the desired action, as shown below:We can also pass multiple actions as parameters to the perform function, as shown below:In this test first, it’ll type “Hello” on the view and then perform the click action. Only if both actions are executed without any error will the test succeed.We’ve completed basic testing with the standard layout. What if we have complicated views, like a list? To write test cases on lists, we have another function called onData, similar to onView.Let’s say we have a list with a string ArrayList, and we need to find the item with text Kotlin and perform click functionality on that particular item. Have a look:In a real-time scenario, things won’t be simple like this. Let’s say we have a listview-item with multiple clicks, and we need to test click functionality of a view with id edit in the item with content Kotlin. Have a look:RecyclerView objects behave differently than AdapterView objects; we can’t use onData() to test RecyclerView.To create an interaction between RecyclerView and Espresso, we need to add the espresso-contrib module, which has RecyclerViewActions to perform actions like click, scrollTo, and more useful stuff.To integrate the espresso-contrib module, add the following line under the dependency node in the app level build.gradle file.The first step to work with RecyclerView is to find RecyclerView using onView function and the id of RecyclerView, as shown below:Let’s start with a simple scroll test. Our object here is to scroll the RecyclerView to a specific position. To do that, we use scrollToPosition on recyclerviewAction, as shown below:To explore a bit more, let’s write a test where our objective is to click the item with content Kotlin. Have a look:Here hasDescendant function is the key. It’ll check the descendant views of every item in the list, and if it gets a match, then it’ll perform the click action.Unfortunately, there is no direct function out of the box to test the inner view clicks of the RecyclerView item. To achieve this, first we need to create a custom action that extends ViewAction, as shown below:Now using the above custom, we can perform item-level view clicks, as shown below:That’s all. I hope you learned something useful. Thanks for reading.",android,https://betterprogramming.pub/android-espresso-for-beginners-57628a15f8b4?source=tag_archive---------1-----------------------
Responsive Image Gallery Using HTML And CSS,"w3hubsJun 3, 2020·1 min readTemplate Name: Responsive Image Gallery Using HTML And CSS.High Resolution: — Yes.Compatible Browsers: — All Browser.Source Files included: — HTML, CSS and Images(unsplash).Responsive layouts is first priority for a web developer/designers. well, here we design a responsive image gallery in HTML and CSS with an attractive user-friendly interface.In this element, we used CSS flexbox properties with justify-content and alignment properties to make perfectly aligned. Also, we used width properties for the column side, and for space we used margin. Here we also used box-shadow effects on mouse hover case. To make responsive we used media queries from 1000px till the mobile screen side.Make it yours now by using it, downloading it, and please share it. we will design more elements for you.Originally published at https://w3hubs.com on June 3, 2020.",html,https://medium.com/@w3hubs/responsive-image-gallery-using-html-and-css-9900f0f52d0e?source=tag_archive---------8-----------------------
Product Life Cycle: Working with Feedback,"Zarina MJun 4, 2020·2 min readTo help gain a better understanding of the feedback and testing process, I decided to recruit some colleagues to look at an application I previously built. I collected user feedback and feature requests to help improve my application while also practicing communicating with clients and meeting their needs. For some background: the application is a calendar/to-do fusion that functions like a planner — flip through the calendar to your desired month and day then enter your event or task.I had two colleagues take a look at the project, and this is what they said:To my surprise, the feedback I received was well-aligned with how I felt about the app myself; there were no major issues but it could use a lot of minor tweaks. I took the advice above and used it to set some basic guidelines. Firstly, I needed to fix aspects that impeded the user from using the app. Secondly, I needed to implement changed that would improve the user’s experience. Lastly, I could work on “nice to have” aspects that would make the experience fun or aesthetically pleasing.To be straightforward: I made those changes. I was lucky enough that the issues with my application were mostly small aesthetic issues, meaning I didn’t have to refactor anything major. That being said, I did decide to hit pause on two major ideas proposed to me: converting my application into React and adding Firebase. I only had a few days to implement the changes suggested to me, and also figured refactoring an initially jQuery project into React should have it’s own dedicated blog post. Regardless, I think it’s important to note that correcting what seemed to be “minor” aesthetic issues did make a substantial difference. Correcting alignment issues, adding a variation of fonts, and adding animations all contributed to my application giving off a more polished look. As someone who always avoided dealing with aspects of design, I learned that tweaking things until they look just right is totally worth it in order to achieve a professional looking project.",html,https://medium.com/@zarinabliss/product-life-cycle-working-with-feedback-5fefbf201721?source=tag_archive---------5-----------------------
Animated_Html &CSS : Do beginning,"Barry YUJun 3, 2020·2 min readawesome !! My profolioStep1 :Html Design UI/UXAwesome Tool : figma未來趨勢的ＵＩ/UX工具 ,美國矽谷已經是最流行了. 非常強大.看過youtuber 透過這工具可以直接幫你帶出ＣＳＳ可以線上和很多設計師共同協作. COOL!目前也是本人自己也是剛開始接觸Step2 :Written Code on your Visual codeCSS and HTMLStep 3 :一開始的框架設計建議在UＩ工具上設計在後續的編寫HTML及ＣＳＳ上設定Background-color ,該 用顏色做一個區分：Header > container > Logo+nav-btn + log-sign在Container 設定 display:flex讓log:{color:red} , nav-btn { color :green} , log-sign { color:yellow }Step4 :display : flex …麻煩你可以透過做過的專案經驗帶入節省時間. 或者 利用DevTool 及時調整修改下面的例子： 用了3個 display : flex . 看顏色就可以暸解很複雜的flex.Open your Html on Chrome , also using DevTool ,could be Modify Element CSS on Styles Item , soon to change your index.html . adjust you FlexOpen your Html on Chrome , also using DevTool ,could be Modify Element CSS on Styles Item , soon to change your index.html . adjust you Flex",html,https://medium.com/@placid-olivine-mink-632/animated-html-css-do-beginning-6f3caec1980a?source=tag_archive---------7-----------------------
311 Week 1 — Node and REST,"Peter GilkeyJun 1, 2020·2 min readHow do you organize your code? What are some suggestions you find on the web?I keep things organized by project, which means separate repositories or branches. In the project directory, I may have an extra sub folder if there are multiple script files. In my script files, I try and keep functions together in the order that they may be used. Within functions I keep variables grouped at the top, and the rest of the code in order as executed.Can you describe your workflow when you create a web page or web app?Planning should always come first, to get a general idea of what is being built. I try to break it down into several main goals, and steps to achieve them. Once that’s set, I try to wireframe my general idea of the page layout. Depending on the project, I may either start with my HTML and CSS, or add JavaScript when necessary.You can’t work out how to solve a coding problem, where do you find the answer? Why? How do you know?I always try logging to the console to see what is being executed and on what line. If I can’t solve it in the console, I’ll go to google with multiple versions of my question in mind. It’s possible that someone else has had the same problem and asked the question in a different way.What problems have you solved that didn’t involve you coding?Git was pretty confusing to me at first. Plenty of mistakes have been made in the terminal, and it took some time to understand what went wrong and how to fix it.Talk about your preferred development environment. (What IDE or text editor they enjoy, and why?)I’ve tried several coding environments including Atom, Brackets, and Notepad++. However my current pick is VS Code. It’s the least sluggish upon opening in my opinion. The UI is nice, and it offers a lot of extensions to download.How are you keeping up with the latest developments in web development?As we are learning, we are being introduced to current technologies that are used in web development. Whether it’s a library or a software, there is a lot to understand and keep up with.",html,https://medium.com/@peterg7042/311-week-1-node-and-rest-a7888628a6cc?source=tag_archive---------10-----------------------
How to use CodePen components inside your webpage?,"ShahsamaJun 1, 2020·4 min readWith the world rapidly evolving by the minute, staying on top of the in-demand skills’ curve is a voluminous task. Web Development is one such skill that is almost essential to the modern developer. With a market size of about $40Bn in 2020, the need and opportunity for quality web developers are ever so evident.Whether you are new to the world of web development or an accomplished developer, you might have come across CodePen. CodePen is an online community for testing and showcasing user-created HTML, CSS, and JavaScript code snippets.Once you have a good foothold on HTML, CSS, JS/Jquery, it’s not necessary to design each component of your webpage from scratch. CodePen apart from being a code editor also has unique functionality, It functions as an open-source learning environment, where developers can create code snippets, called “pens,” and test them. Most of the time CodePen would have tons of “pens” made by the developers that you can add to your website to make it more lavish.In this article, We will look at a small example of how you can use any “pen” from CodePen and add it to your site.Step 1: Open CodePen.ioStep 2: Type anything in the search boxYou can type anything that you wish on adding to your webpage. It can be a responsive slider, a team card, about section, animations, etc. In this article, We will add a beautiful slider. So go on and search beautiful slider in the search box.You’ll get a good astonishing options to chose from. We selected https://codepen.io/supah/pen/zZaPeE.Step 3: Code extractionNow as you can see, the Codepen editor shows us the amazing code that built this slider. But do you think copying the code simply from this editor and pasting it in yours would work? The answer is NO. Most of the time the “pens” that you see use external libraries that the CodePen editor does not show. So simply copying them would result in inaccuracies that would be hard to resolve if we don’t import these libraries.To do that, navigate to the bottom right corner of your screen and click on Export -> Export as .zip. This will download a zip file of the same pen in your system. Extract it using any extracting tool such as Winrar/Winzip and see the files.The zip contains two folders dist and src along with a readme and license text file. You can explore/read them for your interest. Now select the “dist” folder and there you’ll see index.html, style.css, and script.js files. Open all of these in a text editor(Sublime, Atom, Brackets, etc).On opening the HTML file with any text editor, we see many external libraries apart from style.css and script.js.Add these lines inside your index.html along with the body section( I created a sample HTML page with a “CodePen Slider” title that will be followed by this CodePen slider) and you have successfully extracted the main code.Now save the file, don’t forget to add “script.js” and “style.css” in the same directory. If you have those files in any other location, update the href and src according to that.Step 4: Open the index.html fileKudos!!! You just added a “pen” to your website. Pat yourself on the back. Play around with the code. Change the source of images, text on top of images according to your liking/requirement.Conclusion:In this article, you learned how to use “pens” from CodePen inside your website easily. If you want to add any “pen” apart from the example in this article, the process remains the same. Keep building beautiful websites. Thanks for reading!!!",html,https://medium.com/@shahsama542/how-to-use-codepen-components-inside-your-webpage-d46edfc13808?source=tag_archive---------6-----------------------
HTML Introduction || Coding Tag,"Coding TagJun 3, 2020·2 min readIn this course , we will learn all the latest concepts and all mandatory topics for a beginner through which they can learn HTML online , and run simple to advanced level programs, to be an expert one day.we will cover step by step topics to guide you to every possible topics in HTML . SO,Stay tuned with every updated aspect.Let’s start with the HTML Basics.HTML stands for Hyper text markup language ,though HTML is not a programming languageHTML is used to create and layout web pages as it comprises of a markup language and a hypertext. The function of any HTML document consists of small markup tags, it starts with a start tag and closes with an end tag.Example:<!DOCTYPE HTML><html><head><title> Sample of HTML Format </title></head><body><h1> My First Heading </h1><h1> My Second Heading </h1><p> My first paragraph. </p></body></html>You can open your HTML documents in browser like IE,Opera, Google Chrome, Safari , Firefox-responsibility to read HTML code and show your unique works .<!DOCTYPE> represents document type and helps browsers to display web pages properly.<html><head><title> Welcome Page </title></head><body>Hello Friends, This is my welcome page.</body></html>Now after this small html code document you can start your practice and learning from now . Being a beginner type all the code above in any editor like notepad and save with an extension (.html), and after the saving part your unique work will be shown by any browser.You can easily prepare for Interview questions answers list of HTML and HTML 5 by implementing all examples.Originally published at http://codingtag.wordpress.com on June 3, 2020.",html,https://medium.com/@codingtag.com/html-introduction-coding-tag-794bcb91a6d1?source=tag_archive---------13-----------------------
Creating a simple web slideshow with HTML and Javascript,"Today let’s look at creating a simple photo slideshow for a website in plain HTML and Javascript. Sure, most website CMS’s would already have plugins for this sort of thing but let’s say you were doing something really simple and not using a CMS.Please note that I am not dealing with any sort of styling at the moment, nor am I worried about implementing mobile first or any of that. Why? Well for one, I assume you’re already using a existing system to handle that. (Such as Bootstrap or Bulma). Secondly, I don’t want to get into any debates about what css library is better or any of that. I’ll leave the styling up to you for now and maybe cover it later if anyone wants.The <template> tag in HTML might not be something you’re all that familiar with. I know I wasn’t until recently.It’s a tag that allows you to have content in your HTML that is hidden from the user that can be manipulated and used with Javascript. It is ideal if you have some content that you want to use over and over again but don’t want to have to “build” or “construct” it each time. You can put the content into a template and then use Javascript to get that content and display it however you like.In our case, for this simple slide show, we will have a template for each of the images to be displayed in the slide show. This way, once the Javascript is complete and working, you can add or remove images in the slide show simply by adding a template or removing a template.As mentioned above, the HTML for this is going to be very simple.We’ll put a container element that will be where the images are displayed. Something like this.<div id=”slideshow”></div>I will leave the styling and placement up to you.Then we will put in a template section for each of our images that we want to have in the slide show.Something like this.<template><img src=”images/pic001.png”></template>Repeat this as many times as you need to include all your images. pic002.png, pic003.png, etc, etc, etcIf you open the page in your web browser now you’ll notice that all those img tags aren’t visible at all. That’s because the template tag hides that content from the viewer until it is displayed using Javascript.Let’s now create a very simple Javascript object that will control the slide show. It isn’t going to have any animations or transitions, but it will allow us to control some things. We will be able to specify the container element that the slideshow will appear in. (We created this earlier with an id of “slideshow”)We’ll also be able to set the time each slide is shown for by passing in the number of seconds when we start the function. In addition we’ll have functions to change the slide delay, as well as advance or go back manually if you want to allow that.I’ve created a Javascript file which is imaginatively named: slideshow.jsIf you’ve never dealt with objects in Javascript before, then it can be a little confusing to get your head around. In this case, our object is also a construction function. So let’s start by creating a new function.function SlideShow(container_id, delay = 5) {Then, we will add some object properties which will transform this from a normal function to an object (while still also being a function ;) )this.index = 0; this.templates = document.querySelectorAll(“template”); this.slideDelay = delay; // In Seconds this.interval = null; this.container_id = container_id;index will hold the current position of the slideshow and will increase or decrease as the slides change. Pretty straightforward.templates will get an array of all the template tags in our html that we added earlier. These will contain the images for the slide show.slideDelay holds our specified time to show each slide in seconds. We passed this to the function and also had it default to 5. So each slide should show for the specified number of seconds before moving on to the next one.interval will contain a reference to the interval we create to automatically switch between slides.container_id is the html element that we will display our images in.With those variables declared. Now, let’s create a method that will take our template and display it in the container element. We’ll use this whenever we move forward or back in the slideshow.this.updateSlide = function(template) { // Clear the container element document.getElementById(this.container_id).innerHTML = ‘’; // Update the container element with the template content document.getElementById(this.container_id).appendChild(template); }Now let’s create a method that will move to the next image in our slide show whenever it is called. We will automate this with the delay we will be setting, but it could also be attached to a link or button in order to manually advance the slide show.this.Next = function() { // Increment the index this.index++;  // Test if the index exceeds the templates we have if(this.index > this.templates.length — 1) { // If the index is past the number of templates, // reset it to 0 to start again this.index = 0; } // Clone the template fragment for this instance const templateClone = this.templates[this.index].content.cloneNode(true);// Update the new template to the container this.updateSlide(templateClone); };It’s reasonably straightforward. When the function is called, we increment the index variable. We then check to make sure we haven’t gone past the number of slides we have and if we did, we reset back to the start. We then clone the template tag that matches the index and call our updateSlide function to display it to the page.Now a function to go back?this.Previous = function() { // Increment the index this.index — ;  // Test if the index exceeds the templates we have if(this.index < 0) { // If the index is less than 0 // reset it to the last template in the array this.index = this.elements.length — 1; } // Clone the template fragment for this instance const templateClone = this.templates[this.index].content.cloneNode(true);// Update the new template to the container this.updateSlide(templateClone); }This is much the same as the Next function, except here we de-increment the index (because we’re going backwards) and we then need to make sure we haven’t gone below 0. If we did, we jump back to the end of the templates array so that it loops around.We won’t use the Previous method in this example, but it’s there for the future. You could extend this simple example on your own and allow the direction to be changed permanently rather than just going back one slide at a time.We’ll also include a function that should let us change the delay while running.this.changeDelay = function(delay) {// Clear the existing interval we currently have clearInterval(this.interval);// create a new interval with the new delaythis.interval = setInterval(‘this.Next()’, this.slideDelay * 1000); // call the next function with interval in milliseconds }This function clears the existing interval and creates a new one with the new delay we provide. We’ll pass this delay in seconds, but the system needs it in milliseconds so we multiply our seconds by one thousand to get the correct value.Almost done now. The last step is to initialise the first slide that we see right away, set our initial delay value and activate the interval. We’re still inside our SlideShow object/construction function so we can just add those commands here.// Initialise the first slide var initialTemplate = this.templates[0].content.cloneNode(true); this.updateSlide(initialTemplate); // Set the initial interval this.changeDelay(this.delay);Then just end our SlideShow function. (Make sure you save the file alongside the HTML file we created to start with)}That’s our simple slideshow object done and we’ve already setup the HTML. We just need to bring in our javascript to the HTML page and call our SlideShow object constructor.So back in the HTML page. Down the bottom, just above the </body> tag let’s add this.<script src=”slideshow.js”></script> <script> SlideShow(“slideshow”,5); </script>As you can see, we’ve included the javascript code we just wrote and then we’ve called the function from that code. We passed to it the element to hold the slideshow and a delay of five seconds.If all went well, you should see the images change every five seconds and loop round to the start again when it gets to the end.You can go ahead and add links or buttons to manually advance, or a way to change the delay. Or you can set it how you want and leave it so it can’t be changed. It’s up to you.This could be useful on a website for a photographer, or an image sharing site or for any number of uses. I’m sure you can think of a way to use this.In the future we might look at extending this to add some styling and extra functionality such as randomisation, fullscreen mode and remembering where a user was up to when they come back. Any other ideas?",html,https://blog.devgenius.io/creating-a-simple-web-slideshow-with-html-and-javascript-b4feec8636ce?source=tag_archive---------5-----------------------
Set up a static website for (almost) free in minutes using AWS,"Deepam JainJun 1, 2020·8 min readDisclaimer: This post is not sponsored by any organization in any way and only contains the author’s personal opinions. The author must not be liable for your use of the information contained in or linked from this article.Before we dive in, let’s quickly go over the 3 fundamental elements of any website:There is a ton of other things that come into play (such as SEO, Analytics, etc.) but let’s skip that for another discussion.Let’s start with a simple web layout — this is the appearance of your website and is typically written in HTML, CSS, PHP, etc.For the purposes of this discussion, we will use a free HTML template that can be found here. However, once you get familiar with the basic concept, there are thousands of free/paid HTML templates available online so you are free to choose and edit the layout you like.After downloading the HTML theme, unzip the folder, and open two instances of ‘index.html’ file — one in the browser (simply by double-clicking), the other one in a text editor (use ‘Open With’ option; I am using Notepad++).The ‘index’ page in the browser should look like the above image. This page contains some details about someone named ‘Mark Parker’ by default — we need to customize it as per our own requirements.Let’s go to the text editor, search for ‘Mark Parker’:Once you find it, replace it with ‘Unicorn’ (or your name):Save the text editor and hit refresh on the browser, you will notice that the page has been updated.This process can be replicated to update all the information contained in the HTML as you need. If you are familiar with HTML/CSS, you can write your own web page from scratch as well.Once you have finalized your web layout, save it, and get ready for the next step.Now, the HTML you created in the previous step is stored on your local computer. But in order for anyone on the web to access it, we will need to put it on a shared server where anyone can access it — or, in other words, ‘host’ it on the web. This is where AWS (or, Amazon Web Services) comes in.If you don’t know AWS, it’s the world’s leading on-demand cloud (web) service platform, which works on a metered pay-as-you-go basis. AWS offers many services, however, we need only one for hosting our static web page on the web.What’s so different about AWS you ask? Well, for one, a new AWS user gets 12 months of ‘free-tier’ services which will allow you to host your website for almost free. Even after 12 months, the cost of hosting a static webpage would be less than around 50 cents — just a fraction of any other hosting service platform.So let’s jump right in.First, you need to make sure that you have an AWS account created. If not, you can sign up for free here (note that you will need an active credit card for AWS to place a temporary hold).Once logged in, you will see the AWS Management Console. For hosting the static webpage, we need AWS’ service called S3. Let’s find that service in the search bar and open it. You can also locate it under the ‘Services’ option on top-left.Under S3, we need to create a new bucket (in simple terms, a file folder) that will store our HTML files on AWS.So let’s click on ‘Create Bucket’ and enter a bucket name (I am using ‘www.yournewcoolwebsite.com’) — this should correspond to the name of your website (Also, note that your bucket name should be unique globally, if it is not AWS will throw an error and ask you to use some other name).The next option to be configured is the region where your HTML contents are going to be stored — you can change it to whatever region you like, but it is recommended that you set it to a region where your website is anticipated to be accessed most often. Once you are done, hit Next.Let’s skip the ‘configure options’ tab, for now; hit next and move on to the ‘set permissions’ tab.Now we are at the most important step in this process. In the permissions tab, make sure to un-select the ‘Block all public access’ option and check the acknowledgment box (as done below)— meaning you want your bucket to be accessed by anyone on the internet.In the ‘Review’ tab, let’s quickly review everything and hit ‘Create Bucket’. In the next step, we need to upload the HTML page and related content in the bucket we just created.You will be taken to the AWS S3 homepage by default. If not, you can head to AWS Management Console and search for AWS S3 as we did previously. Once you are there, click on the bucket name (www.yournewcoolwebsite.com).Now, we need to upload the files we had configured in the ‘Website Layout’ step. You can also drag and drop the files. Please ensure to upload all the files and folders that came with the HTML folder and hit ‘Next’.In the ‘Set Permissions’ tab, make sure that all of your content is publically readable under the ‘Manage public permissions’ dropdown and hit ‘Next’.On the ‘Set Properties’ page, there are other options available to expand your usage and add any encryption — we will not need any of those options as part of this demo so let’s hit ‘Next’. Go to the ‘Review’ tab and hit ‘Upload’.Now, click on the ‘Properties’ tab and head to the ‘Static website hosting’ option. Once you click on it, select ‘Use this bucket to host a website’, then copy the URL specified in front of ‘Endpoint’, specify the ‘Index document’ to be the ‘index.html’ page. If you have an ‘Error document’ included in the HTML zip folder, feel free to specify that under the second option and hit ‘Save’. Now your static webpage is live and can be accessed from the web.To visit the webpage, copy and paste the URL copied in the previous step in a new browser tab. You should now be able to see your HTML layout — this is now live on the web.This step finishes up the hosting. However, we still need to make sure that your domain name (www.yournewcoolwebsite.com) is pointing to the above URL. Let’s go to the next step to understand how to configure that.This is one piece in the entire process that you will be required to pay an upfront fee since professional domain names such as those ending in .com are not free. But if you don’t want to pay, you can always find free domains online such as here.However, most domain names are only $1 a month so hopefully, it still fits in your budget and is worthwhile for your online professional presence.To quickly recap: domain name is the web address of your website (such as www.yournewcoolwebsite.com) which you need to register with a DNS provider. This is the name that somebody will need to put in the browser to reach your website.There are tons of companies that allow the registration of a domain name such as GoDaddy, NameCheap, etc. Fortunately, for us, AWS also offers this service which is called ‘Route 53’.Similar to the first step, let’s head to the AWS Management Console, search and open ‘Route 53’.Once you are there, hit the ‘Domain Registration’ option and click on ‘Register Domain’ (or, if you already have a domain, go to the ‘Transfer Domain’ option and transfer your current domain to AWS).Now, let’s search for the domain we want to register (in this case yourcoolnewwebsite.com), click add to cart, fill up other details such as contact detailsYou can select the duration for which you want to buy the domain name — AWS will most likely give you a reminder 30 days before the expiry date.Once you purchase the domain, it is yours to keep — if you decide to opt for some other hosting service in the future, you can simply transfer this domain to that hosting service.Upon purchasing or transferring the domain, AWS creates a hosting zone for your domain name.Head over to the ‘hosted zones’ tab on the left side: from the drop-down, select your domain name and click on ‘create record set’. This will open a panel on the right side to specify the values.Specify the details as follows — Name: www; Type: IPv4; Alias: Yes; Alias Target: select the S3 bucket endpoints (or, the S3 bucket name we just created) from the dropdown; Routing Policy: Simple; Evaluate Target Health: No — Hit ‘Create’.The propagation takes anywhere from 2–30 min. to refresh. However, once that is done, your website should be live.You can now go and check your new website out using the website address.If you are interested in the more in-depth step by step process, there is an official Youtube tutorial containing all relevant information here.AWS also offers other services to make sure that your website is readily and swiftly accessible globally. If you are interested to learn more about it, feel free to check out ‘AWS Cloudfront’.Lastly, if you made it thus far, I want to thank you for your patience — hopefully, this tutorial was helpful for you in setting up your first website.",html,https://medium.com/@djain.cbs/set-up-a-static-website-for-almost-free-in-minutes-using-aws-3f38fc5f4c5?source=tag_archive---------5-----------------------
How to build a responsive mega menu,"Muhammad Rauf.Jun 2, 2020·5 min readBuild a responsive mega menu in HTML, CSS, and Javascript. First of all, nothing can be more reliable than a flat, one-level menu. Though, the difficulty is that a simple menu is suitable for small sites only. But, when we need to build navigation paths for something that has more than ten pages, we are forced to create multi-level menus.Let’s consider that a mega menu is something that has a thick structure with various node levels. It is well known as the type of menu found on any large or middle-sized website where content managers need to represent a lot of data impossible to compress into a simple, one or two-level menu. Generally, mega menus include different media content to better visually represent the navigation categories of the website.So, Let’s begin How to build a responsive mega menu with the help of HTML, CSS, and Javascript. First, you need to create a file with the name index.html or whatever you want. After that take a look below.HTML CODE:CSS CODE:Javascript Code:Alright, we’ve done everything. So, you just need to copy the above HTML, CSS, and Javascript code and put it into your index.html file. After that, hit the save button from your text editor and then open index.html file into your browser. You will see a responsive mega menu. Good luck.",html,https://medium.com/@softcodeon/how-to-build-a-responsive-mega-menu-acbedaf9c355?source=tag_archive---------7-----------------------
How to create CSS Gradient background generator in JavaScript from scratch.,"Aneta StojanowskaJun 4, 2020·3 min readStuck at home, can’t go to events and can’t hang out with friends. I could gaze out of the window and wait, but then I found this platform called Zero To Mastery by Andrei Neagoie with lots of interesting stuff about Web Development. A community with more than a hundred thousand wannabe developers striving to acquire the latest and the best technologies on the market.I started with creating a HTML file with elements such as header h1, h2, h3 and the script tag. Cool things that come with HTML and CSS are the input type “color” and the CSS linear gradient. Both syntaxes are essential to create a color picker. The input type “color” indicates the input to get a color picker and the CSS linear gradient to create the gradient where input is color. You need at least two color pickers to build the gradient function.To make the background change and the gradient wheel interactive we need JavaScript. I created a new JavaScript file where I will call the function of the background gradient later on. I do this only because in the future I’ll be adding different functions to my application and want to be more extensible. But you can easily add the script in your HTML file for now.By shuffling within the wheel, I change the background color and make the webpage more interactive. When the user finds his or her preferred blend, the particular CSS code can be copied and pasted. JavaScript can read from the DOM and can affect it. In our case, I want to be able to read the values (the colors) from both inputs.When the user changes the color in our color picker, I want to notice this and listen to the event every time the input value changes. This way I can detect it.While testing the console being updated, let’s get into action. Inspect the webpage in order to know which tag changes the background. In our case the background is in the body, so each time both colors change I grab the background tag. I add an id in the body of the HTML and keep testing if I’m actually selecting the body tag. Now when both inputs change, it fires up the linear gradient.Create the function setGradient to park the background changing actions. Keep in mind to DRY repeat syntax when extracting the function. When shuffling in our color picker, I just need to add the particular values of the CSS format. As we select the gradients, this value is being updated.Now you should be able to see the gradient background changing and the CSS code.You can add a style to make your own generator looking nice. For example add a special font style, add a random button which generates two random numbers for the colour inputs or even bootstrap the web page so it can adapt to your desktop and mobile devices. And if you stuck in one or another way there is probably someone who already found a solution for your coding problem, the internet is your solace.Feel free to check my customised generator in Bootstrap where I followed above steps and add style and font in CSS here.I look forward to doing more of that stuff in the next months.",html,https://medium.com/@anetastojanowska/how-to-create-css-gradient-generator-in-javascript-html-513ee9ef1e6?source=tag_archive---------1-----------------------
How to Create Breadcrumb Navigation in HTML,"Muhammad Rauf.Jun 4, 2020·3 min readHow to Create Breadcrumb Navigation in HTML. So, in this tutorial, we will how to create a breadcrumb using HTML and CSS. First of all, we need to know what is breadcrumb and why we use them into our website/blogs.Breadcrumbs are a dependent navigation assistant which helps the users easily read the relation between their location on a page and higher-level pages. The phrase is used from the tale of Hansel and Gretel where the kids drop a trail of breadcrumbs to track their way back. With breadcrumbs, if you’ve reached a page you don’t want to be there, then you can easily find your route back. Breadcrumbs are not completely website-only elements. Microsoft has introduced Breadcrumbs in Windows Vista and it has been a feature since then in every Windows edition.A breadcrumb track is a helpful tool for both web developers and SEO experts. It also helps Google bots to properly understand the website hierarchy, and it helps users to understand their current position on the site.However, not all websites implement this navigation tool. Possibly many do not consider it to be necessary because of its minor and unclear impact on SEO.So, let’s begin to create a breadcrumb to make our website handsome.CSS CODE:Now our good style breadcrumbs active. You just need to create a file like index.html and copy above HTML and CSS code and paste it into your HTML file. So, open your file in the browser and see magic. If you want to add this breadcrumb you just need to put this code into your article where you want or place it into your web index.html file below the body tag. Please let’s know, in the comment section how this article was helpful for you.",html,https://medium.com/@softcodeon/how-to-create-breadcrumb-navigation-in-html-4917b11ad8b1?source=tag_archive---------4-----------------------
"Create an Image slider with HTML, CSS and JavaScript","I am currently learning web development with the OdinProject curriculum. There’s a task which is to create a simple image carousel. It should contain arrows on each side to advance the image forward or backward. It should automatically move forward every 5 seconds. It should contain the little navigation circles at the bottom that indicate which slide you are on (and they should be click-able to advance to that particular slide).First, let’s create an html file named index.html.In the html file, we have a container that serves as a frame for each slide and each slide contains an image.Let’s add the styles. I’ll assume you have basic knowledge of CSS for you to want to build an image slider. I’ll try to make the styling basic and simple.The slide display property is set to none which makes them not visible now. The slide container and heading are centered too. We will add the functionality in the JavaScript to make the slides visible.Now, let’s style the next and previous buttons and the dots for navigation. Also, add an active class to style the dot for the slide that is currently being displayed.All we have displaying now is the heading, next and previous buttons, and the four dots.It’s time to add the functionality. Create a file named index.js and this to it.We created a variable named currentSlide that stores the index of the current slide to determine the current slide.We also created a variable called slides to store each slide into a array which enables us to iterate over them and another variable named dots to store all the dots in an array.Then we created a function named init that accepts a parameter n. The parameter will be currentSlide passed into it. Inside the function, we iterated through slides and set each slide’s display property to none. While iterating through the slides, we also iterate through dots and remove the class active from each do. When done setting each slide’s display property to none and removing the class active from each dot, we then set the display of current index according to the currentSlide, to block and add the active class to the dot of the current index using currentSlide variable.And lastly, we add an event to the window to run the init() function when the HTML content is done loading.We add this to the index.js file.We created a function named next to change the current slide to the next one. Here, I used the ternary operator instead of if-else statement. Inside the function, we checked if the currentSlide is greater than or equal to the last index of the slides (4 -1 = 3) which is an array. If it is true, we reset the currentSlide to 0, else we increment currentSlide variable and we run the init() function with currentSlide value.For the prev() function, we check if the currentSlide variable is less than or or equal to zero. If it is true, we set currentSlide to last index of the slides (4 -1 = 3), else we decrement currentSlide.And finally we add click event on next and previous button. When you click on the next button, it runs the next() function and when you click on the previous button, it runs the prev() function.To make the slide change automatically, we set a timer that runs the next() function every 5 seconds. Add this to index.jsWe also want to make the dots clickable to advance to the the next slide. Add this to index.jsHere, we iterate through the dots variable and for each dot, we add an click event and run the init() function passing the index of the dot that is clicked as the parameter and also setting currentSlide to that index.Yes, that is all. We now have a working image slider.This is my first ever article. Let me know what you think about it by leaving a response.Thank you for reading.",html,https://levelup.gitconnected.com/create-an-image-slider-with-html-css-and-javascript-3bf2c3e84060?source=tag_archive---------0-----------------------
Web Hosting Using Python Part 4 (Routes and Links in Flask),"Hello everyone, in the previous story we looked at how we can render external files in the Flask environment and what are the benefits in doing so (Link here). Since we plan to use Flask as a server for our website, it is highly likely that our website will have multiple webpages, and those webpages will be linked together. Let’s take a look at how to add multiple webpages to our server.The TaskFor this and the following few stories let us consider that we have the task to create a website that allows a user to sign-in, sends them an email upon sign-in, stores their credentials in a SQL database and allows the users to login later.Adding Multiple HTML PagesHere, the first page is the Home Page that is shows to the User by default. Note on line 11, a link to another HTML page is referenced in the url_for() function. Clicking this link will redirect us to the ‘signup’ page once it is created.Above we have created a sign-up page which will be shown when the user clicks the redirect link from the home page or tries to access the signup page directly (we will talk about this later in the story). Note that this page includes a traditional HTML form with the only difference on line 10 where the function url_for() is used to redirect to the success page when the form is filled and posted. We will create the ‘success.html’ page (which is displayed upon entering the form details correctly ) in the next step. The use of the url_for() function is mentioned in the previous story.In the code snippet above, we have created an HTML page which will be displayed once the user successfully signs up. Note on line 10, the name of the user who signed up is displayed. We will take a look at how to access the user name from the form to the ‘success.html’ page in a short while.Adding the pages to the Flask ServerOnce we have created the new HTML files (which needs to be located in the templates folder), we now need to route these pages on the server.www.datadriveninvestor.comOn line 4 in the above example we can see that parameter passed is ‘/’, which signifies that the below written line of code will be executed when the given link is tried to be accessed. Since the parameter only includes ‘/’, hence the below written code is accessed when the server starts. Therefore it is the default page of our server.Here the page ‘signup.html’ is loaded once we type our localhost address (127.0.0.1:5000) follwed by ‘/signup’ in the address bar. This page can also be accessed if we click the link in the previous page.We fill the details in the form to complete the signup process and move to the next page.Acknowledging User Sign UpRemember in the Sign Up template, the form points to the success.html template with the method post which signifies that once the form is filled and the submit button is pressed, we are redirected to the success.html page. We can now capture the form values, add error handling to the page.Getting User InputsOn line 13, we invoke the success function in case the form is submitted. The if condition on line 14 ensures that the function is invoked only if the form is first posted and an error is given to the user in case they try to access the page directly. Lines 15–18 are used to store the variables that are filled in the form in the previous page by the user. The else statement on line 19 handles the error when the user tries to access the page directly without filling the form first. The argument ‘data’ in line 21 is used to pass these variables in the ‘success.html’ file. We will take a look at how to use these variables on the html page. Also remember to import request on line 1.Displaying Sign Up success to the UserNote here on line 10, ‘{{data[0]}}’ is used to access the first name entered by the user in the previous page. The data is the name of the variable we passed earlier and ‘0’ signifies the index of first name in the data variable.User Sign Up SuccessError HandlingAs shown above, an error message is displayed to the user if an attempt to access the ‘success.html’ is made directly without accessing out the form first because the method involved is not ‘POST’.Thanks for ReadingAs you can see, we have added multiple templates to our server and linked them together. In the next stories I plan on adding the functionality of sending an email to the user upon successful sign up, storing the user credentials in a hashed form in a SQL database and giving user the option to login later.Please comment your views and suggestion below.",html,https://medium.datadriveninvestor.com/web-hosting-using-python-part-4-routes-and-links-in-flask-de72ce1fc0ff?source=tag_archive---------7-----------------------
Free & simple Netlify forms for your frontend site,"Bharat RamnaniJun 2, 2020·5 min readYou are a frontend developer, in love with your beautiful site that you created yourself and deployed to Netlify. Now you want to add a Contact Me section to it, but don’t want to deal with backend development and deployment and maintenance for it. Third-party form services could be your answer, but they are too costly for your little site which is not going to have thousands or even hundreds of form responses. Hacking a Google Sheet script is also not a solution now since we are not living in the 2010s anymore. So what do you do?You do Forms. Netlify Forms!I’ll skip the intro and directly show you two common implementations: one to a barebones HTML site, and other to a site which is built using some framework (such as Angular).I’ll use Angular here since that is what I used recently with Netlify Forms. But the steps remain the same for others (like React).List of active forms in your project (like the contact form here):How your submissions will appear inside a form:You can expand each submission to view the details captured (like name, email and phone shown here):A lot more is possible with Netlify Forms, but we’ll leave it for another day. Feel free to email me at bharatramnani94@gmail.com if you want to learn more about topics related to this, or any other topic for that matter.If you want to learn more, refer to the original documentation for Netlify Forms at Netlify Docs.Note: This post originally appeared at https://blog.bharatramnani.com/integrate-netlify-form",html,https://medium.com/@bharatramnani/free-simple-netlify-forms-for-your-frontend-site-c14d2c3811b6?source=tag_archive---------6-----------------------
Create a Simple Expense Manager with JavaScript,"Building an Expense Manager is probably one of the best introductory JavaScript projects.And you never really learn a language until you build something of your own.So, let’s begin. This article assumes you have basic knowledge of HTML, CSS, Bootstrap 4, and JavaScript. A few helpful references can be found at the end of this article if you need them.All the code we’re going to use in this tutorial is on Github. I would urge you to complete this tutorial and use the final code for reference until then.Meet John. John earns a decent amount by working as an auto mechanic at one of the largest auto repair chains in the country. But he ends up spending his monthly income on things he can’t keep track of.John wants us to help him track his finances and understand how much percentage of his salary he ends up saving, investing and spending.The first task we need to do is settle on a design for our Budget Manager. I usually try creating my designs or if nothing works, head over to Dribbble or Behance to find inspiration from a variety of designs. I use Figma to create designs for my projects and you can find a lot of great tutorials online.The reason why I always begin with the design is that it makes writing code simpler. Selecting a design helps you think in a structured manner which leads to quicker development.Let’s start by creating an index.html in your project folder.Follow the steps here to include bootstrap in your project. To make our design look better, we are using the Open Sans font. This is completely optional.Now that our scripts are ready, let’s start by analyzing our design. To begin with, we need two containers, the blue one on the left which takes up about 40% of the screen and the right one, which fills the rest of the screen. Thank god for the Bootstrap Grid!Let’s start with the blue left container.Insert code from the below block inside your body tag. This will create the following:Oh and, please feel to use your local currency. John the mechanic would like to use Indian Rupee (INR) for this example. 😃Great! Now let’s move on the part where John will add his savings or expenses to create a list.This container will contain a title, a dropdown to select the type of expense, a couple of input fields for John to record his expenses and a list which will display the entries with the date. Add this code below the left-container.That’s it for our HTML! We just completed 1 out of 3 files required for our Budget Manager. Excellent job reaching here. Next up, we will be giving our HTML some style. See that style.css we imported in the <head> above? Let’s style our HTML to create a simple and elegant design.Psst, a friendly reminder: I see you are following along for the last 15–20 minutes. How about a glass of water to keep you hydrated?As we use Bootstrap, most of the CSS-stuff is taken care of for us. Bootstrap handles a lot of positioning and design, so there’s not really much left to do except work with margins, font-sizes, and colors, the look, and feel.Although one interesting property of CSS is creating gradient backgrounds, and can really come in handy when you need to throw in some color into your project.Check out the CSS for the left-container , the background property can take a liner-gradient function which accepts a direction, and the colors you want your gradient to use.The direction has a default value of top-to-bottom .Sometimes, a linear-gradient is not exactly what you have in mind. CSS also allows a radial-gradient, a great reference can be found on W3Schools.Okay, so we are now done with most of the HTML & CSS part of the expense manager. Next, we’ll set up our app.js to add interactions and functionality to our project!JavaScript is the most important part of this project. The HTML CSS defined how our expense manager will look, but now we need to work in the logic. The app.js is where all the magic happens!Before we begin, take a few moments to think over what functionalities we need to add in the app.js . Currently, nothing happens when we select an expense type or when we add a description and expense value and click on the button. Yes, you guessed it right, we need to add eventListeners . We also need to show the current month.We will be writing different functions, functions that are solely responsible to handle the UI and the logic to calculate the month’s budget. Let’s call them controllers.Our project will have 3 controllers.Create an app.js file and add the script tag to the bottom of your index.html just before closing the body tag.Let’s go over an important concept of JavaScript functions before we start writing our controllers.Immediately Invoked Function Expressions (IIFE)IIFEs are functions in JavaScript which run as soon as they are defined. They are “immediately invoked” when your script file is run. Hence, they do not need another function to call them.Pre ES6:ES6:Awesome! We are now ready to add the magic to our well designed, gorgeous looking Expense Manager project.Let’s jump right into creating our controllers for the project.The main controller will take care of the following:The UI controller will focus on:The expense controller will focus on:Create the three controllers as outlined below. The main controller takes in two parameters, the UI controller and the expense controller. This will help the main controller control the flow of data between them.The HTMLStrings function is an object that keeps track of the class names and element ids used in the HTML files. I do this because it makes referencing HTML elements easy in the project and reduces potential hours of debugging spent to find a spelling mistake.Now, let’s set up the eventListeners . List down the elements that will require a click event listener. The expense type dropdown options and the submit button.Alongside, we’ll also create the functions which will be called when the eventListener is triggered.Let’s dig deeper into each function.The setupEventListeners function selects the dropdown elements and the submit button to add a click listener to each of them. These listeners are activated when you click on them. Each click listener performs a specific task, such as setting the expense type or instructing the other two controllers to perform some tasks.The addExpense function asks the UI controller to get the input from the HTML, verifies if the input is valid (not null or 0) and asks the UI controller to add a new list item with the correct input. It also asks the expense controller to re-calculate the values from the new input. Then it asks the UI controller to update the overall expense for the month.You might wonder, we can directly let the UI and expense controllers talk to each other, and yes we can, but it may not be considered a good practice. Controllers should remain independent of each other except for the main controller. This makes it easier for the developer to understand and debug the flow.Great, that’s it for the main controller.The UI controller is the biggest of the three as it works with HTML and CSS classes to provide the right look and feel to our project.Let’s take a look at all the functions:The expense controller has a simple task. It maintains four values: savings, investments, expenses and the total monthly budget.That’s it! You have just completed the basic functionality for the expense manager. By now, you should be able to log multiple entries in the savings, investments and expense types and your budget should be calculated accordingly. Great job!Now, it’s time for our bonus section!Michael is always ready for a doughnut! 🍩Source: GIPHYThere are multiple libraries out there that help you create beautiful charts. Libraries help you quickly achieve your goal and are always a better option than writing the same functionality all over again.We will be using the Chart.js library which is a simple and flexible library for designers and developers. The library uses the HTML <canvas> element to render your charts.Insert a new div which will hold our chart at the bottom of the left-container.We need to link our controllers to update the chart whenever a new record is inserted into our expense manager.In the addExpense function in the main controller, add a new function call which will instruct the UI controller to update the chart with new values.The displayChart in the UI controller method will create a new chart. We need to pass the type of chart, the data (labels and the dataset), and some options in case we need to customize the chart.Awesome! Now try entering a few values into the system to see your beautiful doughnut graph!This is how our expense manager should look like at the end.The full code for the project can be found on Github.Thank you for staying through the end of the two parts! I hope you found this article helpful in your programming journey!Happy Coding! 😃",html,https://levelup.gitconnected.com/create-a-simple-expense-manager-with-javascript-4e2cf2097fba?source=tag_archive---------1-----------------------
3 Easy Ways to Write Better CSS,"For UI development, it’s pretty easy for CSS files to grow as the design demands for your project evolve. If you’re working in an agile environment, one that is categorized by pivoting changes frequently and consistently, this design evolution is typically more gradual. Given the gradual, consistent change, it’s common for a developer to “add on” new styles rather than rewrite your existing ones.Say you have a design of three boxes with a black border. In the next sprint, someone says they want the second box to have a red border. Your instinct might be to simply add a new class to separate that box from the others and give it a border style property as solid 1px red. This works, but it is not optimal when you consider the following.Most projects will be larger than the example above. If you doubled the size of the project, your CSS will remain the same but you’d need to add another class name to another div. The more boxes you need, the more class names you have cluttering your HTML.In the above example, there is only one style variance for one element. Imagine if they wanted to add text to those boxes with varied text-alignment based on which position the boxes are in. Using the above process, you’ll end up adding more classes for more changes and both your HTML and CSS files will grow and become less organized and flexible. Which brings us to our last consideration.As mentioned above, with project growth and change in design demand, it is important to have a foundation that is flexible and as light weight as possible. It’s not unreasonable to add more styles/classes should the needs of your app change/grow, but anywhere that you can replace convenient code for efficient/reusable code will greatly improve the flexibility of your app and therefore require less work when changes are needed. In order to create an ecosystem that is sustainable and flexible, syntax is very important.Below is the UI for the project we’ll be developing throughout this blog post.We have three boxes with red borders, a title (h1) and a subtitle (p). Box one is left aligned, box two is center aligned and box three is right aligned.Here is the initial HTML.Here is the CSS.Given the code above, let’s explore three ways that we can refine and condense our CSS to ensure our app is more flexible, lightweight and condensed.We can start by identifying repetitive attributes. Whenever you see PARAMETER-DIMENSION (such as margin-top), there is an opportunity to condense code. Unless it’s a case where only one dimension is needed and the others are negligible, or if all dimensions require the same parameter, a more efficient way to write the dimensions is to consider the acronym TRBL.In CSS, you can follow this pattern for writing variable dimensions for attributes like margin and padding. Below are a few areas where this pattern can come in handy.In .containerChild we can use border to define the color and the style, while we can use border-widthT to define the width of the TRBL borders.This is great, but we can refine our shorthand even further. Since the values for top and bottom are the same, and the values for left and right are the same, we can rewrite this using only two parameters.As of now, we have saved ourselves 6 lines using the TRBL technique. The next way to refine your CSS proves that sharing really is caring.You may have noticed that some of our elements have similar styles. Let’s take another look at h1 and p.Is this case, the only difference between the two elements’ styles is the color. To avoid the repetition of the other styles, we can use commas to separate the selectors and then give them a shared group of styles.By using a comma to separate the selectors h1 and p we can consolidate similar styles. As for the different styles, we can add a selector for the element with different styles (in this case, p) and then give it only the attribute(s) we want to change.At this point, we have a total savings of 8 lines. It doesn’t seem like much but our original CSS was only 33 lines. That’s a little more than a 24% reduction in code size. If you apply that rate of savings to a style sheet that has, say, 1000 lines, that’s a savings of more than 240 lines.Combinators can ensure your CSS is precise by maximizing element associations without overwhelming your HTML.The use of CSS combinators can help you keep your HTML lighter and more flexible. A lot of classes and id’s can hinder style re-usability and add to markup. Our app has a few instances where we can replace class selectors with combinators..containerChild is the class selector used to target the three boxes within the parent .container. This can be accomplished using the child combinator (>), to select the child div’s within .container.This won’t replace/reduce CSS code, but it allows us to remove the .containerChild class name from the boxes. The same can be done using a combination of the child combinator and the nth-child selector to target the styles for two and three.We are able to capture the box div’s as well as their h1 and p tags. Again, this doesn’t save us lines of CSS code, but check out our new HTML.We are now able to achieve our original result with just one class name. As of now, we have reduced our lines of CSS by about 21%, and reduced our use of presentational values in markup by 87.5% ( 8 elements with class names to just one)!With our HTML more lightweight and our CSS more refined, there is still one other potential area of efficiency that we have improved upon.Imagine if each of the boxes are meant to represent list items? Or blog post thumbnails? Now, unless you’re planning to hard-code each list item into your HTML (a solution that is not dynamic and potentially unsustainable) you’ll need to use JavaScript to render each of the items and apply the styles outline above.By removing the need for class names and inline styles, we have removed multiple lines of JavaScript for every iteration we make through the data. Say the list of items had a length of 100. Since we’re using the combinators, we won’t need to use element.className = ... for any of our elements (aside from .container). Before combinators, we would need to add the classes to our elements programmatically.That’s three instances of element.className that we are able to avoid for 100 items, which translates to a reduction of 300 operations (3 * 100 = 300).The three quick ways to write better CSS are as follows…Apply the acronym TRBL (Top, Right, Bottom, Left) anytime you are trying to apply a style of varying dimensions such as padding, margins, and borders.Using commas, you can consolidate shared styles with multiple elements without having to specify those styles multiple times for each element.Combinators can help you can avoid heavy use of class names and id’s in your HTML. This can make your HTML simpler, more flexible, as well as reduce the amount of programmatic operations needed should you want to incorporate JavaScript.Source code for this post can be found on CodePen.Note: HTML structure provided in this post is not meant to represent HTML that meets accessibility standards. It is being used for convenient and demonstrative purposes relating to CSS refinement. For more on these guidelines please read the W3C standards guideline.medium.comskilled.devwww.w3.org",html,https://levelup.gitconnected.com/3-ways-to-write-better-css-ce4c8a1294fa?source=tag_archive---------0-----------------------
3 Ways to Embed PDF in Website Using HTML,"Bikash PandaJun 4, 2020·2 min readIn this article, we discuss How to Embed PDF in Website Using HTML. In HTML there are 3 ways to embed PDF in HTML.Table of ContentsUsing <iframe> you can set height and width of the PDF.<iframe src=”PDF_SOURCE_URL” width=”100%” height=”100%”>HTML tag has some attributes which are used to manage the embed PDF on the HTML <iframe>. Attributes are explained below,PDF embed code<a href=”http://files/nameOfFile.pdf"">http://files/nameOfFile.pdf</a></code> tag</h3><! — /wp:code →<! — wp:paragraph →<p>Using HTML object PDF embed operation,</p><! — /wp:paragraph →<! — wp:code →<pre class=”wp-block-code”><code><html><body><object width=”400"" height=”500"" type=”application/pdf” data=”/my_pdf.pdf?#zoom=85&scrollbar=0&toolbar=0&navpanes=0""><p>Insert message here, if the PDF cannot be displayed.</p></object></body></html>Above we do the PDF embed operation using different tags of HTML.Note:Using <embed> and <object> tag is old fashioned and also considered as deprecated.So, we using <iframe> for PDF embed on our website, which is an easy way to do the same.I hope, I cleared all the things about how PDFs are embedding on the HTML website using <iframe> HTML tag.If you guys facing any issue on this please let me know on comments.Also Check:Happy Coding..!",html,https://medium.com/@phpcodertech/3-ways-to-embed-pdf-in-website-using-html-d3cddff696be?source=tag_archive---------7-----------------------
What is CSS? | Top 4 Advantages of CSS (cascading style sheet),"iTriangle TechnolabsJun 3, 2020·3 min readCSS stands for Cascading Style Sheets. HTML and CSS are behind the technique of creating a webpage. The use of HTML gives the webpage a shape and the use of CSS gives the webpage a new and attractive look. HTML and CSS are always used together. Without CSS, we can use html but without html, css cannot be used.HTML and CSS is a computer language that is very simple and can be easily taught. To write the code of html and css we need a text editor such as Notepad. After writing these codes, a web browser is required to view it through internet.Many tags are used in HTML such as header tag <h1>, font tag <font>, table tag <table>, image tag <img> etc. All these tags are also used together with css to show them more well in the browser.Using CSS, we can show the text of the webpage in good color, control the space between the styles and paragraphs of fonts, the images in the background and the use of what color in the background will give the webpage a good look. Css is used to set things. The css html document gives a completely new look which attracts more users.We will not have to write css code repeatedly to create a different web page. And by using the css code written only once, we can create as many web pages as we want, in which we save a lot of time.If we use css, we do not need to write the attributes of the html tag again and again. Just once according to the rule of css, by writing the attributes of the tag and applying it in the web page, that tag will appear correctly everywhere. Therefore, the tag will not have to write the same code again and again to appear in different places on the web page, and if there is less code, then the web page will load quickly in the browser.To change the style of web page completely, just changing the style code of css once will automatically change all the elements used in html at once and change all elements one by one will not be necessaryplatform independent means that we can use css in any platform like windows, linux, macintosh etc. And it also supports all the latest browsers.This was CSS and some related information that is used to create a web page. Hope you have helped a lot in this article about What is CSS. If you want more information related to this, then you can comment below.iTriangle Technolabs is top-ranked and expertise web development company, which follows the current and popular trends in the it industry to deliver clients a website with all trending features.If you require a CSS Development with all the trending features in the market, our team is happily available for you 24 by 7.Originally published at http://itriangletechnolabs.com on June 3, 2020.",html,https://medium.com/@itriangletechnolabs/what-is-css-top-4-advantages-of-css-cascading-style-sheet-8bb15ad3dfdf?source=tag_archive---------14-----------------------
Today I Learned in Web Dev — Day 1,"Joshua BennettJun 4, 2020·2 min readA react-router Link injects its own HTML (in the form of an anchor tag) into your Document Object Model. This surrounding anchor tag can hamper CSS. In my project I ended up removing the Link altogether for separate reasons, but initially I had to style the anchor tags instead of the Component’s own parent div for hover effects and such. It’s not ideal because it’s confusing where this CSS should be put. Does it belong in the original Component because that’s where the Link lives, or in the child Component since that’s Component using the Link.I encountered this issue while trying to have some items centered in a box. I noticed that the bottom part had much more space than the top part which made no sense to me. Using the Chrome Inspector I saw the margins were put together. I ended up just changing my use of margin’s in the collapsed children to using padding within the elements inside those children to avoid the issue. More info on it can be found here: https://css-tricks.com/what-you-should-know-about-collapsing-margins/This issue has popped up several times. The first time was because I had used a very general CSS selector such asInitially it was fine, but then since this was so high level it affected later divs without me realizing. I just have to be careful when being so broad with my selectors. I also was reusing classvnames without even realizing it. I don’t like making my class names so verbose, and with SASS it shouldn’t be so much a problem. I just need to make sure I’m utilizing SASS properly, and to be safe being very specific with my class names.I had a large container that had items in it, and I wanted to use padding on the parent container to keep the items within away from the container. This made it a nightmare to try to add a background-color to a child element that wanted to fill the width of the parent since it had that padding on it. There’s a workaround using CSS Gradient, but it’s not very specific and probably doesn’t translate well to other screen sizes. Instead I opted to remove the padding from the parent and adding an extra wrapper just to color the background then adding the padding to each children within the sections. It’s more work and more verbose, but isn’t as hacky of a solution.",html,https://medium.com/@earthsvisitor/today-i-learned-in-web-dev-day-1-ff80985ef11?source=tag_archive---------9-----------------------
HTML Email Template for Beginners,"Suprabha SupiJun 3, 2020·1 min readFor any HTML email developer, we all know how difficult it is to create a fully working and compatible HTML email for newsletters or campaigns.The knowledge I have gained over the past 6–10 months, to create beautiful and compatible HTML email for my company’s campaigns.Some useful resources that I used:HTML Email CheckPutsMailThere are few CSS properties, which will not work in email template. You can refer the below link for all the CSS properties which you can use while creating any email template:Campaign Monitor CSSThere is one easiest way to create an email template by using table element.Reference:smashingmagazine.com for layoutI hope you enjoyed the article. If you have any question, please feel free to ping me on @suprabhasupi 😋🌟 Twitter | 👩🏻‍💻 suprabha.me",html,https://medium.com/@suprabhasupi/html-email-template-for-beginners-34b209279548?source=tag_archive---------5-----------------------
Form submission — why it’s crucial to use a button of submit type,"Few days ago I stumbled across a form component implementation in Angular where a submission was accomplished with the aid of an ordinary button (of button type) using a click event handler. I advised the developer to change the button’s type (to submit one) and register an event handler for the ngSubmit event at the form element. In this blog post, I will indicate three reasons why to favour a button of submit type over an ordinary one.Let’s take a look at a very simple login form:Naturally, you can submit the form by clicking the button. In addition, you can also trigger the action by clicking the Enter key while being focused on any input element within the form due to the presence of the button of submit type within the form element. However, if you use an ordinary button (of button type), you cannot make use of this handy feature:Angular marks an instance of the FormGroupDirective, which gets created when you apply the [formGroup] selector to the form element, as submitted when the appropriate event has been triggered. Chances are that your code relies on the submitted property value, e.g. you have a requirement to render validation errors only if a form has been submitted:However, if you do not use a button of submit type, the submitted property will not be properly updated and the validation errors will not be rendered in the above example.Angular allows developers to choose when a form’s model gets updated based on corresponding values in a view. One of the options is to update the model when a form has been submitted:Before submission:After submission:Similarly as with the submitted property, if you use an ordinary button instead of the one of submit type, the model will never be updated based on the view values.Live example:I hope you liked the post and learned something new 👍. If so, please give me some applause 👏",html,https://medium.com/javascript-everyday/form-submission-why-its-crucial-to-use-a-button-of-submit-type-b43511d92671?source=tag_archive---------0-----------------------
Adding a Patrollin Share Button (Javascript / HTML),"Rick From PatrollinJun 3, 2020·1 min readThe dev team here at Patrollin has been working extremely hard on finding ways to improve the platform, one of the biggest requests that we have received is offering the ability to quickly share links to Patrollin (from another website). Below is the code and an explanation.The JavascriptThe shareOnPatrollin function instantiates a url variable that takes the Patrollin Sharer URL and adds a url and text parameter. Causing a window to open up, like this one.Short/Sweet/Simple :) If you have any questions post a status with the hashtag #help at Patrollin.com (or from within our Android App).",html,https://medium.com/@rickfrompatrollin/adding-a-patrollin-share-button-javascript-html-fb93415b3003?source=tag_archive---------11-----------------------
HTML Never Been This Simple,"Moffat NyabutoJun 2, 2020·2 min readThis got be interesting…For a while I have delved in learning html coding in this amazing site — pirple.comMy much effort finally came to fruition and I can not hide by excitement and right below here is the piece of work that I have developed. They say sharing is caring, and here I am.As the world quickly evolves to digitization, you should too learn the art before you get rendered obsolete.And here we go…<!DOCTYPE html><html> <head>  <meta charset=”utf-8""> <meta name=”viewport” content=”width=device-width”> <title>Moffat web</title></head> <body>  <h1>My All Time Favorite Read</h1> <! — First headline → <p> I have done a couple of books and it has been hard to  narrow down to one favorite one, but here goes one among many. </p> <! — This is line break — !> <br><strong> Rich Dad Poor Dad</strong> by Robert Kiyosaki </br><! — underlining here → <h2> <u>Why I settled on the Book</u></h2>  <ul> <li>The book talks about the importance of financial literacy - cuccrrent worlds urgent neeed.</li> <li>Talks about how one can achieve financial dependance</li> <li>Lays bearhow to build wealth through investing in assets</li> <li>Highlights how to start and own businesses</li>   <! — strike through → <li><strike>Among other reasons</strike>😉</li> </ul>  <h3>Synopsis of the Book</h3> <p> Robert Kiyosaki tells the story of his two Dad’s in his childhood.  His own father and the father of his best friend. While he loved both, they were very different when it came to dealing with finances. </p>  <p> Look around and you’ll see plenty of financially ignorant people in  your own life. Just take a look at local politicians. Is their city  in debt? Your mayor might be a great mayor, but unfortunately, no one ever taught him how to deal with money. </p>  <p> The first step towards building wealth lies in the mindset of  managing risks, instead of avoiding them and learning about investments  will teach you that it’s better to not play it safe, because that always means missing out on big potential rewards.   <! — Line break → <br> Don’t start big, just set aside  a small amount, like $1,000 or even $100, and invest it in stocks, bonds,  or even tax lien certificates. Treat the money as if it’s gone forever and you’ll worry less about losing it. </br> </p><! — Text scrolling →<marquee scrollamount=4> <h3> This information is just priceless</h3> </marquee>I would so much like to hear from you. Please comment below your favorite book and let’s get to learn.<p><form> <input type=”text” name=”email” placeholder=”Name”> <input type=”text” name=”email” placeholder=”Write your email”> </p> <p> <textarea name=”Message” placeholder=”Write your comment here”></textarea> </p></form></body></html>Feel at ease to leave your comment or your thought and I will be happy to get in touch with you.",html,https://medium.com/@moffat.nyabuto/html-never-been-this-simple-1d460d901a4c?source=tag_archive---------13-----------------------
How to get a free shirt (and make accessible web sites) 🆓 👕🚨,"Reece BoydJun 2, 2020·3 min readA simple perspective to make a web site accessible is to achieve two goals:Achieving #1:It’s mostly about tabindex .Besides that, it’s mostly about visual indicators.Achieving #2:It’s mostly about aria-label’s.Ideally your button’s are descriptive enough: “Log in”, “Log out”, “File a claim”, “Chat with us”, etc. But sometimes they’re not: “Get started”.Get started with what? Simple fix: just add an aria-label to the element, for example: “Get started filing a claim.”And that’s pretty much it. Of course there’s a lot more bonus points you can rack up with accessibility but just doing the above two things — which are almost entirely achievable out of the box by using the right html elements or with just minor tweaks to tabindex and aria-label’s — you will already have a more accessible site than most. The tools below will easily take your accessibility practices to the next level.  Tools:Scanners that will help you meet the above 2 criteria and more:Each issue discovered with Axe comes with a Learn more link that provides awesome perspective on how to resolve that issue and why it matters.Screen reader that will help you test your aria-label’s (or lack thereof):What’s next?If you’ve made it this far, thanks for reading and enjoy the free shirt! I’ll write a follow-up that covers some extra accessibility practices that will take your site to the next level. As a teaser, I’ll leave you with two links on my favorite*: the skip link.*It’ll be your favorite too if your site has a header.",html,https://medium.com/@reecealanboyd/how-to-get-a-free-shirt-and-make-accessible-web-sites-3e4a22047bca?source=tag_archive---------8-----------------------
Take Screenshots With Unprecedented Accuracy in Chrome,"ZhengJun 3, 2020·3 min readPretty much every device supports taking screenshots, and the shortcut is usually really easy to remember.And on a computer, like a Mac, PC, or Chromebook, you have the option of taking screenshots that only cover a portion of the screen.You’ll probably find that you take partial screenshots more often than you take full screen screenshots — especially when posting on social media or blogging, as there might be some sensitive information that you don’t want to show. Full screen screenshots can also take up valuable space.And although the resulting partial screenshot looks fine, you can never get it to be exact. There will always be some extra space on the side, or it could be shifted a bit…… eyeballing can only be so accurate.I found this cool extension a while ago for Chrome. It’s called Element Screenshot, and it lets you take screenshots of any HTML element — resulting in unprecedented accuracy! Here’s how to use it.chrome.google.comClick “Add to Chrome”, then “Add extension”.It can “read and change all your data on the websites you visit”, but I haven’t run into any privacy-related issues yet and it’s based on an open-source library here:github.comIt’s always good to look at what it says before hitting “Agree” or “Confirm” or “Add extension”, but for Element Screenshot it should be fine.You can go to any website you like. I’m going to go back to apple.com/mac.www.apple.comYou might need to refresh the page so that Element Capture can be activated.Usage is very simple.2. Click on the element that you want to screenshot…3. … And the screenshot will open in a new tab.To copy the image, just press Command + c (you don’t even need to press on the image!) To save the image, press Command + s.(those shortcuts were for Mac. Replace with Ctrl if you’re on PC or Chromebook).That’s it! Thanks for reading!",html,https://medium.com/@aheze/take-screenshots-with-unprecedented-accuracy-in-chrome-29472b6fa874?source=tag_archive---------6-----------------------
Enhancing HTML audio tag with JavaScript,"The audio tag in HTML allows audio to be embedded into a webpage and offers some options in terms of the functionality it provides. You can, for example, enable controls so that the user can change volume, play and pause and so on. You can have it autoplay or (assuming controls are enabled) it can be up to the user to play it. It’s a pretty straightforward thing.Normally you would implement it in the HTML something like this.<audio controls><source src=”media/song1.mp3"" type=”audio/mpeg”></audio>You can also provide multiple source tags, and it will play the first one that is valid.<audio controls><source src=”media/song1.mp3"" type=”audio/mpeg”><source src=”media/song2.mp3"" type=”audio/mpeg”><source src=”media/song3.mp3"" type=”audio/mpeg”></audio>In that example, if song1 and song2 did not exist but song3 did, then the audio element would load song3. If song1 didn’t, but song2 and song3 did, then it would play song2. It simply plays the first one that is valid.The fact that we can specify multiple sources for an audio tag can be used to our advantage though if we want to enhance what can be done with the audio control.So let’s make use of the multiple source tags and some simple JavaScript to make the audio tag play a playlist of songs that we provide.Start by setting up the audio tag with multiple source tags for the audio you want available.<audio controls><source src=”media/song1.mp3"" type=”audio/mpeg”><source src=”media/song2.mp3"" type=”audio/mpeg”><source src=”media/song3.mp3"" type=”audio/mpeg”><source src=”media/song4.mp3"" type=”audio/mpeg”><source src=”media/song5.mp3"" type=”audio/mpeg”></audio>I’ve got 5 songs in this example, but you could have 10, 20 or hundreds if you wanted to.What we’re going to do is write a simple script that will make it so that once the audio element finishes playing the audio, it will move onto the next one in the list automatically. So, it plays song1 then moves to song2, 3, 4, 5. Once song 5 is finished, it jumps back to the start and starts again from the beginning.In out HTML, just above the </body> tag, let’s add a <script> tag and then declare some variables.<script>var musicSources = null;var musicControl = null;var musicIndex = 0;musicSources will be an array of those sources we have under the audio tag.musicControl will reference the audio element itself.musicIndex will just simply refer to what particular musicSource we are up to.Now, let’s create a function that we will call at the end which will define how the audio control behaves.function SimpleMusicController() {Then we will populate the variables we created before.musicSources = document.querySelectorAll(“audio source”); musicIndex = 0; musicControl = document.querySelector(“audio”);Here, we are using a built in JavaScript function “querySelectorAll” which will return an array containing all the source objects it finds as children of an audio tag.We reset the index to start at 0.Then we use “querySelector” to get the first audio tag on the page. (Yes, we’re assuming in this case that we only have one we want to control.)Now we want to add an event listener to the audio tag that will be triggered when the audio file that is being played, ends. This could be because it’s finished playing, or the user has skipped to the end to force it to finish or because a script has told it to end. It doesn’t count the user pausing the audio however.For our purposes, when the audio ends, we want to get the next in the list and tell it to start playing that one.musicControl.addEventListener(‘ended’, function(e) { musicIndex++; if( musicIndex > musicSources.length — 1 ) { musicIndex = 0; }  var mSrc = musicSources[musicIndex].src; musicControl.src = mSrc; musicControl.play();  }, false);As you can see in the function above, we’ve added the eventListener and provided it a function to run when the event triggers.Within that, we start by incrementing our index.We then check that the index hasn’t gone too far and if it has, we set it back to the beginning. Zero.With the new index determined. We then get the audio file source from the relevant source tag, set that as the source for the audio tag and tell the audio tag to play it.Simple.If we close off our SimpleMusicController function there}and load the page in the web browser. Assuming all the audio files are accessible you should be able to play it, skip to the end and then watch as it starts playing the next song automatically.Nice.More?Oh alright. Let’s add a little more.Let’s make it show what song is currently playing, and what’s playing next.To do this, we’ll need to add a couple of <div> tags under our <audio> tag. (Or wherever suits you for your page). We will also need to make use of data attributes on our source tags.Add a <div> tag to show what is currently playing.<div id=”now_playing”></div>We’ll just reference this by id for now.And another for what’s coming up next.<div id=”up_next”></div>Layout, styling and the ids can all be up to you. We’ll reference the ids in the code though so make sure you update that if you change the ids here.What are data attributes?Data attributes allow us to add extra attributes to our HTML that can be easily used in our code, without actually breaking away from HTML standards. You could read more about data attributes on MDN.Using data attributes we will add content to give us the song title and the artist. (This assumes you’re playing songs. If not you’ll be able to adjust it to suit your use case.) So our source tags will look more like this.<source src=”music/song1.mp3"" type=”audio/mpeg” data-title=”Song One” data-artist=”John Doe”><source src=”music/song2.mp3"" type=”audio/mpeg” data-title=”Song Two” data-artist=”Jane Doe”>…etc…Very imaginative titles as you can see. Using these data attributes though we can easily reference and display the artist and song title for the current and next songs in those divs we created earlier.Let’s add a new eventListener which will listen to the “play” event. We’ll do this inside the SimpleMusicController function just before the end.musicControl.addEventListener(‘play’, function(e) { // Update now playing with title and artist document.getElementById(‘now_playing’).innerHTML = ‘Now Playing : ‘ + musicSources[musicIndex].dataset.title + ‘ by ‘ + musicSources[musicIndex].dataset.artist;  // Get the info on the next song to be played and show it tmpIndex = musicIndex + 1; if( tmpIndex > musicSources.length — 1 ) { tmpIndex = 0; } document.getElementById(‘up_next’).innerHTML = ‘Coming Up : ‘ + musicSources[tmpIndex].dataset.title + ‘ by ‘ + musicSources[tmpIndex].dataset.artist; }, false);We start by updating the HTML shown in the now_playing div. Setting it to ‘Now Playing : ‘ then adding in the song title and artist by referencing those data attributes.You’ll notice here that the data attributes we set were:data-title=”Song One” data-artist = “John Doe”Yet in our code here we are accessing it with:musicSources[musicIndex].dataset.titlemusicSources[musicIndex].dataset.artistWe don’t need to use the “data-” bit when accessing these properties using JavaScript. This makes it a very nice way of extending HTML tags to include extra properties that can easily be accessed by name in the code.After setting the currently playing song, we create a temporary index to determine the next song. Much the same as we did in the “end” function by also checking if we’ve gone past and looping back to the start if needed.We then update the “up_next” div to show the title and artist of the song that will be played next.Call the SimpleMusicController function on the page and you should see that when you start playing the title and artist shows, as well as the next song title and artist and it automatically moves onto the next song when one finishes.Great!Excellent. So we’ve take a basic HTML audio tag and written a very simple script to create a playlist and play through a list of audio sources. It’s kept very simple for now with lots of room to expand in the future. I’ll probably do more in the near future where we expand on this and add extra features. Things like the ability to show a playlist, the ability to randomise the song that gets played, stopping at the end instead of looping around and things like that. We might also look at some more advanced controls and… I’ve got a cool idea we could use to turn this into some extra special which I’ll work on putting together. More details later on that.",html,https://blog.devgenius.io/enhancing-html-audio-tag-with-javascript-ff4b5fce1b55?source=tag_archive---------6-----------------------
The “C” In CSS: Why Is It Important?,"It’s a usual day at work. You’re at your desk, creating the new design sent in by the UI guy, Mike. You wonder why does the UI team have to change the UI so often, you are the who ends up writing media queries for each screen! Anyway, you’re trying to work with a certain CSS property and it just doesn’t seem to work.That’s weird 🤔. You look up from the screen and everyone around you is busy working. An evil thought crosses your mind and you end up adding a !important to the CSS value. Or maybe an inline CSS. There, problem solved!The cascade in Cascading Style Sheets plays an extremely important role and affects how styling methods are applied to your elements. This story is going to help you understand how the cascade works and how wonderful a concept it is!CSS Specificity is a set of the rules applied to CSS selectors (properties) in order to determine which style is applied to an element. The more specific a CSS style is the likelier it is to be present on the element’s style.There are many benefits of understanding CSS Specificity. Most importantly, it helps you understand why your styles aren’t being applied, and it helps you write less CSS code.CSS specificity is determined byHere’s a great article that explains how to calculate the specificity for an element.Coming back to the cascade, the cascade is concerned with where and how you write the styles. The image below shows the decreasing priority of the styles from top to bottom. The !important keyword has the highest priority while inheritance has the lowest.Let’s break down and understand each concept in a reverse manner, starting from the lowest priority, inheritance.A quick example to understand inheritance is that you gain certain properties from your parent such as your last name. Similarly, HTML elements can inherit style from other elements. This is a parent-child relationship, where-in the child inherits the properties of its parent elements.In the above example, we’ve given assigned a color style to the parent div, which has been inherited by its child div. In other words, the style from the parent has cascaded down to the child.But, all CSS properties cannot be inherited. There are some properties that cannot be inherited by a child element. A typical example of a non-inherited CSS property is the border property.While the paragraph element will have a border, the emphasized text will not inherit the border style and thus, not have a border.An excellent reference to CSS properties is provided by MDN. To see if a particular property can be inherited, look for the (“Inherited: yes”) or (“Inherited: no”) in the Specifications section.Inheritance has the lowest priority among the CSS styling methods. If the child has a style of its own, then the inherited value of the parent is ignored, even if the parent style has the !important keyword, as shown in the example below.Style sheets are of two types, external and internal. Both of them have the same priority, but there’s a catch. When the priorities are the same, CSS applies the ordering rules to determine which CSS should be applied.For example, the background property is used in all three classes. The priority and the specificity are the same, so the last declared style takes precedence.CSS rules follow a certain order, from left to right and top to bottom. This means that CSS prioritizes the right-most CSS and the bottom-most CSS.Here, the rightmost CSS is applied to the div, meaning that the div has a blue border.Similarly, the CSS at the bottom is applied to the div, meaning that the div has a blue font.The way you select your elements will also play a role in determining which CSS rules are applied. IDs (#myID) have precedence over Classes (.myClass) and Classes have precedence over tags (div).Consider the example below, the text is red even though the class and div selectors are specified after the ID. Specificity has a higher priority than the ordering rules, so it does not matter where you place your CSS, specificity will always take over ordering rules.Inline styles have the second-highest priority, after the !important keyword. Inline CSS can be overridden only by the !important keyword, so if your CSS isn’t applied even after adding an inline style, there must be a !important hidden somewhere in your stylesheet. 😃Within inline styles, normal ordering rules are applied, from left-to-right and top-to-bottom as we saw earlier.The !important keyword is used to override all specificity and ordering rules and has enough powers to give frontend devs sleepless nights. So use it responsibly, as Uncle Ben told Peter Parker.In some cases such as when using a CSS framework like Bootstrap, using the !important keyword to enforce your own styles can be acceptable.Phew, that’s a lot of new information.Understanding the cascade might seem a bit over-kill but now that you know how much power it wields, I bet you will be writing cleaner and more organized CSS. Your future self will thank you when you revisit your CSS to change some styles later. 😁Happy Coding!",html,https://levelup.gitconnected.com/the-c-in-css-why-is-it-important-8d3900d6827b?source=tag_archive---------3-----------------------
Squarespace and Custom Tables with HTML and CSS,"ForTheLoveOfTechJun 1, 2020·3 min readSome things to consider before continuing:On your Squarespace page, add a Code block in the area you want your table to appear.After you add the Code block, the editor should look thisFrom that view, you can start adding your table HTML. You can also add your CSS inline here, but to keep your code clean, we add CSS in the Design tab(more on that later).An example of a table you can add inside of the Code block:Your Code block should look like this. Keep in mind your table won’t look like this right away without your CSS.Now that you have added your HTML we can move onto the Design tab located on the left side of your Squarespace menu. Click on Design > Custom CSS.The Custom CSS section lets you style your custom HTML using classes. The CSS below styles my table with color, URL underline, background colors, and a range of other styles.Here is the full CSS for my table:Click save at the top and you’re all done adding your custom table and styling it.",html,https://medium.com/@fortheloveoftech/squarespace-and-custom-tables-with-html-and-css-5f574d7f2820?source=tag_archive---------3-----------------------
Rendering Unicode Symbol In React,"James CoxJun 2, 2020·3 min readRecently I built a simple SPA (single page application), https://mathmaster.netlify.app/, with React. When quarantine began, and suddenly every student was given home-school status, we all had to get creative! I put my improving React skills to good use, and built a math generator game aimed at children my daughter’s grade level (she’s 7, just finishing first grade).One random issue I discovered was rendering Unicode symbols in React isn’t the same as HTML (or CSS). I wanted the “divide” symbol, U+00F7, instead of the JavaScript operator, “/”, which was pretty clear to me when my daughter had no idea what “/” meant in terms of the math problems she was used to seeing.There are many ways you can encode a Unicode symbol. At first I didn’t realize this, so I looked for the obvious ones: HTML and CSS. In CSS, the divide symbol is represented like:HTML:But as I FINALLY figured out, you need to format the Unicode specifically with the JavaScript encoding.Notice inside the curly braces the unicode is inside a string, and the “U” is after the backslack and lowercased. Here is how my JSX looks altogether for the correctly solved success message:Renders the following in the browser:And for redundancy’s sake, this is how I code the divide symbol when it’s displayed as the problem to be solved:Which renders:I figured this out by basic trial and error. Because JSX is like a hybrid of JavaScript and HTML, I thought I could try different forms of encoding, specifically the HTML versions described in this handy reference: https://www.htmlsymbols.xyz/unicode/U+00F7gets you something like:It might seem obvious to others, but I admit it took me at least an hour to first FIND the JavaScript formatting for Unicode, and then how to implement it correctly (inside a string, inside the curly braces). I hope this post might save someone else the hour or so it took me to figure this out!",html,https://medium.com/@jamesncox/rendering-unicode-symbol-in-react-76b1b4ffcee4?source=tag_archive---------0-----------------------
5 HTML basic Tags.,"Muhammad TalhaJun 2, 2020·2 min readThe use of these basic five tags help in learning about HTML.After attending EID-UL-FITER in a happy and enjoying way now i am completely settled for my classes continuation.So, today i will talk about the basic five codes that i used in my project with some snaps from there.How the subject is written in HTML.How it look like.Five Tags which are used are explained below.No:1=The use of <big> tag is for making a word or sentence a little greater in size from the normal text.No:2=The use of <cite> tag is for writing a sentence or word in italics fonts.No:3=<b> tag is used for making a word bold.No:4=<br> tag is used for cutting a proper sentence into two parts and the cut part continues in another new line below.No:5=The <del> tag is used for deleting a text which is unnecessary or focused text.All these tags are practically shown in the snaps above. And the other tags are completely explained in my story I published before.",html,https://medium.com/@talhaafzal7262/5-html-basic-tags-9c418d979863?source=tag_archive---------12-----------------------
Bootstrap 5 Alpha — What’s New & What Changed?,"Umair ArshadJun 25, 2020·2 min readBootstrap 5 alpha is out so it is known as 5.0. I will just share that has been changed. You can explore it on your own more.LogoThe very first notable change is the change in the identity of bootstrap with a new logo. The new logo is simple and valuableJQuery dropped Completly With respect to the modern web development jquery is completely dropped now with the latest integration of bootstrap. This is nothing but positivity in terms of news that now bootstrap will be lightweightCSS custom propertiesBy using CSS custom properties Bootstrap 5 dropped support for Internet Explorer. Now Bootstrap 5.0 added a handful of components and layout options. See the exampleImproved customizing docsIn the new Bootstrap 5.0 alpha bootstrap providing more support as compared to old documentationsUpdated formsEvery checkbox, radio, select, file, range, and more includes a custom appearance to unify the style and behavior of form controls across OS and browser(Bootstrap Documentation)Utilities APIIn v5 bootstrap carried the v4 global classes. Bu using the API based approach they created a language and syntax in SASS",jquery,https://medium.com/@umairarshadbutt/bootstrap-5-alpha-whats-new-what-changed-723c934ee787?source=tag_archive---------0-----------------------
Some preliminary discussion about jQuery,"Mohammad ShaifJun 9, 2020·4 min readWhat is jQueryjQuery is a fast and concise JavaScript Library that simplifies HTML document traversing, event handling, animations and Ajax interactions for rapid web development.How to use jQueryHTML document traversing, event handling, animating and Ajax interactions for Rapid Web Development.There are two versions of jQuery available for downloading:JQuery Selectors:jQuery selectors are one of the most important parts of the jQuery library.jQuery selectors allow you to select and manipulate HTML element(s).jQuery selectors are used to “find” (or select) HTML elements based on their name, id, classes, types, attributes, values of attributes and much more. It’s based on the existing CSS Selectors, and in addition, it has some own custom selectors.Here we can see that we are executing an event by selecting the id in a p tag as a selector.JQuery Events:jQuery is tailor-made to respond to events in an HTML page. All the different visitors’ actions that a web page can respond to are called events.An event represents a precise moment when something happens.Examples:Here, when the document is ready, you need to run a function where you can run a function by clicking the dabble button, where you can toggle the class div.Difference between focus and blurFor this, at first, we think that when we take a picture in one place, we focus on our face and then the other places on the side become blurred. In the same way, when I select something in the document window, it focuses and blurs everything else.JQuery EffectsThe hide and show seen in the picture above are usually Effects. Now we will know a little detail.We can also use more than one effect at a time if we want. For this, we can write the name of another effect with a dot after writing an effect. To create an animation with effects, we can specify the time inside the effect bracket by considering 1000 = 1 second as shown in the figure.jQuery Many effects work togetherHere we have used a p tag to work with many effects at once. Here we can write many effects with commas by holding the p tag with on and then with the second bracket inside the first bracket. Here we can also see that this has been used. By which this element is indicated.jQuery HTMLjQuery contains powerful methods for changing and manipulating HTML elements and attributes. One very important part of jQuery is the possibility to manipulate the DOM.jQuery comes with a bunch of DOM related methods that make it easy to access and manipulate elements and attributes.What is the difference between jQuery append() and prepend()Inserts content at the beginning of the selected elements is called prepend and Inserts content at the end of the selected elements is called append.How to change the serial number of ‘ol’ by adding prepend to the list item.When we add something to the list item of an ‘ol’ using prepend, it will be added to the front. This will change the number of the list item. The previous serial number was one. Now that the new element has been added, the serial number of the new element will be one.What is the difference between jQuery remove() and empty()remove() means — Removes the selected element (and its child elements) empty() means — Removes the child elements from the selected elementSuppose there are some people in a flat, the job of the remover is to make the flat disappear, and the job of the empty is to move the people in the flat.If there is any parent tag of remove HTML, it will be deleted. But empty, he will only delete the child inside the parent.",jquery,https://medium.com/@mohammadeshaif28/some-preliminary-discussion-about-jquery-b0561b3397fc?source=tag_archive---------4-----------------------
The topics I leart today,"MD ARIF HOSSAINJun 9, 2020·5 min readToday’s topicsjQueryOne important thing to know is that jQuery is just a JavaScript library. All the power of jQuery is accessed via JavaScript, so having a strong grasp of JavaScript is essential for understanding, structuring, and debugging your code.To change this text to “Hello World!”, we can use JavaScript code like this:But in jQuery, we can use like this:Isn’t it good? But maybe you don’t need to learn it in 2020 if you using any framework like Vue, Angular, etc and also in React library. But you can look if you want to know about jQuery.Downloading jQueryGo this link to download jQuery: https://jquery.com/download/Please Download the compressed version of jQuery. You can’t able to edit the file in compressed version, but we also don’t need to edit the file.You can also download jQuery using npmor yarnAdding jQuery to your siteYou can rename your jQuery file as jquery.js and then add to html file like the above image.We target here at document, means full page. When document is ready, we call a anonymus function. Inside the function we target body tag and add a h1 tag using append() method.Selectors and FiltersLet’s try in a tag:Here we target h1 tag and add css() method to add css for the tag. Inside the css() method we pass two parameter. First one is property name and second one is value of the property. We add border with 3px dotted type with green color. I think you can understand.Let’s try another thingYou can see, if there have more h1 tag, it will access all h1 tag together. How can we specific one?For first one, h1:firstFor last one, h1:lastWe can target using class and id insted of tag name.Replacing ContentCreate a div with id textBox .Now we can replace the text and also add another tag by replacing Hello jQuery text.You can see, we declare a constant named newText and set the value of it a paragraph tag. Then we add a h1 tag inside the p tag using append() method. After that we call the div with it’s id and set the newText constant into the intire div tag using html() method. You can look at the dev tool to clear the concept.Handling EventsHere we style our div and make two event using on() method. For click event, we make a function and also for mouseleave event.Default value:After click the div:For leave the mouse:Hide/Show Eventswe use show() method to show and hide() method to hide the div.Default result:After click the show:Now if you click on hide, the div will hide.AjaxWhat is Ajax?Ajax is a JavaScript library. AJAX stands for Asynchronous JavaScript And XML.AJAX just uses a combination of:My goal to learn recently:",jquery,https://medium.com/@proarif/the-topics-i-leart-today-7abda64a2f0e?source=tag_archive---------2-----------------------
Getting Environment Ready for jQuery and React Native,"Sagor MahtabJun 10, 2020·2 min readjQuery is a small library of JavaScript. It was tremendously popular back in the first half of the last decade. Later it started to became less necessary to use jQuery as JavaScript itself has adopted many of the jQuery features and became more powerful with JavaScript’s latest updates. But still nowadays we may require jQuery for working with many of jQuery’s elegant plugins. So, let’s start using jQuery.You have 2 options to install jQuery in your projects.Method 1 is recommended for your development and learning purpose because it may help you if you’re out of internet for any reason and Method 2 is recommended to use when you’re deploying your site as the CDN file may already be downloaded on the client’s side from another website using jQuery and thus your client’s browser would not need to download the CDN file again which will load your site more faster.Before we start executing our jQuery files we would want to make sure that our jQuery file runs after loading the DOM. On Vanilla JS we have something like this.Unfortunately, the code doesn’t run until all images are finished downloading, including banner ads. To run code as soon as the document is ready to be manipulated, jQuery has a statement known as the ready event:React Native is used for creating both Android and iOS native apps. To work with React Native real quick use the expo framework. First of all download the expo framework globally byThis will take some time. Then runand you’re ready!",jquery,https://medium.com/@sagormahtab/getting-environment-ready-for-jquery-and-react-native-323cf84390c8?source=tag_archive---------0-----------------------
Some Topics to be a better Front-End Developer,"Sayed DelowarJun 10, 2020·2 min readReact-Native is a framework to build apps for Android and iOS. It allows us to create real native apps using JavaScript and React.React Native apps are faster to build, easier to use, offer better quality than Hybrid apps and are cheaper than Native app, without having to develop one.Here are the general steps for setting up the environment -jQuery is a lightweight, “write less, do more” JavaScript library. jQuery library contains the following features -There are several ways to start using jQuery on our web application -Google CDN:Here -Example -$(this).hide() - hides the current element.$(""p"").hide() - hides all <p> elements.AjAX just uses a combination of:Sass Example -That’s all for today…Thank you.",jquery,https://medium.com/@sayed.buet97/some-topics-to-be-a-better-front-end-developer-e16f8c66f6f4?source=tag_archive---------1-----------------------
How to GET and POST images Asynchronously using AJAX in Django — in Detail,"Jaya Shankar BandaruJun 19, 2020·4 min readHey Guys 👋 ,I am Jaya Shankar, a Coding enthusiast, last week I was working on a project related to building a website using Django in my project I needed to send and request Images asynchronously (basically chatting application). I googled🔎 to learn it but I couldn’t find everything in one place so I decided to write this article now let’s get our hands dirty💻I assume that whoever reading this article has some basic knowledge in Python, Javascript and Django Framework, let’s get startedThe first thing we do whenever we need to use images in the Django project is to set MEDIA_ROOT & MEDIA_URL just copy the below code and paste it in your settings.py fileJust to give more sense the next section is the code that I wrote which is needed to move furtherand also add the following line in urls.py so it can know where the media files are locatedif its first time for you to add MEDIA_URL then I would suggest to take the above code and look where the images are being savedThis is a basic Django model I created which has only one field (‘ImageField’)here we use upload_to option to specify a subdirectory of MEDIA_ROOT to use for uploaded files.The above one is a basic form for the ImageUpload model we just createdit's a good practice to use forms if you are not familiar my advice is to learn them because they make our work more easyFor now views.py as only one function(index_view) whose sole purpose is to create ImageForm object and render it in ‘index.html’Here we added two buttons(#submit,#showImages) for which we will be writing functions later and be sure to set the form enctype=“ multipart/form-data” and include csrf_tokenA screengrab of the rendered HTML:now let’s write some JavaScript to upload images to the server via AJAX to keep things simple I am using JQuery for AJAX so be sure to include JQuery script in your HTMLIn index.jswe will be uploading images only when a user clicks submit button-> processData is set to false so that JQuery Lib does not stringify the dataFormData is an object to represent HTML form data.make sure to add the route to ‘/add_image’ in urls.pyAt the end of this article, you can find my ‘urls.py’ fileNow let’s go to our views.py where we will be writing code to store the uploaded images into the databaseIn views.pyIn index.jswe will be making AJAX request for images when a user clicks ‘ShowImages’. buttonnow let’s write code in views.py to handle the requesthere in the above function first we get all the images as QuerySet and store them in ‘images’ and then since we need only where the images are located (URL) so we append all the URLs into a list (image_urls) and return the data as JSON to the clientif we go to the JS we can see on success(getting back a response) the success function gets executed in the just function we are just appending the images into the HTML fileThe below file is part of the one of the apps folderMy apologies for not including error handling because I wanted to keep the code as short as possible ✌️if you have any doubts and found any errors /better way to do in code please mail me: bjayashankar2102@gmail.comif you are interested in looking at the code: click here",jquery,https://medium.com/@bjayashankar2102/how-to-get-and-post-images-asynchronously-using-ajax-in-django-in-detail-17f25f4c3dca?source=tag_archive---------0-----------------------
JavaScript vs jQuery — Learn DOM manipulation without using jQuery,"Originally posted on https://faisalrashid.tech/blogs/JavaScript-vs-jQueryjQuery has been the savior for so many new and coming Web Developers, including myself. If you wanted to learn Web Development back in the day, learning jQuery was an absolute given. This was mainly because jQuery took much of the cross-browser compatibility issues out and enabled developers to write code without having to worry about whether the features that they are implementing will work on all browsers.But with improvements in browser standards and most of jQuery’s API’s integrated into JavaScript, jQuery has become a little redundant. Moreover, with native browser API’s it is so much easier to debug code, and being native, most of these API’s offer better performance than jQuery’s API’s. Besides, you will have one less library to add to your script imports. If you’re still not sold on parting with jQuery maybe this answer will help.So, if you’re considering a move away from jQuery, I have compiled a list of common jQuery methods and API’s that people use, with their Vanilla JS alternatives (Vanilla JS is a fancy name for plain JavaScript code without the use of any libraries). Let’s dive in!The bread and butter of jQuery is it’s amazing ability to query DOM elements. This is demonstrated below:However, you can achieve the same thing with JavaScript using it document.querySelector() and document.querySelectorAll() methods. Below is their implementation.This method always returns the first element that matches the selector. If you’re expecting to query multiple elements, you will have to use the querySelectorAll() method.querySelectoryAll() will return an array of matched elements, hence the use of forEach to iterate over each element..html() method of jQuery is used to get or set the HTML content of a node/element.To do the same in JavaScript you can use the .innerHtml property of any HTML element.Let’s say you need to get the HTML content of a div element with class main-div. You could do this in JavaScript like this.Similarly to set the HTML content of the same div, you could do something like this:Remember, if you want to change the Html of multiple elements with the same selector, you can use the querySelectorAll() method like this:Notice that I pass in the index to the anonymous function here. You don’t really need it in this case, but it’s a good practice to keep the index of each element of the array handy. You never know when you may need it..val() method of jQuery is used to get or set the value of an input element.To get or set the value of an input element in JavaScript, you can use the value property of that element.To set the value of an input element with class input-box to ‘new’To get the value of the same elementjQuery offers .text() method to get all the content of an element without the markup. e.g., Using jQuery to get the text of <p>Hi</p> element will return the string ‘Hi’. This can be very useful when you’re trying to get the content actually visible in the web-page.That being said, every HTML element has a property “innerText” that essentially holds the same value.One of the most exciting features of jQuery is the ability to modify the styling of our DOM. This is highly useful when you’re trying to have visual feedback to a user interaction.Modifying styles can be done by one of two ways:Adding/Removing Classes:To add or remove a class you will most definitely use .addClass() or .removeClass() methods that ship with jQuery.To do the same thing in JavaScript, you can use the classList property of an element. This can be done as shown below.To add the class testClass to an element with Id testElementTo remove the class testClass from the element with Id testElementAdding/Modifying CSS:To directly change the CSS of an element we use the .css() method provided by jQuery. Once again, this is fairly simple to do in JavaScript.To set a style for an element, we simply set the value of that style as exposed by the style property of that element.This will set the color of the element with Id testElement to white.Note: Since the styles of elements are exposed as JavaScript object properties, you won’t be able to use properties such as background-color directly as there is a “-” in the property name. In these situations, you should simply use the camelCased version of these properties. e.g.,This will set the background-color of element with Id testElement to black.Setting and retrieving values of the attributes of HTML elements is once again very useful when developing a web application. You may need to get the value of a data- attribute that is storing important data while working with your application. jQuery provides this ability by exposing .attr() method. You can use it with one parameter to get the value of an attribute or use an overload with two parameters to set the value of an HTML attribute.In JavaScript, every attribute of an HTML element is exposed as a property of that element. This means that to set the value of the src attribute of an img element, you can simply do the following:It’s that simple. For attributes such as data- you can use another approach,To get the value of the same element,The getAttribute() and setAttribute() bear a striking resemblance to jQuery’s attr() method. So, getting used to these shouldn’t be that difficult.Once again, two of the most widely used jQuery methods are the .show() and .hide() methods. These methods do exactly what they say they do, hide or show HTML elements. In their essence, these methods are simply modifying the display style property of elements. So, we can use simple style modification to achieve the same thing.To hide an element:To show a hidden elementOne of my favorite and most used jQuery functionality is sending HTTP requests using AJAX. However, we have to understand that jQuery.ajax() is a wrapper around existing JavaScript functionality. There is no doubt that jQuery makes using AJAX a breeze, but it’s not why we’re here, is it?To send a POST request to the URL ‘home/sendmessage’ with the data {id: 23, name: ‘faisal’}, we’d do the following:That’s it. Your post request will be sent to the server.If you’re a jQuery developer, you will know that these aren’t all the methods and features that jQuery ships with. There are definitely more, but I’ve simply tried to address the most common ones just to give you an idea that life without jQuery in the browser is possible.I hope that it’s enough to push you towards using more and more JavaScript.",jquery,https://levelup.gitconnected.com/javascript-vs-jquery-learn-dom-manipulation-without-using-jquery-d0ca948c2c6b?source=tag_archive---------1-----------------------
Skill Upgradation For Job Satisfaction,"AD BARMANJun 10, 2020·12 min readPHP:The PHP Hypertext Preprocessor (PHP) is a programming language that allows web developers to create dynamic content that interacts with databases. PHP is basically used for developing web based software applications. This tutorial helps you to build your base with PHP.Why to Learn PHP?The key advantages of learning PHP:>>PHP is a recursive acronym for “PHP: Hypertext Preprocessor”.>>PHP is a server side scripting language that is embedded in HTML. It is used to manage dynamic content, databases, session tracking, even build entire e-commerce sites.>>It is integrated with a number of popular databases, including MySQL, PostgreSQL, Oracle, Sybase, Informix, and Microsoft SQL Server.>>PHP is pleasingly zippy in its execution, especially when compiled as an Apache module on the Unix side. The MySQL server, once started, executes even very complex queries with huge result sets in record-setting time.>>PHP supports a large number of major protocols such as POP3, IMAP, and LDAP. PHP4 added support for Java and distributed object architectures (COM and CORBA), making n-tier development a possibility for the first time.>>PHP is forgiving: PHP language tries to be as forgiving as possible.>>PHP Syntax is C-Like.Characteristics of PHP:Five important characteristics make PHP’s practical nature possible − — Simplicity — Efficiency — Security — Flexibility — FamiliarityHello World using PHP:<html> <head> <title>Hello World</title> </head>  <body> <?php echo “Hello, World!”;?> </body></html>Applications of PHP:>>PHP performs system functions, i.e. from files on a system it can create, open, read, write, and close them.>>PHP can handle forms, i.e. gather data from files, save data to a file, through email you can send data, return data to the user.>>We add, delete, modify elements within our database through PHP.>>Access cookies variables and set cookies.>>Using PHP, you can restrict users to access some pages of your website.>>It can encrypt data.PHP — Variable Types:PHP has a total of eight data types which we use to construct our variables −Integers − are whole numbers, without a decimal point, like 4195.Doubles − are floating-point numbers, like 3.14159 or 49.1.Booleans − have only two possible values either true or false.NULL − is a special type that only has one value: NULL.Strings − are sequences of characters, like ‘PHP supports string operations.’Arrays − are named and indexed collections of other values.Objects − are instances of programmer-defined classes, which can package up both other kinds of values and functions that are specific to the class.Resources − are special variables that hold references to resources external to PHP (such as database connections).PHP — Operator Types:PHP language supports following type of operators. — Arithmetic Operators — Comparison Operators — Logical (or Relational) Operators — Assignment Operators — Conditional (or ternary) OperatorsPHP — Loop Types:Loops in PHP are used to execute the same block of code a specified number of times. PHP supports following four loop types.for − loops through a block of code a specified number of times.while − loops through a block of code if and as long as a specified condition is true.do…while − loops through a block of code once, and then repeats the loop as long as a special condition is true.foreach − loops through a block of code for each element in an array.PHP — Arrays:There are three different kinds of arrays and each array value is accessed using an ID which is called array index.Numeric array − An array with a numeric index. Values are stored and accessed in linear fashion.Associative array − An array with strings as an index. This store element values in association with key values rather than in strict linear index order.Multidimensional array − An array containing one or more arrays and values are accessed using multiple indices.Laravel:Laravel is an open-source PHP framework, which is robust and easy to understand. It follows a model-view-controller design pattern. Laravel reuses the existing components of different frameworks which helps in creating a web application. The web application thus designed is more structured and pragmatic.Advantages of Laravel:Laravel offers you the following advantages, when you are designing a web application based on it −>>The web application becomes more scalable, owing to the Laravel framework.>>Considerable time is saved in designing the web application, since Laravel reuses the components from other framework in developing web application.>>It includes namespaces and interfaces, thus helps to organize and manage resources.Features of Laravel:ModularityLaravel provides 20 built in libraries and modules which helps in enhancement of the application. Every module is integrated with Composer dependency manager which eases updates.TestabilityLaravel includes features and helpers which helps in testing through various test cases. This feature helps in maintaining the code as per the requirements.RoutingLaravel provides a flexible approach to the user to define routes in the web application. Routing helps to scale the application in a better way and increases its performance.Configuration ManagementA web application designed in Laravel will be running on different environments, which means that there will be a constant change in its configuration. Laravel provides a consistent approach to handle the configuration in an efficient way.Query Builder and ORMLaravel incorporates a query builder that helps in querying databases using various simple chain methods. It provides ORM (Object Relational Mapper) and ActiveRecord implementation called Eloquent.Schema BuilderSchema Builder maintains the database definitions and schema in PHP code. It also maintains a track of changes with respect to database migrations.Template EngineLaravel uses the Blade Template engine, a lightweight template language used to design hierarchical blocks and layouts with predefined blocks that include dynamic content.E-mailLaravel includes a mail class which helps in sending mail with rich content and attachments from the web application.AuthenticationUser authentication is a common feature in web applications. Laravel eases designing authentication as it includes features such as register, forgot password and send password reminders.RedisLaravel uses Redis to connect to an existing session and general-purpose cache. Redis interacts with session directly.QueuesLaravel includes queue services like emailing a large number of users or a specified Cron job. These queues help in completing tasks in an easier manner without waiting for the previous task to be completed.Event and Command BusLaravel 5.1 includes Command Bus which helps in executing commands and dispatch events in a simple way. The commands in the Laravel act as per the application’s lifecycle.Laravel — Application Structure:The application structure in Laravel is basically the structure of folders, sub-folders and files included in a project. The analysis of folders and files, along with their functional aspects is given below −AppIt is the application folder and includes the entire source code of the project. It contains events, exceptions and middleware declaration. The app folder comprises various subfolders as explained below −Console — Console includes the artisan commands necessary for Laravel.Events — This folder includes all the events for the project.Exceptions — This folder contains all the methods needed to handle exceptions.Http — The Http folder has sub-folders for controllers, middleware and application requests.Jobs — The Jobs directory maintains the activities queued for Laravel application.Listeners — Listeners are event-dependent and they include methods that are used to handle events and exceptions.Policies — Policies are the PHP classes which includes the authorization logic.Providers — This folder includes all the service providers required to register events for core servers and to configure a Laravel application.Bootstrap — This folder encloses all the application bootstrap scripts. It contains a sub-folder namely cache, which includes all the files associated for caching a web application.Config — The config folder includes various configurations and associated parameters required for the smooth functioning of a Laravel application. Database — This directory includes various parameters for database functionalities.Public — It is the root folder which helps in initializing the Laravel application.Resources — Resources directory contains the files which enhance your web application.Storage — This is the folder that stores all the logs and necessary files that are needed frequently when a Laravel project is running.Tests — All the unit test cases are included in this directory.Vendor — Laravel is completely based on Composer dependencies, for example, to install Laravel setup or to include third-party libraries, etc. The Vendor folder includes all the composer dependencies.Sass:Sass (which stands for Syntactically Awesome Style Sheets) is an extension to CSS. It doesn’t really change what CSS can do, we won’t suddenly be able to use Adobe Photoshop blend modes or anything-but it makes writing CSS a whole lot easier. Saas includes various features such as variables, nested rules, mixins, inline imports, built-in functions to manipulate color and other values, all with a fully CSS-compatible syntax.Why to Use SASS?>>It is a pre-processing language that provides indented syntax (its own syntax) for CSS.>>It provides some features, which are used for creating stylesheets that allow writing code more efficiently and is easy to maintain.>>It is a superset of CSS, which means it contains all the features of CSS and is an open-source pre-processor, coded in Ruby.>>It provides the document style in a good, structured format than flat CSS. It uses re-usable methods, logic statements and some of the built-in functions such as color manipulation, mathematics and parameter lists.Features of SASS:>>It is more stable, powerful, and compatible with versions of CSS.>>It is a superset of CSS and is based on JavaScript.>>It is known as syntactic sugar for CSS, which means it makes an easier way for users to read or express the things more clearly.>>It uses its own syntax and compiles to readable CSS.>>We can easily write CSS in less code within less time.>>It is an open-source pre-processor, which is interpreted into CSS.Webpack:Webpack is a highly famed module bundler for JavaScript applications. Since Webpack is an open-source, this makes it free of cost. Although it is a bundler suited for JavaScript, it can also transform front-end assets such as CSS, HTML, and images if required plugins are available. Webpack allows us to create our code in modules and it compiles all packages into one or multiple bundles or modules. With Webpack, we can use the modules without thinking too much about support. It is an extremely powerful piece of technology that can do impressive things with loaders and plugins.Why you should learn Webpack? — Beneficial for building complex front-end applications — Elimination of dead assets — Splitting code made easier — Controlling how assets are processed — Stable production deploys — Excellent speeds when used accuratelyRuby:Ruby is the successful combination of − — Smalltalk’s conceptual elegance, — Python’s ease of use and learning, and — Perl’s pragmatism.Ruby is  — A high-level programming language. — Interpreted like Perl, Python, Tcl/TK. — Object-oriented like Smalltalk, Eiffel, Ada, Java.Why Ruby?Ruby originated in Japan and now it is gaining popularity in US and Europe as well. The following factors contribute towards its popularity − — Easy to learn — Open-source (very liberal license) — Rich libraries — Very easy to extend — Truly object-oriented — Less coding with fewer bugs — Helpful communitySample Ruby Code:Here is a sample Ruby code to print “Hello Ruby”# The Hello Classclass Hello  def initialize( name ) @name = name.capitalize enddef salute puts “Hello #{@name}!” end end# Create a new objecth = Hello.new(“Ruby”)# Output “Hello Ruby!”h.saluteRails: — An extremely productive web-application framework. — Written in Ruby by David Heinemeier Hansson. — We could develop a web application at least ten times faster with Rails than we could with a typical Java framework. — An open-source Ruby framework for developing database-backed web applications. — Configure your code with Database Schema. — No compilation phase required.Rails Strengths: — Metaprogramming — Active Record — Convention over configuration — Scaffolding — Built-in testing — Three environmentsJquery:jQuery is a fast and concise JavaScript Library created by John Resig in 2006 with a nice motto: Write less, do more. jQuery simplifies HTML document traversing, event handling, animating, and Ajax interactions for rapid web development. jQuery is a JavaScript toolkit designed to simplify various tasks by writing less code.Core Features of jQuery:DOM manipulation − The jQuery made it easy to select DOM elements, negotiate them and modifying their content by using a cross-browser open source selector engine called Sizzle.Event handling − The jQuery offers an elegant way to capture a wide variety of events, such as a user clicking on a link, without the need to clutter the HTML code itself with event handlers.AJAX Support − The jQuery helps you a lot to develop a responsive and feature-rich site using AJAX technology.Animations − The jQuery comes with plenty of built-in animation effects which you can use in your websites.Lightweight − The jQuery is very lightweight library — about 19KB in size (Minified and gzipped).Cross Browser Support − The jQuery has cross-browser support, and works well in IE 6.0+, FF 2.0+, Safari 3.0+, Chrome and Opera 9.0+Latest Technology − The jQuery supports CSS3 selectors and basic XPath syntax.Selenium:Selenium is an open-source and a portable automated software testing tool for testing web applications. It has capabilities to operate across different browsers and operating systems. Selenium is not just a single tool but a set of tools that helps testers to automate web-based applications more efficiently.Advantages of Selenium: — Selenium is an open-source tool. — Can be extended for various technologies that expose DOM. — Has capabilities to execute scripts across different browsers. — Can execute scripts on various operating systems. — Supports mobile devices. — Executes tests within the browser, so focus is NOT required while script execution is in progress. — Can execute tests in parallel with the use of Selenium Grids.GO:Go is a general-purpose language designed with systems programming in mind. It was initially developed at Google in the year 2007 by Robert Griesemer, Rob Pike, and Ken Thompson. It is strongly and statically typed, provides inbuilt support for garbage collection, and supports concurrent programming.Features of Go Programming:The most important features of Go programming are listed below −>>Support for environment adopting patterns similar to dynamic languages. For example, type inference (x := 0 is valid declaration of a variable x of type int)>>Compilation time is fast.>>Inbuilt concurrency support: lightweight processes (via go routines), channels, select statement.>>Go programs are simple, concise, and safe.>>Support for Interfaces and Type embedding.>>Production of statically linked native binaries without external dependencies.TypeScript:TypeScript is JavaScript for application-scale development.” TypeScript is a strongly typed, object oriented, compiled language. It was designed by Anders Hejlsberg (designer of C#) at Microsoft. TypeScript is both a language and a set of tools. TypeScript is a typed superset of JavaScript compiled to JavaScript.Features of TypeScript:TypeScript is just JavaScript. TypeScript starts with JavaScript and ends with JavaScript. Typescript adopts the basic building blocks of your program from JavaScript. Hence, you only need to know JavaScript to use TypeScript. All TypeScript code is converted into its JavaScript equivalent for the purpose of execution.TypeScript supports other JS libraries. Compiled TypeScript can be consumed from any JavaScript code. TypeScript-generated JavaScript can reuse all of the existing JavaScript frameworks, tools, and libraries.JavaScript is TypeScript. This means that any valid .js file can be renamed to .ts and compiled with other TypeScript files.TypeScript is portable. TypeScript is portable across browsers, devices, and operating systems. It can run on any environment that JavaScript runs on. Unlike its counterparts, TypeScript doesn’t need a dedicated VM or a specific runtime environment to execute.Benefits of TypeScript:Compilation − JavaScript is an interpreted language. Hence, it needs to be run to test that it is valid. It means you write all the codes just to find no output, in case there is an error. Hence, you have to spend hours trying to find bugs in the code. The TypeScript transpiler provides the error-checking feature. TypeScript will compile the code and generate compilation errors if it finds some sort of syntax errors. This helps to highlight errors before the script is run.Strong Static Typing − JavaScript is not strongly typed. TypeScript comes with an optional static typing and types inference system through the TLS (TypeScript Language Service). The type of a variable declared with no type may be inferred by the TLS based on its value.TypeScript supports type definitions for existing JavaScript libraries. TypeScript Definition file (with .d.ts extension) provides definition for external JavaScript libraries. Hence, the TypeScript code can contain these libraries.TypeScript supports Object-Oriented Programming concepts like classes, interfaces, inheritance, etc.Components of TypeScript:At its heart, TypeScript has the following three components −Language − It comprises of the syntax, keywords, and type annotations.The TypeScript Compiler − The TypeScript compiler (tsc) converts the instructions written in TypeScript to its JavaScript equivalent.The TypeScript Language Service − The “Language Service” exposes an additional layer around the core compiler pipeline that is editor-like applications. The language service supports the common set of typical editor operations like statement completion, signature help, code formatting and outlining, colorization, etc.",jquery,https://medium.com/@adbarman/skill-upgradation-for-job-satisfaction-2bd041adefab?source=tag_archive---------2-----------------------
Install Bootstrap & jQuery in Angular,"Md.Sayed AhammedJun 2, 2020·1 min readBy using the command line1. Installation of jQuerynpm install jquery — save2. Installation of Bootstrapnpm install bootstrap— saveAfter installation of libraries includes jQuery and Bootstrap in your project.Goto project root directory and open angular.json file.“styles”: [ “node_modules/bootstrap/dist/css/bootstrap.min.css”, “src/styles.scss” ], “scripts”: [ “node_modules/jquery/dist/jquery.min.js”, “node_modules/bootstrap/dist/js/bootstrap.min.js” ]Successfully you have included jQuery and Bootstrap libraries. After completing the restart angular server.",jquery,https://medium.com/@sayed.cse17/install-bootstrap-jin-angular-a33701880417?source=tag_archive---------1-----------------------
Laravel StateCity Package,"Uyo-obong AkpanJun 12, 2020·1 min readI will love to share my php/ laravel package with youI chip away at ventures that require States and their particular cities in Nigeria. Each time I experience this, I will google to check whether I will gat an answer that suite my requirements. For the most part, I will see world, state, cities, however I needn’t bother with all the nations.That is the thing that immediate me to assemble all States in Nigeria and their separate Cities. I thoroughly comprehend that most Nigeria Developers face this issue, I chose to bundle for others to utilize too.On the off chance that you need to add to the library you welcome, simply fork the repo and roll out your improvements at that point push and send a PR I will survey and consolidation.Visit https://packagist.org/packages/uyoobonga/statecityor https://github.com/uyo-obong/statecity",jquery,https://medium.com/@uyoobonga/laravel-statecity-package-d7042f968bf3?source=tag_archive---------0-----------------------
"HTML5 | CSS3 | Bootstrap4 | ES(5,6) | J query + Ajax | SQL | MySQL | PHP","Muhammad ShoaibJun 9, 2020·2 min readMore than 1 year of experience in developing web applications in PHP. I have done a lot of projects covering a vast variety of domains related to PHP Web application design and development | PHP Website Customization | Custom functionality implementation and PHP web maintenance support.EXPERTISE:✔ HTML | HTML5 ✔ CSS | CSS3 | Animations✔ Bootstrap | Bootstrap 4 ✔ Jquery + Ajax ✔ JavaScript (ES5/ES6)✔ SQL | Mysql ✔ PHP✔ UI/UX, Responsive✔ GitA Demo page:A birds-eye-view of some projects:Worry about my work? Here is the gitgithub.comHopefully, you enjoyed the blog.Have a nice timeBa bye.",jquery,https://medium.com/@muhammad.shoaib135/html5-css3-bootstrap4-es-5-6-j-query-ajax-sql-mysql-php-1d1b96f5250b?source=tag_archive---------6-----------------------
Learn jQuery the Easy-way?,"JamJun 20, 2020·2 min readAccording to Wikipedia, “jQuery is a JavaScript library designed to simplify HTML DOM tree traversal and manipulation”.The purpose of jQuery is to make it much easier to use JavaScript on your website.Before you start studying jQuery, you should have a basic knowledge of:Syntax:Suppose, you want to select a class. you need to write a class name with a dot (.)jQuery has many functions for DOM Manipulation. such as: HTML(), addClass(), after() etc.Example:jQuery has many events, like: click, focus, hoverThese events are often self-explanatory and normally similar to javascript events.Example:jQuery ajax is similar to HTML get and post request. but the request header is a little different with ajax, we can make requests to the server, receive responses without reloading the page. That is not possible with normal HTML form submit.Example:Always use the jQuery event inside a document.ready and write jquery code after adding jQuery Reference.If you write the jQuery code before adding jQuery Reference, jQuery code will not work.jQuery is very popular and widely used for its simplicity. Happy coding.",jquery,https://medium.com/@moulayjam/learn-jquery-the-easy-way-44ac035158a3?source=tag_archive---------0-----------------------
How to validate a form using AJAX,"Develoweb CoJun 11, 2020·4 min readUsing AJAX will allow us to validate the form without refreshing the page. We’ll also be using jQuery and PHP for the validation. A backend language like PHP is essential to make your form secure, so that a user can’t just manipulate the code through ‘Inspect element’.AJAX and PHP requires a server for it to work, so make sure you have either a local server or an online server running.First, we’re going to create a basic HTML contact form in our index.html file, with inputs for name, email and message, and a submit button.The form ‘action’ is set to form.php, because the backend validation using PHP will be in a separate form called form.php (or any other name you’d like).How you want to style your form is entirely up to you, and I’m just going to use some basic styling to make it look nicer.The more important styles are our styling for error and success messages. You can use the following code, or style it to your preference.First, we want to include the jQuery CDN in our <head> tagsNext, we’ll run the following jQuery function.By using this, we disable the default action of the form, which is to redirect to the form.php page. Next, we store the values of the input fields in variables.We use the AJAX method, load(), to load pass our form data into our form.php file. Note that the first name is the POST name we’re passing to our document, and the name on the right is the actual value of the input from our variables.The data that our form.php file loads, will be displayed in our ‘.form-message’ class.Create a new file called form.phpInside the PHP tags, we check if the submit button was clicked, and get the data from the POST method variables we defined in our jQuery code. We save these values in their respective PHP variables.The built-in method to send emails in PHP required certain parameters, namely the subject, receiver email, header (optional) and the email content itself. We create variables for each of these, which will be used to send the email.Next, we set variables that will check if there is an error in any of the inputs, and set them to false by default.We use the built-in PHP method, empty(), to check if any of the of the variables are empty, and echo our error message with HTML if it is. We also check which specific inputs are empty, and set their respective error variables to true.Then we use the method filter_var and the built-in email validation function to check if the email is not a proper format. If the email is invalid, we echo our error message and set the $errorEmail variable to true.If none of these are true, then it means that everything was correct, and the form is ready to be submitted! We can echo our success message, and use the mail method to send an email using the data we previously defined in our variables.Below the PHP tags in the form.php file, we’ll write some jQuery code to handle the adding and removing of the styles for the form validation.Firstly, we select all our input fields and remove the ‘input-error’ class, so that there is no error shown before or after the form has been submitted.Then we create javascript variables for each of our errors, and equal them to our PHP variables, so that we can use them in our jQuery code.For each error, we check if it is true and add an ‘input-error’ class to the id of its respective input field. In our stylesheet, we have defined this class to add a red border around the input, indicating the error to the user. We do the same for the email error as well.Lastly, if all the errors are false, we set the values to blank. This ensures that once the user has successfully submitted the form, the inputs are cleared.And that’s it, we’re done! With that said, there are definitely other more efficient and secure ways to do this, but this is the method that I use and it works well for me.",jquery,https://medium.com/@develoweb.co/how-to-validate-a-form-using-ajax-78a149073299?source=tag_archive---------0-----------------------
Product Life Cycle: Working with Feedback,"Zarina MJun 4, 2020·2 min readTo help gain a better understanding of the feedback and testing process, I decided to recruit some colleagues to look at an application I previously built. I collected user feedback and feature requests to help improve my application while also practicing communicating with clients and meeting their needs. For some background: the application is a calendar/to-do fusion that functions like a planner — flip through the calendar to your desired month and day then enter your event or task.I had two colleagues take a look at the project, and this is what they said:To my surprise, the feedback I received was well-aligned with how I felt about the app myself; there were no major issues but it could use a lot of minor tweaks. I took the advice above and used it to set some basic guidelines. Firstly, I needed to fix aspects that impeded the user from using the app. Secondly, I needed to implement changed that would improve the user’s experience. Lastly, I could work on “nice to have” aspects that would make the experience fun or aesthetically pleasing.To be straightforward: I made those changes. I was lucky enough that the issues with my application were mostly small aesthetic issues, meaning I didn’t have to refactor anything major. That being said, I did decide to hit pause on two major ideas proposed to me: converting my application into React and adding Firebase. I only had a few days to implement the changes suggested to me, and also figured refactoring an initially jQuery project into React should have it’s own dedicated blog post. Regardless, I think it’s important to note that correcting what seemed to be “minor” aesthetic issues did make a substantial difference. Correcting alignment issues, adding a variation of fonts, and adding animations all contributed to my application giving off a more polished look. As someone who always avoided dealing with aspects of design, I learned that tweaking things until they look just right is totally worth it in order to achieve a professional looking project.",jquery,https://medium.com/@zarinabliss/product-life-cycle-working-with-feedback-5fefbf201721?source=tag_archive---------0-----------------------
How to Loop Through jQuery Objects?,"As someone who started his coding career by diving head-first into python, I have always been an admirer of clean and concise code. That is one of the reasons I enjoy using jQuery with it’s concise one-word functions and methods.jQuery object is an array-like collection returned when new HTML elements are created or existed ones are selected. According to jQuery’s website,When creating new elements (or selecting existing ones), jQuery returns the elements in a collection. Many developers new to jQuery assume that this collection is an array. It has a zero-indexed sequence of DOM elements, some familiar array functions, and a .length property, after all.In this step-by-step guide, we will be looking at how to loop through a jQuery object of existing elements using .each() and nesting it with $(this) selector. The official syntax of .each() is as follows, where it takes a function as its argument — the function you want to apply on each element of the jQuery object.We we will be creating three buttons which, when clicked, toggle between styled and unstyled states. To preview it, head over to the result tab below and try clicking the buttons.Let’s start by creating three buttons with basic HTML and CSS:So far, our buttons have basic styling as shown in the first picture below. To add the class “.button-style” to each button so it matches the ones in the second picture below, we will be using jQuery’s .each() function and $(this) selector.Now that our workstation is ready, let’s begin with our jQuery/Javascript.Create a jQuery object which has all the anchor tags inside it by using the selector $(“.[Class Name]”), which in our case becomes $(“.button”). For easy reference, assign the object to a variable, buttonList.Apply the .each() function on the buttonList which will take another function as it’s argument. We define that function in the next step.Because we want the magic to happen when we click each button, we have to nest the .click() method inside the function we are passing to the .each() method.Finally, add the .toggleClass() function as we want the buttons to toggle between styled and not styled. Again, we will be using $(this) selector to apply that function on whichever button is our code on as it is looping through the buttonList.To summarize it all, we created a jQuery object containing all our buttons. Then, we applied .each() method, inside which the .click() method was responsible for applying the function only when each button is clicked. Lastly, we put the function .toggleClass() inside, to change the button from not styled to styled and vice versa.I hope this guide helped you realize the power of looping with jQuery. If you have questions or concerns, feel free to ask me in the response section below and I will try my best to respond.",jquery,https://levelup.gitconnected.com/how-to-loop-through-jquery-objects-3ace47e1f6d0?source=tag_archive---------0-----------------------
Why you should know about jquery in 2020,"Oaes KuruniJun 9, 2020·2 min readIn 2020, you probably think that you don’t need to know about jquery but if you see this report which is jQuery is used by 97.5% of all the websites whose JavaScript library we know. This is 75.6% of all websites. MoreToday I would like to discuss 5 benefits of using Jquery on your next project or you will consider it to learn if you are not using any other popular library or frameworks in your project. Common information about it, jQuery is a lightweight JavaScript library. jQuery takes a lot of common tasks that require many lines of JavaScript code and wraps them that you can call with a single line of code.The primary jQuery library has been created to be kept tight and focused and this eliminates all non-essential features. This means that your business website will only download the necessary features that it needs. In case you need additional features that are not included, then you can download them online.Google and other search engines use page loading time as an important factor that affects search engine optimization. That’s why web developers must ensure that their code is short and as clean as possible. jQuery files are normally stored separately from the webpages. This enables developers to make any modifications to the whole website and help pages load faster.Developers find jQuery to be lean and easy because the library has been built using shorter and simpler code. When using jQuery, you do not have to be experts in their field for them to come up with excellent website styles. Web developers who have done testing and coding of CSS files will appreciate how easy jQuery implementation is.Another benefit of using jQuery is it can handle lots of cross-browser bugs and problems that are experienced while programming with JavaScript. Handling cross-browser issues in web development can be quite a challenging experience. This is because you may find web design elements working perfectly in one browser and breaking down completely in another browser. The jQuery library deals with most of these cross-browser issues.It provides lots of utility functions that assist in coding string iteration, array manipulation, trimming, and other features. These functions help in providing seamless assimilation between jQuery and JavaScript. With these key utility features, the entire coding process will be easier and hassle-free.read morelearn jquery from beginner to advance hereHappy coding :)",jquery,https://medium.com/@oaeskuruni27/why-you-should-know-about-jquery-in-2020-52721bd117a2?source=tag_archive---------3-----------------------
Crop Image with Croppie,"I had been building a feature for a project where i had to use image cropper. The requirement was like, the user had to upload image with a fixed aspect ratio ( 600 * 170 for me). So, i searched in google and youtube to find out tutorials. But what i found was not convincing enough. I choose to use JQuery Croppie for the requirement but the documentation was not good enough to understand properly. I had to struggle a bit. But good news for you, you don’t have to struggle like me. Because i am going to write a descriptive tutorial so that you can easily use the imaging cropping functionality anywhere. So, Let’s began.I am going to use django for backend but you can use any framework for handeling the backend, because i am gonna focus mostly into how to integrate the feature into front-end and pass it to backend.Setting up HTML. N.B: I used bootstrap 4, Jquery CDN, Croppie CDN.2. Setting Body:The output will be as below.3. Setting up script files.4. Setting Up the JavaScript File ( Fun Part)5. Back-end Code:6. Final Output:7. Conclusion:Thank you and have a good day!!!!",jquery,https://medium.com/analytics-vidhya/crop-image-with-croppie-26146a4b933?source=tag_archive---------0-----------------------
How to create Drag and Drop boards with jQuery and Sortable.js,"YurywalletJun 3, 2020·4 min readAfter becoming familiar with Agile task-boards concept I started to use Trello not only for work tasks but also to organize things in my life - like books to read, movies to watch, daily task to accomplish and so on.Having some experience in web-development, I was wondering how this “Drag and Drop” manipulation feature works?So I started to google phrases like “php drag and drop” and found a great example of jQuery “Trello Like Drag and Drop Cards for Project Management Software” by Vincy which you can find via the link.The example has everything needed for making your personal interactive task-boards:After I recreated the abovementioned solution, I’ve realized that the only problem was - it wasn’t working on mobile devices due to jQuery limitations. Actually, it appeared that web-version of Trello has also quite different functionality for mobile compared to desktop probably due to differences caused by capturing “touch” and “click” events.Therefore, some modification or even other solution was needed to make a mobile friendly Trello-like UI. While searching through StackOverflow and directly via Google for the ways to solve the issue, I found amazing JavaScript solution Sortable JS and it works great out-of-the-box. You just need to copy the code for the “Shared lists” (the cards and tasks are just in fact lists with items) from GitHub and that is it.The missing part to fully replicate the functionality of jQuery task-board solution was that the database management (backend part) had to be developed. And to be more precise, the task was to get the information about the item ID (that is dragged) and the final list destination ID — two things needed to store the changes to the database.At this point I decided to marry two solutions — take backend from the Vincy’s solutions and the Drag and Drop functionality provided by Sortable.js. As there are not that many examples for Sortable.js it took me some time to get acquainted with the library.All the manipulations with the list are defined in the function Sortable, so to grab the IDs onEnd function needs to added to the options and modified to capture the event and get the desired information (check the options provided).Also the data-task-id need to be defined for the “list item” and data-status-id for the “list name” inside the corresponding <div>.Placing the following peaces of code into the “index.php” file instead of original <div class=”task-board”>… </div> in the first solution finally solves the puzzle.As a result, I’ve got a working interactive card-like status-based task board tool both for Desktop and Mobile. The Drag and Drop option for card-based task grouping is intuitive and comfortable to use which makes it worth implementing in diferent projects.I personally applied this code to create my daily task organizer. By changing styles and adding forms for adding and removing items and boards I ‘ve got a result compared to my Trello experience.And now I can drag the task “Write about Drag and Drop task-boards” to “Done”.Please find some file available at Git",jquery,https://medium.com/@yurywallet/how-to-create-drag-and-drop-boards-with-jquery-and-sortable-js-49604b3242bc?source=tag_archive---------0-----------------------
Why was JQuery removed from Bootstrap 5?,"abhishek kumarJun 28, 2020·1 min read“We’re sharpening our focus on building tools that are more future-friendly, and while we’re not fully there yet, the promise of CSS variables, faster JavaScript, fewer dependencies, and better APIs certainly feel right to us.jQuery brought unprecedented access to complex JavaScript behaviors to millions (billions?) of people over the last decade and a half. Personally, I’m forever grateful for the empowerment and support it gave me to continue writing front-end code, learning new things, and embracing plugins from the community. Perhaps most importantly, it’s forever changed JavaScript itself, and that in itself is a monument to jQuery’s success. Thank you to every jQuery contributor and maintainer who made that possible for folks like me.Thanks to advancement made in front-end development tools and browser support, we’re now able to drop jQuery as a dependency, but you’d never notice otherwise.”We’re sharpening our focus on building tools that are more future-friendly, and while we’re not fully there yet, the promise of CSS variables, faster JavaScript, fewer dependencies, and better APIs certainly feel right to us.”Originally published at https://codinglikethunder.blogspot.com.",jquery,https://medium.com/@abhishek529127/why-was-jquery-removed-from-bootstrap-5-c2e4f8efe481?source=tag_archive---------0-----------------------
"A basic introduction to Vue.js, Ajax, jQuery","Ornob RahmanJun 9, 2020·8 min readVue.js is a JavaScript framework for building user interfaces. Compared to other frameworks and libraries, like Angular or React, Vue is more approachable and has not as steep of a learning curve.Vue is also modular and will allow you to split your code into reusable components. Each of them having their own HTML markup, CSS, and JavaScript code.We’re going to start with some very simple HTML and JS code, which looks like this:In index.html:In main.js:We need a way to take the variable product from our JavaScript and have it shows up in the h1 of our HTML.Our first step is to include Vue in our project, which we’ll do by adding this line in our index.html file, just above the main.js script.Index.html:Now in our main.js file, we’ll write like this:Then in our index.html, we’ll use our first JavaScript expression:After saving this, we’ll see “Socks” appear on our webpage.Now the question is how did the Javascript show up in our HTML. Let’s dive deeper into this:The Vue Instance:A Vue instance is the root of our application. It is created by passing an options object into it. Just like it sounds, this object has a variety of optional properties that give the instance the ability to store data and perform actions.Plugging into an Element:The Vue instance is then plugged into an element of your choosing, forming a relationship between the instance and that portion of the DOM. In other words, we’re activating Vue on the div with the id of app by setting '``#app``' as the element ( el ) that our instance is plugged into.Putting our data in its place:A Vue instance has a place for data, in its data property.The instance’s data can be accessed from inside the element that the instance is plugged into.Using an Expression:If we want our product to appear in our h1, we can put product inside these double curly braces.Inside the double curly braces, we’re using a JavaScript expression.What is Expression?Expressions allow us to utilize existing data values, together with logic, to produce new data values.When Vue sees the expression {{ product }}, it recognizes that we are referencing the associated Vue instance’s data, and it replaces that expression with the value of product instead, in this case: “Socks”.The reason Vue is able to display product ‘s value immediately is because Vue is reactive. In other words, the instance’s data is linked to every place that data is being referenced. So not only can Vue make its data appear within the HTML that references it, but that HTML will be updated to display new values any time that referenced data changes.AJAX is a technology used by interactive web applications to make HTTP requests to a server without causing page transitions.AJAX stands for Asynchronous JavaScript And XML because early implementations often used XML as the data format. Since then, simpler more efficient formats such as JSON (JavaScript Object Notation) have gained popularity.In traditional web applications, the browser renders a series of HTML pages that directly result from server operations. Each time a user enters some data and submits a form, the browser makes a call to the server so that it can perform some operation or calculation. The results of that call are rendered in HTML and displayed as a new page.The use of AJAX allows JavaScript on a web page to issue HTTP requests to a server without ever leaving the page. Modern, interactive web applications use this technique to update the web page without causing the distracting flicker that is seen when the browser loads a new page.A typical example might be a shopping site with a cart that can be updated without leaving the page. The data returned from an AJAX call does not have to be in HTML format, because it is not automatically rendered by the browser. Instead, it is parsed in JavaScript and appropriate changes are made to the page to display the data.Making an AJAX call:The core functionality of AJAX is implemented in the XmlHttpRequest object. The sample code, shown below, makes a call to the webserver to obtain the data to display in a list box. The response data is in HTML format for the sake of simplicity because it allows the text returned from the server to be applied directly to the <div> tag.Sending Parameters with AJAX:AJAX is often styled as a remote procedure call mechanism in which some kind of parameter list is constructed for a server-side call and the results are returned in a convenient format such as XML, CSV, or JSON.The code below shows how two numeric parameters can be passed to a server using a simple comma-delimited format. The encoded parameters are supplied as a string parameter to the send method and become the payload of the resulting HTTP POST request message:Using AJAX with a JavaScript Library:There are some problems with AJAX which are:Given these challenges, most developers use a library to handle the cross-browser issues and simplify what is essentially ‘boilerplate’ code. One of the most widely used is jQuery which provides AJAX support as well as many other features.Let’s see how we could re-work one of our earlier examples using jQuery. Almost any HTML element can be the target recipient of an AJAX request in jQuery a. If our server returns HTML we can simply bind our <div> output tag directly as shown below:jQuery is a fast, small, and feature-rich JavaScript library included in a single .js file. It provides many built-in functions using which you can accomplish various tasks easily and quickly.jQuery Important FeaturesAdvantages of jQuery:jQuery Library:To download the jQuery library, go to jquery.com. Then click on the Download jQuery link or Download menu. This will open download page. Here, you can download the latest versions of jQuery. You can download compressed or uncompressed versions of the jQuery library. Compressed versions should be used in the production environment whereas the uncompressed versions should be used in development. The compressed version minimizes the library by eliminating extra white space, line feed, and shortening the variable and function names. So, the compressed version is not readable. An uncompressed library is a readable file that is useful while debugging.After downloading an appropriate version of the jQuery library, you can use it by taking a reference for it on your web page. The jQuery library is eventually a JavaScript file. So you can include it like a normal JavaScript file using script tag as below.Include jQuery Library from CDN:You can also reference the jQuery library from public CDN (content delivery network) such as Google, Microsoft, CDNJS, jsDelivr, etcReference jQuery from CDN:There might be times when these CDN goes down for some reason. Therefore, you need to have a fall back mechanism that downloads the jQuery library from your web server if CDN is down.The following example shows how to include jQuery library reference from CDN with fall back.Example: Use CDN with FallbackjQuery() Function:When you open your web page in a browser and load the jQuery library successfully, it adds a global function named jQuery(). The global scope means the scope of the window, so the global jQuery() function can be called like window.jQuery() or jQuery() anywhere in your web page. $ is an alias of jQuery function, so you can also use $() as a short form of jQuery().jQuery, window.jQuery, $, and window.$ are the same.window.jQuery = window.$ = jQuery = $The jQuery() (aka $) function takes two parameters, selector and context.A selector parameter can be CSS style selector expression for matching a set of elements in a document. For example, if you want to do something with all the div elements then use $ function to match all the div elements as shown below:Example: Selector ParameterHere, you just pass the tag name of the elements you want to find. jQuery (aka $) function will return an array of elements specified as a selector.The second parameter in jQuery() function is a context. A context parameter can be anything, it can be a reference to another DOM element or it can be another jQuery selector. The jQuery will start searching for the elements from the element specified in a context. The jQuery selector may perform faster if you provide a context parameter. However, the context parameter is optional.",jquery,https://medium.com/@ornob011/a-basic-introduction-to-vue-js-ajax-jquery-149a93913131?source=tag_archive---------1-----------------------
Basic Learning about PHP And jQuery,"Md PiashJun 9, 2020·5 min readPHP is a server side scripting language. that is used to develop Static websites or Dynamic websites or Web applications. PHP stands for Hypertext Pre-processor, that earlier stood for Personal Home Pages.PHP scripts can only be interpreted on a server that has PHP installed.The client computers accessing the PHP scripts require a web browser only.A PHP file contains PHP tags and ends with the extension “.php”.PHP Syntax<?phpecho ‘Hi I Am Abrar’;?>A script is a set of programming instructions that is interpreted at runtime.A scripting language is a language that interprets scripts at runtime. Scripts are usually embedded into other software environments.The purpose of the scripts is usually to enhance the performance or perform routine tasks for an application.Server side scripts are interpreted on the server while client side scripts are interpreted by the client application.PHP is a server side script that is interpreted on the server while JavaScript is an example of a client side script that is interpreted by the client browser. Both PHP and JavaScript can be embedded into HTML pages.You have obviously heard of a number of programming languages out there; you may be wondering why we would want to use PHP as our poison for the web programming. Below are some of the compelling reasons.Other benefit that you get with PHP is that it’s a server side scripting language; this means you only need to install it on the server and client computers requesting for resources from the server do not need to have PHP installed; only a web browser would be enough.PHP has in built support for working hand in hand with MySQL; this doesn’t mean you can’t use PHP with other database management systems. You can still use PHP withPHP is cross platform; this means you can deploy your application on a number of different operating systems such as windows, Linux, Mac OS etc.In terms of market share, there are over 20 million websites and application on the internet developed using PHP scripting language.This may be attributed to the points raised above;The diagram below shows some of the popular sites that use PHPFile extension and Tags In order for the server to identify our PHP files and scripts, we must save the file with the “.php” extension. Older PHP file extensions include.phtml.php3.php4.php5.phpsPHP was designed to work with HTML, and as such, it can be embedded into the HTML code.The server interprets the PHP code and outputs the results as HTML code to the web browsers.In order for the server to identify the PHP code from the HTML code, we must always enclose the PHP code in PHP tags.A PHP tag starts with the less than symbol followed by the question mark and then the words “php”.PHP is a case sensitive language, “VAR” is not the same as “var”.The PHP tags themselves are not case-sensitive, but it is strongly recommended that we use lower case letter. The code below illustrates the above point.We will be referring to the PHP lines of code as statements. PHP statements end with a semi colon (;). If you only have one statement, you can omit the semi colon. If you have more than one statement, then you must end each line with a semi colon. For the sake of consistency, it is recommended that you always end your statement(s) with a semi colon. PHP scripts are executed on the server. The output is returned in form of HTML.jQuery is a lightweight, “write less, do more”, JavaScript library.The purpose of jQuery is to make it much easier to use JavaScript on your website.jQuery takes a lot of common tasks that require many lines of JavaScript code to accomplish, and wraps them into methods that you can call with a single line of code.jQuery also simplifies a lot of the complicated things from JavaScript, like AJAX calls and DOM manipulation.The jQuery library contains the following features:· HTML/DOM manipulation· CSS manipulation· HTML event methods· Effects and animations· AJAX· UtilitiesThe jQuery syntax is tailor-made for selecting HTML elements and performing some action on the element(s).There are lots of other JavaScript libraries out there, but jQuery is probably the most popular, and also the most extendable.Many of the biggest companies on the Web use jQuery, such as:If you use Galleria to build a photo portfolio for your website to display on the iPad and iPhone, you will need to link to the jQuery library on Google’s content delivery network (CDN).The easiest method to make jQuery work on your web pages is to use the <script> tag to link to the jQuery library, like this:2. Scroll to the bottom of your web page and click to insert the cursor just above the </body> tag.You can add the jQuery link in the <head> tags at the top of the page, but the best practice is to add jQuery code at the end of a web page, to help reduce the amount of time visitors spend staring at a blank screen, waiting for the page to load.3. Insert the following code to link to the jQuery libraries on the Google server:<script type=”text/javascript” src=https://ajax.googleapis.com/ajax/libs/jquery/1.5.2/jquery.min.js></script>4. Save your web page.",jquery,https://medium.com/@mdpiash/basic-learning-about-php-and-jquery-3c4709934e36?source=tag_archive---------5-----------------------
前端心得：從 jQuery 到 React hook,假設有一個計數器按鈕，它顯示的初始值是0，每次點擊，顯示數值都會加1：如果我們用jQuery，代碼大概會是這樣：上面的代碼其實有三個步驟：我們來改寫下jQuery 代碼，以便更清晰地看到整個流程：如果改用React，我們的代碼大致是這樣寫：那麼，相比之下，React 優於jQuery 的地方是哪些？為什麼當下整個前端的趨勢是從jQuery 遷移到React 等等框架的？對比jQuery 與React 的代碼，我們可以看到，二者都需要獲取當前狀態值，也都需要計算新的狀態值。但是在jQuery 下，我們是這樣更新HTML：而 React 代碼裡，我們的 HTML 是個 JSX 模板，我們只要設定新狀態值，React 就會幫我們填充數據。React 重要觀念：畫面呈現與 state 必須一致！jQuery 會將 HTML 與 JS 分開管理，如上面 counter 的例子，如果複雜一點的應用程式，就會難以維護。單單只看 index.html 這個檔案，你完全猜測不出來 counter 這個 id 到底隱含了什麼行為。事實上，「顯示邏輯」和「模板」有很強的一致性，他們不應該被拆開。而擁有這兩者的物件，我們稱做他為元件( component )。將 JS 和類 HTML 語法混雜在一起，如上面的 react counter，可以直觀從單一檔案中看到 UI 與邏輯。組件化建立在封裝上：一個應用程式的狀態會非常多，而且會頻繁的變化。如果我們傾聽某一個事件，就要針對這一個狀態進行 DOM 的處理，必須不斷增加 event listener。而在 React 中，他將這一件事的處理，拆成兩個關注點：1. 當監聽某一個事件，先改變狀態（ state ）的資料，改變後會進行重繪2. 針對所有資料，進行排版（ JSX ）重繪將會改變所有的 DOM，而改變 DOM 是很耗效能的一件事。React 使用 Virtual DOM 的技術可以比對 state 的改變，最小程度地修改 DOM。然而實務上 render 次數的控制還是 react app 效能的關鍵FB 推出 React 就是因為畫面管理極其困難，React 本身是單純的 UI 解決方案，結合 react-router-dom 和 redux 才算是完整的框架看完強大的 React 是否有種前端世界萬世太平的感覺？我們時常必須維護那些一開始非常簡單，但後來變成充滿無法管理的 stateful 邏輯和 side effect 的 component。每個 lifecycle 方法常常包含不相關的邏輯混合在一起。舉例來說，component 可能會在 componentDidMount 和 componentDidUpdate 中抓取資料。但是，同一個 componentDidMount 方法可能也包含一些設置 event listener 的不相關邏輯，並在 componentWillUnmount 執行清除它們。會一起改變且彼此相關的程式碼被拆分，但完全不相關的程式碼卻放在同一個方法裡。這讓它很容易製造 bug 和不一致性。react lifecycle在許多情況下，因為到處都是 stateful 邏輯，不可能把這些 component 拆分成更小的 component。而測試它們也很困難。這是許多人偏愛把 React 跟一個獨立的 state 管理函式庫（ Redux ）結合的其中一個理由。然而，這常常引入了太多的抽象，要求你在不同檔案間跳來跳去，而且讓重用 component 更加困難。為了解決這個問題，Hook 讓你把一個 component 拆分成更小的 function，這基於什麼部分是相關的（像是設置一個 subscription 或是抓取資料），而不是強制基於 lifecycle 方法來分拆。你還可以選擇使用 reducer 來管理 component 的內部 state，使其更具可預測性。可以發現設置document.title的邏輯是如何被分割到componentDidMount和componentDidUpdate中的，訂閱邏輯又是如何被分割到componentDidMount和componentWillUnmount中的。而且componentDidMount中同時包含了兩個不同功能的代碼。那麼Hook如何解決這個問題呢？就像你可以使用多個state的Hook一樣，你也可以使用多個effect。這會將不相關邏輯分離到不同的effect中：可以在使用 Effect Hook 討論更多相關內容。React 沒有提供一個方法來把可重用的行為「附加」到一個 component 上 (舉例來說，把它連結到一個 store)。如果你已經使用 React 一段時間，你或許會熟悉像是 render props 以及 higher-order components，這些試著解決這個問題的模式。但是這些模式要求你在使用它們時重新架構你的 component，這可能很麻煩，而且使程式碼更難追蹤。如果你在 React DevTools 上查看一個典型的 React 應用程式，你很可能會發現一個 component 的「包裝地獄」，被 provider、consumer、higher-order component、render props 以及其他抽象給層層圍繞。下面是 render props 的範例：使用 Hook，你可以從 component 抽取 stateful 的邏輯，如此一來它就可以獨立地被測試和重複使用。Hook 讓你不需要改變 component 階層就能重用 stateful 的邏輯。這讓在許多 component 之間共用或是與社群共用 Hook 很簡單。以下是自訂 Hook 的範例：詳細內容可以參考：打造你自己的 Hook 。除了使重用、組織程式碼更加困難以外，我們發現 class 可能是學習 React 的一大障礙。你必須了解 this 在 JavaScript 中如何運作，而這跟它在大部分程式語言中的運作方式非常不同。你必須記得 bind 那些 event handler。如果沒有不穩定的語法提案，撰寫的程式碼會非常繁瑣。優點缺點超建議大家看 React conf 2018 的演講， Dan 真的很帥參考資料：1. from jQuery to React : https://blog.zfanw.com/from-jquery-to-react/2. react 特色 : https://medium.com/4cats-io/2016-%E5%B9%B4%E3%81%AE%E5%89%8D%E7%AB%AF-%E7%98%8B%E4%BB%80%E9%BA%BC-reactjs-4727a6ecc85a3. react hook : https://zh-hant.reactjs.org/docs/hooks-intro.html,jquery,https://medium.com/traveling-light-taipei/%E5%89%8D%E7%AB%AF%E5%BF%83%E5%BE%97-%E5%BE%9E-jquery-%E5%88%B0-react-hook-89d7c8e8d58?source=tag_archive---------0-----------------------
Altering content based on the date using JavaScript,"Hi! I’m Erika!Jun 18, 2020·2 min readWhen you work for a company that sells things, you’re going to end up building landing pages that have sale prices or bonus points that are based on a date range.At first, we were coding and scheduling separate code blocks based on the dates. But! I was able to figure out javascript to grab the date and show the sale item or not.Here is an example of grabbing the date and the month, and then showing a different headline if the current date is between the 12 and the 17th of February.I’ve included screenshots of the results in the console based on today1. We need to store the date in a variable:2. Then we need to split this date into two variables; day and the month. I used .getDate() and .getMonth() to pull this information from the variable that I stored the current date and time in. Just a note getDay() results in the day of the week, not the day of the month.You’ll notice here that although it is June (6th month) that date is stored as an object and therefore starts at 0, resulting in June as 5. This tipped us up when we were first implementing the code.3. With this data, I created an if statement to populate the H2 tag with the correct copy. I used a class to ensure that I populated the correct H2 tag, as there were many on the page.You can grab more information from the Date(); object using other methods, which you can find here.",jquery,https://medium.com/@hiimerika/javascript-altering-content-based-on-the-date-5312e2896623?source=tag_archive---------1-----------------------
Responsive Image Gallery Using HTML And CSS,"w3hubsJun 3, 2020·1 min readTemplate Name: Responsive Image Gallery Using HTML And CSS.High Resolution: — Yes.Compatible Browsers: — All Browser.Source Files included: — HTML, CSS and Images(unsplash).Responsive layouts is first priority for a web developer/designers. well, here we design a responsive image gallery in HTML and CSS with an attractive user-friendly interface.In this element, we used CSS flexbox properties with justify-content and alignment properties to make perfectly aligned. Also, we used width properties for the column side, and for space we used margin. Here we also used box-shadow effects on mouse hover case. To make responsive we used media queries from 1000px till the mobile screen side.Make it yours now by using it, downloading it, and please share it. we will design more elements for you.Originally published at https://w3hubs.com on June 3, 2020.",css,https://medium.com/@w3hubs/responsive-image-gallery-using-html-and-css-9900f0f52d0e?source=tag_archive---------13-----------------------
Importance of SVGs in Web Development,"While developers creating their applications they use the SVG format for many icons and vector images. There are many benefits of using SVG instead of regular PNG or JPEG files.If you and your team trying to make responsive website SVGs are the best option to serve your illustrations, icons and images.SVGs are resolution-independent, so we can use them whatever their size is. Increasing and decreasing their sizes is very easy. Unlike JPG or PNG, SVGs retain their quality no matter what size of screen or resolution is used to show them.Different scalabilities with the same file would be very helpful on responsive web pages. As a developer, you can use the same file source everywhere with just a quick adjustment of the scale.Unless they’re sized on a big screens, the file size of SVGs is very small compared to PNG or JPEG files:As you can see, the PNG is nearly ten times larger. When you need to use a lot of images on your website the difference in size is huge.Note: The size of an SVG can be increased without any file-size increase.As you can see above, the sizes of SVG and PNG vary widely. When you’re developing a website with many graphics and images there will be a huge difference between using SVGs instead rather than JPEGs or PNGs.If we have 30 images and icons on our landing page, we can easily reach up to four to five MB using JPEGs or PNGs. With SVG it would be just 100–200kb. The difference is huge!Believe me, no one wants to wait an extra 15–20 seconds when they first land on your webpage!There’s another great benefit of using SVGs. While coding you can change their properties just as you want, with a single file source.This SVG includes height, width, cx, cy, r, stroke color, stroke-width and fill properties. These values can be easily changed using the single file source.As you can see below, using the same file you can add a hover effect for the icon:SVG will definitely increase the speed of your website — a factor that has a very powerful impact on your audience.How many of you have waited more than 30 seconds to load a single page? These days, most people will assume they’ve lost their connection after just five seconds.Here some webpages I use for finding SVGIf you’re using any others please comment below.Thanks for reading!",css,https://betterprogramming.pub/do-svgs-really-matter-154240f5435c?source=tag_archive---------3-----------------------
A Couple of Tips for Writing a Responsive Website,"Aidan RicciJun 2, 2020·2 min readWhat is Responsive Web Design? Why is it useful?Responsive Web Design refers to an application’s ability to adapt to a user’s screen size, screen resolution, and device orientation. This means that it allows a web app to look good regardless of where and how it is being displayed. This article will cover how to achieve this goal with a couple of different approaches.The first step you should take when writing a responsive website is to set the viewport in the HTML head:<meta name=”viewport” content=”width=device-width, initial-scale=1.0"">By setting the viewport, you will give your browser information on how to control the scaling of your website.Units of Measurement in CSS:Before we really get started, it’s important to note that there are two different types of measurements in CSS, absolute and relative.Examples of absolute measurements are: in , cm , mm , pt , pc, and px (pixels). Unless writing a print, it is recommended with px, as real-life measurements don’t scale accurately on the screen. Keep in mind that these measurements are not generally a good idea for responsive design.Relative measurements include: em, ex, rem, vh, vw, ch, vmin, vmax, and %. While all of these have their uses, I personally stick with vh (viewport height), vw (viewport width), and the % for most of my relative CSS needs.How to Get Responsive Images:The first way to achieve a responsive image is using the width or max-width properties.by setting width to 100%, the image will scale to 100% of its parent container, regardless of what size that makes the image. However, this means that if the original image is too small for its size it will get blurry/pixelated.Here’s where max-width comes in. With max-width of 100%, the image can never become bigger than its actual size, although it can downscale if needed.Another option is to show different images depending on the size of the browser. For example, you could do something like this:<img src=”alien.jpg” media=”(max-width: 600px)”> <img src=”alien1.jpg” media=”(max-width: 1500px)”>Doing this ensures that the image shown always fits well with the size of the browser it’s contained within.note: Don’t use the % to scale images using height, it can sometimes cause wonky problems.Using Viewport Width to Create Responsive TextAnother important part of writing a responsive website is creating responsive text.One simple way of doing this is to use vw instead of px for your text, like this:<h1 style=”font-size:10vw”>Hello World</h1>Using Media Queries to Change CSS Based on Screen SizeMedia Queries are CSS descriptors that ensure CSS rules are only used for certain sizes of browser. In the example below, the tags only apply if the size is greater than 500 pixels wide. This is another case where sticking with width is generally good.@media screen and (min-width:500px) {your CSS rules here}",css,https://medium.com/@aiqr25/a-couple-of-tips-for-writing-a-responsiv-584ed14e54a4?source=tag_archive---------13-----------------------
How to Make an Animated Day and Night Toggle Switch,"Providing an option for your users to toggle between a dark and light theme can be a great way to let them:We will look at how you can create an animated toggle button with only CSS. We will also be looking at how you can use CSS variables to define different color schemes. And by the end of this tutorial, you will be able to build a light switcher, like the one below:Create an index.html file at the root of your project and fill it with the following:We’re going to have three different resources:Let’s go over each and see what we need to add for them. At first, we want to create the toggle.Open up your styles.css and define the following styles for the .toggle:I have to mention two important things here. First, you should set cursor to pointer to provide visual feedback to the user that the element can be interacted with. Secondly, we change the icon — which is in the content of the before pseudo-element — based on an .active class.Of course, we don’t want to toggle the class through DevTools, so let’s add the event listener in JavaScript.For this to work, we need the following function attached to the .toggle element.At this point, we are able to toggle the icon on an off. To spice things up, let’s also add some animation.The animation will work in the following way: We scale the icon down to 0. When it hits 0, the icon is essentially invisible. This is when we need to toggle the .active class to change the icon. Then we scale the icon back to its original size.Add the following to your style.css file:We will animate the icon in 300ms to make the animation snappy. To handle this through JavaScript, we will also need to modify our event listener a bit.First, we need to add the animate class to start the animation. At the half of the animation — which is set to 300ms in styles.css, so we need to use 150 here — we toggle the .active class. At the end of the animation, we can remove the .animate class.To make the background transition feel like a wave, we can use a CSS trick instead of changing the background-color property. We will animate the box-shadow instead. For this to work, we need a new element. Add the following line to your index.html file:Since we are already doing some animation on the .toggle element, we don’t want to animate its box-shadow. Otherwise, we would get weird effects. Let’s define the styles associated with the .wave.Some important things I would like to point out:If we start changing the size of the box-shadow, we can see that the background slowly turns into darkness. Let’s also update the event listener in JavaScript:All that’s left to do is to add the color definitions so we have different styles for different themes. Open up your colors.css file and fill it with the following content:As you can see, we don’t have anything, besides the colors. All we are doing here is rewriting the CSS variables for different themes. Again, we want to toggle a class, but this time, on the HTML element itself.This is the last thing to add in the .toggle event listener. So how would you use these variables inside CSS? Instead of hard-coding colors, you can use the var function to retrieve the values from the defined colors.Make sure you add a transition property to smoothly animate color changes. All that’s left to do is to test everything out.In roughly 10 lines of JavaScript code, you are able to create a day-night toggle switch to help your users in various ways. It’s easy to implement and highly customizable. With the help of CSS variables, you can also create multiple themes by simply adding another class with another set of color schemes.If you would like to experiment with the finished project, you can clone it from GitHub. Thank you for taking the time to read this article. Happy coding!",css,https://blog.bitsrc.io/how-to-make-an-animated-day-and-night-toggle-switch-848993d24bb6?source=tag_archive---------0-----------------------
How to get a free shirt (and make accessible web sites) 🆓 👕🚨,"Reece BoydJun 2, 2020·3 min readA simple perspective to make a web site accessible is to achieve two goals:Achieving #1:It’s mostly about tabindex .Besides that, it’s mostly about visual indicators.Achieving #2:It’s mostly about aria-label’s.Ideally your button’s are descriptive enough: “Log in”, “Log out”, “File a claim”, “Chat with us”, etc. But sometimes they’re not: “Get started”.Get started with what? Simple fix: just add an aria-label to the element, for example: “Get started filing a claim.”And that’s pretty much it. Of course there’s a lot more bonus points you can rack up with accessibility but just doing the above two things — which are almost entirely achievable out of the box by using the right html elements or with just minor tweaks to tabindex and aria-label’s — you will already have a more accessible site than most. The tools below will easily take your accessibility practices to the next level.  Tools:Scanners that will help you meet the above 2 criteria and more:Each issue discovered with Axe comes with a Learn more link that provides awesome perspective on how to resolve that issue and why it matters.Screen reader that will help you test your aria-label’s (or lack thereof):What’s next?If you’ve made it this far, thanks for reading and enjoy the free shirt! I’ll write a follow-up that covers some extra accessibility practices that will take your site to the next level. As a teaser, I’ll leave you with two links on my favorite*: the skip link.*It’ll be your favorite too if your site has a header.",css,https://medium.com/@reecealanboyd/how-to-get-a-free-shirt-and-make-accessible-web-sites-3e4a22047bca?source=tag_archive---------14-----------------------
311 Week 1 — Node and REST,"Peter GilkeyJun 1, 2020·2 min readHow do you organize your code? What are some suggestions you find on the web?I keep things organized by project, which means separate repositories or branches. In the project directory, I may have an extra sub folder if there are multiple script files. In my script files, I try and keep functions together in the order that they may be used. Within functions I keep variables grouped at the top, and the rest of the code in order as executed.Can you describe your workflow when you create a web page or web app?Planning should always come first, to get a general idea of what is being built. I try to break it down into several main goals, and steps to achieve them. Once that’s set, I try to wireframe my general idea of the page layout. Depending on the project, I may either start with my HTML and CSS, or add JavaScript when necessary.You can’t work out how to solve a coding problem, where do you find the answer? Why? How do you know?I always try logging to the console to see what is being executed and on what line. If I can’t solve it in the console, I’ll go to google with multiple versions of my question in mind. It’s possible that someone else has had the same problem and asked the question in a different way.What problems have you solved that didn’t involve you coding?Git was pretty confusing to me at first. Plenty of mistakes have been made in the terminal, and it took some time to understand what went wrong and how to fix it.Talk about your preferred development environment. (What IDE or text editor they enjoy, and why?)I’ve tried several coding environments including Atom, Brackets, and Notepad++. However my current pick is VS Code. It’s the least sluggish upon opening in my opinion. The UI is nice, and it offers a lot of extensions to download.How are you keeping up with the latest developments in web development?As we are learning, we are being introduced to current technologies that are used in web development. Whether it’s a library or a software, there is a lot to understand and keep up with.",css,https://medium.com/@peterg7042/311-week-1-node-and-rest-a7888628a6cc?source=tag_archive---------15-----------------------
A Better Way to Make Layout Grids on the Web,"Csaba “Ed” SchreinerJun 2, 2020·7 min readLayout grids are an excellent way to design neat-looking, organized pages. Yet every CSS layout grid framework I’ve seen so far fails to do what they were meant to do: align all items to a single, global grid on the page, regardless of their position in the DOM. But now, with custom properties (aka CSS variables) and viewport width units, there is finally a way to actually achieve that!Not to be confused with the similarly named display: grid, layout grids are a guide for designers to lay out elements of a page. They consist of a certain number of columns, with gutters between them. Designers align elements to these columns, like this:Most popular design systems, like Google’s Material Design, feature some kind of a layout grid. If you want to know more, this article sums up the history and characteristics of layout grids very well.Here’s how most CSS frameworks, like Bootstrap and Foundation, handle layout grids. Let’s say you have a 12-column grid, and want to make a desktop layout with two content columns, one 8 grid columns wide and the other 4 grid columns wide. Like this:To do this, you put a class, for example .col-xl-8, on the green element, to specify that you want it to be 8 columns wide (on extra large screens). Similarly, you put a .col-xl-4 class on the orange one. The framework then calculates how many percent that number of columns would mean, for example in a 12-column grid, 8 columns would constitute a width of 66.67%. Then, it puts either width: 66.67% or, more recently, flex-basis: 66.67% on that element.This works perfectly, at least at the topmost level of elements. But let’s say you then want to fit a bunch of other elements to the grid, like this:You’d assume that you would need to put a .col-xl-2 class on those purple elements, right? But that doesn’t work. All that does is put a flex-basis: 16.67% on those elements, which sets their to be 16.67% of their parent’s width. This type of sizing falls apart on anything except top level elements. Granted, there are hacks to work around this, but usually they are way more trouble than they are worth.When designers think about layout grids, they envision a single system that everything on the page fits to. But a site’s HTML structure is a complicated monster. You have elements inside containers inside wrappers. Using this percentage-based sizing, you can lay out your basic set of columns, but once you try to align anything to the grid that’s nested deeper in the structure, what happens instead is that you end up creating a sub-grid inside your top-level grid.This is especially painful when you are working with a component-based architecture. You want your components to be completely independent of each other. Which means that your component will have no idea how deep it is in the DOM, so there is absolutely no way for it to fit to the layout grid with the percentage-based method.So… does that mean that when you use components, there’s no way to use layout grids at all? Luckily, there is.The biggest problem with implementing layout grids was that we had no easy way to pass information from the top level elements to deeper in the DOM. And that’s exactly what custom properties can do. If you set a custom property on an element, that cascades down to their childrenThe other game changer is viewport width values. With these, you have an easy way to refer to the page width from anywhere in the DOM when the grid needs to be fluid.Let’s build a responsive layout grid with these! I’m going to show you how to implement a responsive grid that has 6 columns and a 16px gutter on small screens, 8 columns and a 24px gutter on medium screens, and 12 columns and a 32px gutter on large screens. It will be fluid on smaller viewports and have a fixed 1256px maximum width on the largest viewports.Let’s start with defining these parameters as custom properties. First, the smallest screen.All right, we have a basic set of parameters. Now we need to calculate a column width, so that we can refer to it. Here’s how we do that:Bit of a long statement there, but it works. You take the total page width, subtract the width of the gutters, and what you end up with is the width of the columns. Divide that by the number of columns, and you get the column width.This means we can now refer to these values from anywhere within our CSS. If we want to set something to be, say, 2 columns wide, we just need to add 2 column widths and 1 gutter width (since there is a gutter between those columns). Similarly, if you want to set something to be 4 columns wide, you add 4 column widths and 3 gutter widths. Like this:That’s great! But it’s still kind of a bit of a long calc() for us to be using every time. We can shorten it though. Let’s define some shorthands for this. For example, we can define a --g1c variable to always mean one column width, a --g2c to mean two columns with one gutter between, and so on.These are a lot shorter to type — although they might be a bit harder to understand. Feel free to skip these shorthands in your code if you aren’t sure about that tradeoff.All right. So we now have grid settings for our smallest viewport, and a way to calculate every necessary value. To add the other viewports, we just overwrite the original custom property values with other values inside media queries.And that’s all you need! You have a responsive, global layout grid that you can refer to from anywhere in the DOM.I used the grid system above to build a responsive article layout. Check out how easily the nested elements can fit to the grid! Visit the example on Codepen to check out how it behaves on different viewport widths.At this point, you might be wondering…Surprisingly — yes, it is!At the time I’m writing this article, according to caniuse.com, custom properties are fully supported by 94.80% of the browsers used globally and viewport width units are fully supported by 95.99%.Unfortunately, this implementation does have one small drawback. When calculating page width using viewport width values, the browsers don’t adjust for the width of the vertical scrollbar. This means that when the grid is fluid, the scrollbar cuts into the right margin:When we first implemented this type of layout grid, we spent a couple of days researching whether we could stop this from happening, and we weren’t able to find a really good solution. It would be possible to use JS to account for the scrollbar’s width, but that might take a bit away from the elegance of this method being CSS-only.Ultimately, we decided to ignore this problem, based on the following reasoning:As a designer, this method feels way closer to my ideal concept of a layout grid implementation than anything else I have seen before, and I’m really proud of how simple it could become, thanks to custom properties. I hope this article inspires you to try this out for your own, and helps you build beautiful, neatly arranged websites in an easy way!Credit for the original idea goes to my friend Ákos Kún, who both developed our first, JS-based implementation of a viewport width-based grid, and also gave me the idea to try doing the same using custom properties.",css,https://medium.com/@csaba.schreiner/a-better-way-to-make-layout-grids-on-the-web-daf9a9651e9a?source=tag_archive---------8-----------------------
Rendering Unicode Symbol In React,"James CoxJun 2, 2020·3 min readRecently I built a simple SPA (single page application), https://mathmaster.netlify.app/, with React. When quarantine began, and suddenly every student was given home-school status, we all had to get creative! I put my improving React skills to good use, and built a math generator game aimed at children my daughter’s grade level (she’s 7, just finishing first grade).One random issue I discovered was rendering Unicode symbols in React isn’t the same as HTML (or CSS). I wanted the “divide” symbol, U+00F7, instead of the JavaScript operator, “/”, which was pretty clear to me when my daughter had no idea what “/” meant in terms of the math problems she was used to seeing.There are many ways you can encode a Unicode symbol. At first I didn’t realize this, so I looked for the obvious ones: HTML and CSS. In CSS, the divide symbol is represented like:HTML:But as I FINALLY figured out, you need to format the Unicode specifically with the JavaScript encoding.Notice inside the curly braces the unicode is inside a string, and the “U” is after the backslack and lowercased. Here is how my JSX looks altogether for the correctly solved success message:Renders the following in the browser:And for redundancy’s sake, this is how I code the divide symbol when it’s displayed as the problem to be solved:Which renders:I figured this out by basic trial and error. Because JSX is like a hybrid of JavaScript and HTML, I thought I could try different forms of encoding, specifically the HTML versions described in this handy reference: https://www.htmlsymbols.xyz/unicode/U+00F7gets you something like:It might seem obvious to others, but I admit it took me at least an hour to first FIND the JavaScript formatting for Unicode, and then how to implement it correctly (inside a string, inside the curly braces). I hope this post might save someone else the hour or so it took me to figure this out!",css,https://medium.com/@jamesncox/rendering-unicode-symbol-in-react-76b1b4ffcee4?source=tag_archive---------2-----------------------
Animated_Html &CSS : Do beginning,"Barry YUJun 3, 2020·2 min readawesome !! My profolioStep1 :Html Design UI/UXAwesome Tool : figma未來趨勢的ＵＩ/UX工具 ,美國矽谷已經是最流行了. 非常強大.看過youtuber 透過這工具可以直接幫你帶出ＣＳＳ可以線上和很多設計師共同協作. COOL!目前也是本人自己也是剛開始接觸Step2 :Written Code on your Visual codeCSS and HTMLStep 3 :一開始的框架設計建議在UＩ工具上設計在後續的編寫HTML及ＣＳＳ上設定Background-color ,該 用顏色做一個區分：Header > container > Logo+nav-btn + log-sign在Container 設定 display:flex讓log:{color:red} , nav-btn { color :green} , log-sign { color:yellow }Step4 :display : flex …麻煩你可以透過做過的專案經驗帶入節省時間. 或者 利用DevTool 及時調整修改下面的例子： 用了3個 display : flex . 看顏色就可以暸解很複雜的flex.Open your Html on Chrome , also using DevTool ,could be Modify Element CSS on Styles Item , soon to change your index.html . adjust you FlexOpen your Html on Chrome , also using DevTool ,could be Modify Element CSS on Styles Item , soon to change your index.html . adjust you Flex",css,https://medium.com/@placid-olivine-mink-632/animated-html-css-do-beginning-6f3caec1980a?source=tag_archive---------12-----------------------
What is CSS? | Top 4 Advantages of CSS (cascading style sheet),"iTriangle TechnolabsJun 3, 2020·3 min readCSS stands for Cascading Style Sheets. HTML and CSS are behind the technique of creating a webpage. The use of HTML gives the webpage a shape and the use of CSS gives the webpage a new and attractive look. HTML and CSS are always used together. Without CSS, we can use html but without html, css cannot be used.HTML and CSS is a computer language that is very simple and can be easily taught. To write the code of html and css we need a text editor such as Notepad. After writing these codes, a web browser is required to view it through internet.Many tags are used in HTML such as header tag <h1>, font tag <font>, table tag <table>, image tag <img> etc. All these tags are also used together with css to show them more well in the browser.Using CSS, we can show the text of the webpage in good color, control the space between the styles and paragraphs of fonts, the images in the background and the use of what color in the background will give the webpage a good look. Css is used to set things. The css html document gives a completely new look which attracts more users.We will not have to write css code repeatedly to create a different web page. And by using the css code written only once, we can create as many web pages as we want, in which we save a lot of time.If we use css, we do not need to write the attributes of the html tag again and again. Just once according to the rule of css, by writing the attributes of the tag and applying it in the web page, that tag will appear correctly everywhere. Therefore, the tag will not have to write the same code again and again to appear in different places on the web page, and if there is less code, then the web page will load quickly in the browser.To change the style of web page completely, just changing the style code of css once will automatically change all the elements used in html at once and change all elements one by one will not be necessaryplatform independent means that we can use css in any platform like windows, linux, macintosh etc. And it also supports all the latest browsers.This was CSS and some related information that is used to create a web page. Hope you have helped a lot in this article about What is CSS. If you want more information related to this, then you can comment below.iTriangle Technolabs is top-ranked and expertise web development company, which follows the current and popular trends in the it industry to deliver clients a website with all trending features.If you require a CSS Development with all the trending features in the market, our team is happily available for you 24 by 7.Originally published at http://itriangletechnolabs.com on June 3, 2020.",css,https://medium.com/@itriangletechnolabs/what-is-css-top-4-advantages-of-css-cascading-style-sheet-8bb15ad3dfdf?source=tag_archive---------16-----------------------
My top “eureka” moments while learning CSS,"Daniel — JS CraftJun 2, 2020·2 min readThere were defining moments in our evolution as a species. Like the moments when we discovered fire, or when we discovered electricity. Exactly like this, we have some breakthrough moments when we learn something new.In its essence CSS is simple. We have selectors and properties that we combine to style our HTML documents.However, there are still moments when I’ve felt like I have discovered that missing piece of the puzzle that was explaining why things were not behaving as intended.So, my main breakthrough moments:1. The CSS box model: In the beginning, it was not clear to me that if you have an element with width: 100px and border: 5px solid that element will need in total 110px of space. Things like the border, margin, and padding are not included in the width of an element. And we can change this with the box-sizing property .2. How CSS deals with conflicting rules: What if we have:What color our paragraphs will be? The two main principles that are used by CSS to decide the winning rule are specificity and cascading. Or course initially I was getting frustrated and was using !important all over the place. But getting a grip on how these principles work made writing CSS a more enjoyable experience.This medium account will be soon discontinued and I will publish new articles only on js-craft.io. If you like this content please sign up for my newsletter I and will keep updated with new suff about CSS, React and Javascript.3. Using CSS grid and flexbox: layout is hard in CSS. Maybe the most complicated subject. However, with the addition of CSS grid and flexbox things got way better. What took before a lot of math and “brainpower” become simpler with these two. You can see here also a way on how to decide when to use the CSS grid and when to use flexbox.Of course, every learning experience is unique, but for me, these were the things I wished somebody told me when I’ve started making stuff with CSS.Cheers and happy learning!This medium account will be soon discontinued and I will publish new articles only on js-craft.io. If you like this content please sign up for my newsletter I and will keep updated with new suff about CSS, React and Javascript.",css,https://medium.com/@daniel.js.craft/my-top-eureka-moments-while-learning-css-28e20ea4459e?source=tag_archive---------12-----------------------
5 Features of Sass That Will Make You Love CSS,"I love being a full stack engineer. I appreciate backend and frontend code equally and for very different reasons. Backend applications amaze me by the sheer fortitude with which they operate, querying and manipulating and returning exactly what is needed from mass amounts of data.Frontend has a special place in my heart though because of the artistry that goes into a beautiful UI/UX design. Breathing life into these is just pure fun. My feelings towards the aesthetic aspects were not always so effusive, though. HTML drove me somewhat crazy before I found JavaScript (and more specifically React), as did CSS until I found Sass.Sass, or, Syntactically Awesome Stylesheets, humbly refers to itself as CSS with superpowers — a statement with which I would agree. Sass is the extension language needed for styling anything larger than a basic hobby application. It provides scoping which in turn creates consistency, easy variable declaration, flow control, and more.Sass is a preprocessor scripting language that gets compiled into classic CSS script. The below is from the official documentation, and sums up exactly why Sass is a necessary tool in modern frontend engineering:CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don’t exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again.In plain English, Sass provides tools for cleaner, more manageable styling, and much needed functionality that is not yet available with standard CSS.Sass files should use the extension .scss, and in reality the script contained within these likely won’t look too dissimilar from CSS. As I said above, it is the extension language that you need, meaning that while it provides additional functionality, all CSS script is valid in Sass files. Setting background colors, font styles and container widths will be nothing new for you.Now, for the best 5 features in Sass…Vanilla CSS does support the use of variables, but Sass greatly simplifies them. Using Sass, you declare the variable name with $ in front, and then assign it a value (not a property, but a value, ie #F5F2D0). I tend to use the practice of assigning all variables globally in a _variables.scss file, making it incredibly easy to play with different colors through the application while maintaining consistency.This is my preference, but Sass also supports local variables. Any variable declared at the beginning of the stylesheet is global, and anything declared within a block (between curly braces) is local to that element. Given this, I import my global variables sheet at the top of each of my individual stylesheets, to make the same set of variables available throughout.Sass also provides a !default flag that allow users to customize a Sass library while still maintaining the library’s stability, as well as a couple of advanced functions that aid in determining whether a variable exists.Learn more about Sass Variables.Most of the standard operators that you are used to working with in programming are supported in Sass:I find that what makes Sass special is not its features on their own, but rather the combined force of them. For instance, you can see how operators and variables might make some of the math that goes into CSS a bit more simple. Properties that relate to element size like height, width, margin, and padding, often are dependent on one another, and we have all had the headache of trying to calculate this and that to make the page look just so. You need certain elements to line up, or one element to stretch the width of two others. By setting variables and then working with those rather than hard coding values, adjusting sizes and layouts becomes a much more manageable and dynamic process.For example, in the situation of needing one element to stretch the width of two others, you can set the widths of the two using variables, and then add the two variables together to set the width of the stretched element.Learn more about Sass Operators.This is where things get really fun. Most of what I have said above is so useful because of the availability of at-rules. At-rules are awesome. There are too many to go through here (the full list can be found in the documentation) but we will cover some of the most important ones. CSS already supports a number of very useful at-rules, such as @import and @font-face which I recommend familiarizing yourself with first if you have not already. Sass at-rules syntactically work the same way.The use rule can be used to import other stylesheets as modules, the components of which can then be accessed in a modular fashion. The namespace for the module is just the last component of the path, but can be aliased using ‘as’.This variable:Can be used in another file like this:Or:The Sass documentation really explains these rules best:Mixins allow you to define styles that can be re-used throughout your stylesheet. They make it easy to avoid using non-semantic classes like .float-left, and to distribute collections of styles in libraries.The mixin rule will define the styling, and the include rule will trigger the usage of the mixin block. Mixins can accept arguments, making them fully customizable in their reuse. This rule is very useful when you have multiple components with similar but not identical properties, and the similarities are not arbitrary (ie, they are related somehow — if one changes, the other will need to change too).For instance, let’s say you want to create a series of containers that are identical but have different color schemes. You could use mixins like this:Sass functions are going to look and feel like any other basic functions that you are writing in your code. They accept arguments and can encapsulate iteration, for loops, mathematic operations, and at the end, return a value.These are incredibly helpful when calculating dimensions, defining color schemes (inverting, lightening, or darkening an existing color), or really any dynamic styling that requires more than basic mathematics or variable use.These at-rules enable flow-control in your stylesheets. Again, they each are going to look and feel very similar to other if/else, each, for, and while statements that you are writing.The flow control rules are often used inside of functions to conditionally or iteratively return a value. Variables can represent objects or arrays, which can then be iterated over.Typically used with other at-rules, such as conditions or functions, the error at-rule will throw an error when one occurs. As someone who once spent 4+ hours debugging a Heroku deployment because of a syntax error in my vanilla CSS (no, I am not exaggerating), I am a big fan of this at-rule.As I said earlier, what makes Sass great is the combined force of its features. This is definitely true in the case of interpolation. Interpolation is supported in any part of a Sass stylesheet, and can be used in conjunction with functions and variables. You can use interpolation to dynamically refer to properties, classnames, values, elements, and more.For example, you can create a function that determines whether you want to use margin or padding on a given class, and interpolate the result of the expression as follows in order to set the property dynamically.This may be a bit of an obvious one, but it really is the headline here. Sass is a preprocessor, and it flawlessly compiles all of these special variables, functions, mixins, style modules, and rules into everyday CSS. You do the prep, they do the heavy lifting.You can find the full documentation for Sass here, and I recommend spending some time looking through it in order to fully wrap you head around its capabilities.",css,https://levelup.gitconnected.com/5-features-of-sass-that-will-make-you-love-css-25e707253ea5?source=tag_archive---------1-----------------------
The 🍔 menu,"Mikael AinalemJun 2, 2020·10 min readThe Hamburger Menu widget is on every other site nowadays. It has become synonymous with the web and, perhaps even more so, with web development. Have, for instance, a look at Dribbble or Codepen. There you’ll find a fair share of examples. They come in all shapes and sizes where one is more elaborative than the other. Developers and designers can’t seem to get enough of the widget.The Hamburger Menu is not without controversy. Some hate it, and some love it. Numerous articles are debating it and its alternatives. Some argue that its proper place is in the history books. Regardless of its fate, it continues to have widespread use. Over and over, it keeps showing up in new sites. It’s especially popular for mobile views where menus typically are hidden.There are quite a few variants out there exploring different kinds of animations. I’ve created a couple myself. Here are a few of my creations:In its most simple incarnation, the Hamburger Menu comes as straight, parallel lines. Usually, they’re three. These lines stay in some form of a clickable container. Shapes and sizes of menus may vary, but their use is the same. Clicking them toggles the hamburger’s state. This interaction makes the menu go back and forth between its opened and closed state. The conventional way to portray the opened state is by showing an X. It signals to the user that tapping/clicking the button again closes the menu.Ever so often, there’s an animation going between these two states. Buttons like these are excellent opportunities for web developers to delight their users. Generally speaking, it’s the perfect place to add an animation. Animating the button’s state transition is not just pleasing to the eye; it also serves a purpose. It’s a good UX to give users feedback on touch and click interactions.SVG line animation is, as the name implies, a technique to animate lines. Or, more specifically, SVG paths. It creates an appearance of drawing a line on the screen. The method emerged mid-last decade, and it has remained popular since. This article from 2014 explains the technique in detail: https://css-tricks.com/svg-line-animation-works/.The effect is ideally suited for the Hamburger Menu as the widget is, most often, created with lines. This article discusses how to use the technique to animate between the two different states of the Hamburger Menu.First of all, let’s start by drawing the three lines. Drawing vectors requires a vector editor. I.e., if you’re not a hard-core SVG coder and like to do it by hand. I use Inkscape to draw my vectors.The first thing to do is to find a suitable size for the SVG drawing. Using a 100 x 100 pixel SVG document is a good idea. Working with even numbers makes it easier to relate and work with sizes and proportions. It’s, in theory, possible to go with whatever size when drawing these vectors. Remember, the S in SVG stands for scalable.When drawing the lines, let’s also reserve some wiggle room for the animation on the sides of the menu. This space is reserved for later on when animating the lines. In the editor, the pen tool creates new strokes.Was it too quick for you? If so, here’s the recipeThe numbers above, e.g., the stroke width, need not be the exact numbers, used here, should you recreate this example. The important thing is choosing values that create something visually appealing. It is possible to tweak these numbers and fine-tune them. They should be such as the hamburger matches the style of the other content on the web page. It’s the web developer’s job to create a consistent look and feel building web pages.A look at the produced code should look something like follows. These essential parts below should be found somewhere in the SVG code.Next up is creating the X close icon. The way to build it is by extending the top and bottom lines. The idea here is to make the SVG document include both shapes at the same time. Combing the opened/closed states goes perfectly together with the SVG line animation technique. It makes it possible to animate between the different shapes of the menu. The creation, at this point, might look like a bird’s nest. Don’t worry. It’ll make sense later on discussing the animation. The following three parts should be part of the model:The way to extend the line is to break up the process in three steps.The first step is to extend the top line with an edgy line that includes part of the X icon. The drawing, at this point, doesn’t need to be perfect. It just needs to be good enough, so you spot half the X.After extending the path, the next step is to move the connecting nodes to their appropriate coordinates. The connecting line can be arbitrary. However, the X icon should be proportional to the hamburger. In this example it’s a line going from (25px, 25px) to (75px, 75px).Last but not least, converting the line to a Bezier curve makes it smooth. First, let’s look at what it looks like doing the above procedure for the top path. Here again, the pen tool once again comes in handy.Once the top line is in place, it’s time to do the same procedure for the bottom line. Since everything needed is already on the screen, it then makes sense to reuse the work above. Duplicate, flip, and position the copy does the trick to replace the previous bottom line. Bringing objects forward & back is one way to access the right path at the right moment.When the model is complete, it’s time to move it over to HTML. Luckily, HTML is interoperable with SVG. That means that anything SVG directly works inside any HTML document. Copying the SVG code into the markup makes it appear on the screen when loading the page in a browser.Placing references, HTML selectors, on the SVG DOM nodes, is what to do next. Doing so opens up the markup to the wonderful world of CSS. Or, more specifically, it enables manipulation of the SVG paths. With CSS, these SVG elements can have styles, be animated, and inspected. Below is what the structure of the CSS rules looks like after adding classes:Another thing to do is to center the menu and to remove the default margin. This is not a must, but it makes it more pleasant to work with the menu. A flexbox container, conveniently places the widget in the center of the browser window.An essential part of the SVG line animation technique is knowing the length of the paths. Here is where the Dev Tools shines. The inspector’s console window can, in numerous ways, access DOM elements. One of the more convenient ways is the nifty focused element shorthand $0. It gives direct access to the currently focused node in the inspector. The function call getTotalLength, callable on any SVG path, measures the lines.Measuring the lines gives the lengths 207px and 60px. It should come a no surprise to see the number 60 again. It is the original length of the hamburger lines. Another thing to note is that the first and the last lines are equally long except for minor rounding errors. This outcome is what to expect as they are duplicates. Both of them are 207px long when rounded upwards. These two numbers, 207 & 60, are the values needed to get started with the SVG line animation effect.A great way to get accustomed to the line animation technique is by using the inspector. The style editor lets you change the CSS rules with immediate feedback. Visualizing the change makes it easier to get a feel for how the CSS affects the SVG paths. The instant feedback loop quickly helps to hon in on desired values. The goal here is to use the model, created above, to find the following two sets of values:These two sets of values, in turn, represent the end-points in the animation. Interpolation between these two extremes is what creates the actual animation.The first thing to do, working with the animation, is to set the stroke-dasharray rule for one of the long lines. The stroke-dasharray rule takes a range of values that describe dashes and gaps. The animation effect in this article needs only two values. They are one dash and one gap. One way to find the dash/gap value pair is to set both values to the full length of 207px and work backward. The keyboard’s up and down arrows stepwise alter the value in the editor. Stepping through the first value in the set of values reveals the hamburger.The same procedure, as above, goes for the X icon. This time the second rule, stroke-dashoffset, comes into play. The offset pushes the line forward to reposition it. The diagonal line in the X is slightly longer than the lines in the hamburger. For this reason, the line needs an extension. Adding a handful of pixels adjusts its length.Now, let’s have a look at the results. Below are the different sets of values found:The Dev Tools is a powerful ally when it comes to animation. With the inspector, it’s possible to test, fine-tune, and record animations. The style editor allows modifying CSS to re-run animations immediately. All inside the browser. The Dev Tools provides the perfect playground for exploring and crafting animations.The simplest way to get objects moving on the screen is by using CSS transitions. They’re easy to use. One specifies what other CSS rules to animate. Setting the target rule, together with duration and easing, immediately enables animations. It is very convenient. Once in place, the only thing needed to start an animation is to change the value of the target rule. Here’s what the transition rule looks like for the Hamburger Menu:Notice the specific easing used. The cubic Bezier easing above comes from the Material UI guidelines. It’s a bit more punchy compared to the regular ease easing.Here’s what fine-tuning in the Dev Tools might look likeThe event handler is the last step in creating the animation. It’s what makes the menu interactive. Toggling the “opened” class triggers the animation automatically. The only thing needed is a place to trigger the state transition. The SVG element itself serves as a perfect place to insert the handler.After publishing the first version of this article, Dennis Lembrée pointed out, that the above version is not accessible. Accessibility is important, so let’s fix the fundamental parts. The right thing to do is to wrap the menu inside a button and move the event handler to the button. With buttons, some of what’s needed for the menu to be accessible comes for free. The menu becomes focusable and it automatically enables keyboard tab navigation.Adding an additional aria-label is also very helpful. It gives contextual information to users relying on screen readers.And here’s the complete menu on CodepenThat’s it! If you reached this far, you can hopefully now know a bit more about Hamburger Menus. Thanks for reading, and the best of luck with your animations!Cheers,Mikael",css,https://uxdesign.cc/the-menu-210bec7ad80c?source=tag_archive---------0-----------------------
HTML Email Template for Beginners,"Suprabha SupiJun 3, 2020·1 min readFor any HTML email developer, we all know how difficult it is to create a fully working and compatible HTML email for newsletters or campaigns.The knowledge I have gained over the past 6–10 months, to create beautiful and compatible HTML email for my company’s campaigns.Some useful resources that I used:HTML Email CheckPutsMailThere are few CSS properties, which will not work in email template. You can refer the below link for all the CSS properties which you can use while creating any email template:Campaign Monitor CSSThere is one easiest way to create an email template by using table element.Reference:smashingmagazine.com for layoutI hope you enjoyed the article. If you have any question, please feel free to ping me on @suprabhasupi 😋🌟 Twitter | 👩🏻‍💻 suprabha.me",css,https://medium.com/@suprabhasupi/html-email-template-for-beginners-34b209279548?source=tag_archive---------11-----------------------
windows多重選單_amos金魚15集改_阻止事件冒泡_移除事件監聽,"yuJun 2, 2020·3 min readcdpn.io主要把左右開合改成上下開合transform: translateY(100%); 改用Y藏在畫面下方transform: translateY(0%); 讓它顯示出來height: 70%; top: 30%; 縮短長度讓上方騰出點空間— — — — — — — — — — — — — — — — — — — — —所有的ul都定位在父層li身上(沒錯沒錯!!)….絕對定位會找最近的相對定位/固定定位!!然後再搭配left:100%就會出現在父選單右側了!!border: none; (不占空間)background: transparent; 去除input/button的背景outline: none; 去除input/button的外框(無外框)line-height與height相同時，會選水置中。text-decoration: none;……無裝飾(去除底線)color: white;……直接覆寫超四種狀態的色彩妹妹選擇器(跟屁蟲選擇器)#side-menu-switch:checked+.side-menu#side-menu-switch:checked+.side-menu label當checked時選到的誰要做甚麼css!!(checkbox最好跟相要處理的dom在緊連label如果沒要跟著處理css時可以隨便擺!!js 處理遇到的挑戰有在做關閉選單時，因為start按鈕也屬於windowsBar的一環，所以會傳遞到windowsBar的事件監控事件的冒泡…會一層一層傳遞，所以需要stopPropagation停止事件傳遞因為我想要點選windowsBar / desktop / start都能關閉選單需要在開啟選單時，在windowsBar / desktop註冊事件監聽，搭配一個關閉選單的winBarHandler函數當判斷選單是開啟時，點選windowsBar / desktop會執行menu_close()函數並且移除事件監聽!!if (flag) {menu_close();windowsBottomBar.removeEventListener(‘click’, winBarHandler);win_desktop.removeEventListener(‘click’, winBarHandler);}",css,https://medium.com/@racingf1/windows%E5%A4%9A%E9%87%8D%E9%81%B8%E5%96%AE-amos%E9%87%91%E9%AD%9A15%E9%9B%86%E6%94%B9-%E9%98%BB%E6%AD%A2%E4%BA%8B%E4%BB%B6%E5%86%92%E6%B3%A1-%E7%A7%BB%E9%99%A4%E4%BA%8B%E4%BB%B6-d3e470d06d06?source=tag_archive---------15-----------------------
Top 17 frequently asked CSS Interview questions and answers,"Do DungJun 3, 2020·8 min readDung Do Tien - Jun/03/2020We have selected 17 frequently asked questions in the CSS interview to help you synthesize your knowledge of CSS and prepare well for your interview. We hope it helpful to you.Answer:The box-sizing property defines how the width and height of an element are calculated (related to the box model in CSS). As you know by default:- Width of an element = border left + padding left + width of content + border right + padding right.- Height of an element = border top + padding top + height of content + border bottom + padding bottom.Let see an example:Okay, you can see that:Now we change padding 0 to 20pxNow the width and height of this div have changed:* Summary: you will see a problem total width and height of that element is increase when adding more padding and border, This will cause the boxes to the right of it to be pushed back to the right or down the line. It may be broken GUI of your page if you add more padding or border. To fix this problem please follow more below.Answer:The CSS Box Model All HTML elements can be considered as boxes. The CSS box model is essentially a box that wraps around every HTML element. It consists of: margins, borders, padding, and the actual content. You can see the picture below:This is a <div> tag, it has content w : 794px and h: 160px, padding : 50px 20px 50px 20px, border 5px and margin: 50px 30px;So what is the total width and height of this box?To calculate the width & height of a box we have formula is :Now we apply this formula to calculate the width and height of the above box:Answer:The display block always displays 100% of the line. even if the total content does not show 100% of the line, it will replace with margin and do not allow other elements to display on the same line as it.See an example :<p> tag is a block tag and I set width to 200px.You can see it always display all of the lines, although I have set the width to it.3.2. Display inline-blockInline block it only displays width depends on the content inside of it or depends on you set the width to it. You can display many tags inline-block in the same line.You can see the example below to understand:Answer:Using @font-face of css3 to import font. See an example below:You can refer below q&a to get more info:How to add custom font into Html using CSS?Answer:We have 3 ways to import CSS in Html page:Also, you can use javascript to add CSS in the Html page.How to add css to html page for BEST performanceAnswer:Because of some reasons below:Answer:Selectors are patterns used to select one or more elements in your Html page which you want to style.We have many selector support in CSS some popular selectors are:you can refer here to see moreAnswer:Between class and id selector in CSSAnswer:::after ::before ::first-letter ::first-line::selectionFor example, I style for the first letter of <p> tagSee the result here.Answer:Answer:Answer:Answer:A responsive website is an approach to web design that makes web pages render well on a variety of devices and windows or screen sizes. It must auto adjust and adapt to any device screen size such as laptop, tablet, and mobile.To make a responsive website use can using CSS, In Css3 has supported @media screen to help you make responsive websites easily.To get more code and step by step to do you can refer below article:Best way to design web responsive with Grid View in CSSAnswer:To center a div we have many ways to do, I can write down 6 ways as below:You can refer below article to see details for each way.How to center a div tag in CSSAnswer:To create a circle with a <div> tag you can use border-radius property of CSS to do. but have a note width and height of the <div> tag must be the same.For example:You can see the result here.Answer:Answer:A pseudo-class is used to define a special state of an element.For example:- Style an element when a user mouses over it- Style visited and unvisited links differently- Style an element when it gets focus- Select the first child or last child element…Some pseudo-classes are::active :checked :disabled :empty :first-child :focus :hover :visited …Originally published at https://quizdeveloper.com.",css,https://medium.com/@dodung1221/top-17-frequently-asked-css-interview-questions-and-answers-a6029abc7840?source=tag_archive---------10-----------------------
CSS Fundamentals: The CSS Grid Guide,"CSS Grid is a modern layout system that we can use when laying out pages.It’s often compared with Flexbox. And whilst they are both excellent systems for working with complex layouts, there is one major difference: CSS Grid works on 2 dimensions (rows and columns), while Flexbox works on a single dimension only (rows or columns).If you only need to define a layout as a row or a column, then flexbox will likely suit your needs. When working in both dimensions — it’s time for CSS Grid!🤓 Want to stay up to date with web dev?🚀 Want the latest news delivered right to your inbox?🎉 Join a growing community of designers & developers!Subscribe to my newsletter here → https://easeout.eo.pageWe activate the grid layout by making an HTML element a grid container:Our HTML:In our CSS, we simply set its display property to grid:A grid layout consists of a parent element, with one or more child elements.There are a set of properties which can be applied to the container element, as well as any child elements (being each individual item in the grid).Throughout this guide we’ll work with the following code:HTML:And our CSS styles:The most common container properties are grid-template-columns and grid-template-rows. With these properties we define both the number of columns & rows as well as the width of each.For example, let’s tell our grid to layout its items (child elements) in 4 columns at 200px wide, and 2 rows with a height of 150px each.And lets now make it a smaller 3x3 grid:Often you’ll be working with elements with no fixed size. For example, you could have a fixed navbar followed by a flexible content section, then a fixed footer section. For this we can use auto and the layout will adapt to the size of our content:We can add spacing between grid items using grid-column-gap and/or grid-row-gap:We could also use the shorthand grid-gap to set both at once:We can control how much space each grid item takes up in the column or row with the following properties:Let’s see an example:Here’s we’ve added classes to the first & sixth items in our grid.The numbers correspond to the vertical line separating each column. So by setting grid-column-start to 1 and grid-column-end to 3, we’re telling our element to start at the first line & end at the third.Similarly we’ve told our sixth element to start at the 3rd line and end at 5.This of course also applies to grid-row-start and grid-row-end, with the cells expanding across rows instead of columns.We can repeat the above using the shorthand properties of grid-column & grid-row, like so:And we could take this even further by using grid-area as a shorthand for grid-column and grid-row. This would only apply in cases where we need an item to span both rows & columns:Would become:With the order being: grid-row-start > grid-column-start > grid-row-end > grid-column-end.Another option we have when positioning our items is span:With grid-column: 1 / span 2 starting at line 1 and spanning across 2 columns.One of the great benefits of grid is the ability to easily create highly flexible layouts.Fraction units give us the ability to build layouts without needing to specify fixed dimensions.For example, lets divide a grid into 3 columns of equal width, each taking up 1⁄3 of the available space:Too simple!We can use any of the CSS length units. So feel free to use a mix of percentages, pixels, rem, em and fractions:We can use repeat() to specify the number of times a row or column will be repeated, and the length of each.It’s a handy way to quickly put together a layout & it also reduces lines of code! For example, you could define 3 columns of equal width as follows:We use minmax() to specify a minimum or maximum width for a grid track.Let’s say you want a column to be between 100px and 300px, followed by a 1fr column:The value for min has to be smaller than the value for max. And fr units can’t be used for the min value, but they can be used for the max!By using a 1fr as the max value, you’ll ensure that the track expands and takes up the available space:Used this way, minmax() allows us to create grid tracks that adapt to the available space, but that don’t shrink narrower than a specified size.Now if the browser is resized, the 1st column won’t shrink to less than 250px.You can also use the auto, min-content and max-content keywords as the min or max values.We use justify-content to align the whole grid inside the container.There are a number of values we can work with:Keep in mind that the grid width has to be less than the container width for the justify-content to work!Let’s see an example of each:We use the align-content property to vertically align the whole grid inside the container.Our grid height needs to be less than the container height for this property to work.We can use grid-template-areas to define named areas & move them around inside the grid, and also to expand grid items across multiple rows and/or columns.Let’s use grid-template-areas to build a typical layout with a header up top, a sidebar to the left of the main content, followed by a footer:And the code used:HTML:CSS:Notice that despite the header being the last element in our HTML, it’s still at the top of our page. This is because we’ve defined it’s position in CSS with grid-template-areas using the grid-area property.If we want the sidebar to move below our main content on mobile devices, we can easily do so using a media query:Are you ready to take your CSS skills to the next level? Get started now with my new e-book: The CSS Guide: The Complete Guide to Modern CSS. Get up-to-date on everything from core concepts like Flexbox & Grid, to more advanced topics such as animation, architecture & more!!Thanks for reading! 🎉🎉🎉",css,https://itnext.io/css-fundamentals-the-css-grid-guide-1efe31542cfe?source=tag_archive---------5-----------------------
"Create an Image slider with HTML, CSS and JavaScript","I am currently learning web development with the OdinProject curriculum. There’s a task which is to create a simple image carousel. It should contain arrows on each side to advance the image forward or backward. It should automatically move forward every 5 seconds. It should contain the little navigation circles at the bottom that indicate which slide you are on (and they should be click-able to advance to that particular slide).First, let’s create an html file named index.html.In the html file, we have a container that serves as a frame for each slide and each slide contains an image.Let’s add the styles. I’ll assume you have basic knowledge of CSS for you to want to build an image slider. I’ll try to make the styling basic and simple.The slide display property is set to none which makes them not visible now. The slide container and heading are centered too. We will add the functionality in the JavaScript to make the slides visible.Now, let’s style the next and previous buttons and the dots for navigation. Also, add an active class to style the dot for the slide that is currently being displayed.All we have displaying now is the heading, next and previous buttons, and the four dots.It’s time to add the functionality. Create a file named index.js and this to it.We created a variable named currentSlide that stores the index of the current slide to determine the current slide.We also created a variable called slides to store each slide into a array which enables us to iterate over them and another variable named dots to store all the dots in an array.Then we created a function named init that accepts a parameter n. The parameter will be currentSlide passed into it. Inside the function, we iterated through slides and set each slide’s display property to none. While iterating through the slides, we also iterate through dots and remove the class active from each do. When done setting each slide’s display property to none and removing the class active from each dot, we then set the display of current index according to the currentSlide, to block and add the active class to the dot of the current index using currentSlide variable.And lastly, we add an event to the window to run the init() function when the HTML content is done loading.We add this to the index.js file.We created a function named next to change the current slide to the next one. Here, I used the ternary operator instead of if-else statement. Inside the function, we checked if the currentSlide is greater than or equal to the last index of the slides (4 -1 = 3) which is an array. If it is true, we reset the currentSlide to 0, else we increment currentSlide variable and we run the init() function with currentSlide value.For the prev() function, we check if the currentSlide variable is less than or or equal to zero. If it is true, we set currentSlide to last index of the slides (4 -1 = 3), else we decrement currentSlide.And finally we add click event on next and previous button. When you click on the next button, it runs the next() function and when you click on the previous button, it runs the prev() function.To make the slide change automatically, we set a timer that runs the next() function every 5 seconds. Add this to index.jsWe also want to make the dots clickable to advance to the the next slide. Add this to index.jsHere, we iterate through the dots variable and for each dot, we add an click event and run the init() function passing the index of the dot that is clicked as the parameter and also setting currentSlide to that index.Yes, that is all. We now have a working image slider.This is my first ever article. Let me know what you think about it by leaving a response.Thank you for reading.",css,https://levelup.gitconnected.com/create-an-image-slider-with-html-css-and-javascript-3bf2c3e84060?source=tag_archive---------0-----------------------
3 Easy Ways to Write Better CSS,"For UI development, it’s pretty easy for CSS files to grow as the design demands for your project evolve. If you’re working in an agile environment, one that is categorized by pivoting changes frequently and consistently, this design evolution is typically more gradual. Given the gradual, consistent change, it’s common for a developer to “add on” new styles rather than rewrite your existing ones.Say you have a design of three boxes with a black border. In the next sprint, someone says they want the second box to have a red border. Your instinct might be to simply add a new class to separate that box from the others and give it a border style property as solid 1px red. This works, but it is not optimal when you consider the following.Most projects will be larger than the example above. If you doubled the size of the project, your CSS will remain the same but you’d need to add another class name to another div. The more boxes you need, the more class names you have cluttering your HTML.In the above example, there is only one style variance for one element. Imagine if they wanted to add text to those boxes with varied text-alignment based on which position the boxes are in. Using the above process, you’ll end up adding more classes for more changes and both your HTML and CSS files will grow and become less organized and flexible. Which brings us to our last consideration.As mentioned above, with project growth and change in design demand, it is important to have a foundation that is flexible and as light weight as possible. It’s not unreasonable to add more styles/classes should the needs of your app change/grow, but anywhere that you can replace convenient code for efficient/reusable code will greatly improve the flexibility of your app and therefore require less work when changes are needed. In order to create an ecosystem that is sustainable and flexible, syntax is very important.Below is the UI for the project we’ll be developing throughout this blog post.We have three boxes with red borders, a title (h1) and a subtitle (p). Box one is left aligned, box two is center aligned and box three is right aligned.Here is the initial HTML.Here is the CSS.Given the code above, let’s explore three ways that we can refine and condense our CSS to ensure our app is more flexible, lightweight and condensed.We can start by identifying repetitive attributes. Whenever you see PARAMETER-DIMENSION (such as margin-top), there is an opportunity to condense code. Unless it’s a case where only one dimension is needed and the others are negligible, or if all dimensions require the same parameter, a more efficient way to write the dimensions is to consider the acronym TRBL.In CSS, you can follow this pattern for writing variable dimensions for attributes like margin and padding. Below are a few areas where this pattern can come in handy.In .containerChild we can use border to define the color and the style, while we can use border-widthT to define the width of the TRBL borders.This is great, but we can refine our shorthand even further. Since the values for top and bottom are the same, and the values for left and right are the same, we can rewrite this using only two parameters.As of now, we have saved ourselves 6 lines using the TRBL technique. The next way to refine your CSS proves that sharing really is caring.You may have noticed that some of our elements have similar styles. Let’s take another look at h1 and p.Is this case, the only difference between the two elements’ styles is the color. To avoid the repetition of the other styles, we can use commas to separate the selectors and then give them a shared group of styles.By using a comma to separate the selectors h1 and p we can consolidate similar styles. As for the different styles, we can add a selector for the element with different styles (in this case, p) and then give it only the attribute(s) we want to change.At this point, we have a total savings of 8 lines. It doesn’t seem like much but our original CSS was only 33 lines. That’s a little more than a 24% reduction in code size. If you apply that rate of savings to a style sheet that has, say, 1000 lines, that’s a savings of more than 240 lines.Combinators can ensure your CSS is precise by maximizing element associations without overwhelming your HTML.The use of CSS combinators can help you keep your HTML lighter and more flexible. A lot of classes and id’s can hinder style re-usability and add to markup. Our app has a few instances where we can replace class selectors with combinators..containerChild is the class selector used to target the three boxes within the parent .container. This can be accomplished using the child combinator (>), to select the child div’s within .container.This won’t replace/reduce CSS code, but it allows us to remove the .containerChild class name from the boxes. The same can be done using a combination of the child combinator and the nth-child selector to target the styles for two and three.We are able to capture the box div’s as well as their h1 and p tags. Again, this doesn’t save us lines of CSS code, but check out our new HTML.We are now able to achieve our original result with just one class name. As of now, we have reduced our lines of CSS by about 21%, and reduced our use of presentational values in markup by 87.5% ( 8 elements with class names to just one)!With our HTML more lightweight and our CSS more refined, there is still one other potential area of efficiency that we have improved upon.Imagine if each of the boxes are meant to represent list items? Or blog post thumbnails? Now, unless you’re planning to hard-code each list item into your HTML (a solution that is not dynamic and potentially unsustainable) you’ll need to use JavaScript to render each of the items and apply the styles outline above.By removing the need for class names and inline styles, we have removed multiple lines of JavaScript for every iteration we make through the data. Say the list of items had a length of 100. Since we’re using the combinators, we won’t need to use element.className = ... for any of our elements (aside from .container). Before combinators, we would need to add the classes to our elements programmatically.That’s three instances of element.className that we are able to avoid for 100 items, which translates to a reduction of 300 operations (3 * 100 = 300).The three quick ways to write better CSS are as follows…Apply the acronym TRBL (Top, Right, Bottom, Left) anytime you are trying to apply a style of varying dimensions such as padding, margins, and borders.Using commas, you can consolidate shared styles with multiple elements without having to specify those styles multiple times for each element.Combinators can help you can avoid heavy use of class names and id’s in your HTML. This can make your HTML simpler, more flexible, as well as reduce the amount of programmatic operations needed should you want to incorporate JavaScript.Source code for this post can be found on CodePen.Note: HTML structure provided in this post is not meant to represent HTML that meets accessibility standards. It is being used for convenient and demonstrative purposes relating to CSS refinement. For more on these guidelines please read the W3C standards guideline.medium.comskilled.devwww.w3.org",css,https://levelup.gitconnected.com/3-ways-to-write-better-css-ce4c8a1294fa?source=tag_archive---------0-----------------------
好用的背景漸層網站,"yuJun 1, 2020·2 min readwww.gradient-animator.combackground-size: 400% 400% 預設會將你的圖片x,y軸(left, top)各放大400%~600%，在搭配 background-position:0% 50%---X,Y軸的定位做變化(所以需要需要放大SIZE)Gradient Angle調整linear-gradient角度(第一個參數)...(90度為左到右/180度為上到下)Scroll Angle調整animation裡 background-position的x,y軸(left, top)定位的變化!!0%{background-position:0% 50%} …左下角50%{background-position:100% 50%} ...右下角(扣除寬度80~90%差不多，但這樣效果也一樣)100%{background-position:0% 50%}....左下角預設由粗體字這樣~~background-position定位參考MDN: https://developer.mozilla.org/zh-CN/docs/Web/CSS/background-position按下Save CSS as Gist 就會出輸到你的GitHub帳戶摟~~(不存檔在右邊的code就是了)以下是我存檔的https://gist.github.com/GradientAnimator/5acd090ca345be75e59351c9d88013be",css,https://medium.com/@racingf1/%E5%A5%BD%E7%94%A8%E7%9A%84%E8%83%8C%E6%99%AF%E6%BC%B8%E5%B1%A4%E7%B6%B2%E7%AB%99-4448d54d8587?source=tag_archive---------13-----------------------
CSS Fundamentals: How to center elements,"Centering in CSS has traditionally been a cause for frustration. It’s an everyday task yet there are so many possible solutions! The approach you take will differ depending on whether you’re centering horizontally, vertically or both!In this tutorial, we’ll look at which methods are best in each scenario.🤓 Want to stay up to date with web dev?🚀 Want the latest news delivered right to your inbox?🎉 Join a growing community of designers & developers!Subscribe to my newsletter here → https://easeout.eo.pageInline elements like text (and links) are super simple to center horizontally using the text-align property:This works for inline, inline-block, inline-flex, inline-table, etc.A block-level element can be centered if both its margin-left and margin-right properties are set to auto (assuming the element has a width). It’s often done with the margin shorthand:However, the modern way to center block level elements (anything that isn’t text) is to use Flexbox!Let’s assume we have HTML like so:Add the following CSS:This will horizontally center any element inside the .container element.Vertical centering has traditionally been quite tricky. And more often than not it’d be accomplished with code like so:This indeed works as it moves the child element back up half the distance of of its height. And it positions the middle of the child element at the middle of its parent.However, we can do this much more simply with Flexbox:This will vertically center any element inside the .container element.To center both vertically and horizontally we simply combine these techniques!To perfectly center an element, in the middle of its container:If using CSS Grid, we can also do this easily:Anything can be centered in CSS!Once you know the basics of Flexbox and Grid, you’ll see just how simple it is! 👍Are you ready to take your CSS skills to the next level? Get started now with my new e-book: The CSS Guide: The Complete Guide to Modern CSS. Get up-to-date on everything from core concepts like Flexbox & Grid, to more advanced topics such as animation, architecture & more!!Thanks for reading! 🎉🎉🎉",css,https://itnext.io/css-fundamentals-how-to-center-elements-15ae82946aa0?source=tag_archive---------7-----------------------
CSS Glitch Effect,"Today, let’s see how to make a text glitch using the CSS<!doctype html><html><head> <meta charset=”utf-8"" /> <meta name=”author” content=”Web Engineering” /> <title>CSS Glitch Effect</title> <link rel=”stylesheet” type=”text/css” href=”style.css” /></head><body> <div title=”Comment what You want to learn next “> Comment what You want to learn next  </div></body></html>Make sure both the title and the material within the div is the same. If both are different, they can override each other.I’ve linked css as an external css, so you can have an internal css that depends on you.You can use the div tag within the body tag where you need to add the glitch.Since the key component is CSS, the CSS section is going to be long.body{ display: flex; width: 100vw; height: 100vh; align-items: center; justify-content: center; margin: 0; background: #131313; color: #fff; font-size: 96px; font-family: sans-serif; letter-spacing: -7px;}div{ animation: glitch 1s linear infinite;}@keyframes glitch{ 2%,64% { transform: translate(2px,0) skew(0deg); } 4%,60% { transform: translate(-2px,0) skew(0deg); } 62% { transform: translate(0px,0px) skew(5deg); }}div:before,div:after{ content: attr(title); position: absolute; left: 0;}div:before{ animation: glitchtop 1s linear infinite; clip-path:polygon(0 0,100% 0,100% 33%,0 33%);}@keyframes glitchtop{ 2%,64% { transform: translate(2px,-2px); } 4%,60% { transform: translate(-2px,2px); } 62% { transform: translate(13px,-1px) skew(-13deg); }}div:after{ animation: glitchbottom 1.5s linear infinite; clip-path: polygon(0 67%,100% 67%,100% 100%,0 100%);}@keyframes glitchbottom{ 2%,64% { transform: translate(-2px,0); } 4%,60% { transform: translate(-2px,0); } 62% { transform: translate(-22px,5px) skew(21deg); }}Since we use the animation property, it has to be defined with the keyframes.Each @keyframe at-rule determines what will happen at different times during the animation. For example, 0 percent is the beginning of the animation and 100 percent is the end of the animation.We split div as div: before and div: after. And div: before, for the top of the glitch-text, and after, for the bottom of the glitch-text.The clip-path property in the CSS allows you to designate a particular section of the item to be shown, rather than the whole field.If anything is not clear or you want to point out something, please reach meMy Website → https://saravananvijayamuthu.herokuapp.com/Weather app → https://gadot.vercel.app/Github → https://github.com/SaravananVijayamuthuLittle Occult Affairs → https://www.amazon.in/dp/8194619920/ref=cm_sw_r_cp_apa_i_GEHnFbXWYB45D",css,https://medium.com/saravananraghul/css-glitch-effect-ba9981b1f5b?source=tag_archive---------9-----------------------
ViewEncapsulation Emulated and override css from a component in Angular,"This article will be to clarify how to override a css from an Angular component coming from a 3rd party library as PrimeNG, AngularMaterial or other libararies that in some cases don’t expose a way to do.Let’s analyse an example:In this image:The selector generated for the element with the class or-PP(green line in image) is using a class selector combined with an attribute that makes it strong (that’s how angular emulates the shadow dom — css specificity).Also if you declare a class or-PPin the parent of `test.component` (hello.component) it will not use the test component attribute `_ngcontent-c2` but the parent one `_ngcontent-c1` and this is why any class declared in the parent can’t affect anything inside a child component.So this `test.component` could be a 3rd party component where we want to apply a css change for some elements inside.So what to do?Bonus: You can make solution 3 specific per view (context), you have to add a wrapper class in the path of your target component then globally use it to wrap the real class or css selector you want to override.You can find a draft of a live demo hereKeep up with our content by following us @FrontEndTricks & don’t forget to give it some 👏👏👏.~ Thank you ~",css,https://medium.com/front-end-tricks/viewencapsulation-emulated-and-override-css-from-a-component-in-angular-bac6e4019d99?source=tag_archive---------4-----------------------
Create a Simple Expense Manager with JavaScript,"Building an Expense Manager is probably one of the best introductory JavaScript projects.And you never really learn a language until you build something of your own.So, let’s begin. This article assumes you have basic knowledge of HTML, CSS, Bootstrap 4, and JavaScript. A few helpful references can be found at the end of this article if you need them.All the code we’re going to use in this tutorial is on Github. I would urge you to complete this tutorial and use the final code for reference until then.Meet John. John earns a decent amount by working as an auto mechanic at one of the largest auto repair chains in the country. But he ends up spending his monthly income on things he can’t keep track of.John wants us to help him track his finances and understand how much percentage of his salary he ends up saving, investing and spending.The first task we need to do is settle on a design for our Budget Manager. I usually try creating my designs or if nothing works, head over to Dribbble or Behance to find inspiration from a variety of designs. I use Figma to create designs for my projects and you can find a lot of great tutorials online.The reason why I always begin with the design is that it makes writing code simpler. Selecting a design helps you think in a structured manner which leads to quicker development.Let’s start by creating an index.html in your project folder.Follow the steps here to include bootstrap in your project. To make our design look better, we are using the Open Sans font. This is completely optional.Now that our scripts are ready, let’s start by analyzing our design. To begin with, we need two containers, the blue one on the left which takes up about 40% of the screen and the right one, which fills the rest of the screen. Thank god for the Bootstrap Grid!Let’s start with the blue left container.Insert code from the below block inside your body tag. This will create the following:Oh and, please feel to use your local currency. John the mechanic would like to use Indian Rupee (INR) for this example. 😃Great! Now let’s move on the part where John will add his savings or expenses to create a list.This container will contain a title, a dropdown to select the type of expense, a couple of input fields for John to record his expenses and a list which will display the entries with the date. Add this code below the left-container.That’s it for our HTML! We just completed 1 out of 3 files required for our Budget Manager. Excellent job reaching here. Next up, we will be giving our HTML some style. See that style.css we imported in the <head> above? Let’s style our HTML to create a simple and elegant design.Psst, a friendly reminder: I see you are following along for the last 15–20 minutes. How about a glass of water to keep you hydrated?As we use Bootstrap, most of the CSS-stuff is taken care of for us. Bootstrap handles a lot of positioning and design, so there’s not really much left to do except work with margins, font-sizes, and colors, the look, and feel.Although one interesting property of CSS is creating gradient backgrounds, and can really come in handy when you need to throw in some color into your project.Check out the CSS for the left-container , the background property can take a liner-gradient function which accepts a direction, and the colors you want your gradient to use.The direction has a default value of top-to-bottom .Sometimes, a linear-gradient is not exactly what you have in mind. CSS also allows a radial-gradient, a great reference can be found on W3Schools.Okay, so we are now done with most of the HTML & CSS part of the expense manager. Next, we’ll set up our app.js to add interactions and functionality to our project!JavaScript is the most important part of this project. The HTML CSS defined how our expense manager will look, but now we need to work in the logic. The app.js is where all the magic happens!Before we begin, take a few moments to think over what functionalities we need to add in the app.js . Currently, nothing happens when we select an expense type or when we add a description and expense value and click on the button. Yes, you guessed it right, we need to add eventListeners . We also need to show the current month.We will be writing different functions, functions that are solely responsible to handle the UI and the logic to calculate the month’s budget. Let’s call them controllers.Our project will have 3 controllers.Create an app.js file and add the script tag to the bottom of your index.html just before closing the body tag.Let’s go over an important concept of JavaScript functions before we start writing our controllers.Immediately Invoked Function Expressions (IIFE)IIFEs are functions in JavaScript which run as soon as they are defined. They are “immediately invoked” when your script file is run. Hence, they do not need another function to call them.Pre ES6:ES6:Awesome! We are now ready to add the magic to our well designed, gorgeous looking Expense Manager project.Let’s jump right into creating our controllers for the project.The main controller will take care of the following:The UI controller will focus on:The expense controller will focus on:Create the three controllers as outlined below. The main controller takes in two parameters, the UI controller and the expense controller. This will help the main controller control the flow of data between them.The HTMLStrings function is an object that keeps track of the class names and element ids used in the HTML files. I do this because it makes referencing HTML elements easy in the project and reduces potential hours of debugging spent to find a spelling mistake.Now, let’s set up the eventListeners . List down the elements that will require a click event listener. The expense type dropdown options and the submit button.Alongside, we’ll also create the functions which will be called when the eventListener is triggered.Let’s dig deeper into each function.The setupEventListeners function selects the dropdown elements and the submit button to add a click listener to each of them. These listeners are activated when you click on them. Each click listener performs a specific task, such as setting the expense type or instructing the other two controllers to perform some tasks.The addExpense function asks the UI controller to get the input from the HTML, verifies if the input is valid (not null or 0) and asks the UI controller to add a new list item with the correct input. It also asks the expense controller to re-calculate the values from the new input. Then it asks the UI controller to update the overall expense for the month.You might wonder, we can directly let the UI and expense controllers talk to each other, and yes we can, but it may not be considered a good practice. Controllers should remain independent of each other except for the main controller. This makes it easier for the developer to understand and debug the flow.Great, that’s it for the main controller.The UI controller is the biggest of the three as it works with HTML and CSS classes to provide the right look and feel to our project.Let’s take a look at all the functions:The expense controller has a simple task. It maintains four values: savings, investments, expenses and the total monthly budget.That’s it! You have just completed the basic functionality for the expense manager. By now, you should be able to log multiple entries in the savings, investments and expense types and your budget should be calculated accordingly. Great job!Now, it’s time for our bonus section!Michael is always ready for a doughnut! 🍩Source: GIPHYThere are multiple libraries out there that help you create beautiful charts. Libraries help you quickly achieve your goal and are always a better option than writing the same functionality all over again.We will be using the Chart.js library which is a simple and flexible library for designers and developers. The library uses the HTML <canvas> element to render your charts.Insert a new div which will hold our chart at the bottom of the left-container.We need to link our controllers to update the chart whenever a new record is inserted into our expense manager.In the addExpense function in the main controller, add a new function call which will instruct the UI controller to update the chart with new values.The displayChart in the UI controller method will create a new chart. We need to pass the type of chart, the data (labels and the dataset), and some options in case we need to customize the chart.Awesome! Now try entering a few values into the system to see your beautiful doughnut graph!This is how our expense manager should look like at the end.The full code for the project can be found on Github.Thank you for staying through the end of the two parts! I hope you found this article helpful in your programming journey!Happy Coding! 😃",css,https://levelup.gitconnected.com/create-a-simple-expense-manager-with-javascript-4e2cf2097fba?source=tag_archive---------2-----------------------
Week 4: Before & After I shall Hide 🤨,"Samiran KonwarJun 1, 2020·7 min readIf you haven’t yet, here is Week 3This week I plan to complete task 1.1.1, which is to build the new User Interface of the Simulator. Talking a bit about the new UI, originally it was not really a part any task-list of the projects that the organization had listed. When I joined the organization channel, reading their discussion I found out that they were revamping the UI of the base website, the original UI had gone a bit outdated, in a world run by the internet trends really move fast. I noticed that they had no plans to revamp the simulator UI yet, & it looked a bit outdated too. It mainly uses jQueryUI 1.12.1 & it’s a bit old, in fact, it was actually released back in 2016. 👀So why not bring something more to the table? If you want to you can always bring something more apart from what the organization seeks initially, which might increase your chances of being selected!🤝So I started making a prototype for the simulator, trying to give it a modern feel, that would stand out against its competitors. I’m no professional UI/UX developer, but I do use a lot of applications with modern User Interface, we all do, they give me a general idea of how I should build UI for CAD application. Even though it had a lot of inconsistency & design errors, later after the results were announced the orgs actually accepted my prototype & one of my mentor really liked it :) he’s a UX designer himself.Later during weeks 1 & 2 he taught me a thing or two, helped to improve the prototype in order to meet all standards such as consistency, sizes, etc but mainly accessibility standards. He introduced me to WCAG 2.0 (Web Content Accessibility Guidelines), I haven’t thought things like these actually exist!👶Apart from having a nice modern look to this prototype, one of the benefits, which is a very important one, is the increase in the workspace area. The new User Interface has an increase of about 7.94% visible workspace area! That’s really a lot taking into consideration that nothing is being taken out that was previously in the original UI. Okay that was a lot of bragging 😅, now coming to:Day 18:This day was spent doing 2 things, firstly having a custom close icon, the cross (X) of a window. In CSS, styling stuff is fine but if you really want something to be really customized, instead of finding & overriding styles applied by browser engines, the best trick is to hide the element & make use of ::before & ::after pseudo-elements in CSS, you can read about them here.I really love these, I can’t imaging having something custom built without the help of these pseudo-elements. I exported the desired icon for close as SVG from Adobe XD & set it like this🤫 , requires a bit of positioning to do then you’re good to go!It’s like having a duplicate element of something that you add CSS to, they come fresh & clean, no CSS gets inherited from the element it was created from, but inherits the behavior, what I mean by that is, if you create a ::after or ::before of a button, make it visible in the browser & click it, the actual button gets clicked! 👊 & you can add your own CSS rules to that pseudo element, like I did above, just remember to set content property or they wont exist in the DOM.The second one is formatting the markup, it’s a bit boring so I will not talk about it :)Day 19:Today I spent fixing a big bug(a production blocker may I say) that actually turned out to be small. I had to revisit old commits in order to find out since when the bug was introduced that went unnoticed by me. 🌚The bug was that whenever I type something on any input fields, it would type fine, but when I press backspace to delete something it won’t delete any character instead it would exit the simulator. Sounds scary right? Is my browser haunted? Or is it malware! 😨,Hey I got virus & thread protection on! 🙁. Okay jokes apart, I went to surf to find the cause of this issue.Whenever you happen to come across any bug always think it through first, ask yourself a few questions, why when where &how? or you could use a debugger but I ain’t have no time to learning debugging & stuff so I asked myself those questions,After searching the internet I found out the answer to why, & its because I use Firefox…Okay.Before you start saying things about Firefox stop. Firefox is awesome & I love it, it’s just that Firefox has a default action to exit your webpage on backspace. But that shouldn’t happen when I have focus on elements that prevents it (event propagation), such as input fields! what is then causing it? I ran some scripts from stack-overflow to check if I’m losing focus on the input field, nope!Got a bit frustration, I went back visiting old commit branches & found the commit that introduced it.I looked at changes I made til that point, just like HTML changes, style changes, plugins that I included, I undone them & tried to reproduce the bug. Found out that the bugs only reproduces when the simulator (grid area) was behind the panel 🤨. I knew it had something to do with the simulator or canvas area. just like this properties panel..So, I searched the simulator’s listeners.js file for the term ‘e.keyCode == 8’, that’s the keyCode of backspace & there she was on line 147, the cause.Day 20:Today I did some usual work, added ripple effect to buttons, added custom ::selection, worked on the quick access panel & some fixes.The ::selection selector in CSS seems a bit interesting, It lets you change the color off how texts are selected! Let me demonstrate it for you,This is how normal selection of text looks in medium: (warning below images might be visually confusing 😆)& after setting these 2 rules in the browser dev toolsThis is what I got:Neat! It’s useful if your website has text & the default selection styles doesn’t meets your likeness this is a handy selector for selection 🧐.Day 21:Hmmm 🤔 If you’re learning web dev (like me), and you like smooth transitions, I’ll share a cool thing (like me 😁).This is one of the think I did today, I’m happy I did it. I added a line of CSS rule to every element that has an hover effect.This:transition: all 0.2s ease-in-out;All refers to what property you want the transition to be applied to, 0.2s refers to the duration of transition (obviously) & ease-in-out is just a timing function.What it does is that.. wait let me just show youWithout transition applied:With transition applied:Tell me you see the difference? I uploaded these as gif, so frame-rate dropped to 25fps, but I assume you get it!It’s just a one line of CSS but man it gives a bit animation like feel! Sure you can do lot more with it!Day 22:The UI is almost build, and today is all about giving it a deep thorough test!I wrote about 1400+ lines of CSS until now😅, & it’s complete.Day 23:Since task 1.1.1 is complete it’s now time to select a new task from my task list. Week 5 comes tomorrow & I spent today thinking about the best way to go about for my next task. In the coming week/s I’ll be moving out of CSS and more into JavaScript🌈. Task 1.2 is all about developing a hotkey binder(custom key-press shortcut) for the simulator. Wait for next weeks blog to know more about it! It’s one of my favorite on the task-list & I’m very excited to working on it! In week 5 blog’ you’ll get to know more about it 🙃.So what’s next?",css,https://medium.com/@abstrekt/week-4-before-after-i-shall-hide-6096118bd2dc?source=tag_archive---------11-----------------------
CSS: The gradient patterns that are not really gradients,"MugbertoJun 3, 2020·5 min readFor beginners, it is intuitively difficult to guess that the pattern above is created with CSS gradients. They will be like, “how possibly can it be gradients while all I see are solid colors?” After all, gradients are supposed to be smooth transitions between colors. That is how I used to think two months ago before I start an HTML & CSS course at Microverse. Since then I started playing around with gradients and created shapes that are almost impossible to create with regular HTML elements. In this article, I have decided to share with my fellow CSS & HTML beginners the technique I used to create the pattern above. Let’s get started.Oh, wait!Let us make sure we have a basic understanding of how the CSS radial-gradient() function works. With this function, we can create a radial gradient which is a type of gradient where transitioning colors form circular or elliptic bands around a central point/origin. In CSS there are other types of gradients that are worthy of attention but for the sake of this article’s shortness will only look at radial gradients.The radial-gradient() function returns a special image type gradient. For this reason, gradients are created with the background-image property, not with the background-color property.background-image: radial-gradient(<shape> <length> at <position>, <color-stop-list>);The CSS radial-gradient() function takes in several values that determine how the resulting gradient looks like. These values are classified as follow:Now, we know basically how to use the radial-gradient function. If you are willing to learn more check this link: https://developer.mozilla.org/en-US/docs/Web/CSS/radial-gradient. However, we know enough to continue…It’s ok now, we can start…Let’s create a simple HTML file with only one div element within the body and give it a class “pattern” to be used as a selector.Then, let’s link it to a CSS file with the following code:This creates a 50 rem wide green circle and places it at the center of the document.Now, let’s create our first radial gradient. In our style, just below the background-color, we add a background-image property and radial-gradient() function as its value. The function’s values are set as follow:<shape> : ellipse ;<length> as 40% 30%;<position> as top:<color-stop-list> salmon, transparent.In CSS, transparent is a color. If you didn’t know, now you know!Notice that we didn’t set the position for the color. When we have colors with unspecified positions on the list. The first is at 0% and the last 100%. Other colors in-between will be positioned evenly.Now we have a salmon color at the center which fades gradually and becomes completely transparent as it reaches 40% of the container’s radius horizontally and 30% vertically. That’s where lies the ellipse we defined in the function.Next, we are going to make the surface of the ellipse opaque by pushing the salmon color’s position from 0%( at the origin) to 98% of <length>. Since it is the first color in the <color-stop-list> , There is no transition along the 98% of the <length>. This results in an opaque salmon color.background-image: radial-gradient(ellipse 40% 30% at center, salmon 98%, transparent);The transparent color is positioned at 100%, we will have 2% wide transition. Hence giving us a smooth edge.Another cool feature about backgrounds in CSS is the fact that we can create layered background-images and still be able to see through transparency. This is achieved by assigning a list of comma-separated values to the background-image property. These values in our case are gradient functions. We are going to take advantage of this feature by adding 3 more ellipses of the same size. We give each of them a distinct color and position(top, bottom, left, and right). We also need to swap the <length> values (40% 30% to 30% 40%) for the left and right positioned ellipses.So far so good. Let us add a little bit of flavor by making this pattern repeating. This is achieved by using two CSS background properties:Done!And that’s it, we have the background pattern we were after. In this article, the main goal was not to fully understand the CSS gradient. Rather, it was to attract your attention to CSS gradients. If you are attracted check this for more on gradients. They are plenty of cool stuff you can create with this tool. So, it’s up to you. Get creative!",css,https://medium.com/@mugberto/css-the-gradients-that-are-not-really-gradients-f0db6bf41280?source=tag_archive---------9-----------------------
Learn CSS animation ASAP | as simple as possible,"If you are a web developer and you want to give life to your website, then the CSS animation is one of the essential items that you should use. It also enhances the user experience and, as you know, the better the user experience, the higher the satisfied visitors.So, welcome to this simple CSS animation journey. In this article, we are going to cover the basics of CSS animation and create some simple ones from scratch to see how it is easy to work with CSS animation.Switching styles from one state to another.Here’s a simple example: Let’s imagine that we want to smoothly change the color of the button when hovering over it as below:Transition-property: use to change desired CSS properties when the transition happens.Transition-duration: the time that animation occurs.Transition-timing-function: how the change in the property will occur. You can use a different kind of timing-function:Note: If you look above options accurately, you will notice that “ease” and “ease-in-out” have the same explanation. But you should know that they have some minor differences in action. Let’s see their implementations:Transform: translateX(500px): when hovering over each button, the position of the button will be changed 500 pixels in the x-axis.Also, we can use the transition-delay property for starting the animation after a specific duration. (e.g., transition-delay: 2000ms; the animation will start after 2 seconds)Note: If you noticed from the above code snippet, we get the help of “transform” property to change the position of the elements. If you are not familiar with transform property or you forget how to use it, then you can take a look at the article below to learn or remind it again.medium.com2. Professional way: write all features in one single line: (transition: transition-property transition-duration transition-timing-function transition-delay)Note: Imagine you have multiple transition properties like color, background-color, and padding for one HTML button. Maybe you want to write all of them in one single line then you can pack each transition property with its options and separate each one via comma as below:Transition Can use with all the properties that are changing gradually. It’s better to say; it can use if the property can vary from one state to another. You can see some CSS properties below that can/cannot use with the transition:Check this link for more details.Indeed, CSS animation is your imagination and also your creativity. If you want to be an expert then you should play with CSS animation properties and, of course, take time to work with it as well. With enough time spent perfecting your skills you could build all of your animation dreams if you want. And as for becoming the CSS animation master, the next step is to learn CSS transform animation. For learning CSS transform animation, you can read this article that explains it comprehensively.",css,https://codeburst.io/learn-css-animation-asap-as-simple-as-possible-374b7874d4dd?source=tag_archive---------3-----------------------
How to Create Dynamic Backgrounds With the CSS Paint API,"Modern web applications are heavy on images. They are responsible for most of the bytes that are downloaded. By optimizing them, you can better leverage their performance. If you happen to use geometric shapes as background images, there is an alternative. You can use the CSS Paint API to generate backgrounds programmatically.In this tutorial, we will explore its capabilities and look at how we can use it to create resolution-independent, dynamic backgrounds on the fly. This will be the output of this tutorial:Let’s start by creating a new index.html file and filling it up with the following:There are a couple of things to note:A paint worklet is a class that defines what should be drawn onto your canvas. They work similarly to the canvas element. If you have previous knowledge of it, the code will look familiar. However, they are not 100% identical. For example, text-rendering methods are not yet supported in worklets.While here, let’s also define the CSS styles. This is where we reference that we want to use a worklet:I’ve added a black border so we can better see the textarea. To reference a paint worklet, you need to pass paint(worklet-name) as a value to a background-image property. But where did pattern come from? We haven’t defined it yet, so let’s make it our next step.Open up your pattern.js and add the following content to it:This is where you can register your paint worklet with the registerPaint method. You can reference the first parameter in your CSS that you defined here. The second parameter is the class that defines what should be painted on the canvas. This has a paint method that takes three parameters:Our next step is to get something showing up, so let’s draw the rectangles. Add the following into your paint method:All we’re doing here is creating a nested loop for looping through the width and height of the canvas. Since the size of the rectangle is 20, we want to divide both its height and width by 20.On line 4, we can switch between two colors using the modulus operator. I’ve also added some drop shadows for depth. And finally, we draw the rectangles on the canvas. If you open this in your browser, you should have the following:Unfortunately, apart from resizing the textarea and getting a glimpse of how the Paint API redraws everything, this is mostly still static. So let’s make things more dynamic by adding custom CSS properties that we can change.Open your styles.css and add the following lines to it:You can define custom CSS properties by prefixing them with --. These can then be used by the var() function. But in our case, we will use it in our paint worklet.To make sure that the Paint API is supported, we can also check for support in CSS. To do this, we have two options:To read these parameters inside pattern.js, you need to add a new method to the class that defines the paint worklet:To access these properties inside the paint method, you can use properties.get:For the color, we need to convert it into a string. Everything else will need to be converted into a number. This is because properties.get returns a CSSUnparsedValue.To make things a little bit more readable, I’ve created two new functions that handle the parsing for us:All we need to do now is replace everything in the for loop with the corresponding prop values:Now go back to your browser and try to change things around.Why might the CSS Paint API be useful for us? What are the use cases?The most obvious one is that it reduces the size of your responses. By eliminating the use of images, you save one network request and a handful of kilobytes. This improves performance.For complex CSS effects that use DOM elements, you also reduce the number of nodes on your page. Since you can create complex animations with the Paint API, there’s no need for additional empty nodes.In my opinion, the biggest benefit is that it's far more customizable than static background images. The API also creates resolution-independent images, so you don’t need to worry about missing out on a single screen size.If you choose to use the CSS Paint API today, make sure you provide polyfill, as it is still not widely adopted. If you would like to tweak the finished project, you can clone it from this GitHub repository.Thank you for taking the time to read this article. Happy coding!",css,https://betterprogramming.pub/how-to-create-dynamic-backgrounds-with-the-css-paint-api-ebd733254014?source=tag_archive---------3-----------------------
Squarespace and Custom Tables with HTML and CSS,"ForTheLoveOfTechJun 1, 2020·3 min readSome things to consider before continuing:On your Squarespace page, add a Code block in the area you want your table to appear.After you add the Code block, the editor should look thisFrom that view, you can start adding your table HTML. You can also add your CSS inline here, but to keep your code clean, we add CSS in the Design tab(more on that later).An example of a table you can add inside of the Code block:Your Code block should look like this. Keep in mind your table won’t look like this right away without your CSS.Now that you have added your HTML we can move onto the Design tab located on the left side of your Squarespace menu. Click on Design > Custom CSS.The Custom CSS section lets you style your custom HTML using classes. The CSS below styles my table with color, URL underline, background colors, and a range of other styles.Here is the full CSS for my table:Click save at the top and you’re all done adding your custom table and styling it.",css,https://medium.com/@fortheloveoftech/squarespace-and-custom-tables-with-html-and-css-5f574d7f2820?source=tag_archive---------7-----------------------
Problems in styling list of child elements in CSS,"Santanu BiswasJun 1, 2020·3 min readWhen we are working as front-end developer we face some styling problems for list of child elements in every project. I am going to discuss here one most occurring Css styling situation where to use (:first-child/:last-child/:first-of-type/:last-of-type) Css selectors with example.#1 (:first-child/:last-child)Assume there are unknown number of paragraphs list with in a div container. Here in below example,Requirements: all paragraphs need padding between 40px and border between . So we given(1) ‘padding:20px 0’ for each p. (2) Then for first p in div container we have to remove padding-top and same like (3) for last p in div container, we have to remove padding-bottom, and border-bottom also.This kind of situation we need first-child & last-child Css pseudo-class.:first-child — This Css selector is works for only if selected element is the first element of its parent. Here .child:first-child represents the child class element which is the first element of it’s parent. here, ‘<p class=”child”>First Child</p>’ . Here, If the first element of div is not belongs to .child class then this selector will not work.:last-child — This Css selector is works for only if selected element is the last element of its parent. Here .child:last-child represents the child class element which is the last element of it’s parent. here, ‘<p class=”child”>Fourth Child</p>’. Here, If the last element of div is not belongs to .child class then this selector will not work.Note: Here we have shown example by Css class(.class:first-child/.class:last-child) selectors. It’s applicable for others type of selectors also. For example (div p:first-child/ div p:last-child)#2(:first-of-type/:last-of-type)Assume there are unknown number of paragraphs list and other html tags in a div container. example like below,Requirements: You need only for first and last paragraph element within that div need colour to be red. We can solve it below way in Css.This kind of situation we need :first-of-type & :last-of-type Css pseudo-class.:first-of-type — This selector only works if selected element is the first specified tag type of its parent element. here we given selector of p tag type(.container2 p:first-of-type) that means paragraph type first tag with in a parent element. In above example that is ‘<p>First Paragraph</p>’.:last-of-type — This selector only works if selected element is the last specified tag type of its parent element. here we given selector of p tag type (.container2 p:last-of-type) that means paragraph type last tag with in a parent element. In above example that is ‘<p>Fourth Paragraph</p>’.Note: Remember this selector for element tag type wise that means selector of elements tags not for Css class selectors. example: (.css-class:first-of-type / .css-class:first-of-type will not be valid)",css,https://medium.com/@biswassantanu1996/problems-in-styling-list-of-child-elements-in-css-e643db336e29?source=tag_archive---------12-----------------------
Curved Background Using CSS,"w3hubsJun 2, 2020·1 min readTemplate Name: Curved Background Using CSS.High Resolution: — Yes.Compatible Browsers: — All Browser.Source Files included: — HTML, and CSS.CSS has so many awesome properties to make any elements attractive. So here we designed simple Curved Background Using CSS and with the help of basic HTML tags.Here we used border-radius CSS properties to make this background rounded/curved. Also, we used flex properties to make our content properly centred and perfectly aligned. To make mobile-friendly we used media queries for the mobile screen side.Make it yours now by using it, downloading it, and please share it. we will design more elements for you.Originally published at https://w3hubs.com on June 2, 2020.",css,https://medium.com/@w3hubs/curved-background-using-css-w3hu-8e26ce7f24f8?source=tag_archive---------4-----------------------
The “C” In CSS: Why Is It Important?,"It’s a usual day at work. You’re at your desk, creating the new design sent in by the UI guy, Mike. You wonder why does the UI team have to change the UI so often, you are the who ends up writing media queries for each screen! Anyway, you’re trying to work with a certain CSS property and it just doesn’t seem to work.That’s weird 🤔. You look up from the screen and everyone around you is busy working. An evil thought crosses your mind and you end up adding a !important to the CSS value. Or maybe an inline CSS. There, problem solved!The cascade in Cascading Style Sheets plays an extremely important role and affects how styling methods are applied to your elements. This story is going to help you understand how the cascade works and how wonderful a concept it is!CSS Specificity is a set of the rules applied to CSS selectors (properties) in order to determine which style is applied to an element. The more specific a CSS style is the likelier it is to be present on the element’s style.There are many benefits of understanding CSS Specificity. Most importantly, it helps you understand why your styles aren’t being applied, and it helps you write less CSS code.CSS specificity is determined byHere’s a great article that explains how to calculate the specificity for an element.Coming back to the cascade, the cascade is concerned with where and how you write the styles. The image below shows the decreasing priority of the styles from top to bottom. The !important keyword has the highest priority while inheritance has the lowest.Let’s break down and understand each concept in a reverse manner, starting from the lowest priority, inheritance.A quick example to understand inheritance is that you gain certain properties from your parent such as your last name. Similarly, HTML elements can inherit style from other elements. This is a parent-child relationship, where-in the child inherits the properties of its parent elements.In the above example, we’ve given assigned a color style to the parent div, which has been inherited by its child div. In other words, the style from the parent has cascaded down to the child.But, all CSS properties cannot be inherited. There are some properties that cannot be inherited by a child element. A typical example of a non-inherited CSS property is the border property.While the paragraph element will have a border, the emphasized text will not inherit the border style and thus, not have a border.An excellent reference to CSS properties is provided by MDN. To see if a particular property can be inherited, look for the (“Inherited: yes”) or (“Inherited: no”) in the Specifications section.Inheritance has the lowest priority among the CSS styling methods. If the child has a style of its own, then the inherited value of the parent is ignored, even if the parent style has the !important keyword, as shown in the example below.Style sheets are of two types, external and internal. Both of them have the same priority, but there’s a catch. When the priorities are the same, CSS applies the ordering rules to determine which CSS should be applied.For example, the background property is used in all three classes. The priority and the specificity are the same, so the last declared style takes precedence.CSS rules follow a certain order, from left to right and top to bottom. This means that CSS prioritizes the right-most CSS and the bottom-most CSS.Here, the rightmost CSS is applied to the div, meaning that the div has a blue border.Similarly, the CSS at the bottom is applied to the div, meaning that the div has a blue font.The way you select your elements will also play a role in determining which CSS rules are applied. IDs (#myID) have precedence over Classes (.myClass) and Classes have precedence over tags (div).Consider the example below, the text is red even though the class and div selectors are specified after the ID. Specificity has a higher priority than the ordering rules, so it does not matter where you place your CSS, specificity will always take over ordering rules.Inline styles have the second-highest priority, after the !important keyword. Inline CSS can be overridden only by the !important keyword, so if your CSS isn’t applied even after adding an inline style, there must be a !important hidden somewhere in your stylesheet. 😃Within inline styles, normal ordering rules are applied, from left-to-right and top-to-bottom as we saw earlier.The !important keyword is used to override all specificity and ordering rules and has enough powers to give frontend devs sleepless nights. So use it responsibly, as Uncle Ben told Peter Parker.In some cases such as when using a CSS framework like Bootstrap, using the !important keyword to enforce your own styles can be acceptable.Phew, that’s a lot of new information.Understanding the cascade might seem a bit over-kill but now that you know how much power it wields, I bet you will be writing cleaner and more organized CSS. Your future self will thank you when you revisit your CSS to change some styles later. 😁Happy Coding!",css,https://levelup.gitconnected.com/the-c-in-css-why-is-it-important-8d3900d6827b?source=tag_archive---------5-----------------------
How to use CodePen components inside your webpage?,"ShahsamaJun 1, 2020·4 min readWith the world rapidly evolving by the minute, staying on top of the in-demand skills’ curve is a voluminous task. Web Development is one such skill that is almost essential to the modern developer. With a market size of about $40Bn in 2020, the need and opportunity for quality web developers are ever so evident.Whether you are new to the world of web development or an accomplished developer, you might have come across CodePen. CodePen is an online community for testing and showcasing user-created HTML, CSS, and JavaScript code snippets.Once you have a good foothold on HTML, CSS, JS/Jquery, it’s not necessary to design each component of your webpage from scratch. CodePen apart from being a code editor also has unique functionality, It functions as an open-source learning environment, where developers can create code snippets, called “pens,” and test them. Most of the time CodePen would have tons of “pens” made by the developers that you can add to your website to make it more lavish.In this article, We will look at a small example of how you can use any “pen” from CodePen and add it to your site.Step 1: Open CodePen.ioStep 2: Type anything in the search boxYou can type anything that you wish on adding to your webpage. It can be a responsive slider, a team card, about section, animations, etc. In this article, We will add a beautiful slider. So go on and search beautiful slider in the search box.You’ll get a good astonishing options to chose from. We selected https://codepen.io/supah/pen/zZaPeE.Step 3: Code extractionNow as you can see, the Codepen editor shows us the amazing code that built this slider. But do you think copying the code simply from this editor and pasting it in yours would work? The answer is NO. Most of the time the “pens” that you see use external libraries that the CodePen editor does not show. So simply copying them would result in inaccuracies that would be hard to resolve if we don’t import these libraries.To do that, navigate to the bottom right corner of your screen and click on Export -> Export as .zip. This will download a zip file of the same pen in your system. Extract it using any extracting tool such as Winrar/Winzip and see the files.The zip contains two folders dist and src along with a readme and license text file. You can explore/read them for your interest. Now select the “dist” folder and there you’ll see index.html, style.css, and script.js files. Open all of these in a text editor(Sublime, Atom, Brackets, etc).On opening the HTML file with any text editor, we see many external libraries apart from style.css and script.js.Add these lines inside your index.html along with the body section( I created a sample HTML page with a “CodePen Slider” title that will be followed by this CodePen slider) and you have successfully extracted the main code.Now save the file, don’t forget to add “script.js” and “style.css” in the same directory. If you have those files in any other location, update the href and src according to that.Step 4: Open the index.html fileKudos!!! You just added a “pen” to your website. Pat yourself on the back. Play around with the code. Change the source of images, text on top of images according to your liking/requirement.Conclusion:In this article, you learned how to use “pens” from CodePen inside your website easily. If you want to add any “pen” apart from the example in this article, the process remains the same. Keep building beautiful websites. Thanks for reading!!!",css,https://medium.com/@shahsama542/how-to-use-codepen-components-inside-your-webpage-d46edfc13808?source=tag_archive---------10-----------------------
Clickable and Editable UITextView,"Elsayed HusseinJun 2, 2020·1 min readUITextView is a great control, we are using it alot, and data detecting ( e.g Links, Phone Numbers, Addresses, ..) from its nice features.When you need make UITextView clickable you should make isEditable with false, but in some situations we need to make it detectable and clickable in same time like iPhone’s Note app.We can’t detect links in UITextView and UITextView’s isEditable = true, so we will achieve that by workaround.First we will create a UITapGestureRecognizer and assign it to textview and in UITapGestureRecognize's action we will do two things:In UITapGestureRecognize's action we will get a position of tap and check if tap position is outside UITextView's text so show the keyboard, and place the cursor in that position, but if tap on link, so do link action like open the link URL.Last thing, we should back to normal UITextView state when dismiss keyboard so we need to implement UITextViewDelegate’s textViewDidEndEditing method:github.com",ios,https://medium.com/@bokhary/clickable-and-editable-uitextview-8f7c8649a3f0?source=tag_archive---------16-----------------------
Interview Questions and Answers in iOS — Part 3,"Naveen SharmaJun 1, 2020·18 min readQ. Introduction to Swift Programming Language?A. Swift is a general-purpose, multi-paradigm, object-oriented, functional, imperative, and block-structured programing language developed by Apple Inc. for iOS, iPadOS, macOS, watchOS, tvOS, and Linux. It was introduced at Apple’s 2014 Worldwide Developers Conference (WWDC) with version Swift 1.0. Swift is designed to work with Apple’s Cocoa and Cocoa Touch frameworks.From swift 2.0 release, Swift adopted the Protocol-Oriented Programming Paradigm.Q. What is Swift's main advantage and features?A. - Open sourced and easy to learn- Fast, safe, and expressive- Type-safe language- Optional Types, which make applications crash-resistant- Powerful built-in error handling - Supports Closures, Generics- Supports pattern matching- Tuples for multiple return values- Fast and concise iteration over a range or collection- Structs that support methods, extensions, and protocols- Functional programming patterns, e.g., map and filter- Advanced control flow with do, guard, defer, and repeat keywordsQ. What is Notification or Broadcasting?A. A notification is a message sent to one or more observing objects to inform them of an event in the program.Q. How could we get the device token?A. There are two steps to get the device token. First, we must show the user’s permission screen, after we can register for remote notifications. If these steps go well, the system will provide the device token. If we uninstall or reinstall the app, the device token would change.Q. Explain the types of notifications?A. There are two types of notifications: Remote(Push) and Local. Remote notification requires a connection to a server. Local notifications don’t require a server connection. Local notifications happen on the device.Q. Local Notification?A.- Local Notification is introduced in iOS 4.- Local Notification is an instance of UILocalNotification/UserNotifications.- In iOS 10 Apple has deprecated UILocalNotification and introduced UserNotifications.- Each application on the device is limited to 64 scheduled local notifications.- We request authorization to send the user notifications through UNUserNotificationCenter by passing 4 options (alert, badge, sound, and car play) defined under UNAuthorizationOptions.- We create notification through the UNNotificationRequest object which requires three pieces of information: an identifier, content, and a trigger.- We use the UNMutableNotificationContent class to create the notification contents such as the title, subtitle, body, sound, and media attachments (if any).- There are three basic triggers UNTimeIntervalNotificationTrigger, UNCalendarNotificationTrigger, and UNLocationNotificationTrigger.- Notifications can also contain media attachments such as images, videos, or audio. Media attachments will appear in the long look and the short look (images).- Actions appear as buttons in the long look of the notification. We create actions using the UNNotificationAction class.- Custom actions must be associated with a category. We create a category using the UNNotificationCategory class. Each category can contain a set of actions to be displayed when the notification is accessed in a long look. We can reuse a category with different notifications, but each category requires a unique identifier.- To load an image URL from the local file system and create our UNNotificationAttachment object. We then assign it to the attachments property.- To load an image URL from the server in case of remote notification we use Notification Service Extension.- To display a custom interface for notification we use Notification Content Extensions.Q. What are three triggers for a Local Notification?A. Location, Calendar, and Time Interval. A Location notification fires when the GPS on our phone is at a location or geographic region. Calendar trigger is based on calendar date broken into date components. Time Interval is a count of seconds until the timer goes off.Q. Difference between Local and Remote Notification?A. Remote Notification is introduced in iOS 3 and Local Notification is introduced in iOS 4.- Local Notification is scheduled by application and delivered to the same device. Push or Remote Notifications are sent by our server to the APNS server, which pushes the notification on devices.Q. What is Remote Notification’s payload size?A. Remote Notification’s payload size is 4kb. If we want to send a high-quality attachment, we should use Notification Service Extension.Q. Explain options available under UNAuthorizationOptions?A. .badge: Display a number on the corner of the app’s icon..sound: Play a sound..alert: Display text..carPlay: Display notifications in CarPlay..provisional: Post non-interrupting notifications. The user won’t get a request for permission if we only use this option, but our notifications will only show silently in the Notification Center..providesAppNotificationSettings: Indicate that the app has its own UI for notification settings..criticalAlert: Ignore the mute switch and Do Not Disturb. We’ll need a special entitlement from Apple to use this option as it’s only meant to be used when absolutely necessary.Q. Explain build in keys of “aps” dictionary of payload?A. alert: This can be a string, or a dictionary itself. As a dictionary, it can localize the text or change other aspects of the notification.badge: This is a number that will display in the corner of the app icon. We can remove the badge by setting this to 0.sound: Name of a custom notification sound’s file located in the app. Custom notification sounds must be shorter than 30 seconds and have a few restrictions.thread-id: Use this key to group notifications.category: This defines the category of the notification, which is used to show custom actions on the notification.content-available: By setting this key to 1, the push notification becomes silent.mutable-content: By setting this key to 1, our app can modify the notification before displaying it.Outside of these, we can add as much custom data as we want, as long as the payload does not exceed 4 kb.Q. What are the biggest changes in UserNotifications?A. - We can add audio, video, and images (Notification Service Extension).- We can create custom interfaces for notifications (Notification Content Extension).- New Notification extensions allow us to manage remote notification payloads before they’re delivered.Q. Explain the notification service extension?A. The notification service extension lets us the chance to change the content in the notification before it is presented.Q. Explain the notification content extension?A. The notification content extension gives us the tools, we have in an app to design the notification.Q. What is the difference between using notification and delegation?A.- Notifications and Delegates are used to accomplish nearly the same functionality. However, Delegates are one-to-one communication while Notifications are one-to-many communication at the same time.- Notification is more like a broadcast rather than a straight communication between two objects.- Notification removes the dependencies between the sending and receiving objects by using the notification center to manage the sending and receiving of notification.- The other difference between notifications and delegates is that there is no possibility for the receiver of notification to return a value to the sender.Q. Difference between Delegate and Data Source?A. A delegate is an object that acts on behalf of, or in coordination with another object when that object encounters an event in the program. A delegate is an object that is the delegate control of the user interface for that event.- A data source is like a delegate except that instead of being delegate control of the user interface, it is the delegate control of data.Q. What is Protocol?A. Protocol are interfaces which define certain methods and properties that an object respond to.- The protocol can be adopted by a class, structure, or enumeration to provide an actual implementation of methods and properties.- If a class adopts a Protocol, It must implement all required methods of the Protocol, it adopts.- A class adopts more than one Protocols.- There are two types of Protocol: Formal, Informal.Adding Property Requirements- A protocol can have properties as well as methods.- A protocol declaration only specifies the required property name and type. It doesn’t say anything about whether the property should be a stored one or a computed one.- A protocol also specifies whether each property must be gettable or gettable and settable.- Property requirements are always declared as variable properties, prefixed with the var keyword.- Gettable and Settable properties are indicated by writing { get set } after their type declaration, and gettable properties are indicated by writing { get }.- A { get set } property cannot be a constant stored property. It should be a computed property and both get and set should be implemented.- A { get } property can be any kind of property, and it is valid for the property to be also settable if required.Adding Methods Requirements- A protocol can have type methods or instance methods.- Methods are declared in exactly the same way as for normal instance and type methods but without curly braces or a method body.- Variadic parameters are allowed.- Default values are not allowed.- We can also add mutating instance methods to protocols. And If we mark a protocol instance method requirement as mutating, we do not need to write the mutating keyword when writing an implementation of that method for a class. The mutating keyword is only used by structures and enumerations.Q. How to make a method inside a protocol optional in swift?A. We can do it in two ways.The pure — by providing the default implementation using protocol extension.The objective — by using the @objc and optional keyword.A drawback of objc compatibility way is that MyProtocolObjc in the above example became a class only protocol. A struct cannot conform to this protocol.Q. Mutating Methods?A. Mutating methods are methods that we use on value types like structs and enums. These methods are allowed to modify the instance it belongs to and any properties of that instance.Q. NSAssert in Objective C or Assert in Swift?A.- NSAssert/Assert is a macro that takes a condition and a message. If the condition is not met/true, then the assertion fails and NSAssert raises an exception with the message provided.- Assert is to make sure a value is what it’s supposed to be.- If an assertion fails that means something went wrong and so the app quits.- One reason to use assert would be if we have some function that will not behave or will create very bad side effects if one of the parameters passed to it is not exactly some value (or a range of values) we can put an assert to make sure that value is what we expect it to be, and if it’s not then something is really wrong, and so the app quits.- Assert can be very useful for debugging/unit testing, and also when we want frameworks to stop the users from doing “evil” things.- It also tells us the class, method, and the line where the assertion occurred.Asserts come in three major flavors:1. Assertions2. Preconditions3. Fatal ErrorsQ. What is auto-layout?A. Auto Layout dynamically calculates the size and position of all the views in the view hierarchy, based on constraints placed on those views.Q. Explain Generics in Swift?A. Generic code enables us to write flexible, reusable functions, and types that can work with any type. Generics are one of the most powerful features of Swift, and much of the Swift standard library is built with generic code. For example, Swift’s Array and Dictionary types are both generic collections.Generics create code that does not get specific about underlying data types. Generics allow us to know what type it is going to contain. Generics also provides optimization for our code.Q. What is the difference between static vs class functions/variables in Swift classes?A. Subclasses can override class methods but can not override static methods.Q. Please explain the final keyword in the class?A. By adding the keyword final in front of the method, property, or subscript, we prevent them from being overridden. Exp. final var, final func, final class func, and final subscript.If we can replace the final class keyword with a single word static and get the same behavior. Any attempt to subclass a final class is reported as a compile-time error.so, static is internally final.Q. Difference between Self and self in Swift?A. When we write protocols and protocol extensions, there’s a difference between Self (capital S) and self (lowercase S). When used with a capital S, Self refers to the type that conforms to the protocol, e.g. String or Int. When used with a lowercase S, self refers to the value inside that type, e.g. “hello Swift” or 786.As an example, consider this extension on BinaryInteger:Remember, Self with a capital S refers to whatever type is conforming to the protocol. In the example above, Int conforms to BinaryInteger, so when called on Int the method effectively reads this:On the other hand, self with a lowercase S refers to whatever value the type holds. If the example above were called on an Int storing the value 8 it would effectively be this:Q. Types of properties in swift?A. Stored Properties and Computed Properties.Q. Explain the Stored Properties in Swift?A. - Stored properties store constant and variable values as part of an instance of a particular class or structure.- Stored properties can be either variable stored properties (introduced by the var keyword) or constant stored properties (introduced by the let keyword).- Stored properties are provided only by classes and structures.The example below defines a structure called FixedLengthRange, which describes a range of integers whose range length cannot be changed after it is created:var rangeOfThreeItems = FixedLengthRange(firstValue: 0, length: 3)//the range represents integer values 0, 1, and 2rangeOfThreeItems.firstValue = 6//the range now represents integer values 6, 7, and 8Instances of FixedLengthRange have a variable stored property called firstValue and a constant stored property called length. In the example above, length is initialized when the new range is created and cannot be changed thereafter because it is a constant property.Q. Can enums have stored properties?A. Enums can have methods, subscripts, and computed properties. But it cannot have stored properties.Q. Explain the Lazy Stored Property in Swift?A. - A lazy stored property is a property whose initial value is not calculated until the first time it is used. We indicate a lazy stored property by writing the lazy modifier before its declaration.- Lazy properties are useful when the initial value for a property is dependent on outside factors whose values are not known until after an instance’s initialization is complete. Lazy properties are also useful when the initial value for a property requires complex or computationally expensive setup that should not be performed unless or until it is needed.- We must always declare a lazy property as a variable (with the var keyword) because its initial value might not be retrieved until after instance initialization completes. Constant properties must always have a value before the initialization completes, and therefore cannot be declared as lazy.- If a property marked with the lazy modifier is accessed by multiple threads simultaneously and the property has not yet been initialized, there is no guarantee that the property will be initialized only once.- We can’t use lazy with let.- We can’t use it with computed properties. Because a computed property returns the value every time we try to access it after executing the code inside the computation block.- We can use lazy only with members of struct and class.- Lazy variables are not initialized atomically and so it is not thread-safe.Q. Explain the Computed Properties in Swift??A. Computed properties calculate a value rather than store it. They provide a getter and an optional setter to retrieve and set other properties and values indirectly. Computed properties are provided by classes, structures, and enumerations.Computed properties are always variables (never constants)Q. What happens if we try to set a computed property in its own setter?A. We cannot do that. It will call the same setter method again and again and it will create an endless loop. The app may crash due to a memory overrun.By definition, a computed property is one whose value we can’t set because, well, it’s computed. It has no independent existence. The purpose of the setter in a computed property is not to set the value of the property but to set the values of other properties, from which the computed property is computed.Q. Explain Read-Only Computed Properties?A. A computed property with a getter but no setter is known as a read-only computed property. A read-only computed property always returns a value, and can be accessed through dot syntax, but cannot be set to a different value.We can simplify the declaration of a read-only computed property by removing the get keyword and its braces:Q. Property Observers?A. Property observers observe and respond to changes in a property’s value. Property observers are called every time a property’s value is set, even if the new value is the same as the property’s current value.We can add property observers to any stored properties we define, except for lazy stored properties. We can also add property observers to any inherited property (whether stored or computed) by overriding the property within a subclass. We don’t need to define property observers for nonoverridden computed properties, because we can observe and respond to changes to their value in the computed property’s setter.We have the option to define either or both of these observers on a property:willSet is called just before the value is stored.didSet is called immediately after the new value is stored.Q. What is the difference between property and instance variable?A. A property is a more abstract concept. An instance variable is just a storage slot, as a slot in a struct. Normally other objects are never supposed to access them directly. Usually, a property will return or set an instance variable, but it could use data from several or none at all.Q. Explain [weak self] and [unowned self]?A. - Weak and unowned are used to solve leaked memory and retain cycles. - Both do not increase the retain count.- If self could be nil in the closure use [weak self].- If self will never be nil in the closure use [unowned self].- By declaring weak we need to handle the case that it might be nil inside the closure at some point. If we try to access an unowned variable that happens to be nil, it will crash the whole program. So we only use unowned when we are positive that variable will always be around while the closure is around.- We could use unowned when the other instance has the same lifetime or longer lifetime.- weak will nullify the pointer whenever the reference is deallocated but unowned won’t do that, so that may result in a dangling pointer.- Because weak references need to allow their value to be changed to nil at runtime, they are always declared as variables and optional.According to Apple-doc:- Weak references are always of an optional type and automatically become nil when the instance they reference is deallocated.- If the captured reference will never become nil, it should always be captured as an unowned reference, rather than a weak reference.Q. Can we make a let as weak?A. No, ‘weak’ must be a mutable variable(var), because it may change at runtime. When we declare something as weak, it becomes optional. It may or may not have a value. It will be mutated [set to nil] once there is no reference to it.Q. Why do we need to specify self to refer to a stored property or a method When writing asynchronous code?A. Since the code is dispatched to a background thread we need to capture a reference to the correct object.Q. Can we make a let as unowned?A. Yes, UNOWNED references can be a let or a var. unowned references will always have a value.Q. What is Optional Chaining in Swift?A. Optional chaining is a process for querying and calling properties, methods, and subscripts on an optional that might currently be nil. If the optional contains a value, the property, method, or subscript call succeeds; if the optional is nil, the property, method, or subscript call returns nil. Multiple queries can be chained together, and the entire chain fails gracefully if any link in the chain is nil.Q. When do we use optional chaining vs. if let or guard?A. We use optional chaining when we do not really care if the operation fails; otherwise, we use if let or guard. Optional chaining lets us run code only if our optional has a value.Q. Explain Guard Statement in Swift?A. Like if-else, the guard is a conditional statement that requires execution to exit the current block if the condition isn’t met. In simple words, A guard block only runs if the condition is false, and it will exit out of the function through the return. If the condition is true, Swift ignores the guard block. It provides an early exit and fewer brackets.Any new optional bindings created in a guard statement’s condition are available for the rest of the function or block. It helps us to safely unwrap multiple optional values.Q. What are the benefits of the Guard Statement?A. There are three big benefits to guard. One is avoiding the pyramid of doom (lots of annoying if let statements nested inside each other). The second benefit is providing an early exit out of the function using the break or using return.The last benefit, guard statement is another way to safely unwrap optionals.Q. Explain Forced Unwrapping?A. When we defined a variable as optional, then to get the value from this variable, we will have to unwrap it. This just means putting an exclamation mark at the end of the variable.We use Forced Unwrapping when we know an optional has a value.Q. Nil Coalescing Operators in Swift?A. The nil-coalescing operator (a ?? b) unwraps an optional “a” if it contains a value, or returns a default value b if a is nil. The expression “a” is always of an optional type. The expression b must match the type that is stored inside a.Note: If the value of a is non-nil, the value of b is not evaluated. This is known as short-circuit evaluation.Q. What is Downcasting?A. When we cast an object to another type in Objective-C, it’s pretty simple since there’s only one way to do it. In Swift, though, there are two ways to cast — one that’s safe and one that’s not.as used for upcasting and type casting to bridged typeas? used for safe casting, return nil if failedas! used to force casting, crash if failed. should only be used when we know the downcast will succeed.Q. Explain the Defer Statement in Swift?A. Defer Statement: Put off (an action or event) to a later time; postpone.A defer block executes only after the current scope (loop, method, etc) exits.Let us execute the simpleDefer() function.simpleDefer()// “Print First”// “Chill, later”Q. What is Tuple in Swift?A. Tuple (Functions with Multiple Return Values)- Tuple is a group of different values represented as one. According to apple, a tuple type is a comma-separated list of zero or more types, enclosed in parentheses.() is the empty tuple — it has no elements. It also represents the Void type.Tuples don’t conform to the hashable protocol. Hence it cannot be used as dictionary keys.Tuple is a value type. When we initialize a variable tuple with another one it will actually create a copy.Please note that tuples are passed by value, not reference.Q. Optional Tuple in Swift?A. If the tuple type to be returned from a function has the potential to have “no value” for the entire tuple, we can use an optional tuple return type to reflect the fact that the entire tuple can be nil. We write an optional tuple return type by placing a question mark after the tuple type’s closing parenthesis, such as (Int, Int)? or (String, Int, Bool)?.Note: An optional tuple type such as (Int, Int)? is different from a tuple that contains optional types such as (Int?, Int?). With an optional tuple type, the entire tuple is optional, not just each individual value within the tuple.Q. In-Out Parameter in Swift?A. Function parameters are constants by default. in-out means that modifying the local variable will also modify the passed-in parameters. Without it, the passed-in parameters will remain the same value.In another word, If we want a function to modify a parameter’s value, and we want those changes to persist after the function call has ended, we define that parameter as an in-out parameter instead.In-out parameters cannot have default values, and variadic parameters cannot be marked as in-out.func swapTwoInts(_ a: inout Int, _ b: inout Int) {}Q. How is an in-out parameter different from a regular parameter?A. An In-out passes by reference while a regular parameter passes by value.Thank you for reading! If you liked this article, please clap to get this article seen by more people.Please follow me on Medium by clicking Follow.I’m also active on LinkedIn, Twitter, and GitHub.",ios,https://medium.com/@imnaveensharma/interview-questions-and-answers-in-ios-part-3-1bd9927aa4db?source=tag_archive---------17-----------------------
iOS 14 | Everything you need to know,"David ParisJun 1, 2020·3 min readApple’s new iOS software update is coming soon and you’re maybe debating whether to update or not once it comes out. Right now I’m going to show you some of the top expected features and much more!Homescreen layoutApple has decided to add a list of all of your apps like you might have seen on some Android devices. You’ll still have the classic homescreen but now you’ll be able to access the app list via the spotlight search.iMessageIt looks like we’re getting some of the features for iMessage we’ve been wanting and talking about for a long time now, such as being able to delete a message for everyone, mark a conversation as unread or tag a specific person in the group chat.WidgetsIt is rumored that Apple is going to let you add and use widgets on your homescreen. It’ll include all of the widgets you’re used to having in a separate section on your iPhone right now but also will add some brand new ones.Default AppsApple is probably going to finally allow us to set the default apps ourselves. For example, you’ll be able to set Google Chrome as the default browser instead of Safari, or Gmail instead of Apple’s email app.Wallpaper managementLast but not least you can expect Apple to add plenty of new features to the wallpaper section as well. Now you’ll be able to customize the wallpapers by adding a blur or adjusting the colors. Also, you’ll be able to download wallpapers from 3rd party apps.Apple will officially announce the new software update this year on June 22nd, during the WWDC 2020 virtual conference. It is expected that the first public beta version will arrive around July 13th, 2020. The iOS 14 is supposed to be compatible with the same devices as the current iOS 13.Thank you so much for reading my article, I hope you found it helpful or entertaining in any kind of way. I would be grateful for any kind of feedback from you. Make sure to share it with your friends and contact me for any other questions.Bye.David Paris",ios,https://medium.com/@thedavidparis/ios-14-everything-you-need-to-know-9c8792d5c802?source=tag_archive---------28-----------------------
"How to keep updated with the latest iOS, Swift tech stack every day","Chandan ShettyJun 1, 2020·2 min readBeing an iOS developer is not easy to catch up on the latest technology since Swift and SwiftUI is evolving rapidly. Below are the blogs I admire and follow every day just before jumping into my daily work.This is curated by Dave Verwer and published every Friday, I really like the comment from the author at the beginning of the blogiosdevweekly.comWeekly iOS newsletter curated by Marius Constantinescuios-goodies.comI am a big Apple fan nothing beats the news and rumors on Apple productswww.macrumors.comNSHipster is a journal of the overlooked bits in Objective-C, Swift, and Cocoa. Updated weeklynshipster.comA weekly video series on Swift programmingwww.objc.ioWeekly Swift articles, podcasts and tips by John Sundell, till now I have listened to all his podcasts :)swiftbysundell.comA community-driven weekly newsletter about what’s happening in the Swift open-source projects at Swift.org. Curated by Bas Broek. Started by Jesse Squires. Published for free every other Thursday.swiftweekly.github.ioBelow two blogs are to get familiar with current tech trendwww.techmeme.comtechcrunch.comIt's difficult to keep updated and follow all the tech blog sites individually, so I use the Reeder4 app for Mac as well as iOS.reederapp.comUsing Reeder app helps me to know what I have missed, here is the sample screenshot of my Reeder app window, I have imported RSS feeds of the above blogs via Feedly.Thanks for reading and happy coding.",ios,https://medium.com/@chandanshetty01/awesome-ios-swift-and-tech-blogs-i-follow-every-day-5f6f9954f5c9?source=tag_archive---------24-----------------------
6 Tips for Better Core Data Performance,"Writing Core Data code with performance in mind helps to prepare your app for the future. Your database might be small in the beginning, but it can easily grow, resulting in slow queries and worse user experience.Since I started writing the Collect by WeTransfer app in 2017, I’ve been writing a lot of Core Data-related code, touching it almost every day. With millions of users adding lots of content, performing Core Data-related code has become an important skill in our team.Over the years, we’ve developed lots of insights and I’m happy to share six tips you should know.One thing we didn’t do from the start is to make use of a background managed object context. We only used the view context to perform any Core Data-related tasks: inserting new content, deleting content, fetching content, etc.In the beginning, our app was relatively small. Only making use of the view context wasn’t really an issue and didn’t result in any visible performance penalties related to Core Data. Obviously, once our app started to grow, we realized that the view context was associated with the main queue. Slow queries blocked our UI and our app became less responsive.In general, a best practice is to perform data processing on a background queue, as it can be CPU-intensive. Examples like importing JSON into Core Data could otherwise block the view context and result in unresponsiveness in the user interface.The solution is to make use of a background managed object context. The latest APIs make it easy to create a new context from your persistent container:I recommend this method over the NSManagedObjectContext(concurrenyType:) initializer, as it will automatically be associated with the NSPersistentStoreCoordinator and will be set to consume NSManagedObjectContextDidSave broadcasts too. This keeps your background context in sync with the view context.You can save this background context on a custom persistent container subclass. This way, you can reuse your background context and only have to manage two contexts. This keeps your Core Data structure simple to understand and prevents having multiple out-of-sync contexts.If you only have to use the background context in a few places, you can also decide to use the performBackgroundTask(_:) method that creates a background context in place:However, this method creates a new NSManagedObjectContext each time it is invoked. You might want to consider using the shared background context if you're dispatching more often to a background context.Writing multi-threaded Core Data code is a lot more complex than using a single view context. The reason for this is that you can’t simply pass an NSManagedObject instantiated from a view context to a background context. Doing so would result in a crash and potential data corruption.When it’s necessary to move a managed object from one queue to another, you can make use of NSManagedObjectID, which is thread-safe:Saving a managed object context commits all current changes to the context’s parent store. As you can imagine, this is not a cheap operation and it should only be used if needed to ensure performance in Core Data.First of all, it’s important to check if there’s even anything to save. If there are no changes to commit, there’s also no reason to perform a save. By creating a saveIfNeeded method, you allow yourself to easily build in a check for this:Apart from using saveIfNeeded instead of save(), you also need to consider whether a save makes sense. Although a context could have changed, directly committing these changes is not always necessary.For example, if you’re importing multiple items into your database, you might only want to save after you’ve imported all items on your background context. A save is often followed by UI updates, and multiple saves after one another could easily result in unnecessary reloads. Besides that, take into account that saved changes in a background context are merged into the view context, blocking the main queue shortly as well. Therefore, be conscious!Fetching data is an expensive task and needs to be as performant as possible to make your app prepared for large datasets. The following code is a common mistake:This code will load all inserted objects into memory while it’s being filtered for content with a name.It’s much more performant to use predicates to only fetch the objects that are needed. The filter above can be written as followed with a NSPredicate:This has two advantages:Predicates are very flexible and should allow you to fetch the desired dataset in most cases while maintaining performance in Core Data.Following up on the previous example, it’s important to set fetch limits when you’re only going to display a part of the dataset.For example, say that you only need the first three names of all the content items. In this case, it would be unnecessary to load all the content items that have a name into memory. We could prevent this by setting a fetch limit:This code will only return the first three content items with a name.Instead of iterating over a dataset deleting each object one by one, it’s often more performant to use a NSBatchDeleteRequest that runs faster as it operates at the SQL level in the persistent store itself.You can learn more about batch delete requests in a previous article I wrote.As with all code you write, it’s important to know how to optimize and debug it once it’s not performing as expected. There are many ways of debugging that are best explained in my dedicated blog post.Writing performant Core Data code from the beginning helps you to prepare your app for future large datasets. Although your app might be performing at the beginning, it can easily slow down once your database and model grow. By making use of a background context, smart fetch requests, and batch delete requests, you’re already making your Core Data code more performant.Thanks for reading!",ios,https://betterprogramming.pub/6-tips-for-better-core-data-performance-d7ff8fc07f36?source=tag_archive---------9-----------------------
Unreal Engine iOS Remote Packaging教學 —上集,"TWBobJun 1, 2020·5 min read用UE做了AR專案，原先只有Android版，但是你知道的，神說要有iOS，於是就有了iOS。過程中也遇到一些問題，查了一些資料，終於成功建置，便整理一份教學給同在苦海浮沉的開發者們。[PC]Windows 10Unreal Engine 4.24[Mac]macOS Catalina 10.15.4Xcode 11.4.1在Apple Developer網站註冊並上繳保護費，取得尊爵不凡的開發者權限。參照Unreal官方教學逐步設定，假設已經創好了一個UE的專案，此部分操作均在PC執行。step 1. 建立Key pair與Certificate Request打開iPhonePackager.exe。以本篇環境為例，路徑為C:\Program Files\Epic Games\UE_4.24\Engine\Binaries\DotNET\IOS選擇New User，點選Create certificate request and a key pair…填寫Apple ID使用的Email Address與名稱點選Generate a key pair並把.key檔儲存在安全的地方點選Generate Certificate Request並把.csr檔儲存在安全的地方step 2. 建立Certificate前往iOS Certificate，點選Certificate旁的+選擇iOS App Development，並點選Continue點選Choose file，指向step 5產生的.csr檔，點選Continue點選Download，將.cer檔儲存在安全的地方step 3. 註冊Device前往iOS Device頁面，點選Devices旁的+輸入Device Name與Device ID(UDID)，點選Continue，裝置的UDID可透過iTunes查詢，在此不贅述確認裝置資訊正確後，點選Registerstep 4. 建立App ID在Identifiers頁面，點選+選擇App IDs，點選Continue輸入Descripion，選擇Wildcard，輸入Bundle ID(建議com.[domainname].*)，點選Continue確認資訊無誤後點選Registerstep 5. 建立Profile在Profiles頁面，點選+選擇iOS App Development，點選Continue選擇step 4建立的App ID，點選Continue選擇step 2建立的Certificate，點選Continue選擇step 3註冊的Device，點選Continue輸入Provisioning Profile Name，點選Generate點選Download，並把.mobileprovision檔儲存在安全的地方本篇比較像備份Unreal官方教學，怕版本更新後會有出入。本來想一篇打完的，但實在是有點冗長，那麼還是分個兩三篇吧。下一篇會接著把本篇所產生的認證檔案匯入Unreal Engine中。",ios,https://medium.com/@shinoda0514/unreal-engine-ios-remote-packaging%E6%95%99%E5%AD%B8-part0-1-4bdfa7bb999a?source=tag_archive---------27-----------------------
Integrating Crashlytics with Carthage in 2020,"Laurie Aviel GrayJun 2, 2020·4 min readCrashlytics and Firebase are usually installed with CocoaPods, but what if you were using Carthage as your dependency management system? Tutorial to the rescue! I’m going to explain this as simply as possible as it was a bit of a pain hunting all the info down so hopefully this can help you out :-]Firstly assuming you have Firebase installed, go ahead and add the dependencies to your Cartfile:Firebase and Crashlytics aren’t technically supported without using CocoaPods but it just requires a few steps so hang in there. You’ll need Fabric as well as Crashlytics to get this to work.Go ahead and do a Carthage Update - note, this can take a while.Once this is complete head to your project folder and inside the Carthage directory drag in the Fabric and Crashlytics Framework. You do this in the General tab inside Frameworks, Libraries & Embedded Content.You’ll want to select Do Not Embed as we're not worried about signing and linking before building the project. This way our dependencies will be linked at build time just fine.Once your update finishes your project might not build (as mine did). This is due to the fact Firebase has changed significantly of late and promises are now used throughout.If you navigate back to your Carthage build directory where the other SDKs are, you’ll notice a new PromisesObjc.Framework file there. This is what we're missing! Just drag that in alongside the other two frameworks we just imported.Crashlytics, like Firebase, does a little bit of configuring as your app runs and before you get control in the AppDelegate. For this you’ll need to head to your Build Phases tab for your target and at the very end of your Build Phases add a new Run Script:This will find the fabric framework and it will use the Google Info.plist that you have setup for your target: this is useful if you’re using a dev, qa, uat and prod target setup for example. Pro tip, you can do that like this:Crashlytics will respect this plist that you’re swapping and it will talk to that target on your Firebase console!Whenever you build an app its either built for debug or production. Each build, lets say version 1.0.1, has a unique ID associated with it and any crash report will tie in perfectly with that ID. The thing is crash reports aren’t super-helpful if you need to know which line, or which file the crash happened. For that you’ll need to enable debug symbols (symbols being the functions, variables etc unique to your code) in the Build Settings for your project in Xcode.Head to Build Settings and make sure All is toggled to see all options. Type Debug Information Format and make sure DWARF with dSYM File is enabled for any build you want to monitor for detailed crashes.Next head to the Firebase console and then Crashlytics. You’ll be prompted to enable Crashlytics and it will ask you to crash your app to view it.Once that’s listening for crashes head back to your AppDelegate and import the library and crash that baby!Follow these steps for a headache-free crash-reporting experience, trust me:Simple as that! All you did was add the library, make sure debug symbols were generated on a crash, upload the reports for each target and console environment on Firebase and make sure you forced a crash on an actual device.If this helped you feel free to share it and have a crash-free day!For more head over to www.code-disciple.com.",ios,https://medium.com/@mrlauriegray/integrating-crashlytics-with-carthage-in-2020-9f6ae90aa9f0?source=tag_archive---------14-----------------------
What is type erasure in Swift,"These are are just some of the questions I found myself asking once I first starting exploring type erasure. Like many other developers, I have been making use of protocols in my code to remove dependencies and make my classes easy to unit test. It wasn’t until I then started to add generics to my protocols that I discovered the need to apply type erasure.Having read many blog posts and guides about type erasure I still came away confused as to what it was, why it was needed and why it seemed to add so much complexity. By trying to add generics to protocols in a project I was working on I finally saw the light! I am going to try and walk you through the topic using an example which is similar to the one I was trying to solve in my project. Hopefully this will help those of you who are looking to understand the topic further in the same way it helped me.I am assuming that as you are here you have a fairly advanced knowledge of Swift and have potentially begun or have been using protocols with generics in your code. Below is a simple protocol called Fetchable. The idea of the protocol is to go and fetch some objects of type FetchType from somewhere and call the completion handler with the result once it’s finished whatever it is doing.Now that we have our protocols lets create a couple of structs to implement the protocol.So here we have created a dummy data class, User. Our fetch struct has implemented the generic protocol and has specified the type of object that will be returned in the protocol using a typealias. Everything here is great, we can implement this protocol as many times as we like and return whatever object types we want without the need to create a new protocol for each one.Now, here in lies the problem. If we wish to hold a reference to an object that has implemented this protocol. How do we know what type it is going to return? See the below example:What you will also find here is that you will see an error, something like the below:Protocol ‘Fetchable’ can only be used as a generic constraint because it has Self or associated type requirementsSo we can’t even use this type as a reference to the object, as it has an associated type which we cannot see at this point and have no way of specifying.Now we could do something like below, however this creates a dependency between SomeStruct and the userFetch object. If we are following the SOLID principles we want to avoid having dependencies and hide implementation details.Ok, so let’s try adding a type like we do with other generic types such as arrays and dictionaries:If you try the above you will probably end up with an error something like this:Cannot specialize non-generic type ‘Fetchable’See generic protocols, unlike generic types cannot have their type inferred in the type declaration. The type is only specified during implementation.So this is where type erasure comes in. In order for us to know the type returned we need to implement a new class that can be used to infer the type of object returned so that we know what to expect when we call fetch.Whoooa! There is a lot going on here so let us go through it piece by piece to explain what is happening.Hopefully some of this makes sense, some of this may be new or confusing especially point 4. Let’s show how we can use our class in this example.Now you are probably thinking, why do all of this? Well, let’s try another example:So here we have created a new object that implements Fetchable and returns a user called Dave. We can then pass this into our SomeStruct using our type erasure class and it works exactly the same. The SomeStruct class doesn’t need to be changed in order to work with the new dave class as it’s type has been erased. In a production app we could inject any class we want as long as it fetches a User, whether that comes from the web, core data, the file system. It doesn’t matter we could switch it at any time without making changes to our SomeStruct class.The last example here is that we can use our Any class for other types, not just User. See the example below:Similar to our user example, we have created a new Product object and a fetcher that returns a product object. However we can re-use our AnyFetchable here but specifying the return type as Product.There is a lot to cover and understand here and hopefully this helps make some sense of type erasure and what it is used for. More importantly how to implement your own Any type erasure class for your own protocols so that they can be referenced in your code.Download the playground and play around with the examples yourself.",ios,https://blog.devgenius.io/what-is-type-erasure-in-swift-6e53fe27145?source=tag_archive---------4-----------------------
Why You Should Avoid Storyboards in Your Apps,"I’ve written an article spelling out the advantages of using loadView() instead of using storyboards. People appreciated the article.The problem? The article works with Playgrounds and a Single View Application. Xcode assumes you’re going to use a storyboard, and deleting it gives an error. What a mistake.To get rid of those annoying storyboards, read on.Difficulty: Beginner | Easy | Normal | ChallengingThis article has been developed using Xcode 11.4.1 and Swift 5.2.2.Storyboards: A visual representation of the user interface of an application.Once you’ve created a Single View Application, you’re presented with something like the following image:What’s the problem? Can you see on the left-hand side? Beside your projects? We have a storyboard there — and the whole point is we aren’t going to be using it. If we delete it, we won’t get any further than a blank screen on the simulator followed by an **NSInvalidArgumentException**.That’s not great at all.I’m going to arrange my classes much like I did in the loadView() articleI can list the code for these classes (now I prefer to download these from the repo, but there you go), but I’m leaving the explanation with the original article, as these are basically just created to give us some code to test during the process. In other words, don’t panic!ViewControllerDetailViewControllerButtonViewDetailUIViewIn order to do this, we can just delete the Main.storyboard file with the friendly-looking backspace key on your keyboard. Just select the file, and then press delete. I know you can do it. Go on!There is a link to the storyboard in the PLIST file and in the deployment info. We don’t want either of those, so we’re going to get rid of them with our delete-me-do button.When you select the target on the left-hand side of the project inspector, you’ll see a drop-down saying “Main.” Get rid of it. Another delete will get rid of the link. No problem (right?).In the project inspector, there’s a rather lovely info.plist file. Select it, and I’ll see you just after the image. Don’t delete it — we’ll modify it.The part to delete is buried inside a rather annoying set of drop-down lists. We’re looking for the creatively named Storyboard Name, and we can delete that line entirely. This will get rid of the entire key and value. You’ll be left with something like this:Yes, I did find it tricky to select a deleted row, so there isn’t too much I can do about that (sorry)scene(_:willConnectTo:options:) needs to be adjusted. By adjusted, I do, of course, mean that we need to change the entire body of the function. The function lives in the SceneDelegate.swift file, which can also be selected from the files on the left-hand side of the project.When you've got that selected, our target function will be at the top of the file — just replace it with the following:And we’re done! You’re still here? Looking for the repo? I guess you’ve got it. Thanks.Phew. There are quite a few steps to go through in order to get rid of that (rather annoying) file. You might be tempted to leave it — but that would be a mistake. You’re not shipping extra code that shouldn’t be in your bundle, right?Anyway, at least you have the tools to avoid this from now. All the best!",ios,https://betterprogramming.pub/avoid-storyboards-in-your-apps-8e726df43d2e?source=tag_archive---------3-----------------------
利用 Swift Error Breakpoint 檢查 error handling 丟出的錯誤,"開發 iOS App 時，我們有時會遇到一些丟出錯誤的程式。當我們使用 do catch 或 try! 時，可以馬上看到錯誤的相關資訊，比方以下程式 JSONDecoder 因解析 JSON 失敗丟出錯誤。從印出的錯誤訊息我們得知 JSON 裡 resultCount 的型別是 Int，但是我們卻宣告成 String，因此解析失敗。但如果我們使用的是不會產生錯誤的 try? 呢 ?很遺憾的，此時我們將無法用 print 印出錯誤，App 也不會像 try! 一樣閃退顯示錯誤資訊。但是別難過，我們可以透過 Swift Error Breakpoint 找出錯誤。當 Swift 程式丟出錯誤時，Swift Error Breakpoint 將讓 App 中斷執行，因此使用 try? 的程式也會被中斷。JSONDecoder 的 decode 丟出錯誤，因此 App 被中斷了。我們可在右下角的 console 輸入 lldb 的指令查詢錯誤資訊。由於產生錯誤的程式是 decoder.decode(SongResults.self, from: data)，因此我們輸入po 指令將印出 decoder.decode(SongResults.self, from: data) 丟出的錯誤。(ps: 可以不用加上 try ）Cool，即使是 try?，我們也可以找到錯誤的原因了。若是想先確認是否正確抓到 JSON 資料，我們也可以將 data 變成 string 印出，方法如下:",ios,https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86/%E5%88%A9%E7%94%A8-swift-error-breakpoint-%E6%AA%A2%E6%9F%A5-error-handling-%E4%B8%9F%E5%87%BA%E7%9A%84%E9%8C%AF%E8%AA%A4-7a5fbf6c7303?source=tag_archive---------12-----------------------
iOS doesn’t like my SD card — a UX review,"Matti RichouxJun 2, 2020·5 min readI recently bought a cool thing off Amazon. It’s an adapter for iPhone, making it easy to transfer photos from your big bulky camera to your phone. The adapter itself is very easy to use: simply insert your SD card and plug the adapter into your lightning port:However, what’s less easy to use is the iOS interface that follows. In fact, it has some really dangerous buttons.Let’s have a look.When a user plugs in hardware to a PC or phone, the operating system should figure out whether the user wants to do something with this newly connected hardware or not.Let’s say you plug in a pair of headphones. Chances are, you just want audio sent through the headphones, that’s about it. You don’t need the OS asking you what you want to do.However, if you plug in a hard drive, a USB stick or, say, a SD card, I’d say the likelihood of you wanting to do something with that media is high.Unfortunately, when I plug in my SD card adapter (with an SD card in it) to my iPhone, nothing happens.Hey iOS, I’d like to do something with the photos on the SD card. Don’t act like I don’t. As un-sexy as Microsoft Windows is (don’t worry, I’m a Windows guy) their designers really understand the intent of the user when he or she plugs in media:In this case, iOS doesn’t. It just sits there.Let’s sketch up a quick and easy draft of what iOS should do when we plug in the SD card adapter:All right, so we’ve navigated our way to the photos app of our iPhone. By the way, this is where the “OK” button in the wire-frame above should have taken us.I’d like to talk about two really dangerous things on this screen.Delete All (wipes all media off your SD card)Let’s remember why we bought the SD card adapter in the first place. I want to import photos and videos from the SD card to my phone.The first, and most prominent choice on this screen is to delete all contents of your SD card.Let’s be real: deleting your entire SD card is usually an action taken very rarely, only really done when 1) you have to, because of lack of space, or 2) you are sure you have a backup.In this screen, the designers of iOS have made it far too easy to make a terrible mistake. Yes, there is a confirmation step after pressing this button, but still.Import All (imports all media to your iPhone)This is a button with much less potential for disaster than its brother “Delete All”. However, the use-case is almost equally difficult to grasp. How often do you want to import all your photos and videos to your phone? Even if you wanted to, would you be able to?Many SD Cards can contain hundreds of gigabytes of data, usually far exceeding any iPhone storage space.Delete All and Import All have no business being this prominent. Begone.All right, I’ve made it this far. Time to import some photos!Not so fast! Something’s off.The iconography here is confusing, and inconsistent. On the left side you see me importing photos. Here, by default, every image has a white outlined check mark. Once you actually select an image, it turns into a blue-filled check mark.Why would you need two types of check marks?This is inconsistent with how iOS works in general (right screen) where you only have a check mark on the images you have selected.But all right, that’s a detail, we can get past that.We’re so close.Smooth sailing from here on out, right?❌Wrong❌I want to import three photos.I select those three, and press import. Then, I get this dialogue box:In this dialogue box, the most prominent choice offered to me is toImport all my photos.Why? I manually selected three photos, because those are the three photos I want to import. Why does iOS insist I import all photos now?Here you’re in a fight with your muscle memory. Usually in this situation, you get a dialogue box with an “okay” on top and a “cancel” below. Too quick here, and you’ll import everything.We pace ourselves and instead press “Import Selected”, which is what we wanted to do in the first place.This time, we should be clear of any crazy User Experience Design tomfoolery, right?❌Wrong❌Once the three images are transferred, another dialogue box hits us. This time, we’re given the choice of deleting the images we just imported from the SD card, or keeping them.Again, iOS insists that it should be very easy to delete something off my SD Card. Very prominently, every time.Here’s my solution.In three easy steps, we are able to import all the photos we want without running the risk of deleting or importing anything we don’t want.Importing / deleting all photos are the least prominent options, and they are bundled under a clear label “SD Card Options” at the bottom of the screen.Thanks for reading!",ios,https://uxdesign.cc/ios-has-some-really-dangerous-buttons-a-ux-review-4d5101d1d8cd?source=tag_archive---------15-----------------------
Complete Guide to Codable — Encodable,"Codable was introduced in Swift 4 which helps you convert your JSON to your model and model back to a JSON object. Codable is a typalias of Decodable and Encodable protocols. Codable is similar to Serialization in JavaIn this tutorial, we will only be covering Encodable in-depthEncodable is used for converting your data model to JSON object with the help of JsonEncoderLet’s take a simple example of how to encode your data-model to the JSON objectHurray! And just by writing a few lines of code, you have your data-model mapped to JSON.JsonEncoder will avoid adding keys to theJSON object ONLY if the variables are optional and their respective values are nil. In the below example, we have made feedDate an optional variable and have set its value as nil. This will avoid adding the key to theJSON object.Let's say the server request needs the JSON keys in snake_case instead of camel_case. This can be made possible with the help of CodingKey.CodingKey tells the JSONEncoder to map the variables present in the data-model to the keys present in CodingKeys. In the below example we have renamed the keys in enum CodingKeys from feedKey →feed_key, feedUrl → feed_url and feedDate → feed_date.Encoding nested data is as simple as encoding a simple data model. Let’s say the API server requests start asking you to send location along with the feed. You can add the location in your data-model which will conform to Encodable. You can add the new Location struct in your PhotoFeed or as a new struct outside the PhotoFeed.This value tells the encoder to show how the encoded JSON object would be laid out; like the element order and its readability.1. prettyPrinted: It shows formatted JSON object with white space and indentation that makes it easy to read.2. sortedKeys: It shows formatted JSON that sorts keys in lexicographic order.Most of the time the server requests not always follow the camelCase naming convention. There is a good chance that you may have a request that follows snake_case. We can set the encoding strategy that determines how data-model variables are encoded in a JSON object. Let’s take an example:1.useDefaultKeys: It's a default strategy. It will use the same keys for encoding that have been mentioned in the data-model.2.convertFromSnake: It will convert the camel-case🐫 variable from your data-model to snake-case🐍 while encoding the JSON object.3.custom: You can have your custom implementation where you can choose the name of all the keys you have in the JSON object. This custom closure is called for each variable that is encoded.We have taken an example where we prefix photo to all the JSON encoded objects.JsonEncoder supports encoding dates to the JSON object. Below are the list of strategies that you can use for encoding dates.1.deferredToDate: Number of seconds elapsed since 1st Jan 2001 represented in double. An extremely rare chance you would be using this.2. iso8601: This is the most widely used date-format. Use this link to understand more about date-format used under iso86013. formatted(DateFormatter): You can add a custom DateFormatter with the required dateFormat that you wish to encode your JSON object.4.custom((Date, Encoder) -> Void): This strategy helps you write custom code for parsing your dates to the encoded JSON object. Let’s take an example where you have to send multiple date formats in the JSON object.5.millisecondsSince1970: It encodes dates in milliseconds since midnight UTC on January 1, 19706.secondsSince1970: It encodes dates in seconds since midnight UTC on January 1st, 1970JsonEncoder supports strategies for encoding your raw data into JSON objects. Here is a list of supported data encoding strategies1.base64 : encodes data using Base 642.custom((Data, Encoder) : encodes data using a custom function implemented by you.By now you already got a gist of how the default custom encoding works and how would you use them. Still find it difficult to understand, comment below and I will add gist for youThis policy is used by a JsonEncoder when it encounters exceptional floating-point values. Say if you have to update the server that some values are not-a-number NaN and have infinite values that might need to be handled differently. Here is how you handle nan, +veInfinity and -veInfinityThat’s pretty much you what you get in Encodable 🙌Did I miss a use-case? Let me know in the comments below!",ios,https://medium.com/flawless-app-stories/complete-guide-to-codable-encodable-f15b408b8eaa?source=tag_archive---------5-----------------------
Apple Watch Series 6 Not Coming This Year Leaker Suggests,"sonoJun 1, 2020·1 min readThe Apple Watch Series 6 will feature an OLED screen like previous models, according to a leaker of upcoming Apple products, suggesting the company isn’t quite ready to use its in-house MicroLED display technology in consumer products.Apple reportedly has a secret manufacturing facility in Santa Clara, California where it is designing and producing test samples of displays that use MicroLED, a technology that will follow OLED. MicroLED screens can result in devices that are slimmer, brighter, and use less power.The technology isn’t expected to reach an iPhone for another year or so, but there is precedent for new screen technologies showing up in the ‌Apple Watch‌ first. When it was introduced in 2014, the ‌Apple Watch‌ had an OLED screen. The technology then migrated to the ‌iPhone‌ X three years later.Read MoreOriginally published at https://bbcstoriesnews.blogspot.com.",ios,https://medium.com/@sonoagarwall/apple-watch-series-6-not-coming-this-year-leaker-suggests-3292d2ade907?source=tag_archive---------32-----------------------
A Smooth Corner Radius in iOS,"Arthur Van SiclenJun 2, 2020·2 min readI remember looking at an Apple interface, comparing it to the things I was creating, and feeling a bit miffed that Apple’s designs were even rounder and warmer than mine. Once I learned about this shape called a squircle and figured out just what was going on, I knew I had to integrate this ultra-round shape into the products I build.I started with Minimal | Notes — our notes app that is designed to evoke the experience of a real notebook. It’s a perfect candidate for the squircle, and I found the process of incorporating this shape surprisingly easy.Here’s an exagerated depiction of what’s going on:To accomplish this, I did two things. I used Figma’s Corner Smoothing tool in every image that makes its way into the Minimal app (visible in Figure 1.1).I also created a UIKit-generated bezier path that incorporates Apple’s corner rounding math. It didn’t take long, and Minimal became squircle-rich. You can see the code below, which works with any instance of UIView or UIView subclass.I’m quite pleased with how it turned out. When I do side-by-side comparisons, the interfaces that incorporate the squircle come out feeling softer, warmer, and friendlier.If you’d like to read more about the squircle, check out this story about Rounded Corners in the Apple Ecosystem and this behind-the-scenes look at some of the math that makes the squircle, well, squircular.",ios,https://medium.com/@arthurofbabylon/a-smooth-corner-radius-in-ios-54b80aa2d372?source=tag_archive---------3-----------------------
Building iOS Chatbot with Dialogflow (API.ai),"In this article, we will be sharing steps to building iOS chatbot with Dialogflow. All you need to build a sample chatbot using Dialogflow and Kommunicate for an iOS app.Below is an example of Kommunicate Support Bot developed in iOS using Dialogflow and Kommunicate. We actually use this bot on our website. If you wish to see the bot live in action, click here.We support both Swift and Objective-C in our iOS SDK, and we have made it very easy to integrate through CocoaPods.1. Build a Telegram Bot Scheduler with Python2. A Conversational UI Maturity Model: a guide to take your bot to the next level3. Designing a chatbot for an improved customer experience4. Chatbot, Natural Language Processing (NLP) and Search Services and how to mash them up for a better user experienceThe actionable rich messaging powered bot can reply based on whether users are on chat for general queries, technical queries, Just checking out or scheduling a demo.You can use your existing Dialogflow bot or checkout bot samples to build a qualifying bot of your own. Download the Kommunicate Support Bot from here and import into your Dialogflow account.Let us know jump into the crux of this post.This is fairly simple. You can get a free account in Kommunicate. Signup and navigate to the Bot section. Click on Settings in the Dialogflow block.This is fairly simple. You can get a free account in Kommunicate. Sign up and navigate to the Bot section. Click on Settings in the Dialogflow block.Upload your Dialogflow provided client keys. In case you are using Dialogflow V1, you can copy and paste your client and dev tokens. Though, we recommend using Dialogflow V2 for the latest capabilities.Set up your bot’s name and profile picture and choose whether to allow the bot to human handoff for your newly created bot. Click Finish bot integration setup and voilà, your bot is now integrated.You can check your newly created bot in two places:Dashboard →Bot Integration → Manage Bots: You can check all your integrated bots here.Dashboard → Bot Integration: Your Dialogflow icon should be green with the number of bots are you have successfully integrated.Once you create a bot then you set it as a default bot in the conversation routing section as shown below. Click on Settings –> Conversation rules –> Then click on bot like below and select your bot.Now, this bot will reply in all the conversations.In this step, you need to add Kommunicate iOS SDK to your app. Installing Kommunicate SDK it comes with pre-configured Dialogflow integration. Follow the below instructions to add iOS SDK in your app:Initialize CocoaPods:Kommunicate is available through CocoaPods. To install it, simply add the following line to your Podfile:This is how pod file looks like finally:Run pod install in your project terminal folder:After finishing the installation, you can initialize the SDK by calling the below method:You can get your unique Application ID in the Installation Section. If in any file, you’d like to use Kommunicate, don’t forget to import the framework with import Kommunicate.Login user to Kommunicate:You need to register the user using below Kommunicate.registerUser method before starting chatting and pass the userId and email of the user.Now, you can send the payload data to Dialogflow through chat screen and get a text response from Dialogflow Agent. Kommunicate provides a ready to use Chat UI so only launching the chat screen is required in this step.You can refer the more documentation for conversation section here. Run the iOS project and chat with the Dialogflow bot. You can easily integrate Dialogflow in iOS apps in a few simple steps. In case you need more information, you can check out the Kommunicate iOS documentation.Kommunicate Sample application: Download the sample app from here which includes ready to use Kommunicate iOS SDK.Dialogflow iOS SDK Client:Dialogflow provides iOS Client SDK that makes it easy to integrate speech recognition with API.ai natural language processing API on Apple devices.If you are looking to develop your own chat implementation and directly you can integrate Dialogflow into your iOS app without Kommunicate SDK, then integrate with the following SDK from GitHub: Dialogflow iOS Client SDK.Liked the story? Hit that clap button and follow me on Medium. Thanks for reading! This article was originally published on Kommunicate blog.",ios,https://chatbotslife.com/building-ios-chatbot-with-dialogflow-api-ai-e9c9fd25bd42?source=tag_archive---------7-----------------------
How to Draw in 3D With SwiftUI,"In an article I published in April, I used the rotate tag in SwiftUI to rotate dominoes. The tag in question can be thought of as a convenient option. I say that because there is a more advanced one that gives you much more control.The first two elements — the degrees of movement and the axis on which you want to turn the object — are exactly the same as with the convenient version. But in the advanced method, you have three other variables you can work with. As such, this tag is one of the most complex to use in the SwiftUI armory. Hopefully, this complexity will clear up as you read on.Let’s start with the anchor. This basically determines which side you want to pivot on. The effect of the anchor is closely linked with the axis on which you are pivoting.In the example above, I changed the degree of pivot around 360 degrees, initially doing so on the Z-axis. Here is the code behind the scenes (note that this gist is for the following piece):You can clearly see the rotation3DEffect tag in code above, with the only variable I am animating being pivotDegree. Here is the code in action, making the green, blue, and purple boxes turn:Of course, there is the Y-axis on which you can also pivot. The next example shows just that:Now I didn’t mention it, but we did use/set the perspective here too, setting it to 0.5. It controls the amount of swing. The higher the number, the greater the swing. Here is a GIF showing a perspective of 1.2 with some words of wisdom:Note: It disappears as we swing through 90 degrees. We’ll use that later.This brings us to the most challenging of the variables: the anchorZ. Set the code running the animation below to the values 10, 20, 30, 40, 50, 60, and 90. I chose those values because they correspond with the size of the rectangle I am moving about. The anchorZ controls the center point of the diameter on which you are pivoting around. You can see the effect most clearly with the red/pink square on the very end, which pulls to the back and then swings forward to the front.In truth, I’ve been simplifying things just a little. You see, the anchor values I’m using are of a type UnitPoint. There are few presets, but you can mix and match your own combos to get some pretty cool effects:The values are a little odd since we’re rotating on the X-axis and varying the Y-value in the UnitPoint variable. A value of zero here is in effect a UnitPoint.top, and value of one is a UnitPoint.bottom. The other values are somewhere in between. In terms of degrees, we’re travelling 360, so a full circle. Note how similar it looks to the previous example. It is using perspective but only a value of 0.5. Our card at the end of this animation swings in front and then back in line.Here is the code for a portion of this piece:Be warned, things can get confusing because many of the attributes are interlinked. So if you’re not careful and change two opposing variables, they will cancel each other out and it’ll seem like you didn’t change anything at all.We covered a good deal of ground on 3D rotations in SwiftUI, but what about an object?Let’s focus on building a cube. It has six sides, although to keep things simple, I’m only going to worry about four of them. Two of our sides will start at 90 degrees, making them invisible, and two of our sides will be at 0 degrees (so face on). As we rotate things, we’ll need to use an offset on two of the faces to keep everything together. To help us get the rotation correct, we use a perspective value of at least 0.5 so we can see if we got it right as it turns. It helps too if we add a line of text to each face so that we can tell which way it is flipping as it turns. Sometimes, it isn’t so obvious. Finally, we use opacity as we build it to mask the faces that are good. Use opacity at the end to get your front and back visually looking right. Reduce the perspective to a minimum too. Otherwise, it simply won’t work.Keep in mind as I said before that many of the parameters interact with each other, so be careful as you move forward not to change more than one at a time.Follow those rules and soon you’ll find yourself looking at a cube just like this one — a cube you can leave as a wire frame or indeed add a skin to.Here’s the code for this GIF:As I said, once built, you can add a skin too and some subtler shading. And there you have it: a 3D object in SwiftUI.",ios,https://betterprogramming.pub/how-to-draw-in-3d-with-swiftui-7989cfcd35fc?source=tag_archive---------5-----------------------
AutoSender Tips and Tricks,"FreeRamble Technology Inc.Jun 1, 2020·5 min readOur iOS text scheduling apps AutoSender and AutoSender Pro have many features and abilities, and offer incredibly flexible use. As we continue to update and improve our app we are consistently bringing in new and exciting features.In this article we are going to take a look at some of the different features that our app offers, how to use them, and what they can be used for.There are many times where you may want a specific message to send to certain people at specified times. For example, if you manage properties you may want to remind tenants at the end of each month to send their rent payments. By using AutoSender you can schedule Daily, weekly, or monthly repeat messages that will be sent at these specific dates and times.To use our repeat messages you must open the app and go to the schedule a message page as you normally would. You can then select “Repeat” from the upper right corner of the scheduling page. Next click “Date” and you will be brought to a page where you can select Weekly or Monthly. If you choose the Weekly option you can choose the days you want the message to send, including every day if you want the message to be sent daily.If you choose Monthly you can then choose specific days of the months that the message will be sent out, meaning if you choose the 20th your message will be sent out on the 20th of each month. However, please note if you choose a date like the 31st that does not appear every month it will only send on months with those dates.Once you select the days you want the message to send you can then click “Ok”, and continue scheduling the message like a regular message. The repeated message will be sent until you either cancel the task or run out of credits.When using the provided cloud number to send messages you are unable to receive texts or send texts that are not scheduled messages as the cloud numbers are shared numbers. However, if you subscribe to a private number you receive a personal number that is exclusively yours.With this number you can send and receive texts as you would with a normal phone number simply by going to the inbox in the app.To subscribe to a private number you can click the “Balance” tab and select the “Numbering” button. You will then be able to choose from a 30 day subscription, 90 day subscription, or a yearly subscription.Once you have chosen your subscription and clicked subscribe you will then be able to choose between a US or Canadian number, once you have chosen you can enter the area code of a region you are interested in and pick a phone number from the available numbers.Once you are subscribed to a virtual number you can access the inbox by clicking the mail icon in the upper left corner of the scheduling page. In the inbox you will then be able to see any text messages you receive to your virtual number. You will be able to start new messages with your contacts or by typing in their number or reply to received messages by opening the chat.Click here for more information regarding sending and receiving messages.There are many times where we may be away from our phones but expecting an important message, whether you are driving, on a flight, etc, it can be stressful knowing you may be missing important texts.By using AutoSenders automated responses you can choose the contacts you want to receive a response and then type in the message you want them to receive, and that text will send out after they text you. For example you could have it set to automatically send a message letting them know you are currently driving and will send them a reply once you are able to.To do this you must first subscribe to a private number as mentioned above, as there is no way to receive messages through cloud number.Once you are subscribed to a private number open the inbox in the upper left corner of the schedule page and then click “Auto-Reply” on the bottom left. Next select the contact(s) you want to receive the message, and type what you want the message to say.When the contacts you chose send you a message they will then receive the reply you set up.Click here for more information regarding automated replies.Creating groups is a great way to allow you to easily message a large number of contacts at once but allow them to receive the message individually and not as a group chat. For example you could have your co-workers in a group so that you can choose that group and message them about work related information.To create a group open the app and click “Setting” in the bottom right corner. Next, click “Manage Groups”. In this screen you can create new groups, manage existing groups, or delete groups. This is a great way to split up your contacts by groups such as family, friends, co-workers, etc.Originally published at https://www.freeramble.com on June 1, 2020.",ios,https://medium.com/@freeramble/autosender-tips-and-tricks-news-2c29246522ed?source=tag_archive---------33-----------------------
AddTrust External CA Root Expiration on May 30th and how to fix it,"If you’re like me, you probably woke up on Monday, June 1st, and found some of your services suddenly failing, whether it’s your mobile app failing to connect to your API or your images suddenly failing to load.In my case, some of our mobile apps (Android, specifically) were reporting errors trying to retrieve data from our APIs. After debugging for a bit I found that it was an SSL issue.Opening the URL in question in Chrome or making requests on iOS, however, showed no issues, and I was greeted to the normal green lock icon representing a normal certificate validation.The issue:Modern browsers and HTTP clients like the iOS one have a better TLS trust verification strategy than older HTTP clients like cURL and the Android built-in one, they will build a chain of trust to the root instead of stopping and failing at the first expired certificate in the chain.So what’s the problem, and how do we debug further?Well to investigate SSL issues we can rely on good old OpenSSL. You can use OpenSSL to fetch all certificate data for a given server by running:The output you will receive should contain a section containing the full list of certificates in the chain:Notice the AddTrust in the 1st and 2nd positions in the certificate chain. These certificates are now expired and are causing the certificate has expired errors in cURL and the Android HTTP client.How do we mitigate this?The proper way to mitigate this is on the server-side, by updating the SSL configuration of the server, or the reverse-proxy if using TLS-termination.You can check your certificate chain using https://whatsmychaincert.com/. Just put the URL of your site there and it will spit out useful information about your certificate chain issues. You can even put in your hostname (domain) and it will auto-remove the expired AddTrust certificates and produce a fixed .crt file you can put into your existing configuration as a drop-in replacement.If you’re not a server administrator or you’re having issues with another platform, and just want to fix your client until the server admin picks up the slack, you have a few options:On Linux-based client environments (Ubuntu and others), you can edit the ca-certificates.conf file:In there you will find 2 references to AddTrust, a quick fix is to just prepend ! before each line and run:to apply the changes afterward.On macOS, a simple way to fix this for me was to edit /etc/ssl/cert.pem , find the AddTrust certificate, and completely remove it.Make sure you only delete lines between and including:This should fix your issues and cURL and other HTTP clients should now be happy to resolve the new USERTrust Root Certificate properly.Hope you found this helpful and I saved you some headache, cheers!",ios,https://medium.com/codechem/addtrust-external-ca-root-expiration-on-may-30th-and-how-to-fix-it-9f70832d6eea?source=tag_archive---------11-----------------------
Sign In With Apple: A Node.js Back-End Implementation for iOS Developers,"One of the most interesting aspects of iOS 13 (the most recent major release) is that it gave us a tool to create our own authorization-based back-end servers for our iOS apps. Sure, we had “Sign in with Facebook, Google, etc.” before, but for an indie project, “Sign in with Apple” is a lot easier and faster to implement — both on the client and server side.Let’s take the following scenario (based on my experience, of course): You’re an iOS dev who has a small app published under your own name. You want to add a server-dependent feature, but you either don’t have a back end yet or you have a simple back end with no user management system. Your feature requires user management. You want to persist specific items in relation to your app’s users. You don’t want to create a user/password-based system, you don’t want to manage JWTs, and you want to keep the iOS code to a minimum.This is what nobody talks about. “Sign in with Apple” has been created with this specific use case in mind.First, let’s make some aspects clear: In this article, we will not be talking about the iOS implementation. That has been covered in myriad articles and tutorials. It took me less than eight hours to write the client-side code in a SwiftUI app. Raywenderlich.com should provide everything you need. It will also not cover deploying a Node.js back end. You can find a lot of tutorials on Google. It’s 10 minutes’ work to deploy a Node server using Express. Here’s an example.We’re going to pick up after you have the iOS code in place and the iOS APIs have provided you with the user’s email as well as Apple’s user identifier and authentication JWT.Let’s first create a new route, /authenticate:Later, we’re going to explore why we called it /authenticate and have a single route instead of /register and /login.We’ve extracted the token, email, and apple_id variables from the request body.The first thing we want to do is query our database about if we have that user:We’re now calling getUser on our database, with the callback passing us either a user or an error. The error is strictly in the DB communication. I handle that by passing it back to the client. The client will show a popup to the user and they can screenshot it and send it to me for debugging. This is my choice. You can handle it however you want. This doesn’t mean we don’t have the user. It means that querying the database failed.You can see that I only passed the apple_id and not the whole user to the fakeDB. This is in relation to one of the privacy features of “Sign in with Apple.” If you already wrote the client-side implementation, you should have seen and read up until this point that SiwA only provides the user’s credentials the first time the user uses it with your app.This is very important, and the reason it’s important is that because of this, you need to write some specific logic to have the user’s credentials in sync with both the server and the client. This is available for both the email and name (if you request them from Apple).If we have a user, we jump to the next step. We now check if we received an email with our request:This is again a quirk or edge case that comes with SiwA. Think of it this way:If you found the user in your database by their apple_id and this time you didn’t receive an email with your request, you need to send the current email from your DB to the client.The next validation in that if checks if the current request’s email is the same one that we have stored in the DB:Again, edge case:At this point, you need to respect the user’s choice. They decided to have their email hidden. You now need to update it both on the client and server side.If this validation fails:We need to update the user’s email in the DB:Then send the user back as the response:This was the login flow. We have a user in the DB with the provided Apple identifier, we update their personal information if needed, and we send a success response once finished.If we didn’t find a user in our DB, this means this is the first time the user is signing in to our app, so we need to handle the token validation and create the user in our DB:First, I created a token service in a different file. Its responsibility is to validate the JWT token received in the request body. For this, we’re going to need the jose and request npm modules:I’m not going to delve deeply into how JWT validation works, but you can find a good explanation on Sarun.My implementation of the verification service is as follows:If the verification fails, we’re sending the error back to the user (how you wish to handle this is your choice).If the verification passes, we can now add the user to our database with their designated apple_id and email.Upon successfully adding them to the database, we send the user back to the client with the provided apple_id and email.That’s it! At this point, your user authentication is implemented. Your back end can now sign in or register users with a single call towards /authenticatefrom your iOS app.In case you have some questions, though, I prepared some answers for the ones that bugged me during development.Well, if you plan to only support SiwA, they’re not needed. Apple provided a simple API on iOS to check the session directly in the app. According to the official docs:“After verifying the identity token, your app is responsible for managing the user session. You may tie the session’s lifetime to successful getCredentialState(forUserID:completion:) calls on Apple devices. This is a local, inexpensive, nonnetwork call and is enabled by the Apple ID system that keeps the Apple ID state on a device in sync with Apple servers.User interaction is required any time a new identity token is requested. User sessions are long-lived on device, so calling for a new identity token on every launch, or more frequently than once a day, can result in your request failing due to throttling.If the user’s Apple ID changes in the system, calls to getCredentialState(forUserID:completion:) indicate that the user changed. Assume that a different user has signed in and log out the app’s currently known user.For apps running on other systems, use the periodic successful verification of the refresh token to determine the lifetime of the user session.”This means that you can (and should) use getCredentialState when your app finishes launching to validate that the same user is logged in on the device and they did not revoke SiwA for your app. If they did revoke, you should perform the sign-out flow on the device (like deleting the saved user credentials and apple_id from the device) and show the proper SiwA screen when it’s needed.Because the principle doesn’t apply here. As in the answer above, you can use the local getCredentialState API to validate the current user, so a new call to your back end isn’t needed. This means that the only use case where you need to perform a login is after the user manually signed out on the device.Sure, you can create both /login and /register and separate the logic from /authenticate in two endpoints on your back end, but you would have the same code duplicated in two methods on the client — one that calls the login endpoint and the other that calls register. You would also need to check on the client side if it’s the first time the user signs in with Apple or not. We did that check on the server side, remember?You don’t. I show the user email on a screen in my app. That’s why I have it persisted. If the user signs out, signs back in, and picks another email address from SiwA, I want to show the correct address (as in the last one they used to SiwA) on that screen. If you don’t need the email or the user’s email, there’s no need to persist them on the client or back-end side.With that said, it’s good practice and safer to verify the email from the request body with the one decoded from the JWT token. So if the user signs in for the first time or it’s a subsequent sign-in after they revoked your app, my recommendation is to send the email to your back end for validation.You need to revoke access for your app.On your iOS device, go to iCloud > Password & Security > Apple ID logins > *Your app* > Stop using Apple ID.As you can see, Apple put a system in place that is very beneficial for the use case where you want to have a simple sign-in system without passwords or other third-party authentication providers, and we should use it. From my point of view, it opens a lot of doors.Happy coding!",ios,https://betterprogramming.pub/sign-in-with-apple-node-js-backend-implementation-for-ios-developers-25a54b79aa68?source=tag_archive---------1-----------------------
"[Swift]Music App #6 Collection View, Table View scrollToRow, Navigation Bar Colour",這個版本的更新中練習了collection view的layout設定及使用，用collection view做出Instagram 使用者首頁的照片格，點擊之後轉跳到下一頁，可以見到點擊的照片及貼文細節，並模仿現在IG的功能，轉跳過去後還可以上下滑動看其他的貼文。Collection view是比table view更新的東西，也比table view 更彈性，雖然很多地方很像table view，但也花了一些時間去熟悉和摸索細節。初學swift和storyboard的時候常會碰到一個問題，就是有時會不小心在storyboard和code針對同一件事做出衝突的設定，導致最後layout、元件設定等無法如預期地方式呈現，在練習這部分的時候也學到了很多東西。練習時還遇到了一個問題：所有模擬器都能順利在一行顯示指定的格數，唯獨在iPhone 8的模擬器上無法。這是因為練習中的collection view不是用collection view controller建立，而是用一般的view controller加上collection view原件，並用auto layout與view controller的四邊固定。viewDidLoad時collectionView的大小是設計時storyboard的型號（我使用iPhone 11 Pro Max），而auto layout會在viewDidLoadSubviews前才計算。我計算每個cell的尺寸時是使用collectionView元件的寬度來計算，因此會受到影響。處理方式有3種：Collection view header的內容使用collectionView:viewForSupplementaryElementOfKind:atIndexPath:來控制。很像設定table view和collection view的cell內容，建立一個header的reuable view常數，型別down cast為header自訂的型別IGPicHeaderReusableView，接著就可以取得並設定header上的元件內容。scrollToRow可以讓畫面跳到指定的table view row。在第一個VC中點下圖片時，會利用prepare for segue將點下的collection view cell的indexPath傳到第二個VC，並在第二個VC中使用table view的scrollToRow將畫面帶到剛剛點下的圖片那個post。要將navigation bar的顏色改成透明，必須寫程式，在viewDidLoad中修改navigation bar的background image和shadow image。github.commedium.commedium.commedium.comTable View scrollToRowdeveloper.apple.commedium.comithelp.ithome.com.twCollection Viewmedium.comitisjoe.gitbooks.iostackoverflow.comNavigation Controllermedium.comUILabelmedium.comlayoutSubviewsmedium.commedium.comwww.appcoda.com.twDownload Datamedium.comdeveloper.apple.comstackoverflow.comstackoverflow.com,ios,https://medium.com/%E5%BD%BC%E5%BE%97%E6%BD%98%E7%9A%84-swift-ios-app-%E9%96%8B%E7%99%BC%E6%95%99%E5%AE%A4/swift-%E7%B7%B4%E7%BF%92-music-app-6-collection-view-table-view-scrolltorow-4ea6e4c39850?source=tag_archive---------19-----------------------
GraphQL for iOS Development,"GraphQL is a query language for APIs. It specifies the ways that a client can communicate with a server. If you are familiar with a REST API, you can think of this as an alternative. REST APIs are typically defined as a set of endpoints that represent resources with which you can perform CRUD operations via HTTP methods… For a blog app (like Medium) these resources may be Authors , Posts , Comments , etc. With GraphQL, resources are NOT defined by API endpoints, they are defined by a schema that is introspected by clients who will then formulate whatever queries they want as long as they are valid according to that schema.If this just sounds like a bunch of jargon to you, I will attempt to simplify it: With REST, the API endpoints determine what data can be exchanged over the network. With a GraphQL endpoint, the client determines what data will be exchanged over the network, as long as its supported by the schema.From a client perspective, GraphQL implementation goes as follows:… More on this later.Our apps are completely fine the way they are! Why would I want to change from using REST to GraphQL?Well this argument is fair, there is a little bit of a learning curve with GraphQL… and you may not solve too many problems by using it over REST. But it does solve one set of problems exceptionally well:Most data models have relationships with one another, to bring back the blog example: Users may have Posts , Posts may have Comments, those comments will have associated Users, and it goes on and on. As iOS developers, we all run into the issue where we need to chain API requests to get all of the resources in order to populate a particular view. We hate this. We want to hit the network as few times as possible, it makes for cleaner, more performant code and a better user experience. A solution to this problem would be to constantly go back to your API authors and ask for “special” endpoints. This is where REST starts to break down. This “special” endpoint doesn’t necessarily represent any particular resource, but it represents a fine tuned response that a particular client will need. API authors may be reluctant to create these “special” endpoints because it detracts from the whole REST paradigm and can lead to some messy code on their end… but for performance reasons, we can usually force their hand.With GraphQL the client can specify exactly what it wants in every request, nothing more, nothing less. The shape of the response will be the same as the shape of the request. This means:GraphQL supports three different kinds of operationsQueries, which fetch data.Mutations, which (you guessed it!) mutate data.Subscriptions, which subscribe a client for realtime updates.To execute any of these, you will have to formulate them using GraphQL Query Language.Here is an example of a query, followed by an example of a mutation. To learn more about GraphQL Query Language syntax, check out the docsThe truth is, GraphQL shines the most over REST when fetching data via queries, but all CRUD operations are, of course supported by the GraphQL spec.If you’re looking at this type of syntax for the first time, you are definitely going to have some questions:Well, the answer is simply GraphQL Schema Introspection. Clients can introspect the schema provided by the API to generate documentation and validate queries. This is one of my favorite things about GraphQL. You don’t need documentation about the request/response data of every endpoint. Once you introspect the schema, you will be able to see everything that is possible with the API.In my workflow, I use a tool called Insomnia (https://insomnia.rest/graphql/). It acts as a GraphQL client that can introspect your schema and generate all of necessary documentation. Since Insomnia will now know about your schema, you can test out some queries and mutations on your server with full autocomplete and real time validation support!Finally, now we can get to the bread and butter of this post. It’s cute that you can use insomnia as a client, but we really want our iOS app to be the client!If you stumbled upon this post by looking for resources on how to implement GraphQL on iOS you may have realized that the majority of paths take you to one Framework: Apollo Client for iOS.This is not necessarily a problem, after all Apollo is probably the industry standard implementation for GraphQL on any platform. I’m going to tell you the problems I have with using Apollo Client. These problems may not apply to your use cases, and if they don’t, you should hop over to their docs ASAP to get started!Apollo Client is VERY heavy handed with your app. It will involve itself in your modeling, network, and caching layers. It will introspect your schema and allow you to write your queries with GraphQL query language in text files. It will take the queries you write and generate models and operations in code for you. You will then use Apollo’s networking and caching abstractions to execute those queries in code. Due to all of this work, you can be absolutely sure that your requests will work and the shape of your models are correct (If of course, the schema of the API doesn’t change underneath you).While it’s AWESOME that Apollo Client will do all this for you… it may not be what you want. Maybe your app already has a pretty extensive networking layer with its own caching mechanisms that you’ve come to rely on. Maybe your app hits a plethora of API’s and you’re slowly moving some of them to GraphQL. Maybe you have existing models that for some particular reason, you don’t want to replace with the ones that will be generated by Apollo Client. These are all reasons that have prevented me from integrating this framework into an app. They may be completely invalid reasons for you, especially in the case that you are starting an app from scratch that is going to rely heavily on GraphQL. These are also *my conclusions about this framework, and if there are ways to use it without running into these issues, I would love to hear them or have them clarified in their documentation.If we want to avoid the overhead of a heavy handed framework, we are going to have to forgo some of its luxuries. There are a few different ways we can write our requests without Apollo-Client. We will still get the benefits of GraphQL queries (reducing our network calls) but we won’t get to *directly* enjoy schema validation to let us know that what we are doing is possible on our API.At the end of the day the GraphQL request is typically a JSON encoded string in the body of an HTTP POST request. Which means we can generate this string ourselves in code and then shoot it off to our API. This isn’t ideal, it even says so in the GQL docs as an argument for the use of GQL variables:“It wouldn’t be a good idea to pass these dynamic arguments directly in the query string, because then our client-side code would need to dynamically manipulate the query string at runtime, and serialize it into a GraphQL-specific format”BUT, it’s really the best we’ve got at the moment without integrating Apollo Client. Luckily there are some open source libraries out there that enable us to build our GraphQL queries in a very Swift friendly way. The one I am currently using is called Sociable Weaver, its a nice lightweight library that lets us use a GraphQL query building DSL powered by the new @functionbuilder Swift language feature. I highly encourage you to check it out!Creating a request (called a Weave in this library) looks something like this:Which will generate the following requestThis is a very basic one, but this library can be used to formulate much more complex requests…So what would the workflow look like if you wanted to skirt Apollo-Client iOS and use a library like SociableWeaver?First, I would recommend getting some sort of GraphQL IDE/Playground/Client whatever you want to call it (As mentioned before I use Insomnia GQL IDE). So you can still introspect your API’s schema and test out requests. This way you know that the requests you plan on building in code will actually be valid. Also, this will give you complete API documentation!Next, use your query building library how you see fit to generate your requests. It’s important to understand how the data comes back from the server so you can decode it properly. In GraphQL you will get back a data field and possibly an operation name that you will have to drill into to get to your response object.Keep in mind this approach may seem hacky to some GQL purists, but I find it very useful for mobile developers who don’t want to undergo huge refactors to start supporting some cool new GQL API. I’m sure even more tools will come down the pipeline for more seamless implementations of GraphQL in our iOS apps!I hope some of this helps and feel free to leave any questions or comments!",ios,https://levelup.gitconnected.com/graphql-for-ios-development-cfeff5bdc043?source=tag_archive---------7-----------------------
LastPass clone — part 12:,"Hey guys, in the previous part we created entities for password and note as well as their respective models and view models. I had to prepare those in a separate post as it would’ve made this article way too long. In this one we will start working with CoreData and do a little bit of refactoring.You should have the source code link in your email inbox if you are subscribed, otherwise click here to subscribe and get it.We will put all core data related stuff in a separate file almost . In the service folder, add a swift file named CoreDataManager.swift containing the following code:This will be our CoreData playground. Then add the following property to the top:We make the initialiser private, because we want to make this class a singleton meaning we will create a shared static function that will return an instance of this class containing a valid context.Add the following code below context to create that shared instance:What we are doing here is initialising the shared instance with the context retrieved from the AppDelegate. If you open the AppDelegate.swift, and look for persistentContainer, you'll find it declared as a lazy variable inside the class.You will get an error saying the following Value of type ‘CoreDataManager’ has no member ‘context’, it makes sense because we haven’t created the context property yet. Add the following to the top, above the init function.A context consists of a group of related model objects that represent an internally consistent view of one or more persistent stores. Changes to managed objects are held in memory, in the associated context, until that context is saved to one or more persistent stores. A single managed object instance exists in one and only one context, but multiple copies of an object can exist in different contexts. Thus an object is unique to a particular context.The above statement means that the context is an object that keep our local and persisted state of our data in sync. Every time we set one or more of our entities, the changes will only be persisted until we save the current state using the context.Next, add the following method below the init:In the above function, we first check if there are changes in the context, if it’s the case we call save() on the context to store those changes.Next, add the following function below the one above:This one will be called every time a user views a particular password’s details. What happening is that we use the password’s id to fetch the whole object from the database, update its lastUsed date to current date and save it again.Next, add the following below:This one will be called every time a user toggles the favourite button in the details view. What happening is that we use the password’s id to fetch the whole object from the database again, update its isFavorite attribute to 1 or 0 (true or false) and save it again.Next, add the following 2 methods below the ones above:The above methods do the same as the ones we’ve created for the password, but these will be used to update the note.Before creating the search function, let’s first add the following properties to the top of thee struct:Here is an explanation of what each of those properties will do:Now let’s implement the search process. Below everything, add the following block of code:Here is what we are doing above:Last but not least in this class, we need created a function that will be used to apply filters. Below the function you’ve created, add the following:The above is pretty self-explanatory as well. We first reset the predicates and sortDescriptor to default values, then set corresponding values in the switch statements.With that, we are done for this part. We are still missing the deletion functionality which is almost the same as the update. I’ll leave that to you, play with the project be creative, be liquid. In the next one we will integrate CoreData in views, stay tuned. Please feel free to share this article, subscribe if you haven't done so already. If you have any question, send me an email. Happy coding.Originally published at https://liquidcoder.com on June 1, 2020.",ios,https://medium.com/flawless-app-stories/lastpass-clone-part-12-dc9692122e80?source=tag_archive---------20-----------------------
"Mocking objects in Swift — fake name, last name or description","Mocking is a technique when we replace real data in an application with some generated or static fake data to handle a specific test case.In this article, I’ll try to show You how to create and use simple service to Mock data (not the services).Why do we want to mock something? Mocking is pretty useful when we write tests or when we don’t have complete Backend but we know the structure or even when we develop UI and we need some filling. Using mock generator we can easily change mocking behavior and — whats more important — find all mocked data to avoid fuckup on a production environment.Ok, let’s assume that we want to create an object with a bunch of properties and all of them should be generated uniquely but also should have some sense. We don’t want situations when in name field we find random hash or in phone number some ‘lorem ipsum’ string. Data should resemble something real.This is the way how I see that case:Of course, there is no ‘randomName’ function in swift. We should prepare a bunch of base properties which holds our real data. With the help of this data, we can select random name or words that make sense. Imagine that You want to generate only German names or Chinese, just change base array and You will get what you need. At this moment names, last names and paragraphs lets us extract real information.In most cases, you need just a simple string. Imagine you have a list of products and you want to generate and check how it will look like with dozen products with different names. Let’s create an extension for our service that returns random word from paragraphs (you can also prepare a separate list with product names).But hey! What if we have longer names, not only one word or some of our products doesn’t have a name? Go deeper. What if we want to generate longer description? We don’t want to create a new function for each property. Our generator should be reusable for different cases.The solution is within reach. We can add parameters that extend our methods. Let’s define some options: - none — we don’t want to modify behavior - length — we want to specify specific length — short, regular or long- nullable — we want our value to have a chance to be nullImagine you need a value to be null but not always e.g. you noticed that in 90% cases your product description is set but there is a possibility that someone skipped it in the input form. You want to take this into account and check what will happen. The function below extract proper option and provide a flag if the value should be null or shouldn’t.Let's come back to our ‘string generator’ and add options handling. Line number 4 checks if the value should be null. If ‘nullable’ option isn’t set it’ll skip. Then — depends on the length — We can return a string with different length. It’s easy like that.The name generator is even simpler. In all methods, we should check the ‘nullable’ option, but length can be skipped. Of course, nothing can stop you from extending it in the case when you have a very long name.To complete this example we need one more thing. We have to inform our app that model can be mocked. If the model conforms to the Mockable protocol it means that it can generate a mock object.You probably noticed that our first example uses much more functions than string or name. In the same way, you can create methods to generate numbers, email, postal code or date. Even the whole object can be mocked in another object.Let me know if you want to see complete code from my mock.framework.If you have any question or suggestion leave a comment.",ios,https://medium.com/macoclock/mocking-objects-in-swift-fake-name-last-name-or-description-ebdbc2194998?source=tag_archive---------25-----------------------
"How to use Fastlane to deploy multiple targets to TestFlight, Xcode iOS🚀🚀","A compact tutorial on how to deploy multiple targets iOS app to TestFlight in 8 easy steps.Let’s get rolling.1. First of all, install Fastlane2. Setting up FastlaneOpen Console -> Navigate your project directory and run:This command going to ask you — ”What would you like to use Fastlane for?”Choose the second option:This choice will set the basic setup that we a going to expand to work with multiple targets.3. Create environment variables per targetFor instance we have two targets App1 and App2. For each of them creates an environment file that will hold an environment variables Scheme name and Bundle. The filename should be in format .env.YourTargetName. Create .env.App1 and .env.App2 files.For example, fou can create both files in the console:Fill up both files accordingly, each with your unique scheme name and bundle of each App target:4. Modify an AppfileOpen the Appfile and change the line:To use defined variable:5. Generate application specific passwordsTo upload builds to TestFlight, you need to generate an application specific password.6. Fastfile and GymfileThe context of rest two files Fastfile and Gymfile are completely generic for all projects. They know to get a scheme and bundle identifier from our files.Fastlane file — is a major Fastlane file that builds and upload to TestFlight for all targets.Gymfile — is specifiled where and how to create a *.ipa files.Download Fastfile and Gymfile from my GitHub repo and place them into fastlane directory.7. Inside Fastlane directory create a directory called “builds”Our Gimfile is configurated to create all *.ipa files in this directory.8. Run and testRun a specific target:Run all targets:Finally, an exciting part!After Fastlane finished to run in my real environment, which includes four targets, I got this message in console:To be honest it’s saved me even more because I can’t perform action after action immediately.In conclusion, there’s no doubt that Fastlane is 100% worth to use.Download all files from GitHubThank you for reading this.Good Luck!P.S. This tutorial was partly base on Integrating Fastlane with multiple target for a single iOS project by SANDEEP MUKHERJEE and on fastlane Tutorial: Getting Started By Lyndsey Scott from raywenderlich.com there you can read more about Gymfile and do_everything loop written in Fastfile.",ios,https://blog.devgenius.io/how-to-use-fastlane-to-deploy-multiple-targets-to-testflight-xcode-ios-41e1b932ef3?source=tag_archive---------2-----------------------
How to Implement Sign In With SwiftUI and AWS Amplify (Part 4),"Before jumping to the code, if you didn’t set up your project with AWS Amplify, please check Part 1. In that part, I go through setting up the project and also adding a sign in with Google. For signing in with Facebook, check Part 2, and for Sign in with Apple check Part 3.Luckily, when signing in with email, we don’t need to configure anything since AWS Amplify takes care of everything for us, so if you’re only using email as a provider on your app, you just need to configure your Amplify project and ignore the configurations for Google, Facebook, or Apple.The steps we need to follow for sign up and sign in are:First, let’s implement the sign-up-with-email method on our SignInViewController.The AWSMobileclient sign-up method is very simple and only requires a username and a password. If you configured the Amplify project to require an email, a name, and a picture, you need to pass those in an array. Here I’m sending an empty string for the picture, but if you have an URL for a picture, feel free to add it for your user:Since we might want to check the tokens when signing in with email (as well as with social providers), notice the refactor of the getToken methods above.We also need to update our app settings to store the email we’ll send a confirmation code. This is done so even if the users close the app, we’ll keep track of the email they entered before:For signing up, we’ll create a button like the other ones we have, but once we press it, we want to show a text field into which the user can enter the required information.We need to add several States that we want to track. We also need a Bool signUpWithEmail to show the TextField once we press the button. We also need to add Bindings for signUpName, signUpEmail, and signUpPassword. See the updated ContentView below:Notice we used a SecureField so the entered password isn’t displayed.Once you press the Button, you should see the TextField and be able to call our sign-up method. AWS will send a verification code to the entered email, so we need to add another method to check that and also a TextField to enter the code.First, we need to update our SignInViewController and add a method for email confirmation:And also, do the relevant changes in your ContentView. Here we add another condition on our if/else. If we stored an email for confirmation in our app settings, we’ll display a confirmation View for the users to enter the code they received in their emails and call the method from SignInViewController:If we run this code, we should see the email was confirmed successfully. This means the email is confirmed but not signed in, so now we need to let the user sign in.Now let’s add a final method to our SignInViewController in order to be able to sign in:We also need a Bool signInWithEmail to show the sign-in text fields once we press a button and the bindings for signInEmail and signInPassword. We also need to add another condition to the if/else so it can display the text fields:Now you should be able to go through the whole process: signing up with a new email, confirming that email, and also signing in with an existing email.If you want to take a look at the whole project, see the GitHub repo.",ios,https://betterprogramming.pub/how-to-implement-sign-in-with-swiftui-and-aws-amplify-part-4-a59144107147?source=tag_archive---------15-----------------------
Extending iOS Apps With Plug-Ins,"App extensions or plug-ins are very common for desktop apps. Web browsers offer a big selection of plug-ins, while many development environments like Visual Studio Code or Android Studio offer far more features in their plug-in marketplaces than in their original apps. Graphic design software also has plug-ins allowing you to extend the functionality of your apps.iOS apps, on the other hand, don’t have many extensions. Usually, you get a product with fixed functionality. Generally, Apple doesn’t seem to like plug-ins. Xcode is one of the few development environments that can’t be extended with plug-ins. You get what you get, and if something new appears, you need to update the whole app.I don’t know why Apple doesn’t like plug-ins, but for iOS apps, there’s an explanation. Before publishing an app in the App Store, which is the main distribution method for iOS apps, Apple reviews them. They have a long list of rules, and if they see that your app doesn’t fulfill at least one of their requirements, they reject it.“2.5.2 Apps should be self-contained in their bundles, and may not read or write data outside the designated container area, nor may they download, install, or execute code which introduces or changes features or functionality of the app, including other apps. Educational apps designed to teach, develop, or allow students to test executable code may, in limited circumstances, download code provided that such code is not used for other purposes. Such apps must make the source code provided by the Application completely viewable and editable by the user.” — Apple DeveloperYou may ask, “What about web browsers?” Web browsers on iOS devices are also very limited in functionality. They can show web content, but they can’t go beyond that. Even more, showing web content is limited by the same guidelines:“2.5.6 Apps that browse the web must use the appropriate WebKit framework and WebKit Javascript.” — Apple DeveloperOK, but what if we want to add plug-ins that don’t change the app’s functionality? Or what if we don’t want an app for enterprise distribution? Is it possible to extend an app’s functionality dynamically? The short answer is “Yes.” And there are several different ways of doing it.For demonstration purposes, let’s make an app with an empty screen, one container, and one button. The button will load our plug-in, run it, and show the result in the container.All plug-ins will do the same thing: Draw a traffic light and switch between red, green, and blue in a loop.We need a protocol of communication with our plug-in. This protocol should include the following features:So, we need a two-way interaction. Let’s see how it can be done.Before we start adding plug-ins, let’s create a base app. Create a new project with only one view controller. This view controller should have only two views:In more complex versions, you can load plug-ins from the web, scan from QR code(s), receive from Bluetooth, or even decode from sound waves. It won’t change anything, which is why I’ll use the easiest option — adding a script to the app bundle.For each type of plug-in, the Swift code will be different. I’ll attach it to the end of each section.The easiest way to extend any app is by using JavaScript. First of all, JavaScript is a popular language. Second, there are other languages (e.g. TypeScript) that compile their code into JavaScript. And third, iOS has an integrated JavaScript interpreter.To run JavaScript code, use the JavaScriptCore framework:JavaScript requires a context:This gives an optional context. It’s a good practice to check if the context was created or not, and if not, to show an alert. But to shorten the future code, I’ll assume that the context will be always created:Let’s include the JavaScript file to our project. We pass a text string to JSContext. The text can come from any source. It can be hardcoded, a file from an app bundle, or the web.Add jsPluginCode.js to your project. Here’s the JavaScript code I wrote for our plug-in:To avoid conflicts with UIKit functions, I called views “widgets.”There are two functions that will be called from Swift:And two functions that our script can call:For this demo, there’s only one widget (Circle) and only one changeable property (backgroundColor). Of course, you can add as many widgets and properties as you like.Our Circle widget has the following properties:The provided JavaScript code creates three circles: red, yellow, and green. One of them has full opacity, whereas the other two have more or less 1/3 of the opacity.The tick function counts in milliseconds. When it reaches one second, it runs loopStep, which changes the state of our traffic light, giving full opacity to another color (another light).If you know JavaScript, this code won’t raise many questions. If not, just read it carefully. You’ll see that it’s not so different from Swift.The iOS code is more complicated. We need to add four items:Separately, we need to add a JavaScript exception handler to log our errors to the console. It will make the debug process much easier.Here’s the full source code of the view controller using JavaScript:Another way to extend an app is with Lua. Lua is a simple programming language that was originally created to extend apps.“Lua is a powerful, efficient, lightweight, embeddable scripting language.” — LuaLua is very popular in game development. Some game engines use Lua as their primary language (e.g. LÖVE). Other frameworks allow you to write part of a game, like the game logic or the game extensions, in Lua (e.g. Polycode).Lua is truly a cross-platform language. Its official framework is written in plain C and can be integrated basically anywhere. Lua is free and distributed under the MIT license.“Lua may be used for any purpose, including commercial purposes, at absolutely no cost.” — LuaThe original Lua framework can be easily integrated in Objective-C, but it’s more complicated in Swift. Fortunately, there are libraries to make it easier. For example, I found one that I’m going to use for our examples in this section.Note: If you prefer to use the original Lua framework, you can download it and connect it to your project using a bridging header.I won’t explain Lua’s syntax. There are many tutorials, manuals, and video courses for that. The logic of the script is 100% identical to the JavaScript code from the previous section:A couple of details:When Lua script is written and included into an app bundle (with the name luaPluginCode.lua), we integrate a library to parse and execute it. It can be done with Cocoapods:Don’t forget to install pods. If you don’t know how pods work, you need to install cocoapods:Then, add the file Podfile to a project folder and run:Great! Now, let’s switch to the Swift code.Import the Lua library:Create a virtual machine:For simplicity’s sake, we’ll add all the functions to a global namespace. There are two functions, like in our previous example:This function has access to the view controller, so all logic can be done inside. We get all the arguments in one args variable, which we decompose to six variables with understandable names.The rest is pretty straightforward. We use this argument to set up a view and add it to our container.As Lua can get the return value, we need to specify it directly. In this case, we return .nothing, but if you need to return a value, you can use .value.A second function to update the widget property:Now, when the functions are done, let’s read our file and save the start and tick functions in view controller properties startFunc and tickFunc. Both have the type Function — or Lua.Function to be precise:All errors will be printed in the console. This includes the errors inside the Lua script. As the lua4swift library outputs all the errors to the console automatically, we can ignore the call value when we don’t need it.Our function calls will look like this:And:The first function is called only once, and the second every 0.01 seconds. Visually, the app looks exactly like the one written in JavaScript.Here’s the full source code of the view controller using Lua:Finally, we reached the fun part.What I’ve shown before is actively used in many apps. Some apps in the App Store are actually written in JavaScript. I’m talking about frameworks like React Native. Other apps actively use Lua. But what if we want to add a binary library to the app?Let’s assume that we download a “fat” library built for armv7 and arm64. We know that this library has two functions: start and tick. The first one accepts two Double arguments, while the other accepts one Double argument. Can we call it? Is it even possible?I want to point out that I’m not talking about Apple requirements. They’ll most likely reject such an app. I’m talking about the technical possibility to “inject” binary code into an existing iOS app.Let’s split it into two steps:Since the old days, there have been two types of libraries: static and dynamic. Static libraries are linked with the main binary during the compilation. As a result, they become a part of the output binary. Obviously, this one is not our choice.Dynamic libraries are located in separate files. They’re loaded after the app starts (usually in the beginning), but they’re not included in the executable binary. If you delete a dynamic library from the app folder or the app bundle, the app usually crashes or shows an error message.In Apple iOS platforms, dynamic libraries are presented as frameworks.I will use this existing project below and add a third view controller. All UI elements will be inside the main part (the same as in the previous examples):I called my framework “DynamicLights.” The code for both parts is very short. It doesn’t require any additional knowledge. It’s a simple iOS app:As you can see, in the dynamic module, I used only the Foundation framework. This is to avoid any references to the UI. Actually, it’s an exact copy of the JavaScript and Lua code from previous sections.FrameworkViewController.swift is also very simple:First of all, let’s remove DymanicLights from the project dependencies:Clean the “build” folder and build the project again. You’ll get an error message:That makes sense. Our main app doesn’t know anything about our dynamic framework anymore.Choose DynamicLights as a build target and build it. In the Project navigator, you’ll see DynamicLights.framework in the Products folder:Open this framework in Finder. In the DynamicLights.framework folder, you’ll find the DynamicLights file (the file name is different if you chose another name for your framework). This file is our dynamic library.Xcode is smart, so if you copy it to an app bundle, it will link it automatically — even if you don’t choose this option explicitly. DymanicLights.framework and any parts of it shouldn’t be in the project bundle.I tested it on the iOS Simulator, which has access to the file system of your Mac, so I just read the library from the file. If you want to test it on a real device, you’ll have to download the file, save it to the App Sandbox at runtime, and load it from there.As I mentioned before, Apple has something against plug-ins and you’ll hardly find extendable iOS apps in the App Store, so Apple doesn’t have an official, documented way to interact with frameworks dynamically. It gets even more complicated if you think about different architectures, Swift versions, signing, and other environmental conditions. That’s why I kind of agree with Apple. This way is really not good.Fortunately for us, Swift is compatible with C. You can’t mix them in the same file, like Objective-C with C, but you can use C functions in Swift code. Now we’re talking about functions such as dlopen, dlsym, and dlclose. To export functions from a framework, you need to add the keyword @_cdecl.In order not to deal with changing global variables (not even sure it’s possible), I added functions to set callbacks. Here’s the full code:Rebuild your framework and copy the DynamicLights file to some known location. Theoretically, you can use the existing path from Xcode’s Derived Data folder, but I wouldn’t recommend it in order to avoid confusion. My path is /Volumes/Extra/Work/DynamicLights.Going back to our view controller, replace import DynamicLights with import Darwin. Darwin is a core element of Apple’s operating systems. As a framework, it gives access to core C functions, including dlopen, dlsym, and dlclose.We have four functions. Two of them set callbacks, while the other two give commands:And two class members:handle has a pointer to a dynamically loaded library. tickFunc is a function that we call 100 times per second (that’s why it’s worth keeping the pointer instead of getting it every time).This is how we load the dynamic library:Considering that the updateProperty and addWidget constants contain the code of our callbacks, this is our initialization code:And this is the timer callback:As you can see, we use some unsafe casts here. If the function prototype doesn’t match in the main and library code, the app will crash. This makes this method much less safe than using our JavaScript or Lua code.Here’s the final version:JavaScript, Lua, and native libraries are not the only ways to extend iOS apps. There are many other scripting languages.For example, there’s the fast and simple Gravity language that looks like Swift. It offers a C library that can be integrated into Objective-C and Swift apps.More complicated languages like Python, Perl, or Ruby can also be used to extend existing apps, but it’s usually not worth it.If you need dynamic layouts, consider using XML or another markup language. You can write a simple engine yourself or use existing solutions.Even though Apple doesn’t encourage the use of app extensions, it’s technically possible to use them.The safest way to extend an app is with JavaScript. Apple has a native way to run JavaScript code, you can use different JavaScript flavors, and you can easily transfer the code to another platform.If you’re not comfortable with JavaScript for some reason, you can use dozens of other scripting languages like Lua or the more exotic Gravity.Layouts can also be read from external files. Using an external layout engine, you can import the layout from an XML or other markup file. HTML can also be parsed and rendered in iOS apps.See you next time. Happy coding!",ios,https://betterprogramming.pub/extending-ios-apps-with-plug-ins-e4119d064f2d?source=tag_archive---------4-----------------------
React Native Android Receive Sharing Intent and IOS Share Extension,"Ajith A BJun 1, 2020·10 min readHello Guys,This is my first story in the medium. I writing this package is my challenge once upon a time. I have joined a company after b-tech. The company should give a second task for me to Implement Pdf Receiving App to build with React Native. On the Company side, I was getting more pressure to complete this task. So I was confused there is no package is available to file receive in react native. there is a package available for text and image receive to React Native Apps. First of all, I was learned java to Receive Sharing Intent then learn Objective-c IOS Share Extension. In Android and IOS, the entire Implementation is Different.Then I have completed this task within 2 weeks. I was the Written Native module in Java and Objective-c and communicate our React Native Bridge.That time I have the aim to build a package for React Native and do not suffer anyone for this feature. So I can build a package from ScratchToday I’ll be explaining how to implement sharing images, files, website links, and text from 3rd party app’s to your react native application.YarnAutomatic installation only for (React Native 0.60.xx less than versions)Note: React Native 0.60.xx or greater version not required to linkNote: Ios and Android on Debbuging time not working at sometimes while App is ClosedAndroid-Installation<Project_folder>/android/app/src/main/manifest.xmlIOS-Installation<project_folder>/ios/<project_name>/info.plist<project_folder>/ios/<project_name>/AppDelegate.m<project_folder>/ios/<your project name>/<your project name>.entitlementsIn Ios is Mandatory to Recieve file from other apps.<project_folder>/ios/<Your Share Extension Name>/info.plist<project_folder>/ios/<Your Share Extension Name>/ShareViewController.swiftNote: Important change the hostAppBundleIdentifier value to your main host app bundle identifier (example in my case: com.ajith.example ) in this ShareViewController.swift2.Create an app group for Share Extensiongithub.comThank YouAjith A B",ios,https://medium.com/@ajith4ab/id-react-native-android-receive-sharing-intent-and-ios-share-extension-daddfc9fb5d6?source=tag_archive---------10-----------------------
Complete Guide to Codable — Decodable,"A quick and easy way to convert your JSON object to Data modelsCodable was introduced in Swift 4 which helps you convert your JSON to your model and model back to a JSON object. Codable is a typalias of Decodable and Encodable protocols. Codable is similar to Serialization in JavaIn this tutorial, we will only be covering Decodable in-depthDecodable is used for converting your JSON object to your data model with the help of JsonDecoderLet’s take a simple example of how to decode a JSON object to your data-modelThe type passed to decode method should conform to Deocodable.Hurray! And just by writing a few lines of code, you have your data-model ready that is mapped from the JSON object. HereJsonDecoder does the heavy lifting 🏋️‍♂️for you by mapping each key from the JSON and setting the values of the variables. The manual chores of getting the key from the JSON dictionary and then setting values in model days are far gone. All thanks to CodableYou can mark the variables as optional for which the JSON object does not contain the value. In below example, the JSON response does not send the postKey. JsonDecoder ensures that when the variable is marked as optional, it checks the value for the key in a JSON object and then set its value if present.CodingKey tells the JSONDecoder to map the keys present in the container with the assigned variables. The above API response has id and photoTnURL, which does not tell what exactly the variable represents. This can be fixed with CodingKey. So we rename the keys id →feedId and photoTnURL → thumbnailUrlStringLet’s say the API response starts sending you the location along with the feed. You can add the location in your data-model which will conform to Decodable. You can add the new Location struct in your PhotoFeed or as a new struct outside the PhotoFeed.JSONDecoderwill help you iterate through all the container values. Use decoder.container(keyedBy:) on the decoder to get all the values present in the CodingKey. Similarly nestedContainer(keyedBy: forKey:) will allow you to iterate through the nested container. Using these methods you can iterate over all the keys in the decoder. Let's see this through an example to flatten out the structureSometimes the JSON response not always follows the camelCase naming convention. There is a good chance that you may have a response that follows snake_case.We can set the decoding strategy that determines how coding keys are decoded from JSON. Let’s take an example to solve this:1.useDefaultKeys: It’s the default strategy. JsonDecoder will use the same variable names for decoding2.convertFromSnake: It will convert the JSON keys from snake-case🐍 to camel-case🐫 keys present in your data-model3.custom: You can have your custom implementation which returns the correspondingCodingKey present in your data-model. The below data-model has keys which have youtube prefixed before every key present in JSON. Using custom decoding strategy, we can remove the prefixed stringyoutube from every JSON object key so it maps to the variable in PhotoFeed.JsonDecoder supports decoding dates from the JSON response. Here is a list of supported date decoding strategies.1.deferredToDate: Number of seconds elapsed since 1st Jan 2001 represented in double. A very rare chance you would be using this.2. iso8601: This is the most widely used date-format. Use this link to understand more about date-format used under iso86013. formatted(DateFormatter): You can add a custom DateFormatter with required dateFormat4.custom((Decoder) -> Date): You can write custom code for parsing your date. Let’s say you have a bad JSON containing two date formats, here is a quick way to resolve it using .custom decoding strategy5.millisecondsSince1970: It decodes date in milliseconds since midnight UTC on January 1st, 19706.secondsSince1970: Decodes date in seconds since midnight UTC on January 1st, 1970JsonDecoder supports strategies for decoding your raw data. Here is a list of supported data decoding strategies1.base64 : decodes data using Base 64 decoding2.custom((Decoder) -> Data) : decodes data using a custom function implemented by you.By now you already got a gist of how the default custom decoding works and how would you use them. Still find it difficult to understand, comment below and I will add gist for youThis strategy is used by a JsonDecoder when it encounters exceptional floating-point values. In your lifetime you must have encountered when your server returns an invalid “NaN”. Similarly how to handle +ve infinity and -ve infinity values (which you might have never encountered), if not handled this would just CRASH 💥 your app.Here is how you handle nan, +veInfinity and -veInfinityThat’s pretty much you what you get in Decodable 🙌Did I miss a use-case? Let me know in the comments below!",ios,https://medium.com/flawless-app-stories/complete-guide-to-codable-decodable-b1ff696da24f?source=tag_archive---------1-----------------------
May 2020,"Another busy month in isolation, with lots of minor updates to the extensions. Hope you are all keeping well in these unprecedented times.If you have been having issues releasing new applications due to the UIWebView deprecation on iOS, make sure you have updated all your extensions. We have had a few of our extensions brought to our attention that we didn’t realise contained UIWebView usages. So over the last month we have been through them all and migrated any using UIWebView to WKWebView.Please let us know if you are still encountering this issue. Note that you should be using the latest release of AIR as well, as the AIR SDK itself contained UIWebView usages.We have updated our extensions utilising older iOS Google libraries. This includes Firebase, Adverts (AdMob), Google Identity, Google Analytics and Push Notifications. Additionally we have released some point updates to Google Play Services on Android.We have been holding these back due to conflicts with AIR (see below), however we made the decision to push forward due to UIWebView usages in the underlying SDKs. So you may find some issues with the latest releases until the issues below in AIR are fixed.We are asking all our customers to put your voice behind these couple of issues in AIR. They are currently either causing issues or blocking further releases to our ANEs. In particular the iOS version of Firebase and related Google services are being affected.Firstly there is a conflict between the version of sqlite used in AIR and the version in iOS since iOS 12. This has become an issue due to the functionality in the newer version of sqlite being used in core Google libraries. When an application is packaged including a version of sqlite this version takes priority over the system version, which means newer api calls are failing.github.comThe second is an issue with the crashing at launch when we use ANEs containing certain types of iOS libs.github.comAll you need to do is to add a reaction to the initial issue report. Hopefully this will push it up the list of goals for Harman.Stay safe out there!",ios,https://medium.com/airnativeextensions/may-2020-f21a646d894c?source=tag_archive---------23-----------------------
🚘 The Magic Behind Anghami’s Car Mode On iOS 🚙,"Amer EidJun 2, 2020·5 min readNothing is more magical than having your favorite music streaming app switch to car mode automatically as soon as you start playing music via your car’s bluetooth stereo. By the end of this article you will discover how we detect when an app is connected to the car stereo via bluetooth or not.Car mode detection allows us to implement our Car Mode feature seamlessly by replacing the conventional music player with a minimalistic and very functional one suitable for usage in cars.Before we dive into the details, a little about me (because I’m sure ur wondering if you can truly trust what I’m saying or not hehehe). My name is Amer and I’m the iOS lead at Anghami.Want to read this story later? Save it in Journal.Anghami is a music streaming platform based out of Beirut, Lebanon - and we are just really really really really really (almost enough reallys) really passionate about music.The purpose of this article is to go over (technically) how we detect when our users are listening to Anghami via their car’s bluetooth and the steps we took that led us to our preferred solution.Core Motion is an Apple framework that allows the developer to detect whether a person is in a car/running/walking/cycling using the phone’s accelerometers, gyroscopes, pedometer, and others sensors.We had several problems with this solution:External Accessory is an Apple framework that connects with accessories connected wirelessly through Bluetooth.This solution doesn’t ask the user for permission but was useless due to car bluetooth stereos not being detected as External Accessories. So on to the next attempt.Core Bluetooth is another Apple framework that “communicates with Bluetooth low energy and BR/EDR (“Classic”) Devices”.This solution does ask for the user’s permission but does not detect bluetooth stereos as anything other than regular bluetooth devices.Since we are a music app, we were able to use AVAudioSession.sharedInstance().currentRoute to get info on the port. This info told us whether the current route used A2DP, HFP, carAudio, etc (Full list of ports https://developer.apple.com/documentation/avfoundation/avaudiosession/port).This seemed like a wonderful solution because it doesn’t ask for user’s permission and can tell us if bluetooth device’s port type is car audio or hands free or anything that is car related.Problem with this attempt is that all car stereos we tested on always returned port type as A2DP, therefore this solution was unsuitable.We noticed most of the car manufacturers had default bluetooth device names for their stereos. We utilized this and compared the bluetooth name to a list of common car manufacturer’s stereo default names.This solution did not ask the user for bluetooth permission and this code could be triggered whenever AVAudioSession.sharedInstance().currentRoute changes (e.g when user disconnects from a bluetooth device, etc).The most apparent problem with this solution is that if you change your device’s name to a car related name, it'll think it’s a car stereo. An example of that is if you change your Bose speaker’s device name to “Kia”, Anghami will think it’s a car.Bluetooth Device Address is a unique 48-bit identifier assigned to each Bluetooth device by the manufacturer. The first 6 letters of the address represents the device manufacturer’s. This helped us pinpoint car bluetooth devices more accurately without relying on the device name.Mining all the bluetooth device names and their MAC addresses allows for a more accurate crosscheck on what bluetooth devices are car stereos and which aren’t.Example: Some BMWs use Harman Kardon stereos in their cars. Their manufacturer’s mac address is 9C:DF:03.This helps us pinpoint that even if Harman Kardon’s device name changes to anything, we can still detect that it’s a car just by relying on the MAC address.There is one last problem which we have yet to solve. What if the bluetooth manufacturer makes household speakers as well as car speakers.An example of that is Alfa Romeos. Some Alfa Romeos have Bose stereos in their cars. The problem here is that we cannot rely on the MAC address of Bose immediately or else it will assume every Bose device (Headphones, home speakers, etc) connected to their phones are cars.In this case, the only solution is to fall back on the device name. Unfortunately this can fail if bluetooth name is deceiving (e.g . Bose speaker’s device name is “BMW” — This causes the app to think it’s in the car).We were able to finally find the best solution possible to automatically switch to car mode. Car mode has already been added to Anghami (turned on by entering audio settings in the music player) but we are currently in the process of implementing this following solution to make car mode, just a tad bit better than before. Coming to an App store near you 😅.I hope this medium article gives future developers some insight into how we detect playing audio via car bluetooth stereos, and hopefully it would save them the trouble of googling all this (and going crazy at the same time).This couldn’t have been possible without the entire Anghami team.Follow Anghami on Twitter or check us out at Anghami.com📝 Save this story in Journal.👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.",ios,https://medium.com/@amereid/car-mode-detection-in-ios-music-apps-875b6fd719e7?source=tag_archive---------13-----------------------
Camera preview and a QR-code Scanner in SwiftUI,"In this post, I will guide you through the creation of a QR-code scanner for iOS, using SwiftUI.We are also going to see how to create a preview of the camera feed and use it as a SwiftUI View. The end result will be compatible with the simulator, Xcode’s Live Preview as well as UI tests!QR-code stands for Quick Response Code. They contain small amounts of information presented in such a way that computers and mobile devices can interpret it. QR-codes are a kind of 2D barcodes called Matrix codes. Some variations exist, like Aztec Codes or Data Matrix codes. QR-codes usually look like this:Let’s begin by creating a new iOS app project in Xcode. You will want to choose the Single View App template and make sure the User Interface technology is set to SwiftUI.The layout of the scanner screen probably depends on your specific app use case, so we are going to create a very simple view which allows us to see the camera preview, the value of any detected QR-codes as well as a button to toggle the torch light on and off.Create the ScannerViewModel, which will hold the state the ViewThe scan interval is a variable which allows us to control how often we are going to retry scanning for a QR-code. This is useful if we want to provide a continuous scan function.The torchIsOn field holds the current state of the device torch light. This tutorial assumes that a torch is always available.The lastQrCode field will hold the string value of the last detected QR-code value. When the user is scanning, this field will update to the last QR-code scanned.Finally a function onFoundQrCode which our QrCodeScanner is going to call whenever a new QR-code has been detected.Now let’s create the ScannerView layoutWe are going to use a ZStack to create a layout where the Scanner showing the camera view will be seen at the back, taking the entire screen and on top of it, we are going to layout some interface elements like the balloon with the recently scanned QR-code or the torch button.We are going to create a View which shows the feed from the device camera. Ideally, we also want to be able to position the Camera View using SwiftUI.Most of the functionality we are going to use comes from the AVFoundation framework. For presently it doesn’t offer any ready made SwiftUI views, we are going to create a traditional UIView and embed it as a UIViewRepresentable in our SwiftUI app.We are going to setup a AVCaptureSession which will use the device back camera as an input and a AVCaptureMetadataOutput as an output, allowing us to receive a stream of metadata objects recognised from the camera input. Those metadata objects are the actual QR-code values we are aiming to gather.Since we are going to use the camera view in SwiftUI layouts, it will be ideal if the implementation plays nice with the simulator (so the SwiftUI preview function will work). Since the simulator lacks a physical camera device, we are going to embed a fallback in the camera view.The delegate will be responsible for handling the metadata output, checking if a QR-code was found and informing the parent View of that QR-code value.Most notably, our delegate object will adopt the AVCaptureMetadataOutputObjectsDelegate protocol in order to listen for new metadata objects which were detected in the camera feed.The delegate will also take advantage of the scanning interval we prepared for in our View Model. As long as the capture session is active and a QR-code image is in view of the camera feed, new metadata objects will be constantly detected. For this, we are going to skip frames and only notify the parent view once per scanning interval.The camera preview is the UIView we are going to use to show a live feed from the camera (or a simulated value for the simulator). This is a UIKit view which will eventually be presented using a UIViewRepresentable in our SwiftUI layout.The view is created for a given AVCaptureSession and includes a AVCaptureVideoPreviewLayer which is based on CoreAnimation and enables us to preview the visual output from the camera session. This is what the user will eventually see as a “camera preview”. When the camera is not available, we are going to substitute the vide preview with a simple UILabel showing the mock data content we’ve provided.This means a QR-code scan can be “simulated” during preview or live view in Xcode, the simulator or a UI test.The last piece we need is a way to show the CameraPreview view in SwiftUI. This can be achieved by implementing the framework-provided UIViewRepresentable protocol.The purpose of this protocol is for us to sort of translate the SwiftUI state and lifecycle events to UIKit. For example, in SwiftUI views are often destroyed and recreated when their state changes while in UIKit a UIView is usually kept around and updated.The bare minimum we are going to need is:💡 Remember to add the NSCameraUsageDescription key to the Info.plist file otherwise using the camera will result in a crash.Let’s tweak the QrCodeScannerView with the following additions:Here is the complete implementation of the scanner view:Now we can use the QrCodeScannerView in the SwiftUI layout. Replace the Text view with the following:Tadaa 🎉",ios,https://blog.devgenius.io/camera-preview-and-a-qr-code-scanner-in-swiftui-48b111155c66?source=tag_archive---------0-----------------------
MySQL database in phpMyAdmin,"Whenever we think of backend, the ‘database’ is the first word that strikes our mind. It acts as a key component in almost all domains. The main aspects of having a database are storing and retrieving data. So here we are going to create a MySQL database and then retrieve the necessary data using SQL queries in phpMyAdmin.So to get started with creating a database turn on the Apache and MySQL modules from the XAMPP control panel. if you are unfamiliar with the XAMPP server then don’t worry, go through “Temporary Web-Hosting using XAMPP and ngrok”.Now, as our XAMPP server has been started successfully, follow the steps given below to proceed further.Step-1: Open ‘localhost’ on your browser.Step-2: Open ‘phpMyAdmin’ from the navbar of localhost dashboard.phpMyAdmin is a free and open-source MySQL administration tool. It provides both the features of performing the operations with a user interface as well as by executing the SQL queries.Step-3: Click on the ‘New’ button on the left side of the page.Here enter the name of the database and then click on ‘Create’.Once the database is created successfully, the name of the database will be shown at the left pane and you will see that the database is empty. A warning will be shown “No tables found in database”.In the next section, we are going to create a new table in our database using the SQL statements.Step-1: Click on the ‘SQL’ button in the navbar.On this page, we can run all the SQL queries.Step-2: Creating a table with whatever name you want, like I have created a table named “Details” that contains three columns: Id, Name and Email_id.Now proceed by clicking on ‘GO’.Step-3: Once the table is created successfully, we will now enter the data in our table.Step-4: Click on ‘GO’. When the data will be entered successfully, you will get the message “5 Rows inserted.”. The number of rows will vary based on the number of rows you enter like ‘5’ in the above example.As the table is created and the data is entered successfully, in the next part we will be running the SQL queries for retrieving the desired data from the table.1. Displaying the whole table using the ‘SELECT’ statement.2. Selecting the column ‘Name’ to display only names in the table.Similarly, you can perform all the desired operations for your MySQL database in phpMyAdmin using the XAMPP server.#Learntocode Happy Learning!😊",mysql,https://medium.com/nerd-for-tech/mysql-database-in-phpmyadmin-282ce4b60587?source=tag_archive---------3-----------------------
Test automation for Laravel 7 and MySQL with GitHub Actions,"GitHub Actions ( https://github.com/features/actions) is a powerful service provided by GitHub for continuous integration and continuous delivery.GitHub Actions allows you to execute some commands when a GitHub event is triggered. For example you can automate the execution of unit tests on your code base when you push your code in the repository.I created an Open Source tool for creating GitHub Actions workflow for Laravel application. The source code is: https://github.com/Hi-Folks/gh-actions-yaml-generator . You can use a demo here: https://ghygen.hi-folks.dev/With GitHub Actions you can:The scenario that I would like to cover is:I have a Web App based on Laravel 7, every time I push some new code on develop branch on the GitHub repository, I would like to execute automatically Unit tests and Feature tests that uses also a MySQL services.Let’s start to build your workflow file from scratch.In your project directory, you need to create a new file in .github/workflows directory:The yaml file will contain 3 main sections: name, on and jobs.You can define the name of your workflow.This name, is used in Github Actions user interface, for grouping reports and for managing workflows.You can define when to launch the workflow.For example you can define , when you push your code on master and develop branch:You could define branches master, develop and all feature branches:Or you could define also the pull request on master branch:With a workflow you can define one or more jobs. A job is a specific task that needs to be executed in the workflow. You can configure to run multiple jobs in parallel or with some dependencies (for example: run the job “Test” only when the job “build assets” is completed)A job can be executed on a specific container that runs a operating system. For Laravel usually I use the latest version of Ubuntu.In the Job ( jobs) section you can add also some service containers. For example, if you are creating a job that needs MySql service you could add service sub section. This service could be used by your scripts and apps that runs in the current job. For example in my case, I’m creating a job for running tests on my Laravel application. To do that, I need also a database. Sometimes I could add sqlite database, that it is easier to configure, but probably if you want to run test with the same database that you have on production, probably you could prefer to have a MySql instance.The most important things to highlight are:Each jobs has multiple steps. In each step you can “run” your commands or you can “use” some standard actions.Following the yaml syntax, each step starts with “minus sign”. Step that uses standard action is identified with “ uses” directive, steps that use custom commands are identified by “ name” and “ run” directive. “ name” is used in the execution log as a label in the GitHub Actions UI, “ run” is used for launch the command. With “ run” directive you could define command with arguments and parameters. With “ run” you can also list a set of commands.Let me walk-through all steps in the steps section.The first step is retrieve the sources. To do that, GitHub Actions has a standard action identified with “actions/checkout@v2”. To perform the action you need to set a step:You can use also actions/checkout@master if you like to live on the edge.In Github workflow you need to think about a new fresh install of your web application every time you execute the workflow. It means that you need to execute all things needed by a fresh installation like:In Laravel you can use .env to store your keys and parameters for the environment configuration. For example: database connection parameters, cache connection parameters, SMTP configuration etc. You can prepare your.env.ci with specific configuration for executing workflows and commit it on the repository. Anyway we will override later some parameter like the connection with the database (some secret paramters like password or access tokens that you don’t want to store in.env.ci).For installing package I suggest you to use these options:Now you have your Laravel application installed in the runner and ready to use the MySQL service. What we are going to do is:Both commands, probably, they will need to access to database. To do that we need to be sure that all parameters are correctly configured. In the last step you can add env sub section where you can list all your parameters. These parameters will override the parameters that you defined in your .env_file. And probably you want to avoid to hardcode some parameters in your workflow _yaml file. For that you could use the Settings -> Secrets functionality in your GitHub repository.In Settings/ Secrets section you can define and list all your “secrets” and you can use those secrets you your yaml file with secrets.DB_PASSWORD. Please note that you need to use the secrets. prefix to access to secrets variables. DB_PASSWORD is the name of your secret.In this case:With this configuration you can execute the migration and the phpunit. As you can see I’m using phpunit in the vendor directory.Please write me your feedback or suggestion in the comment in order to improve this article.Let’s automate everything!I created an Open Source tool for creating GitHub Actions workflow for Laravel application. The source code is: https://github.com/Hi-Folks/gh-actions-yaml-generator . You can use a demo here: https://ghygen.hi-folks.dev/Originally published at https://dev.to on June 4, 2020.",mysql,https://medium.com/swlh/test-automation-for-laravel-7-and-mysql-with-github-actions-467493c2f7ed?source=tag_archive---------0-----------------------
Turning SQL Queries into Automated Regression and Debug Scripts,"harshit jhureleyJun 5, 2020·5 min readData is everywhere. It has always been about data, especially if you work in the Finance and Payment Domain. And If you are a QA engineer, it is important to confirm successful processing and verification.Undoubtedly, Database is a critical element in the highly complex application, and Dedicated database testing is needed to be in place to ensure the system can works as expected and integrate with the services without any issues. It is difficult to form a testing strategy based on manual testing only, As it involves analyzing many characteristics like accuracy, duplication, replication, consistency, validity, constraints, performance, data completeness, and conversion of raw data into meaningful information. When test coverage is of this kind and release testing requires production-like data then the Automation test strategy will sound good to overcome the impediments of traditional database testing.We will use DBFit as a scripting tool and MySQL DB to demonstrate some handful of automation and debugging scenarios.Setting Up DBFit ToolOtherwise, follow the link https://hub.docker.com/r/thinkwhere/dbfit to start the container with the DBFit image.!|dbfit.MySqlTest|!|Connect|localhost|root|root|portfolios|One of the most unhelpful scenarios you can run into is the circumstance over and over where you have to repeat the set of queries to narrow down the conditional flows and catch what went wrong. However, We can avoid this during release by grabbing needful queries and converting them into debug script.Let’s see the first look at data inspection scriptIf you are wondering about how it works then check the below pointsNow click on the test button to generate an inspection report.This time we take a different route and perform validations, data readiness, and apply invariant tests on the table.Table Design Test: Are the data type definitions are as per the data model design specifications? or Is A column that cannot be null has the ‘NOT NULL’ constraint?The above test used to validate table design. Let’s click on the Test button and generate the test result.A test case passed(1) with the correct assertions(26).Query Test: Is the functionality or the business logic works as expected for the application? Is the record check compared to the exact value?The above test used to validate simple queries. Let’s click on the Test button and generate the test result.A test case failed(1) when value mismatched.Replication or Data Diff Test: Data Replication is the process of storing data in more than one node. Replication or data-diff test allows you to perform a comparison between the two tables either on the same server or a different. Is the sync service working? Is data replicated between two DB? The below test will help you in such a case.Let’s click on the Test button and generate the test result.Let’s increase the comparison data between the two tables and see the verification accuracy.Comparison accuracy works well also with the large data set. We can compare thousands of records in one go.It is quite common to have Functions and Stored Procedures employed business logic in the application. Run the database function and procedure with the appropriate input parameters. Compare data in tables as expected results.Let’s click on the Test button and obtain the test result.Timing Test: The timing test is always helpful to analyze queries and procedures performance benchmarking. Is the store procedure operating very slow? Is the query execution take more time than usual? The below test will help you in such a case.Let’s click on the Test button and obtain the test result.This test measures the time it takes 2 seconds to run the procedure. Such tests are useful to measure performance and raise the flag against potential performance problems.Thanks for reading!",mysql,https://medium.com/@harshitj.qa/turning-sql-queries-into-automated-regression-and-debug-scripts-3cf38a615a8f?source=tag_archive---------0-----------------------
Connect to MySQL running in Docker container from a local machine,"Md KamaruzzamanJun 8, 2020·5 min readIf you are working in Software Engineering industry or if you are planning to work in Software Engineering field, probably you have heard about Docker.In 2013, Docker introduced the concept of Container and changed the Software Engineering landscape forever.Containers are a standardized unit of software that allows developers to isolate their app from its environment, solving the “it works on my machine” headache. Docker is still the “de facto” standard for containerization.Another vital use of Docker is that a developer can download and run any containerized application without directly installing it in their local machine. As there is a Dockerized version of almost all necessary applications, Docker helps to try and run applications while keeping your OS lean and clean.MySQL is one of the most popular open-source Databases and one of the “Big Four” relational Databases. It is widely used by industry, academia, and the community alike. In a Blog post, I have made a detailed analysis and ranking of the top ten databases in the industry and MySQL got the top spot. You can read my article if you want to know more about why MySQL is the number one database including its key features, use cases, managed MySQL services, and alternatives:towardsdatascience.comHere I will show how to run a Dockerized MySQL Database and then connect with it from your local machine.You can install Docker in almost all primary OS, be it Linux, Windows, or macOS. Please follow the instruction given in the official docker site to install Docker in your local machine: https://docs.docker.com/engine/install/Docker containers are stateless. So, if you use a Containerized MySQL, then you will lose all your saved Data once you restart the container. One way to avoid the issue is to create a docker volume and attach it to your MySQL container. Here are the commands to create a MySQL container including attached volume in your local machine:The following command will create the volume in your local machine which you can connect with MySQL container later:The following command will pull the MySQL server version 8.0.20 from the Docker registry and then instantiate a Docker container with the name “mk-mysql.” It will also attach the previously created volume “mysql-volume” with the Database and will expose the port 3306 so that you can reach the MySQL database outside the container:You can check whether the container is running by listing the running containers:You can also check the log file of the running MySQL container with the following command:Now, you can connect to the container’s interactive bash shell with the following command:Once you are inside your container, you can connect to your MySQL server and create a new Database as follows:Please note that you have to give the same password we have defined to run the container (my-secret-pw).By default, MySQL restricts connection other than the local machine (here Docker container) for security reasons. So, to connect from the local machine, you have to change the connection restriction:Although for security reasons, it would be better to create a new non-admin user and grant access to that user only.You can use any MySQL Client program to connect with MySQL Server. My personal favorite is phpMyAdmin, which is a simple yet powerful Web MySQL client. Also, instead of installing phpMyAdmin in my machine, I prefer to use the Dockerized phpMyAdmin.You can pull the phpMyAdmin image from docker registry and run the container with the following command:You can check whether phpMyAdmin is running by either listing all running containers or by checking the log files:Based on your OS, your DOCKER_HOST is different. On Linux, it will be your localhost. For Mac/Windows, you can obtain DOCKER_HOST with the following command:For Windows/Mac, you can either connect DOCKER_HOST IP address. The other option is Port forwarding. In Windows, a Docker Machine is a virtual machine running under VirtualBox in your host machine.To enable Port forwarding for MySQL and phpMyAdmin, perform the following steps:Open your browser and visit http://localhost:82 to access phpMyAdmin UI:Once you log-in with the previously configured password (my-secret-pw), you should be able to view the phpMyAdmin Admin view as follows:In the left panel, you can see the previously create Database (MYSQLTEST). Now, you should be able to administrate your Database (create/drop table, run SQL queries, etc.).If you have configured the Docker volume as mentioned, your Database changes will be persisted even if you restart your MySQL container. Otherwise, all the changes you made in your Database will be lost.towardsdatascience.comtowardsdatascience.comtowardsdatascience.com",mysql,https://towardsdatascience.com/connect-to-mysql-running-in-docker-container-from-a-local-machine-6d996c574e55?source=tag_archive---------0-----------------------
How to Build an LMS in 30 days — Part 6,"Aric MazickJun 1, 2020·3 min readThe final core functionality to be developed was event registration and automate the completion of the associated course. This was a big one for the organization since we hold hundreds of live classes each year across the country, sometimes with thousands of participants over the course of a single summer. Event Espresso was the plugin that caught my eyes, as they boasted of integration with LearnDash. Perfect! This would be easy.Except it wasn’t. As an event registration tool, Event Espresso is great and robust. But the LearnDash integration was pretty weak. It allowed LearnDash courses to be “associated” with an event, but that association had no functionality at all. So I had to build it myself.For this, we’d need to dive into the different plugins and learn how they work underneath the hood. Specifically, I was looking for a way to run a function that would check which participants have been checked into events, and then automatically trigger completion of the associated courses in LearnDash with a Cron job.After melting my brain staring at dozens of php files and navigating the WordPress mysql databases, I was ready to take a nap and spend some time on the patio. A couple hours later I came back and worked on this code in the theme’s function.php file:Let’s look at it one step at a time. First, I added a Cron plugin to the WordPress site and added a hook so this function is called every time the hourly cron job is run, and I created a new table in the MYSQL database to contain the event attendance/completion data.Next, we set up a connection to the database and log any errors. If we’re connected, it runs a SQL query:The table structure in event espresso distributes the data I need across a number of tables. I need to correlate CHK_IDs with REG_IDs, REG_IDs with ATT_IDs in another table, then correlate TKT_IDs with ticket_ids, and finally ATT_emails with user_emails to gather all the data we need.Basically, this query aligns the checked in learner’s user profile with the LearnDash Course the event was associated with, then makes sure that the row of data is not already present in the table I just created.The data now needs to be manipulated in php now in order to be processed and trigger completion of the LearnDash Course. Through a lot of trial and error, I came onto this method clean up the course id data.Then I used 2 functions I found in the back end of LearnDash to trigger the completion of a course. This was necessary because you can’t trigger completion of a course in LearnDash. You can only trigger completion of lessons that are contained within courses. So I had to search for the single lesson associated with the course, then mark it as complete. By doing this, the course would be marked as complete as well. (Note: The $lessons variable that is returned is an array and can return multiple lessons if you’ve built the course that way. The way I developed this will only work in single lesson courses. That’s all I need at this time.)Now that we’ve marked those courses as complete for the learners that attended our event, we need to add that data to the new table. That way, next time this function runs, it won’t try to complete the courses on all the same attendees again. We’ll prevent duplicates this way.Note: The line with REPLACE(REPLACE… is there to clean up some text artifacts that come through before adding it into the to the new tableSo now, when an instructor completes a live program, they can log in to the Event Espresso plugin and check in the students that will receive credit. The Cron job will run shortly after this happens, and all attendees will be marked as complete for the course in LearnDash seamlessly.Whew. Time for another nap, take your weekend off, and come back next week for some pilot testing!Click Here to read Part 7.",mysql,https://medium.com/@aric_43174/how-to-build-an-lms-in-30-days-part-6-2b3de7899234?source=tag_archive---------6-----------------------
Execute SQL Query on a MySQL RDS Instance from a Web App with AWS Lambda as Backend,"Let us start by creating a dataset for this demo, I am using Zomato Restaurants Hyderabad dataset. We’ll start by uploading the CSV to our RDS instance using MySQL workbench. This will quickly create a dataset for us to experiment with. Expand on schema_name → Tables → Right-click → Table data import wizard. Follow the self-explanatory steps. Run this select query and you should see your table in the DB.We are going to query something like this from the web app →SELECT cuisine FROM webdata.Restaurant where Name = ‘xxx’;Let us create the backend service which will query the DB taking a parameter from the web app. Here is the lambda-codeNote: you need to install pymysql to connect to MYSQL instance. Simply install on your project folder —pip install pymysql -t .The project structure should look like this after installing pymysql —Python — lambda_function.pyThe event input is to test —Here is the test output —Package this lambda_function.py and the pymysql installation files into a zip and upload it to lambda in aws.Now we simply need to hit this microservice, fetch the response, and display in the web app. Proceed by attaching an API gateway to AWS lambda.Html —Javascript —Demo —Wow! I know its tasty. Go give it a try and drop me a 👏 if you liked it. Until next time save time — keep automating.Add on —More content at plainenglish.io",mysql,https://aws.plainenglish.io/mysql-aws-lambda-webapp-521b16458b93?source=tag_archive---------1-----------------------
How to set up secure file permissions and database permission in ubuntu,"Manish PrajapatiJun 5, 2020·3 min readMany junior developers are ready to set the permission to 777 for their project directory e.g. Laravel. So if you are setting your folder permissions to 777 you have opened your server to anyone that can find that directory. We can fix that easily. follow the next few steps.There are basically two ways to setup your ownership and permissions for your directory on your project. Either you give yourself ownership or you make the web server the owner of all files like apache.Web server as owner (the way most people do it for Laravel directory way) So we are assuming www-data for apache (it could be something else for different web servers) is your web server user.or if you are in particular directory then follow the above command.if you do that, the web server owns all the files for you, and is also the group, and you will have some problems uploading files or working with files via FTP, because your FTP client will be logged in as you, not your web server (www-data), so add your user to the web server user group: assuming that ubuntu is your current user for ftp.Then you set all your directories to 755 or 775 and your files to 644 or 664. In next step we will SET file permissions.for directoryfor filesNow, you’re secure and your website works, AND you can work with the files fairly easily.Stop using /phpmyadmin publicly. we have to stop using it, only localhost can access it where you are with your PC.You can restrict who can access the given location (URL path) using your web-server configuration. For example, if you use Apache on Ubuntu, then edit /etc/phpmyadmin/apache.conf to include Order, Deny and Allow directives (only the relevant part included):Now we have situation where cann’t accessing phpmyadmin from remote URL (http://YOURIPADDRESS/phpmyadmin) but still we want to access phpmyadmin for some reason like database update and view only.Below is a gist for how to enable phpmyadmin only from localhostAllow phpmyadmin only from localhostThis is can be accomplish by creating a tunnel between server using our ssh keys (.pem or .ppk).Go to Connection->SSH->Tunnels in putty (make sure you have a connection string for your server configured with Auth using ppk file)Go to Connection->Dataadd your username for server, in my case my ssh username for my ubuntu server is ubuntu.Now go back to session and save the settings.Now open http://127.0.0.1:8888/phpmyadmin/ You can access the remote db from your local server using tunnel.suggested by @thedijjeNavigate to your pem file and type following commands.exampleNow open http://127.0.0.1:8888/phpmyadmin/ You can access the remote db from your local server using tunnel.🙂Originally published at https://www.koffeewithkode.com on June 5, 2020.",mysql,https://medium.com/@manprajapat/how-to-set-up-secure-file-permissions-and-database-permission-in-ubuntu-745f2e1e0666?source=tag_archive---------12-----------------------
How to Build an LMS in 30 Days — Part 5,"Aric MazickJun 1, 2020·3 min readIt’s time to work on the Moodle challenges. We need to be able to gather completion data, and control the appearance of the User Interface.When it comes to gathering completion data, we could use Moodle’s reporting tools, but then we’d have completion data spread across both LearnDash and Moodle depending on the type of course. It would be much better to have all the completion data in one central location.In order to resolve this, I found an area of the Site Administration in Moodle that allows additional HTML to be added to Moodle pages.If you go to Site Administration, Appearance, Additional HTML, you’ll find what I’m talking about.There are multiple options here on where to add code. I selected the “Before BODY is closed” section as I want it to run after all the HTML elements have been loaded.Here’s the code I put together. Let’s go through it one step at a time:First, the script searches for the appropriate HTML element to identify whether the user is a Site Administrator. We’re only using two different roles in Moodle, so if the user is not an admin, they’re a student. And on all student pages, we want to call the RemoveUIElements() function.Then, if the page detects that the student has a check in the checkbox that displays progress, it calls the TriggerComplete() function that marks the lesson as complete in LearnDash. (NOTE: The way this has been scripted only works with single activity courses in Moodle. If there’s more than one activity, the code would need to be modified to determine if all check boxes are marked before triggering the function.)In the RemoveUIElements() function, you’ll find some of the same tricks that I used before when it comes to modifying UI. We simply search for the element either by id, or by class, and modify the element CSS as necessary to get the UI we’re after.In the TriggerComplete() function, we use the same technique to find the Mark Complete button in LearnDash that we previously hid and we simulate a click on it. Then we search for a link to the next lesson and click it if there is one.In one fell swoop, we’ve been able to clean up the UI, mark a lesson as complete in LearnDash, and automatically redirect the learner to either the next lesson or back to the main course page if all lessons have been completed. Let’s take a look at the new UI:Looking better. It’s cleaner and easier to use for sure. And even better, the completion function works as well. As soon as the SCORM module is completed, the “Your progress box” gets ticked. This automatically progresses the course forward to the next module.The issue with javascript taking just a fraction of a second to execute is still present. But otherwise, the solution works and the UI is not nearly as bad as it was.Looking back at our initial list of requirements for the core functionalities, we’ve solved for a lot. We’re now able to upload all our courses and not worry about which platforms or formats are supported and which are not. We’re using the TinCanny plugin for our internal courses and Moodle for all vendor or legacy courses and all completion data is now contained in LearnDash.As far as assigning courses to different groups within the organization, this was solved easily using the Uncanny Toolkit Plugin. There were a lot of functionalities that allowed us to set up groups and control membership, visibility of courses, and other aspects.Next I’m going to work on Event Registration and automating completion for the associated courses. I’ve been putting this one off, but it’s time to do it.Click here to read Part 6.",mysql,https://medium.com/@aric_43174/how-to-build-an-lms-in-30-days-part-5-e1984154f259?source=tag_archive---------7-----------------------
A Generic Connection Pool,"Hasitha Hiranya AbeykoonJun 3, 2020·4 min readImplementation (open source!!): https://github.com/abeykoon/Generic-Connection-PoolExample application using the pool: https://github.com/abeykoon/Connection-Pool-ExampleLike to read on and get to know what that is better?Obviously, this is not a new subject. When a connection is created using an application level language like Java, it will call underlying OS, open a socket and start negotiating a connection. Thus, establishing a new connection with a third party application or a system is not lightweight. It is time consuming. Moreover, it is resource consuming.If you allow your application to a new create connection every time it needs to communicate with the third party system without any control, one of following will happen. It will lead your application to crash.The reason to this problem is that your application is creating new connections every time it needs to communicate. App does the communication and then just discard it. When the number of requests to communicate becomes increases, you need more connections. Naturally, this leads to slowness and resource hungriness.Why not re-using the connections?When a connection is created we can keep it to serve multiple requests. A set of connections can be established with the remote system and those connection objects can be pooled in a connection pool.Enjoy the implementations, report issues, clone and expand… :)",mysql,https://medium.com/@hasithah/a-generic-connection-pool-21e3b34c1510?source=tag_archive---------2-----------------------
How to Build an LMS in 30 Days — Part 7,"Aric MazickJun 5, 2020·2 min readWe tested twice over two weeks.First, we invited about 40 users from a small area of the business and gathered feedback. It was generally positive, but there were some issues with the tracking of OLT. It seemed to be lagging.When I went into the backend to try to diagnose the problem, I noticed that the completion tracking had totally failed too! After a brief panic, and hours of testing, I noticed that the tracking issues only occurred on Articulate Rise content that was exported in the TinCan format, and the lagging issues only occur when Articulate Rise entrance animations are turned on. Easy fix — we’ll publish to SCORM with Rise, and we’ll turn off the entrance animations.Round 2 of testing was with about 75 team members from a broader area of the business including some senior leaders, IT developers, and a majority of the Human Resources department. Again, feedback was pretty positive and we had no issues with completion tracking or event registration!But, we did run into some errors with the way users were being created in the system with the organization’s Single Sign On platform, which we were using to import users. This required an upgraded SSO plugin for WordPress, and a bit of collaboration with a few Okta developers to resolve.Click here to read Part 8.",mysql,https://medium.com/@aric_43174/how-to-build-an-lms-in-30-days-part-7-89a7737287ed?source=tag_archive---------11-----------------------
How to Build an LMS in 30 Days — Part 8,"Aric MazickJun 5, 2020·2 min readWhen I wrote all this together, it felt a bit too easy — almost as if I didn’t bang my head against the wall constantly throughout the development. Which I definitely did.I spent hours learning and verifying syntax in w3schools.com and poring over Stack Overflow Q&As on topics from JavaScript to HTML, to CSS, PHP, MySQL, and AJAX. I’ve learned a huge amount during the past 2 months and I’ve definitely grown a lot as a developer and learning designer.But it was stressful. The whole “32 days of work” actually turned into me working weekends and nights most of the time to make up for the tight deadline. I thrive on intensity, but now at the tail end of this first iteration, I’m tired and ready to stop constantly dreaming about my code breaking.Suffice it to say, with an LMS, there are a lot of moving parts. And while it is certainly possible to put one together for a medium sized organization on a tight budget (We probably spent about $1500 altogether) and in a short timeframe, I couldn’t honestly recommend it. Complex systems like this need proper time frames and budgets in order to be developed properly.I’d rather have explored using Scorm Cloud to host our eLearning content. It’s the industry standard, but far more expensive than the TinCanny uploader or Moodle. With a bigger budget, we could have considered this option and possibly found a more robust hosting solution.And I’d rather we had several more months to develop the system. Though my JavaScript workarounds certainly do what they’re meant to, they’re not the ideal solutions to the problems. It would be better to directly manipulate the plugins and php files that make up the site, but that would have taken significantly more time for me to learn and develop.All in all though, I am satisfied with what we’ve accomplished and pretty impressed at how productive we were able to be with such a short time frame. I have lots of plans on how to rework and add to this system in the future.Update: Sorry, that’s not actually the end. I added another article a few months later with some updates. Click here to read it.",mysql,https://medium.com/@aric_43174/how-to-build-an-lms-in-30-days-part-8-481aab6c9520?source=tag_archive---------10-----------------------
"Configurar Apache, PHP y MySQL en entorno de desarrollo local","Esta configuración está basada en Debian 9.8Una vez tengamos instalada nuestra máquina virtual con Debian 9.8 (Si no tienes conocimiento de cómo instalarla te dejo el siguiente video: ¿Como instalar debian (terminal) en VirtualBox?)# Instalamos los módulos de apacheapt-get updateapt-get upgradeapt-get -y install dirmngr apache2-utils# Habilitamos los siguientes módulos de Apachea2enmod rewritea2enmod headersa2enmod authz_hosta2enmod dira2enmod expiresa2enmod mimea2enmod alias#Reiniciamos apache/etc/init.d/apache2 restart# Instala PHPapt-get install php#Instalamos las librerías adicionales para PHPapt-get -y php7.0-mysql php7.0-bcmath php7.0-curl php7.0-gd php7.0-imap php7.0-json php7.0-mbstring php7.0-mcrypt php7.0-simplexml php7.0-zip# Actualizamos variable php.inised -i “s/;fastcgi.logging = .*/fastcgi.logging = 0/” /etc/php/7.0/apache2/php.inised -i “s/;date.timezone =.*/date.timezone = America\/Mexico_City/” /etc/php/7.0/apache2/php.inised -i “s/max_execution_time = 30.*/max_execution_time = 120/” /etc/php/7.0/apache2/php.inised -i “s/max_input_time = 60.*/max_input_time = 120/” /etc/php/7.0/apache2/php.inised -i “s/;mbstring.func_overload.*/mbstring.func_overload = 0/” /etc/php/7.0/apache2/php.inised -i “s/memory_limit.*/memory_limit = 512M/” /etc/php/7.0/apache2/php.inised -i “s/post_max_size =.*/post_max_size = 100M/” /etc/php/7.0/apache2/php.inised -i “s/session.cookie_httponly =.*/session.cookie_httponly = 1/” /etc/php/7.0/apache2/php.inised -i “s/upload_max_filesize =.*/upload_max_filesize = 100M/” /etc/php/7.0/apache2/php.ini# Actualizamos el DocumentRoot para habilitar el servidor apacheecho “<VirtualHost *:80>” > /etc/apache2/sites-available/000-default.confecho “ <Directory /var/www/html/>” >> /etc/apache2/sites-available/000-default.confecho “ Options Indexes FollowSymLinks” >> /etc/apache2/sites-available/000-default.confecho “ AllowOverride All” >> /etc/apache2/sites-available/000-default.confecho “ Require all granted” >> /etc/apache2/sites-available/000-default.confecho “ </Directory>” >> /etc/apache2/sites-available/000-default.confecho “ ServerAdmin webmaster@localhost” >> /etc/apache2/sites-available/000-default.confecho “ DocumentRoot /var/www/html” >> /etc/apache2/sites-available/000-default.confecho “ ErrorLog ${APACHE_LOG_DIR}/error.log” >> /etc/apache2/sites-available/000-default.confecho “ CustomLog ${APACHE_LOG_DIR}/access.log combined” >> /etc/apache2/sites-available/000-default.confecho “</VirtualHost>” >> /etc/apache2/sites-available/000-default.conf# Instalamos MySQLapt-key del A4A9406876FCBD3C456770C88C718D3B5072E1F5export GNUPGHOME=$(mktemp -d)gpg — keyserver ha.pool.sks-keyservers.net — recv-keys A4A9406876FCBD3C456770C88C718D3B5072E1F5gpg — export A4A9406876FCBD3C456770C88C718D3B5072E1F5 > /etc/apt/trusted.gpg.d/mysql.gpgapt-key listapt-get updatewget https://dev.mysql.com/get/mysql-apt-config_0.8.8-1_all.debdpkg -i mysql-apt-config_0.8.8–1_all.debapt-get updateapt -y install mysql-server mysql-clientsystemctl enable mysql && systemctl start mysqlecho ‘sql_mode = “STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION”’ >> /etc/mysql/mysql.conf.d/mysqld.cnfrm mysql-apt-config_0.8.8–1_all.debEste bloque contiene contenido inesperado o no válido.ResuelveConvertir a HTML/etc/init.d/mysql restart/etc/init.d/apache2 restartexit 0;Para conocer más acerca de un entorno de desarrollo local, te invito a ver mis vídeos en mi canal de Youtube: R4nbryQue sigas pasando un excelente día.¡Hasta la próxima!",mysql,https://medium.com/configurar-apache-php-y-mysql-en-entorno-de/esta-configuraci%C3%B3n-est%C3%A1-basada-en-debian-9-8-d617ab85fedb?source=tag_archive---------7-----------------------
MySQL External-based Replication in Alibaba Cloud,"Alibaba CloudJun 1, 2020·6 min readBy Ankit Kapoor, Database Senior Solution ArchitectIn this MySQL based article we are going to discuss about:In order for you to fully understand the concepts presented in this blog, you should already be well aware about:External based replication is one of the architecture of replication’s topology between an [ApsaraDB RDS] and on-premises Database server. In this architecture we can have RDS as master and on-prem DB as slave. We can also construct RDS as slave with on-prem DB as its master. In this article we will consider this architecture for MySQL and how can we deploy it in our production and staging environment. In this article, I am going to have Master on RDS MySQL 5.7 and slave on MySQL 8.0 version & reason behind this is to know what are the errors we can face while setting up this architecture and how can we resolve it. Having plain setup is simple and will not produce any errors.Please note that currently we don’t support external based replication of architecture in which RDS is slave with on-prem DB server as Master.Main purpose of having this architecture may vary for different business requirement. One of the business requirements I have seen is where we want one of our database servers to act as warehouse or OLAP and we want it to self-managed. In some cases COST can also be counted as a factor to adopt such architecture.Main purpose of having RDS as a slave is because we want to keep our data safe and thus we opt for managed services. In times of crash recovery or backup recovery or to generate BI -report we can use this RDS and doesn’t worry about the monitoring or managing it.I have replicated below scenario where I wanted to have below architecture:1. Login to your RDS console https://rdsnext.console.aliyun.com/2. Go to Products and locate Relational Database Services.3. Click on create instance.4. Make ensure that you setup below configuration:5. Click next.6. You will land to Instance configuration page.7. Make ensure that you have your VPC and vswitch must be setup already.8. Click on next and you will land to confirm order page.9. Tick Terms and Services and confirm your order.10. Wait for few minutes until your RDS instance gets ready.11. Once ready click on the instance and you will land to below page:12. Click on Configure Whitelist.13. Click on Create Whitelist and add the IP of your on-prem server.14. For testing purpose, I have made this RDS as public and put 0.0.0.0 in whitelisting.15. Make ensure that public endpoint of this RDS must exist so that you can connect from outside VPC. To apply for the public end point, you can follow steps mentioned in below link: https://www.alibabacloud.com/help/doc-detail/26128.htm16. Create Privileged account for RDS MySQL . For this please follow below link: https://www.alibabacloud.com/help/doc-detail/87038.htmWe will need this in creating replication user.17. Connect to the RDS via DMS or MySQL client on ECS or on your local machine. For DMS, you can login via console only.18. Once RDS is setup, please install MySQL on your local machine. You can follow Oracle guide on this. It is solely depend on you how to install MySQL . Either via RPM, binary files or DMZ package.19. Once Database has been started, please enable gtid mode and put below configuration in your cnf file. I am pasting a sample config file for this:20. Login to your RDS DB server.21. Take the backup of RDS database using mysqldump. This has22. been done to make ensure that all transactions are in GTID mode.23. Restore it on your on-prem DB server.24. Once done, please configure the master at on-prem DB server :25. This error has been introduced because of the different versions between master and slave. Let’s see how can we resolve itAs per error, table structure seems to be different. We can fix this error by having similar structure at both end and then run start slave . I have pasted my output . Make ensure that you should take data from source too.26. Hence this will start the external replication.We don’t support Classic file based replication currently. But if you have standalone RDS MySQL instance then you can set classic file based replication with RDS as master or slave.RDS as slave is not supported currently via GTID based but you can achieve this by using DTS tool.For more information you can visit below documentation:https://www.alibabacloud.com/help/product/26590.htm?spm=a3c0i.126076.1204981.3.6295155aFo7R9zAlibaba Cloud ApsaraDB RDS for MySQL is a stable, reliable, and scalable online database service. Based on Alibaba Cloud distributed file system and high-performance SSD storage, ApsaraDB RDS for MySQL features disaster tolerance, backup, recovery, monitoring, and migration capabilities to facilitate database operations and maintenance. To learn more, visit the official product pagewww.alibabacloud.com",mysql,https://medium.com/@alibaba-cloud/mysql-external-based-replication-in-alibaba-cloud-b48b5b0ac9b8?source=tag_archive---------4-----------------------
What exactly is MySql | How to create database and tables in MySql,"Anonymous CreatorJun 7, 2020·1 min readHere we will understand the basic meaning and understanding of mysql . When you have a huge amount of data around you then you need a database to manage it . Simply you need a particular system to manage such data in a proper way . That is why we need database management system .It is basically a container where comes DBMS to interact with user application & database itself to capture and analyze data . We have discussed about what is data , database , how to manage data . What is database management system . MySql is just used as an application to manage data used by the users . There are basically 4 types of databases .For further more , please visit:https://codingyouloud.blogspot.com/2020/06/what-is-mysql.html",mysql,https://medium.com/@faltukimail123/what-exactly-is-mysql-how-to-create-database-and-tables-in-mysql-69f8ebacf54a?source=tag_archive---------1-----------------------
Postgresql Simulating MySQL’s ORDER BY FIELD(),"Yogi SoftwareJun 4, 2020·1 min readWe have been using PostgresSQL in our new product Quiz PlatformWe want to draw to randomize questions from the question pool for each user. We maintain this order for in our cache/DB, we need this question to be displayed in the same sequence.In Mysql, it’s easier to achieve using field functionWell, we can obviously write a function which can help you do the same. Let’s the function name be sortbyid.So here how I get the result from Question table for 3 records with order or id as 2,1,3I’m not MySQL or PostgreSQL expert but hey that what I found, I know they're a ton of way to do it some maybe even better.",mysql,https://medium.com/@yogisoftware/postgresql-simulating-mysqls-order-by-field-c0bfadc9bba2?source=tag_archive---------4-----------------------
SQL for Beginners!,"Vududala Sai KiranJun 1, 2020·2 min readSQL is a language used for manipulating, storing and retrieving data in the database, people search through data to find insights to inform strategySome of the popular paid relational databases include Oracle, Microsoft SQL Server and Sybase. In this article, we will be using SQLite for executing Queries.To create the table, we need to run the following SQL command:To insert the data into the table:SELECT statement retrieves data from a databaseUPDATE is used to modify the existing data in the databaseDELETE is used to delete the existing data in the databaseDrop command is used to drop the existing table in the databaseThe TRUNCATE command is used to delete the data in the table but not the table itselfALTER command is used to add additional columns(data) to the tableI hope you found this article useful. It is supposed to help you get started in SQL!!Thanks for reading. I hope you found something interesting here :)",mysql,https://medium.com/@saikiran37544/sql-for-beginners-9d681d95cfa9?source=tag_archive---------5-----------------------
Azure MySQL with AAD and Managed Identity,"I do not like having to manually manage servers. Therefore, PaaS is what I am focusing on in the public cloud. When it comes to databases, Microsoft Azure has a wide variety of PaaS offerings, including MySQL. For running your web API or web app on PaaS, Azure gives you App Service. If you want to connect both services securely without having to manage passwords, Managed Identity is your friend. The Azure docs contain an article giving some guidance about using Managed Identity together with MySQL, but it is not very detailed and it does not cover App Service. So, I would like to demonstrate in this article how you can use all these PaaS offerings together.Readers who are not familiar with the basics of Azure Active Directory (AAD) authentication with MySQL should read this docs article to understand the fundamental concepts before diving into the code sample shown below.First, we need an ARM Template to deploy some Azure resources. In this example, I decided to setup the following demo environment:Here is the ARM Template that I created for this example. Nothing really special in it. The interesting part will come later.Note that the template above adds a regular connection string to our web app. This is for demo purposes only. The whole point of this example is to not have a connection string with username and password. So in your real-life code, remove the connection string and just go for Managed Identity!You can deploy the template shown above using the Azure CLI as follows:The ARM Template shown above create a user-assigned managed identity in AAD called mySqlIdentity. The web app needs the Client ID aka Application ID of the managed identity. Unfortunately, I have not found a way to get this ID directly inside the ARM template. If you know how this can be done, please contact me. However, it can easily be set after the ARM Template has been deployed in just two lines of Azure CLI code:Now we need to create a user in MySQL representing our managed identity. Unfortunately, this cannot be done with the regular MySQL administrator account. We need an AAD account with MySQL administrative privileges. This account can create the MySQL user for managed identity.If you have an existing AAD user (no Microsoft Account or something similar) that you want to use, you are fine. If not, you can easily create one with the Azure CLI:We need to get the AAD user’s object ID:Now we can assign our AAD user as the AAD administrator of our MySQL database:It would be so nice if all these steps could be done directly in the ARM Template. Unfortunately, I have not found a way to do that. If you know one, please let me know.With the AAD MySQL administrator, we can create a MySQL user for our web app’s managed identity. For that, I recommend opening a Cloud Shell. Note that you have to be signed in with the AAD MySQL administrator that we created before.Let’s start the MySQL CLI and authenticate with the AAD MySQL administrator:I recommend trying az account get-access-token... separately. Copy the token and examine it using e.g. https://jwt.io/. You might get additional insights into how AAD authentication with MySQL works.Once you have MySQL CLI running, create the MySQL user and give it the necessary permissions (note that $CLIENT_ID has to be replaced with the Client ID of the managed identity; see corresponding variable in the Azure CLI scripts shown above):That’s it for the Azure configuration part. We have:Now let us create a simple C# client that we can use to verify that everything works. Note that this client uses the MySqlConnector NuGet package. Note that not all drivers are compatible with MySQL AAD integration. MySqlConnector is.Note that this is by far not production-ready code. It is just a sample demonstrating how it is generally done. Here a few notes:Build it, deploy it to the App Service Web App that our ARM Template created, and try https://yoursite.azurewebsites.net/MySQL/withManagedIdentity. It should successfully return 1.Managed Identity enhances your system’s security because you do not have to set and regularly change DB passwords manually. Integration of Managed Identity with MySQL isn’t super simple, but once you got the things mentioned above figured out, it works pretty well. Hope this article will be useful for you.",mysql,https://blog.devgenius.io/azure-mysql-with-aad-and-managed-identity-698128dc53fa?source=tag_archive---------1-----------------------
The python of wall street Part 2,"reuglewicz jean-edouardJun 3, 2020·5 min readDisclaimer:This project is a eight parts project that I’ll leverage to have the opportunity to implement different technologies I want to explore.You are welcome to re-use any part of this script. But I would not advise using it on the stock market with your money. If you do, I am in no way responsible for whatever may result of it.part 1: extracting data and computing trend indicators part 2: creating an ETL pipeline for data quality and centralizationpart 3: creating a classification model part 4: automatic retraining of the modelpart 5: create apis to access the data part 6: data visualizationpart 7: create docker container for micro services architecturepart 8: process automationETL is a type of data integration that refers to the three steps :The objective is to create one golden record containing all the needed features for further reliable analysis or model creation. As well as an optimized pipeline to ingest new data inside the data lake.Feature generation is the process of creating new features from one or multiple existing features, potentially for using in statistical analysis. This process adds new information to be accessible during the model construction and therefore hopefully result in more accurate model.https://towardsdatascience.com/optimizing-feature-generation-dab98a049f2eTherefore, the transform part of the pipeline will consist in creating more features to better understand the underlying behavior of the studied phenomenom and creating a model that will be better at discriminating between sales and buy opportunities or hold opportunities.At some point, some external information, for instance tweet analysis or wether forecast could be intergrated in your analysis for stocks. And brining them together in one place for further analysis and patterns extraction can be a challenge. ETL process the heterogeneous data and make it homogeneous which in turn makes it seamless for data scientists and data analysts to analyze the data and derive business intelligence from it. It makes it easier to intergrate several sources (APIs, database, files…) in one consolidated repository. Then, it is easier to use the master record for further reliable analysis, as it is considered as the best source of truth and most reliable source of informations compared to putting together one way or an other the data.Moreover, it enables better operations by getting a more granular vision of the workflow for later developement and maintenance.Then, the transform part allows a systematic and coherent data manipulation and features augmentation over time, ensuring better continuity of servicesIn part 1 all the indicators were computed on the fly and the results were presented in a raw way to the consumer via graphs and points on those graphs when the indicators advised whether to buy or sell shares. The objective here is to formalize this pipeline, make it more rbust and resilient and enable training of machine learning models, taking the decision to buy or sell istead of a human being.In the considered case, it is as follow:Having an ETL makes the data wrangling process more atomic and manageable. It also enables the creatin of a relevant and coherent dataset for later analytics or model trainingThe script for this project can be found here: https://github.com/elBichon/midas_project.github.ioThe next step is to use those labelled data in the datalake to train a classification model to buy low, sell high or hold.https://towardsdatascience.com/optimizing-feature-generation-dab98a049f2ehttps://www.springpeople.com/blog/data-warehousing-essentials-what-is-etl-tool-what-are-its-benefits/https://www.investopedia.com/trading/candlestick-charting-what-is-it/https://www.investopedia.com/articles/active-trading/062315/using-bullish-candlestick-patterns-buy-stocks.asphttps://en.wikipedia.org/wiki/Candlestick_pattern",mysql,https://medium.com/@reuglewiczjeanedouard/the-python-of-wall-street-part-2-f5e8c41f7dff?source=tag_archive---------3-----------------------
What is Deno and Create Your First Deno Application,"G.D Udara Lahiru SampathJun 8, 2020·6 min readIn this article I’m going to explain what is Deno and it’s features. Also in this we’re going to develop simple application using Deno with MySql database. Using this application I’m going to demonstrate how we’re working with Deno.In 2018, the creator of Node.js, Ryan Dahl did a presentation for JSConf EU audience. In there he spoke about his journey and experience about Node. In that presentation he mainly focus on 10 main points about some of the decisions he regrets on taking with regard to the design of Node. End of his presentation he introduce a prototype to audience. That is the Deno, first impression to the world.Deno is a runtime for JavaScript and TypeScript that is based on the V8 JavaScript engine and the Rust programming language. It was created by non other than Ryan Dahl, and it is mainly focused on productivity and high security.No. It is not going to replace Node. Simply Node is very well established and gained so much popularity in such a small amount of time, there are so many packages and applications using Node.js everywhere in the world.One of the main feature of Deno is TypeScript support. We don’t need to manually configure TypeScript environment for Deno application. Deno has a compiler and everything built-in for TypeScript. Also as usual we can use Javascript to write code in Deno.Deno scripts cannot access the hard drive, open network connections, or make any other potentially malicious actions without permission.Ohhhhh wait. Is it not clear ? 😳 Will explain with an example in code sectionDeno also implements top-level await. It enables developers to use the await keyword outside of async functions. It acts like a big async function which causes any other modules who import them to wait before they start evaluating their functions.Deno is not depend on a centralised server. Also there is no more use of package.json or npm packagers. Packagers are simply import using URL. Also it’s cool feature is, application on load cached all dependancies to hard drive. Therefor without internet connectivity we can access the URL’s dependancies using cached.Deno provides a set of standard modules that are audited by the core team and are guaranteed to work with Deno. As an example below I mentioned how we import uuid module from Deno standard library.Browser compatibility means that where we overlap in functionality with the browser, As an example fetch() that we use the browser APIs. But it’s does not deviate from standardised browser JavaScript APIs.Deno is supported on Mac, Linux and Windows. The executable file is all that is required to run any Deno program. The ultimate goal of this single file execution is to keep it compact and independent of system libraries.Finally we are done with boring stuff .😔 Let’s do some cording stuff. 😍In this code section we are going to develop one rest API for get users from our MySQL database.Yehh… Les’s do this 😁First of all we need to install Deno to our local development environment. For that check this installation notes form Deno site. Also we need up and running MySQL instance. For this I’m using VS Code as my editor.After successful installation we need to check Deno is running without an error in our local development environment. For that I’m going to run some simple code. Please check below screenshot.Using Deno we can simply run a program by executing URL. In Deno official site they have some nice example’s for testing. Let run one of them and check what is the output.If your using VS Code as your editor. You can install Deno extension. That will be more helpful while you developing.We all set to start our application development. Before start I’m going to explain our final project structure and I’m going to divide this code section in to 6 sub sections. I think it will be more useful while you reading.Section 1In project root we have our server.js file. In there we configure our port to run Deno application. As I explain above Deno feature section we not going to install any npm packagers. We just import from those using URL. Check below code snippet.Section 2In my route file I’m going to declare my routes (Navigator with in application). For this example I declare my /api/v1/users route and bind that in to getUsers method which is in UserController Once I call this route it will return all the users in my database. Simply routing refers to how an application’s endpoints (URIs) respond to client requests.Section 3This section is fully related to controllers and its operations. In our example I created a UserController In this controller we use to process all user related business logics. Inside this UserController we need to call UserService to get user data. Check below code snippet.Section 4The service layer encapsulates and abstracts all of our business logic from the rest of the application. We also have service layer in our application. In this example we are using UserService Inside the UserService we are calling our UserRepository to get the users details. Check below code snippet for UserServiceSection 5Repository layer gives an additional level of abstraction over data access. In our application we are going to use UserRepository and inside of that we are doing all DB related operations. I’ve run simple select query to get all the users from database.I’m not using any ORM to querying in this example. But there is also an ORM library for Deno. You can use that one for your application 😊Section 6The last part I’m going to explain is our DB connection. As I mentioned in the top of this article we are using MySQL database. Deno already has third party module (deno_mysql) for MySQL. This example also I used that module. To get an idea about our database connection check below code snippet.Finally… We are done with all the development. Now we going to run our application.Now we going to see something special 😳. Let’s run the application using below code. That is normal way we execute Deno application.After running our application it’s execute with an error 😒Why we are getting error like this. As I mentioned in Deno Features section Deno is secure by default. Inside our application we need allow network access. If you want to get more details related to Deno permission. Check this official documentation.Now we going to run our application with permission using below code.If there is no errors. You can get output like below. If you get that we are done with Deno sample application.Now we going to test our endpoint using PostmanConclusionSuccessfully we developed simple application using Deno. In this article we discussed about what is Deno and its features. Also used MySQL Deno extension to working with MySQL database. As a last part learned about Deno permissions. Last but not least we test our endpoint using Postman.Hope you guys learned something from my article. If you interested about my article click bellow clap button 👏 to show your support!Thank You Very Much For Reading… 🙂🙂",mysql,https://medium.com/@udaragd/what-is-deno-and-create-your-first-deno-application-3658f626a2c2?source=tag_archive---------2-----------------------
Oracle Experience: Oracle_SID versus TWO_TASK,"AshJun 4, 2020·2 min readOnce common issue related to data processing, involving batch scripts relates to Oracle Path Environment variable. Most of the time we defined and export the Oracle Database Environment variables in our unix scripts for example ORACLE_HOME, ORACLE_BASE, ORACLE_SID. However, there is one scenario where the ORACLE_SID will not work.Let’s say that you have installed the Oracle Client on a server and then your database server is located on a remote server. The scenario is that in my batch processing i want to call SQLPlus with <username>/<password> only, i don’t want to specify the Oracle Identifier each time i want to perform a SQLPlus operation. Inorder to do that, i will add and export the ORACLE_SID and normally that should work.However, if your database server is remote, by specifying the ORACLE_SID, it might still not work. In that case, you will need to define and export the TWO_TASK variable just like the ORACLE_SID and specify your identifier. By doing this, it will resolve the SQLPlus connection problem.export ORACLE_SID=testdbexport TWO_TASK=testdbRunning the command: > sqlplus user/password , will result in the SQL prompt command.If you want to troubleshoot your connection issues, some interesting commands are:a) $TNS_ADMIN (to verify if you are using the proper tnsnames.ora)b) tnsping <oracle identifier>, example tnsping testdb (It does not validate your database connection but verify your tns alias definition)Reference Links:https://docs.oracle.com/cd/E11882_01/server.112/e10839/admin_ora.htm#UNXAR117",sql,https://medium.com/@cvashwin/oracle-experience-oracle-sid-versus-two-task-3cdb97dc73e6?source=tag_archive---------5-----------------------
"Query Smarter, Not Harder","Andrew ColeJun 1, 2020·4 min readRelational databases are one of the most common types of data storage regardless of industry. Using SQL (in any of its forms) to create queries is an extremely effective and accessible way to access data, but it can get a bit confusing sometimes. Queries can grow to become quite complex, requiring multiple methods to correctly obtain correct data. When queries begin to lengthen and grow in complexity, a subquery can be a very helpful method to segment the queries in a much more digestible manner.Our table schema above shows a somewhat complex, but definitely not uncommon, database.We will begin with a general query that will use to return all employees who work in the company’s USA office.Our DataFrame is returned to look like above. We used a straightforward many-to-many join using officeCode as our key. This was a pretty straightforward query, but sometimes utilizing joins and primary/foreign keys can get confusing on large table schemas. Let’s take a look at using a subquery now to write this query in a different and simpler way.A subquery is essentially just a query within a query. We will execute a query statement which has a condition dependent on a completely a separate query. Sounds a little confusing, so let’s take a look at what we’re talking about.Ta-da! This query will output the same DataFrame as in our previous code. Instead of joining to a new table, we are simply selecting features from an initial table created in the subquery. Let’s try a bit more complicated of a problem.Write a query to return all employees from offices with greater than 5 employees.This is a particularly difficult query because it requires us to return a query based off of an aggregate condition, but we aren’t actually returning the aggregate itself. We need to create a condition that only selects employees who match that condition. Normally, we use “HAVING” or “GROUP BY” statements to retrieve aggregate data but in this case we want to filter on the aggregate instead. This type of situation is where subqueries really shine.In this case, we first nested an initial query that contained our conditional statement: “HAVING COUNT(employeeNumber) > 5”. The cursor temporarily makes this newly executed internal query the new table, and we can then just select the necessary features for our answer. A little trick that helped me wrap my mind around this process when I was beginning with SQL is to start with the internal query first, containing our condition, and then go out from there.In the last query, we selected based on an aggregate condition, but what if we want to return an aggregate function result?Write a query to return the average payment of all the customers.In this case, we actually use our subquery to define the mathematical aggregate function. The aggregate function “AVG(amount) AS customerAvgPayment” is aliased and then executed initially in the nested statement. All that is left to do is to select that aggregate function as its own variable and voilà!As always with coding and SQL, there is never a singular way to arrive at an output. There are many methods for writing queries and subqueries were a great way for myself to get a handle on the flow of events in SQL.",sql,https://towardsdatascience.com/sql-subqueries-f2c490bf772c?source=tag_archive---------4-----------------------
Week notes 9 — Greek myths and little work 🔱 ☀️ (31st May),"Alex WaltersJun 2, 2020·3 min readThis week has been the slowest and hardest since this learning journey began 9 weeks ago and I don’t feel that I have very much to say about the progress that I have made in the last 7 days.The end of the week, and the weekend in particular, was dominated by hearing and watching what has been happening in the States after the murder of George Floyd and I am still at a bit of a loss about how to respond privately or publicly. I am not going to say much here are because I don’t think I have much to offer at the moment. I have done some initial research to understand what I could do and this post feels like a good start. I cannot attest to the quality of the contents but it’s somewhere to begin.Before I heard the news I watched a video from the Economist about the extent to which racial divisions have cut through and continue to cut through America. It’s a brutal reminder about the seismic role that racial inequality plays in the modern world and I was shocked about how clearly the past persists. The video contains disturbing images so watch with care.Data scienceMy month long free trial ends this Thursday and I won’t have time to finish the coursework that means I will receive a certificate for my efforts. If I needed the paperwork then I think I would happily pay the £200 a month it would cost to continue. But I realised that I really don’t. The course has been a helpful starting point for me when it comes to querying relational databases and telling stories about the data they contain — the real work comes next as I practice. And I think the same is true of my coding skills too. I am really confident that I have a good grounding in some core concepts so I am glad that I took the time to enjoy the sunshine rather than kill myself in front of a data filled computer screen.It looks like there are loads of ways that I can move my working knowledge of SQL forward and thanks to Alex for suggesting this brilliant tool: https://sqlpd.comMythosThe highlight of my week has been exploring some ancient Greek myths courtesy of Stephen Fry. Part storytelling, part religion and part meaning making, the book reminded me how much I enjoy great stories, however far fetched. One thing that struck me about reading these stories again is how much I wanted to identify with the individual gods as they appeared. Athena for her strategic mind and Hephastus for his craftwork really resonated with me. I saw for the first time how these stories might have helped people that heard them make sense of the world around them but also make sense of themselves too. I found a great sense of humanity in all the stories and as Stephen Fry says, their faults provide all the more reason to endorse them:What makes the Greeks so appealing to us is that they seemed to be so subtly, insightfully and animatedly aware of these different sides to their natures. ‘Know thyself’ was carved into the pronaos of the temple of Apollo at Delphi. As a people — if we read them through the myths as much as in their other writings — they did their best to attend to that ancient maxim.So while they may have been far from perfect, the ancient Greeks seem to have developed the art of seeing life, the world and themselves with greater candour and unclouded clarity than is managed in most civilisations, including perhaps our ownHere’s to that.Alex",sql,https://medium.com/@alexwalters75/week-notes-greek-myths-but-not-much-work-%EF%B8%8F-1b1bce9f86c8?source=tag_archive---------8-----------------------
How to Sustain a Growing Platform and Gain Online Users,"Alibaba CloudJun 1, 2020·10 min readBy Cheng Zhe, nicknamed Lanhao at Alibaba.In this article, we are going to be looking at experiments done at Alibaba, which were intended at helping increase online users on its second-hand buy-and-sell platform, Xianyu, which literally translates to “Idle Fish.” But, before we get ahead of ourselves, let’s discuss some of the dynamics of the buy-and-sell platform of Xianyu:In early 2019, the Xianyu team at Alibaba conducted multiple experiments on user growth, including the following two experiments, shown in the figures below:The team conducted the preceding two experiments with the aim of retaining users on Xianyu for a longer time. The longer time that users spend on browsing on Xianyu, the more likely they are to discover interesting content, including products and posts in our various curated item groupings, or what are referred to in Chinese as “fish ponds.” As such, users may be attracted to return to Xianyu at some later time, and Xianyu can achieve a greater level of user growth. Most of the experiments we conducted produced good business results. However, two problems were also found with these experiments:To solve these issues our team turned to an engineering solution, one that involved a rule-engine based on event streams. For this, we first implemented a layer of business abstraction. The operation personnel analyzed and classified various user behaviors to obtain a common and specific rule and then applied the rule to users in real time as an intervention.We engineered the business abstraction layer to have improved R&D efficiency and operation efficiency. To this end, we developed the first solution, which was a rule engine based on event streams. We took user behavior as being a series of sequential behavior event streams. We can define a complete rule by using a simple event description in Domain Specific Language (DSL) and then incorporate input and output definitions.Let’s look at the second experiment in user growth as an example. This example can be briefly expressed in DSL as shown in the following figure.The rule engine that we implemented could appropriately implement policies for user growth, so we quickly promoted it within Alibaba Group to other business modules and we had also planned to implement it as part of the security service integrated in the Xianyu platform. The following is a description of the consumer-to-consumer (C2C) security service. This security service aims to prevent and stop violations of Xianyu’s terms of use policy, including cutting down on inappropriate posts, as shown below.In the C2C security service, a rule abstraction operation, which is obtained from a series of behaviors.Despite our best efforts, these security rules could not be used in the rule engine. Consider this example rule, for instance. If a user is blacklisted twice within one minute, this user will be marked with a high-risk tag. When the first blacklisting event occurs, the rule engine matches the event. Then when the second blacklisting event occurs, the rule engine also matches this event. As such, the rule should be met from the perspective of the rule engine and subsequent operations can be performed. However, one important aspect is that the blacklistings should be performed by two different users to prevent one user from maliciously blacklisting another with multiple devices.However, this is difficult for the rule engine to discover, as the rule engine only knows that two blacklisting events are matched and the rule is met. This is because the rule engine can match only stateless events and cannot trace back the details of these events for further aggregate computing purposes.Based on the limitations of the rule engine, we re-analyzed and organized our business scenarios. Then we designed a new solution and defined a new DSL based on the well-known general solutions in the industry. As you see, our syntax is SQL-like and we mainly take the following into considerations:Compared with the previous rule engine, the new DSL solution has the following strengths:The example below shows how our new solution resolves the problem we discussed in the previous section.To facilitate engineering, we developed the following overall layered architecture in DSL, written based on the Event Programming Language (EPL): To rapidly achieve minimum closed-loop verification, we selected Blink as the cloud parsing and computing engine. Blink is an enhanced version of Apache Flink, optimized and upgraded by Alibaba.The layered architecture comprises the following layers from the top down:The event collection module intercepts all network requests and behavior-tracking data based on aspects and then records the data in a server log stream. In addition, the event collection module cleanses the event stream by using a fact task to obtain desired events based on the format defined earlier. After that, the event collection module outputs the cleansed logs to another log stream for reading by the EPL engine.Because we adopted an SQL-like syntax and because Apache Calcite is a common SQL parsing tool in the industry, we decided to use Calcite and customized a Calcite parser to perform parsing. For single-event DSL, Flink SQL is obtained. For multi-event DSL, the Blink API is directly called after parsing.After generating computing results, the EPL engine outputs the results to the user outreach module. The user outreach module first selects an action route to determine the action to respond to. Then, the user outreach module delivers the action to a client by using a persistent connection with the client. After receiving the action, the client identifies whether the current user behavior permits the display of the action. If yes, the client directly implements the action and exposes it to the user. The user may perform a behavior after receiving the response. The behavior may affect the action route and provides feedback about the route.Since the new solution was launched, we have implemented it in an increasing number of business scenarios. Here are two examples.From the above example of Fish Ponds, we can see that this solution is somewhat like algorithm recommendation. In the preceding rental example, the rule is too complex and it is difficult to express the rule in DSL. Therefore, the rule is configured to collect only four browses of different houses for rental. After the rule is triggered, the collected data is provided for the business team that developed the house rental application. This is also the boundary we found during the implementation.This complete solution can significantly improve R&D efficiency. Generally, the original R&D process can be completed in four business days by writing code case by case. In extreme situations, if the client version needs to be updated, the process may take two to three weeks. However, when SQL is used, the R&D process only takes 0.5 business days. In addition, this solution has the following advantages:By implementing this solution in multiple businesses, we found its appropriate boundaries. This solution is applicable to businesses that:The current solution has the following disadvantages:Therefore, in the future, we will focus on exploring real-time computing capabilities on the client and the integration of algorithm capabilities.www.alibabacloud.com",sql,https://medium.com/@alibaba-cloud/how-to-sustain-a-growing-platform-and-gain-online-users-7a68491607bf?source=tag_archive---------9-----------------------
Datetime2 — Why You Should (Not) Use It?,"Nikola IlicJun 2, 2020·5 min readA few weeks ago, I was working on a business request to prepare a data integration scenario for customer surveys. After the initial conceptual phase and logical data modelling, during the physical data modelling phase, I was thinking about certain data types that will satisfy both business needs and ensure the best usage of SQL Server resources.My suggestion was to go with Datetime2 datatype for all DateTime columns, while my boss insisted on sticking with good old Datetime. He is an old-school guy, I thought, accustomed to using traditional data types, so I will prove him wrong and convince to switch to newer (and Microsoft recommended, by the way) data type.Before proceeding further, I need to briefly emphasize on main characteristics and differences between Datetime and Datetime2 data types.The following table gives a quick overview of those two:Datetime is a “traditional” data type for storing data about date and time. It takes strictly 8 bytes of memory, 4 bytes for date part, and 4 bytes for the time part.So, let’s check what is going on behind the scenes here:We got following hexadecimal value: 0x0000ABC900C5C100So, if we take the first 8 characters (0000ABC9) and convert that to the decimal system, we are getting 43977 integer value. This value shows a number of days since 1900–01–01, which can be tested as follows:And we see that our previously “defined” date (2020–05–28) is there.Let us check the time part, by taking out the last 8 characters (00C5C100). Converting this to decimal, gave me 12960000. This number represents the number of clock ticks from midnight. And, since the accuracy is 1/300 seconds, we can do the following calculations in order to get more human-readable results:Once we “add” this time part to the previous date part, we get 2020–05–28 12:00:00.000, right as in the first declaration.Datetime2 was introduced with SQL Server 2008, so it is here long enough to draw some comparisons with its “older brother”. The main difference is the way of data storage: while in Datetime type, the date comes first and then time, in Datetime2, 3 bytes, in the end, represents date part!Let’s check with the same data as previously:Now, hexadecimal value is: 0x03002E930224410BSo, we are taking the last 6 characters for a date (3 bytes), but the rightmost byte is the most important within Datatime2, so it goes first: 0B4124. Converting this to decimal and we get 737572. When we add this to a ‘0001–01–01’ starting point of Datetime2 data type, we are getting our date (2020–05–28) back:For the time part, things become more complicated, because it depends on defined precision. In our case, it’s 3 digit precision, but it can go up to 7 digits (which is the default, in case we haven’t defined any specific value). Depending on precision, Datetime2 takes between 6 and 8 bytes of storage.Back to our example:Again, we got results as expected.Ok, this was a brief overview of both data types. When creating tables, in many cases one can’t foresee the amount of data that will be stored in a specific table. Sometimes, you create a table believing that it won’t go above a certain threshold, but after a while, you realize that your assumptions were not correct.However, we were definitely sure that this table will be BIG! When I say big, I’m talking about probably close to 100 million rows after year or two. And, back to our original request to create optimal tables, I was thinking that storing data as Datetime2 with 2 digits precision (which takes 6 bytes), multiplying with at least 5 DateTime columns, can save us a lot of space on the distant run.Less storage needed, bigger precision, larger date range, SQL standard compliant…But, I started to dig deeper. And tried to search the web to find experts’ opinions on this topic. I thought, there has to be something with Datetime2 (I mean, my boss usually knows what he talks about, hehe). So, I’ve found this discussion with quite well-documented shortcomings of Datetime2.The main issue is the lack of (simple) possibility to do basic math operations with dates, such as calculating the difference between two dates in days, hours, minutes, etc.Here is one basic example with Datetime type:If you try to do the same thing with Datetime2, you will get an error:Another really significant drawback is if you try to calculate the 1st day of the current month, which is a quite common scenario:The result is as expected:But, you wouldn’t expect that data type of this is: DATETIME, not DATETIME2!This means that every time you are doing comparisons using these calculations, you will finish with implicit data conversion!Another really important consideration comes from the fact that SQL Server can’t use statistics properly for Datetime2 columns, due to a way data is stored (remember the reverse sequence of bytes previously explained). More on those issues in this great article written by Aaron Bertrand.Wrong estimations lead to non-optimal query plans, which decrease the performance and this is one more significant drawback.So, wrong estimations and bad query plans as a consequence, implicit data conversion in common use scenarios, impossibility to use basic math operations for date calculations…",sql,https://towardsdatascience.com/datetime2-why-you-should-not-use-it-70e50ae2bab9?source=tag_archive---------0-----------------------
Ruby Episode II: The SQL,"John GuestJun 4, 2020·4 min readOriginally posted on October 21, 2019At first glance, SQL can appear to be a little unwieldy and unfriendly. That was my first reaction anyway. SQL (Structured Query Language) is a query language that is used to manage databases and perform various operations on the data in them. The language was developed at IBM by Donald D. Chamberlin and Raymond F. Boyce in the early 1970s. Edgar Codd invented the concept of the relational database and came up with the idea of storing data in tables, indexed by primary key and related by foreign keys to ‘normalize’ the data. The goal of data normalization is to reduce redundancy. SQL does this beautifully.I think it is the stripped nature of the language that causes it to appear archaic or oversimplified which may have been what led to my previously mentioned first reaction to it. It also felt like no matter how many readings, code-along’s or video tutorials I did; I was not really getting comfortable with or fully understanding SQL. It wasn’t until I began to write my own queries that I started to see that perhaps the reason it seemed unfriendly and archaic is that SQL is just so simplified and clean that it lacks some of the flashy and refined feeling that other languages have. If something is simple that doesn’t have to mean that it is lacking. The fact that SQL has been around since the seventies speaks to its value and efficacy. The most complicated part of SQL for me thus far in my experience with it, “JOIN”, is really not that complicated at all.JOIN is used to combine multiple tables of data in a database.There are several types of joins:Left, right, and full joins are known as outer joins. Here are some visuals to explain the differences between them:--A join statement might look something like this:SELECT column_name(s) FROM first_table JOIN second_table ON first_table.column_name = second_table.column_name;Queries like the above allow us to tie data together to be able to ask and answer more challenging questions. This join query demonstrates that the syntax is very simple yet the capabilities that arise from its use are profound. Being able to cross-reference and combine data tables was at the heart of the motivation behind the creation of SQL.Let’s look at some examples that demonstrate the differences between the types of joins:Imagine we have the following tables.An inner join returns only the rows of the joined tables that match the query. (the * indicates all columns in a given table)This will return the teacher that has an id that matches any student teacher_id:Now let’s take a look at an outer join. Left outer join is the most common one:This returns every row of the teacher's table even if the corresponding “student_id” column for that teacher is empty:Another feature that you may notice being used in this query is the “AS” statement. This works by essentially assigning the column to a variable to make the output cleaner and easier to read as well as allowing this new column name to be used in more complex functions.A right join is essentially the same as a left join except it returns all the rows from the right-most column:And as you might have guessed by now; the full join returns all rows from all tables:Understanding how these functions work and how to use them is a major portion of understanding the entire language itself. I have come to love SQL and I look forward to gaining more understanding as I come to understand why it is one of the oldest and most commonly used languages there are.",sql,https://medium.com/@jcguest90/ruby-episode-ii-the-sql-6f07fecb8c65?source=tag_archive---------6-----------------------
PostgreSQL: Rank function introduction.,"sobamchanJun 3, 2020·3 min readEach user can make multiple posts in the application, a super simplified version of the twitter database should look something like this, I guess.Here comes the data-analysis question.How can we find out when and what each user has posted for their first time? This kind of question is very common when you analyze application data, you want to know the very first activity of each user. By achieving this, we supposed to get a table like below.You can see that now we extracted the three posts which are the first posts users created first from the previous table. This extraction can be the base to obtain the service insights like belowEnhancing users to make the first move is the key to activate the service, so you should know how this can be done.We can summarize the required procedure as follows.To achieve this only with PostgreSQL, there are some possible ways but today we will use one of window functions implemented in it, RANK function. Using this, we can get this analysis done simple and easy. This function takes the column to be used to make groups and order method as arguments, and it gives rankings for each group. Its syntax looks like this.As you can see, we give “how to group them” to PARTITION BY and “how to order them” to ORDER BY. Seems very simple right? Let’s adapt this RANK function to our problem then.PARTITION BY takes user_id so that it groups data into each user and ORDER BY id sorts posts in each user group. By executing this SQL, we can obtain the following table.You can see the rank numbers for each user group, to extract only the first posts from this converted table, we can just utilize where to constrain rank = 1.This SQL gives finally what we wanted.I hope you could now see how easily it can be done to extract the first post of each user in the database using PostgreSQL’s RANK function. Based on this extracted information, there will be plenty of analysis you can run to understand your users better.",sql,https://towardsdatascience.com/postgresql-rank-function-introduction-c8ac008506dd?source=tag_archive---------2-----------------------
An Easy Way to Get Started with Databases on Your Own Computer,"Byron DolonJun 5, 2020·8 min readSQL is the third most popular language for developers, based on the StackOverflow 2020 Developer Survey. Everyone knows that you should learn to use SQL, but what about actually creating and managing a database?While you can follow courses that let you play with SQL on a browser with predefined tables (which is not a bad way to learn), it’s also valuable to experience creating, managing, and interacting with a database of your own.The DB Browser for SQLite (DB4S) is a tool that makes relational databases accessible to anyone. It allows you to interact with an SQLite database, the entirety of which is contained in a single file. No servers or extensive configuration are needed to get it running. Both DB4S and SQLite are free!If you don’t have any experience working with databases or SQL, this is a tool that will help you get up to speed with the fundamentals. The DB4S has a spreadsheet-like interface similar to Excel, which means that you can create and edit tables in your own database without using SQL. You also have the option to write custom SQL queries and view the results.Using the DB4S is a great way to get your hands dirty creating and working with relational databases. This piece will get you started with using the tool. The walkthrough is broken down into 7 parts:You can check out the installation details on the DB4S website here. You can download the latest versions for both Windows and macOS. It’s also available for Linux based distributions.If you have Homebrew on macOS, you can also install the latest release with this line:Once you’ve gotten DB4S installed on your machine, open the application and you’re ready to begin!When you open DB4S, on the top left you see a button labeled “New Database”. Click on it, type out a name for your database, and hit “Save” to your desired folder.That’s it! You’ve created your first SQL database. You should now see this:Now let’s create our first table. To do so, we’re going to import a CSV file with metadata for pieces featured on Medium’s popular page. I got the data in an earlier Python project that you can check out here.The CSV looks like this:To create a table with this file, on DB4S, you do:After that, select your desired CSV file and hit “Open”.This will prompt another window for a couple of customization options. Change the name to something that’s easy to remember. If the column headers are already in the first line of your CSV file, tick the box to make sure DB4S recognizes that. It will shift the first row to become the column names. Then hit “Ok”.And boom! You’ve created your first table.Exporting the database file to CSV is just as easy. All you have to do is right-click on the table and click “Export as CSV file”.Or to bulk export more than one table, do this:Now let’s take a look at our table on DB4S. Right-click the table and select “Browse Table”.You will now see the table in a classic spreadsheet-like format.If you want to add a new row, just click “New Record”, and DB4S will create a new row (already with a primary key) for you to fill in with values, just like you would on Excel. In this case, we may want to do so every time a new piece is added to Medium’s popular page. DB4S will even suggest a row value based on the first letters you type in if the value already exists in the database.To delete a row, you just need to select a row (click on a primary key on the leftmost column) and then click “Delete Record”.Once you’re done with all your changes, click “Write Changes” to save all your modifications and update the table in your database.Now on the table, you can see there’s a column called “field1”. This is because my CSV extract already included an index column. As DB4S automatically creates an index, I don’t need this column anymore.Writing SQLite queries will allow you to make some table schema changes, but there is no one line “DROP COLUMN” feature available. The SQLite docs will give you a workaround for how to drop a column, which involves creating a new table with only the desired columns, copying the data from the old table to the new table, and deleting the old table.In DB4S, dropping a column is a lot simpler. To delete a column, in the Database Structure tab, right-click on your desired table and click “Modify Table”.You will see a list of columns currently in your table. Select the column you would like to delete, then click “Remove field” and “Yes”.If we go back to “Browse Table”, we’ll see that “field1” is no longer in the table.Earlier, we added a record with the author “Dana G Smith”. If we wanted to look for all articles published by Dana in the table, we just need to write the name in the “Filter” box under “Author”.You can also add filter by more than one column at a time. If we wanted to find all articles in 2019 that had an 11 minute read time, you would just need to fill in two of the filter columns. The table would refresh automatically to show you just the results you need.You can run all kinds of SQLite queries via the “Execute SQL” tab. For example, if we wanted to see all articles with a reading time of 5, just typing “5” in the filter column like we did earlier would show some unwanted results. In this case, we’ll also get articles that have a reading time of “15”.To fix this, we’ll write a basic SQL query that filters results to only have a reading time of 5.You can easily save this filtered set of results to a CSV. Click the button circled below and select “Export to CSV”.Then, you can configure the CSV output and choose the location to output your file.Your resulting file will now be filtered to only have articles with a reading time of “5”.Say we wanted to create a graph that looked at the total reading time of articles per day over-time. To do so without any SQL, we can use the DB4S plot function. Click on “View” and then select “Plot”.This will open up a new dialog for you to select which columns you want on the X and Y axes of your graph. Below, I selected “Date Published” and “ReadingTime(mins)”. DB4S generated a bar graph to show me the reading time in minutes for every day.You can also generate graphs from your SQL queries. Say we wanted to count the number of articles published every day. We could write a simple “GROUP BY” query and graph the results.First, go back to the “Execute SQL” window demonstrated before. Then fill in your SQL query, run it, click “Plot”, and select the columns for your axis. You’ll see a graph version of the table the SQL query generates.By now, we’ve gone through the complete set-up and usage of your own relational database using DB4S! The simple visual interface of the tool allows you to easily get started with creating and managing an SQLite database. After you’ve gotten comfortable with that, we also looked at how you can actually use the tool to run SQL queries and even visualize data with simple graphs.This piece was meant to help you get started. For further reading, I’d highly recommend checking out the SQLite docs and the DB4S docs.If you want to look at how you can write to a database using Pandas, feel free to check out this piece:towardsdatascience.com",sql,https://towardsdatascience.com/an-easy-way-to-get-started-with-databases-on-your-own-computer-46f01709561?source=tag_archive---------1-----------------------
[AWS] Athena를 조금 더 효율적으로 쓰기 위한 쿼리가이드,"Jaemun JungJun 6, 2020·10 min readAthena(Presto) Query를 조금 더 효율적으로 쓰기 위한 가이드. 주로 하단의 [참조] 링크들을 참조, 번역하여 취합하였음.DML query quota — 20 DML active queries. DML queries include SELECT and CREATE TABLE AS (CTAS) queries.DML query timeout — The DML query timeout is 30 minutes.위와 같이 limit를 세팅하게 된 계기는, Athena query 비용을 모니터링 해보니 가끔 평소의 수십배에 달하는 비용이 부과되는 날이 있었고(일 $20 부과되던 게 어느날 일 $700 로 평균대비 30배 부과된다던가..), 원인을 찾아보니 대부분 데이터에 익숙치 않은 사용자가 partition 조건 없이 전체 S3 table 조회를 수회~수십회 반복해서 시도한 경우가 원인이었다.이러한 실수를 원천적으로 막기 위해서, 위의 limit 설정을 추가하고, 그럼에도 과다한 사용이 일어날 경우 운영자가 알 수 있도록 notification을 추가하였다.또한 Athena는 상대적으로 가벼운 ad-hoc query 목적에 충실할 수 있도록 이러한 활용 가이드를 작성하여 공유하였다.100% 정확한 Unique Count가 필요한 케이스가 아니라면(추세를 보는 것으로 충분한 경우) distinct() 대신 approx_distinct()를 활용해 Approximate Unique Count를 활용할 수 있다.이 방식은 hash값을 활용해 전체 string값을 읽는 것보다 훨씬 적은 메모리를 활용해 훨씬 빠르게 계산할 수 있도록 해준다. 표준오차율은 2.3%.Example :SELECT approx_distinct(l_comment) FROM lineitem;asterisk(*)를 사용한 전체 선택은 지양하고 필요한 컬럼을 지정하여 SELECTParquet file 특성 상 컬럼별로 지정하여 원하는 컬럼만 read 하므로 불필요한 연산을 크게 줄일 수 있다.Issue : Left에 작은 테이블, Right에 큰 테이블을 두면 Presto는 Right의 큰 테이블을 Worker node에 올리고 Join을 수행한다. (Presto는 join reordering을 지원하지 않음)Best Practice : Left에 큰 테이블, Right에 작은 테이블을 두면 더 작은 메모리를 사용하여 더 빠르게 쿼리할 수 있다.Issue : Presto는 Index 없이 fast full table scan 방식. Big table간 join은 아주 느리다. 따라서 AWS 가이드는 이런 류의 작업에 Athena 사용을 권장하지 않는다.Best Practice : Big Table은 ETL을 통해 pre-join된 형태로 활용하는 것이 좋다.Issue : Presto는 모든 row의 데이터를 한 worker로 보낸 후 정렬하므로 많은 양의 메모리를 사용하며, 오랜 시간 동안 수행하다가 Fail이 나기도 함.Best Practice : 1. LIMIT 절과 함께 ORDER BY를 사용. 개별 worker에서 sorting 및 limiting을 가능하게 해줌. 2. ORDER BY절에 String 대신 Number로 컬럼을 지정. ORDER BY order_id 대신 ORDER BY 1Issue : GROUPING할 데이터를 저장한 worker node로 데이터를 보내서 해당 memory의 GROUP BY값과 비교하며 Grouping한다.Best Practice : GROUP BY절의 컬럼배치순서를 높은 cardinality부터 낮은 cardinality 순(unique count가 큰 순부터 낮은 순으로)으로 배치한다. 같은 의미의 값인 경우 가능하면 string 컬럼보다 number 컬럼을 활용해 GROUP BY한다.Example :SELECT state, gender, count(*) FROM census GROUP BY state, gender;Issue : 여러개의 string 컬럼에 like검색을 써야하는 경우 regular expression을 사용하는 것이 더 좋다.Example :SELECT count(*) FROM lineitem WHERE regexp_like(l_comment, 'wake|regular|express|sleep|hello')The stripe size or block size parameter : ORC stripe size, Parquet block size는 block당 최대 row수를 의미한다. ORC는 64MB, Parquet는 128MB를 default로 가진다. 효과적인 sequential I/O를 고려하여 column block size를 정의할 것.https://aws.amazon.com/ko/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/https://www.upsolver.com/blog/aws-athena-performance-best-practices-performance-tuning-tipshttps://www.upsolver.com/blog/6-data-preparation-tips-querying-big-data-aws-athenahttps://searchaws.techtarget.com/tip/Improve-query-performance-cut-costs-with-Amazon-Athena",sql,https://medium.com/@jaemunbro/aws-athena-presto-query-guide-886ce047d710?source=tag_archive---------2-----------------------
A Non Coder’s Journey to Data Science then to Data Engineering — Part II,"Dharran KandaiahJun 3, 2020·10 min readIn the first part, I wrote a brief introduction about my personal background and my experience in trying out online courses which led me to pursue Data Science and the platforms related to it such as Kaggle.medium.comIn this post, I reveal my uncovering of Data Science and provide insights on how I transitioned from Data Science into Data Engineering.After completing a few projects on Kaggle, I started realizing that this process was not as fulfilling as it should have been. Most datasets presented on this website were historical data mostly used for practice. I wanted put my skills to some real-world test. I wanted to scrape raw data and create my own version of the organized dataset. Many resources online told me to extract data from APIs, scrape them from a website or pull them from a data repository. When attempting to do so, this is the exact point where my outlook on this field changed. The data I obtained from the methods above wasn’t like the CSV files I used to work with, from Kaggle. It wasn’t well structured, there were no problem statements and there were no kernels from other users to get inspiration from. Some APIs were well managed while others were a mess. Upon further research I found out that in most organizations, the data pipelines were first well organized by a team of individuals called data engineers. Below is a picture you’ve probably seen in many articles.“A scientist can discover a new star, but he cannot make one. He would have to ask an engineer to do it for him.”–Gordon Lindsay GleggData engineers are crucial individuals working behind the scene to provide organized data for the data scientist to work on; The robin to the batman. Just like how I started my data science journey; I spent days searching up on how to start my data engineering learning path. Unfortunately, data engineering isn’t as popular as it’s close cousin data science. There aren’t many online courses or free learning materials or even if there was, it was not compiled in an orderly fashion. I was demotivated once again. I sounded dejected when speaking to my family and partner stating that this was a dead end as there were no proper learning materials out there for me to pursue. This is when my partner suggested to put the word out there to companies who might be providing internship or individuals on LinkedIn pursuing this field (I owe her a nice dinner for this sound advice).Aha, a light-bulb moment! I had a colleague who used to work in the business intelligence department in my previous company. I was hoping he might provide some tips and suggestions to get started. I dropped him a text and that is very much how I ended up here today writing this article. Let us call him Mr.A for the entirety of this article. I owe a big portion of my progress to him; a big thank you to Mr.A. The following parts of this article will be slightly more technical, explaining about the tools I’ve used along this journey.At this point my Python and SQL skills were fairly decent. I would suggest learning some of the following topics to get you ahead;Mr. A’s advice was very precise and practical. Pick up any project that interest me and he will then recommend the tools needed for a data engineer to work through the project. I usually excel at learning something new when doing it hands-on. After going through many online courses as mentioned in Part I to no avail, I clearly knew I had to revert to a method I was comfortable with; hands-on project-based learning.At the time of writing this article, the world is being attacked by a deadly pandemic called the Coronavirus. I also generally enjoy reading about the financial world during my free time, and therefore I decided to combine both topics and create a data engineering project to explain the economic impact of the Coronavirus on a country.Problem StatementDid the Coronavirus pandemic impact the financial status of a country and its major companies in various industries?HypothesisThe prolonged infection of the Coronavirus among the citizens during the pandemic has impacted the country’s economy and the financial stability of many companies in a negative manner.A simple visualization was created to show the impacts of Coronavirus on various industries in a country. This was achieved by tracking the individual financial performance of several public listed company in various industries.Exploratory Data Analysis (EDA)In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modelling. I had some experience with EDA as I used it frequently while working on Data Science projects previously.A simple EDA is an important step that should be done at the start of the project. This gives us a bird’s-eye view of the shape and size of the data that we are pulling from the source. I created some simple charts and tables to understand the data for this project better. I would suggest using Jupyter Notebook to perform EDA as it has an interactive interface and produces visually pleasing charts and graphs that will paint a better picture of our data. Shown below is a sample snippet of some basic EDA code and visualization to give us an idea on the data trend.Another example shown below showing the stock pricing trends of public listed companies;Data Engineering Skills to Learn for this ProjectI will break down the steps and tools used to complete the project in the order of use;The very first step was to look for the data and its sources. This can be done using the free APIS available on the internet. I completed this task using the Python requests library. I could not find any free APIs for the local stock market, so I started extracting American stocks to begin with. I did some basic visualization on the American stocks pulled from the API using skills I obtained while practicing Data Science previously. To obtain data for the local stock market, I used a Python package called Beautiful Soup to scrape data from various websites.2. Storing the data in a relational database systemExtracting data alone isn’t sufficient, the data had to be stored in a database and the database I used was MYSQL. MYSQL has a nice GUI called Workbench. I used SQLAlchemy to connect my python code editor to my local MYSQL database and push all the data collected in the previous step to the database. There are also other options like msql-connector-python to connect to the local database. The reason I chose SQLAlchemy was because the codes were a lot leaner and it had functionalities I needed for this project.3. Automating the tasksAt this point I had to click the run code button to extract all the data on a daily basis. Wasn’t there a more efficient way to complete this? One thing for sure, computers were far more superior at automating daily tasks. After some research, I found CRON that would automate both the processes above. Basically, a CRON job allowed me to execute a shell script at fixed time intervals and I invoked my python code from inside that shell script. I also found that Airflow was “cooler” and had a lot more functionality in automating data pipeline processes; more of the latter for sure.I was a little ambitious, so I gave Airflow a go. THIS WAS A PAINFUL DECISION. The thing about most installation guides on the internet for these applications are that most of them are catered for the MacOS. A moment of silence for Windows users. All the commands in the guides were for the MacOS and none of them worked on my Windows laptop. I spent 4 full days just trying to install Airflow on my laptop. I used Docker, Ubuntu, Vagrant, VirtualBox, almost everything under the sun. The errors messages from all the different installation methods were far more agonizing than ****** . After trying to break through brick walls with my head, I finally gave in and went with the lesser complex CRON. I will conquer you very soon Airflow, I’ve got my eyes on you.4. Running my code on the cloudOnce I’ve automated this process, I still had to keep my laptop on for this automation to take place. Here come cloud servers to the rescue. Cloud computing platforms are synonymous to your laptop being placed on the cloud (virtual computers); able to run at any time of the day without having to physically turn on or being present in front of the machine. There are 3 main providers; Google, Amazon, and Microsoft. I chose AWS as I found some good resources on how to setup the system.I used EC2 which is the cloud computing platform on AWS and RDS which is Amazon’s version of relational database systems. Under RDS, I used the MYSQL engine. To connect to the EC2 engine from my local machine (aka laptop), I used PuTTY. PuTTY is an open source SSH client used to connect to a remote server. PuTTY is basically a terminal for windows based operating systems. Using PuTTY, SSH and WinSCP (A software that allows secure file transfers between computer and the remote server), I managed to make a connection to the servers and push my Python files to the cloud.Then, I had to make my EC2 interact with my RDS. However, the connection to the RDS machine from EC2 was blocked. This was a little tricky, I realized that we needed to open up the rules on the security group. Learn more here on how to allow access. Make sure you allow inbound access from your local laptop/desktop, as well as access from the EC2 instance.Just like connecting with the EC2, I had to also connect to the RDS from my local machine. I did that using MYSQL Workbench. There were tutorials on AWS official documentation to complete this. After I connected to both EC2 and RDS, I push all my codes to the cloud and made them interact with each other just like how it was interacting whilst being on my local machine.This has been my progress so far folks for my first project. I have a fully running automated program on the cloud servers.github.comIn this post, I explained about the importance of Data Engineering and how I transitioned from Data Science into Data Engineering. I also described my course of action on building a Data Engineering project from scratch with real case examples and use of various data tools.In the third post of this series, I will complete this project while implementing software engineering best practices/principles. I will also improve this project with some of the ideas I shared above to make it a more holistic “Big Data” project.If you enjoyed this series so far, stay tuned for Part III",sql,https://medium.com/@dharrankandaiah/a-non-coders-journey-to-data-science-then-to-data-engineering-part-ii-9653ae91aa34?source=tag_archive---------3-----------------------
Comparing PostgreSQL DigitalOcean Performance & Pricing — ScaleGrid vs. DigitalOcean Managed Databases,"Kristi AndersonJun 4, 2020·6 min readDigitalOcean is a cost-effective cloud provider that caters to, and is widely adopted by the developer community. ScaleGrid is a fully managed DBaaS that supports MySQL, PostgreSQL and Redis™, along with additional support for MongoDB® database and Greenplum® database. Along with many popular cloud providers, DigitalOcean also provides a Managed Databases service. They support PostgreSQL, MySQL and Redis, but for the sake of this article, we are going to focus on their PostgreSQL product. So, which database service is right for your application? In this post, we are going to compare the performance and pricing of DigitalOcean PostgreSQL vs. ScaleGrid PostgreSQL to help you determine the best PostgreSQL hosting service on DigitalOcean.Compare PostgreSQL DigitalOcean Performance & PricingPostgreSQL is one of the fastest growing databases, and #4 database in the world by popularity. As an open source database, it’s a highly popular choice for enterprise applications looking to modernize their infrastructure and reduce their total cost of ownership, along with startup and developer applications looking for a powerful, flexible and cost-effective database to work with.ScaleGrid PostgreSQL provides on average 68% higher throughput for write-intensive workloads and 94% higher throughput for balanced workloads compared to DigitalOcean for PostgreSQL. Read nowScaleGrid PostgreSQL provides on average 42.3% lower latency compared to DigitalOcean for PostgreSQL. Read nowOn average, ScaleGrid provides over 30% more storage vs. DigitalOcean for PostgreSQL at the same affordable price. Read nowIn order to see which DBaaS provides the best PostgreSQL hosting performance on DigitalOcean, we are comparing equivalent plan sizes between ScaleGrid PostgreSQL on DigitalOcean and DigitalOcean Managed Databases:ScaleGrid PostgreSQLInstance Type/RAM: Large — 16GB RAMStorage: 300GBDeployment Type: Single NodeDigitalOcean Region: SFO3Monthly Cost (USD): $240DigitalOcean PostgreSQLInstance Type/RAM: 16GB RAMStorage: 270GBDeployment Type: Single NodeDigitalOcean Region: SFO3Monthly Cost (USD): $240As you can see above, the monthly cost and cluster RAM configurations we are using for ScaleGrid and DigitalOcean are the exact same. Now, let’s take a look at the throughput and latency performance of our comparison.We measure PostgreSQL throughput in terms of transactions processed. Throughput in general is the rate at which something is processed, and in terms of PostgreSQL, this is the number of transactions per second (TPS) your application can handle.ScaleGrid PostgreSQL provides 68% higher throughput for write-intensive workloads and 94% higher throughput for balanced workloads on average compared to DigitalOcean for PostgreSQL.This means that ScaleGrid is able to process almost 2x the amount of transactions as DigitalOcean across the same deployment configurations!Next, we are going to test and compare the latency performance between ScaleGrid and DigitalOcean for PostgreSQL. Latency is the average transaction execution time of your PostgreSQL data. Here is how both providers performed across workload types for the PostgreSQL configurations outlined above:ScaleGrid PostgreSQL provides on average 42.3% lower latency across all workload types compared to DigitalOcean for PostgreSQL.While the performance is significantly better across the board, the most noticeable improvement can be seen in the balanced workload scenario where they achieved over 48% lower latency than DigitalOcean Managed Databases.As we can tell from the above graphs, ScaleGrid PostgreSQL can help you achieve almost 2x better performance in terms of both throughput and latency across these common workload scenarios:PostgreSQL DigitalOcean Throughput AveragesScaleGridRead-Intensive: 3,328Balanced Workload: 7,998Write-Intensive: 4,584DigitalOceanRead-Intensive: 2,056Balanced Workload: 4,121Write-Intensive: 2,736ScaleGrid ImprovementRead-Intensive: 61.9%Balanced Workload: 94.1%Write-Intensive: 67.5%ScaleGrid PostgreSQL averages 74.5% higher throughput performance over DigitalOcean Managed Databases for PostgreSQL.PostgreSQL DigitalOcean Latency Averages (ms)ScaleGridRead-Intensive: 54.1Balanced Workload: 22.5Write-Intensive: 39.3DigitalOceanRead-Intensive: 87.6Balanced Workload: 43.7Write-Intensive: 65.8ScaleGrid ImprovementRead-Intensive: –38.2%Balanced Workload: –48.5%Write-Intensive: –40.3%ScaleGrid PostgreSQL averages 42.3% lower latency performance over DigitalOcean Managed Databases for PostgreSQL.How does ScaleGrid achieve such higher performance for PostgreSQL compared to DigitalOcean’s PostgreSQL service using the exact same deployment configurations? ScaleGrid for PostgreSQL is architectured to leverage-high performance SSD disks on DigitalOcean, and is finely tuned and optimized to achieve the best performance on DigitalOcean infrastructure.Here is the configuration we used for the ScaleGrid and DigitalOcean benchmark performance tests highlighted above:Configuration DetailsBenchmark Tool: PgBenchPostgreSQL Version: 11Scaling Factor: 10,000Query Mode: SimpleNumber of Clients: 180Number of Threads: 10Duration: 1,800 secondssynchronous_commit: offIn order to help you determine which provider is best for you, we compared PostgreSQL performance on DigitalOcean across all three common workload types:A write-intensive workload is one that is primarily dominated by write operations, and has more data written to the disk through queries like UPDATE and INSERT. We used 20% reads and 80% writes in this comparison, using standard TPC-B type transactions where each transaction consists of 3 updates, 1 insert and 1 select operation.A balanced workload is one that has an equally balanced number of read operations and write operations. We used 50% reads and 50% writes in this comparison. A single transaction consisted of 1 select and 1 update operation.A read-intensive workload is one that is primarily dominated by read operations sent to the disk, and typically has more query types like SELECT. We used 80% reads and 20% writes in this comparison. A single transaction consisted of 4 select and 1 update operation.One of the first aspects to consider before adopting any new service is its cost. Fortunately, when comparing ScaleGrid vs. DigitalOcean PostgreSQL, pricing is not a major consideration because both offer extremely affordable plans at the exact same price. Let’s take a look at the various configurations:As you can see from the above chart, both ScaleGrid PostgreSQL and DigitalOcean PostgreSQL offer standalone (1 data-bearing node), primary-standby (2 data-bearing nodes) and primary-standby-standby (3 data-bearing node) configurations.DigitalOcean Managed Database pricing is the same as ScaleGrid’s DBaaS pricing across all PostgreSQL plans, as well as their MySQL and Redis plans, at just $15/GB for their standalone plans. ScaleGrid does offer some advanced configurations, such as standby nodes for their Nano plan, a standalone configuration for their 64GB X2XLarge plan, and an X4XLarge plan with 96GB for even larger-scale deployments.While ScaleGrid and DigitalOcean charge the same amount by RAM, ScaleGrid offers, on average, over 30% more storage for the same price. Additionally, ScaleGrid supports DigitalOcean hosting for MongoDB® database, and is the only DBaaS to support this database cloud deployment.So, now that we’ve outlined performance and pricing, are there any other major considerations when deciding between ScaleGrid and DigitalOcean? Let’s take a look at some key features to wrap up this comparison:ScaleGrid PostgreSQLSupported PostgreSQL Versions: 9.6x, 10.x, 11.x, 12.xPostgreSQL Superuser Access: YesUnlimited PostgreSQL Extensions: YesReplication Strategies: Synchronous & asynchronousMulti-Region Replication: YesPostgreSQL Configuration Management & Tuning: YesHigh Availability: YesContinuous Backup: Coming soonDigitalOcean PostgreSQLSupported PostgreSQL Versions: 10.x, 11.xPostgreSQL Superuser Access: NoUnlimited PostgreSQL Extensions: NoReplication Strategies: AsynchronousMulti-Region Replication: Only for read replicasPostgreSQL Configuration Management & Tuning: YesHigh Availability: YesContinuous Backup: YesFor the sake of brevity, this is a summarized list. To see the full feature comparison, check out our ScaleGrid vs. DigitalOcean PostgreSQL page.",sql,https://medium.com/@kristi.anderson/comparing-postgresql-digitalocean-performance-pricing-scalegrid-vs-caa08a732a38?source=tag_archive---------1-----------------------
Solar System classifications with SQL,"What are the largest objects in the solar system? Well, it depends on how I define ‘largest’ — some people mean the largest as the most massive thing (like the Sun) or the object where the diameter is the biggest (again, the Sun). But, does the largest object need to be spherical? Do we count orbits of planets or Kuiper Belt objects in this idea of ‘largest’? What about the path of human-made satellites like Voyager?Philosophy aside, I define ‘largest’ as the width, or the diameter, of a single object in the solar system. For example, the Sun is the largest object, not the Oort Cloud. In fact, the Sun is the first object listed in solar_system_20, a relational database table I built with the following columns:Recall, this table has facts about the 20 largest objects in the Solar System. I used solar_sytem_20 in two previous tutorials, Touring the Solar System with SQL and Searching for Moons with SQL, to show hands-on examples of aggregate functions, subqueries, aliases, and the GROUP BY and HAVING commands in SQL.I’m going to query the table again in this tutorial. Again in this post, I won’t show the entire table filled with data, but encourage you to read through this post to explore the table. This mimics a real-world scenario where you have a big dataset to query and you can’t possibly view all the records.To get started, let’s remind ourselves what the first few records look like.Result:And, how many classifications are there?Result:Ok, there are four types of objects (from the column classification) in the table: moon, planet, dwarf planet, and star with 9, 8, 2, and 1 objects in each bin, respectively.Suppose we want to eliminate duplicate records from a set of results. For example, say that we want to obtain the classifications without the count of objects. Enter the DISTINCT keyword.Result:Instead of counting the number of classifications on your hand (here we count 1… 2… 3… 4), SQL can count for you by combining SELECT DISTINCT with an aggregate function.If we don’t use DISTINCT:Result:which doesn’t make any sense.Recall that, at the beginning of this post, we found the first two rows of the table. We can also do the reverse and view the last row of the table. Or, if we want to be more data-savvy and use our new SQL skills, we can find the smallest two objects in the table.Result:But what if we wanted to display one or multiple records for middle-range diameters? Enter OFFSET. Let’s see this in action: find the 12th, 13th, and 14th largest objects in solar_system_20.Result:Before I discuss the last topic in this post, I need to prep you. The following images have a different view format. So far in this post, the SQL views have been in a table format, with all the information in single rows.I’m going to switch to an extended view for query results in the next topic. Why? The extended display does not show all the information crowded into one line, but separates one record into multiple lines. This is especially important when text begins to wrap to the next line in a table display — it becomes difficult to read!Read on to see examples of this different display.The array aggregate function collects a set of elements and combines them into an array. I think the best way to describe this is with an example. From the examples above, we know that there are four classifications for 20 total objects. Let’s create a view that displays all the objects in each classification.Result:What does this mean? The names of the objects in each classification are grouped together in an array. Same with the diameter of the objects. So, in the “moon” classification, there are nine objects with the names in an array and the nine diameters in an array.ARRAY_AGG() is an aggregate function (like COUNT()) but the function argument can take more than one input. For example, let’s list the name of the object and each diameter together in the array.Result:In fact, we have all 20 objects from solar_system_20 listed in this view! Now we know the largest objects, their diameters, and their classifications in the Solar System.This brings up the question: can we slice these arrays? Why yes we can! Just add parentheses around arr_agg() and add [lower_bound:upper_bound] after the right parenthesis.Here, we are asking for the 2nd and 3rd objects listed in each classification’s array. The result?Notice that the star classification only has one object so the names column is NULL — meaning there aren’t 2 or 3 objects to list.SELECT DISTINCT, OFFSET, and ARR_AGG() — more techniques to add to your SQL toolbox. Stay tuned for next week's post on using SQL with Python’s Pandas package!For your reference, I have a series of SQL tutorials that cover basic and intermediate queries. Take a look!Like these SQL posts? Have a question? Let’s chat in the comments!",sql,https://medium.com/swlh/solar-system-classifications-with-sql-f1a3a5e4730a?source=tag_archive---------2-----------------------
Day 1–10 SQL Questions challenge,"Mark LiJun 2, 2020·4 min readDay 1–10 SQL Questions challengeJoin Function: probably the most important features in SQL, to combine different tables with unique primary key.For example,That tells us analyst tries to combine ‘game’table and the ‘goal’table and the matching id will ‘id’ in the game table and the ‘matchid’ in the ‘goal’ table.If we want more specific way we could say ON (game.id=goal.matchid)Question 2:Show the team1, team2 and player for every goal scored by a player called MarioQuestion 3 :The table eteam gives details of every national team including the coach. You can JOIN goal to eteam using the phrase goal JOIN eteam on teamid=idQuestion 4 :List the dates of the matches and the name of the team in which ‘Fernando Santos’ was the team1 coachThis questions i got it wrong twice; first version of my code:The error remains me “Ambiguous column name ‘id’, let’s go back to look into the original dataset:There is more than one id as the primary key which tells us the importance of specifying primary key joining.Correct version:Question 5 :List the player for every goal scored in a game where the stadium was ‘National Stadium, Warsaw’This is where get me thinking about the table itself and not focusing 100% on the code. When i typed above codes, i wasn’t confident enough that i could generate correct answer as questions asked for every goal while i couldn’t find this peculiar information. However, when i looked back into the table itself, the table essentially tells about the player who scored goals in which match, which team at what time. Therefore, a quick glimpse of tables and solid understanding of basic information in the table is relatively crucial.Question 6 :This question i am proudly enough to say that it took me more than 15 mins to come up with the solution. a. not remember about the not equal sign != or <> b. then didn’t put distinct player to avoid the repeated namesI learnt that you could always learn by doing it or researching it…For example, just resemble all tables together and dig into the table to see what you could find. You won’t able to get the answer at the very first place but please keep faith that it is the matter of time you become better and excel in this technique.Questions 7 :Show teamname and the total number of goals scored.Wrong version:Another light buble moment for me to remain me my lack of ability to understsand table. I coded the count gtime and group by the match id, which gtime is not countable and matchid will only gives how many goals score in that particular game.In terms of grouping by the teamname gives us the individual unique teamname for all goal scored.Question 8:This one almost burn me out althoug it looks very much the same compared with the first one:Again, count every goal could be used as the proxy for count the player, only player could score the goal while the time doesnt give you the same info.Question 9:That gives me the error of :Column ‘goal.matchid’ is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause.This remains as the questions, why does it need the group by as the clause?Question 10 :(FINALLY)For every match where ‘GER’ scored, show matchid, match date and the number of goals scored by ‘GER’Simiarly, change the match id the GER and that is it..Done, day 1",sql,https://medium.com/@WeiLi39939672/day-1-10-sql-questions-challenge-725cb40e72e2?source=tag_archive---------10-----------------------
SQL-Part 11 | by Kristina Paseru,"Kristina PaseruJun 1, 2020·2 min readThankfully for being able to write here again. Guys, so on this section I am going to share about the usage of INSERT INTO SELECT and CASE in SQL. Let’s discuss it one by one :The INSERT INTO SELECT statement copies data from one table and inserts it into another table. Important to remember that :Take a look on the example below :Input :Output :So, from the example above, I want to insert data into table Pelanggan1 from table Pelanggan.the CASE statement goes through conditions and returns a value when the first condition is met (like an IF-THEN-ELSE statement). So once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the ELSE clause. If there is no ELSE part and no conditions are true, it returns null.Let’s jump into the example to make this clear.Input :Output :Okay, so that is all for this part and see you on the next sections. Thank you :)",sql,https://medium.com/@kristinapaseru/sql-part-11-by-kristina-paseru-962e30eda1e3?source=tag_archive---------12-----------------------
Our experience using SQL with DDD,"At Inato, we designed our software architecture according to Domain-Driven Design (DDD) principles. In this post I’d like to share with you our experience using a SQL database to persist data with DDD.SQL databases are great solutions to persist data in DDD but make evolving the domain model error-prone and time consuming.We usually build software to solve a problem. This problem space is called the domain. To solve this problem, we often introduce concepts that help define the problem and constrain the solution. This process is called domain modeling.Domain-Driven Design is an approach to design software that relies on the following hypotheses:In the layered architecture pattern, we usually have 3 layers:Since we isolate the domain from the rest of the code, it is very natural to follow the layered architecture pattern when implementing DDD. Indeed, the domain is conceptually a sub-part of the application layer.Now you get more context on our approach designing software, let’s talk about our specific experience using SQL in such context.Most of tech companies use a SQL database to persist their app state since it is one of the most reliable database out there.We run our SQL cluster as a managed service in Google Cloud and never experienced performance nor availability issues for the past 2 years we have been using it. Managed SQL databases are available at most cloud providers as a managed service which can let the engineering team focus on the product rather than the infra.SQL does not let you write anything that do not fit your data scheme. This prevents a lot of bugs from happening and will make your query fail rather than corrupt the data.For example, if you have set a column to non-nullable and you inadvertently forgot to fill this data when updating a row, then the SQL server will throw an error, preventing you from corrupting the data.Before implementing a robust back-office, we have leveraged our SQL database by allowing operational people to browse and edit it directly. By using a UI tool like postico, this allowed us to let the ops manage a lot of workflows without requiring the engineering team to build a complex interface.This is not DDD-compliant to let people interact with the database directly but this hack allowed the engineering team to focus more on the end-user and iterate faster on the product at the beginning of Inato.Down below are listed a few problems we have faced when building and evolving our persistence logic. They are not specific to using SQL but are specific to our own implementation. I’m sharing them with you since I believe there is a high probability they occur if you start using a SQL database in the context of DDD.When persisting a domain object, we need to map it to a representation that can be sent to an external system. This process is called the serialization. The process of converting back this representation to a domain object is called deserialization.An example of serialization is to convert an object to JSON:This process is often straightforward for flat objects but can be more time consuming for highly nested objects:As you can see, the problems with such serialization / deserialization design are:This phenomenon is called object-relational impedance mismatch.In the above example, you can see it is not straightforward to persist union types with SQL. You basically have 2 options:An other data structure which does not fit easily in SQL is the map. Maps are often represented by a dedicated table in SQL. However, since this is quite cumbersome to create a table every time we add a map attribute, we often end up serializing the maps as JSON and put them in a column of type jsonb.In DDD, we combine domain objects in aggregates. Those objects are often big, complex and nested data which require complex joined-queries to be retrieved. Complex aggregates often end up being represented with multiple tables. As the domain evolves and complexifies, the SQL query to fetch those aggregates also grow in complexity.In DDD, the model is owned by the domain layer itself rather than by the database. Therefore, the SQL validation of the data model is redundant since the domain is already doing the model validation. Sometimes the double-validation can lead to friction if they are not in phase, especially during migrations.The domain code is the source of truth and we should never rely on the SQL schema.In the above example, the validation is done by the constructor itself and is much more powerful than what can be done in SQL. Such validation needs to be tested and enforces the business rules at the core level.Worse, using foreign keys can even lead to some burden when testing repositories. In the below example, the population table depends on the study table which depends to the project table. If we want to write integration tests for the population logic, it will require having the database setup with real studies and projects. This makes the tests much more intricated instead of being independent.We didn’t design our aggregates to be accessed in a concurrent manner. The problem occurred when multiple concurrent sessions were trying to update the same aggregate. Let’s consider the following scenario:In this example, the modifications made by the Session 1 have been overriden by the Session 2.Thus it is important to support concurrent access by default on all your aggregates. A simple solution might be to lock tables when updating an aggregate.The domain is supposed to evolve with your comprehension of the problem. Therefore, your persistence logic should be able to evolve along with it.If you are familiar with SQL, you know we need to write migrations to evolve the SQL schema. The migration in itself might be a simple (although risky) operation. However, a single migration requires tens of steps to be executed in the right order, without any safe-guard for the developer if he misses any.Let’s consider the following example where we wanna migrate the field a3 from Entity A to the Entity B .That’s it! Our migration is done. As you can see, a lot of burden is put on the developer and he has the full responsibility to not miss any step.An ideal persistent layer would have a simpler workflow when evolving the domain model.SQL is definitely a great technology for persisting state. However, it is a bit too rigid which makes evolving the domain model painful. Since in DDD all business rules should be encoded in the domain layer, there is no need for the additional safety that SQL provides. Moreover, writing serializers / deserializers is very time consuming since we need to write custom bindings to SQL and test them. Finally complex aggregates often lead to complex queries that can be slow and hard to maintain.Of course our design is not perfect, and most of those problems can be avoided while keeping SQL to persist data. However, I think it is worth exploring SQL alternatives (e.g. MongoDB) to see if they fit better with DDD.",sql,https://medium.com/inato/our-experience-using-sql-with-ddd-96c2024d435c?source=tag_archive---------2-----------------------
Day 3–10 SQL Question Daily Challenge,"Mark LiJun 5, 2020·2 min readDay 3 —10 SQL Challenge QuestionList the teachers who have NULL for their department.Note the INNER JOIN misses the teachers with no department and the departments with no teacher.Use a different JOIN so that all teachers are listed.Use a different JOIN so that all departments are listedUse COALESCE to print the mobile number. Use the number ‘07986 444 2266’ if there is no number given. Show teacher name and mobile number or ‘07986 444 2266Use the COALESCE function and a LEFT JOIN to print the teacher name and department name. Use the string ‘None’ where there is no department.Use COUNT to show the number of teachers and the number of mobile phonesUse COUNT and GROUP BY dept.name to show each department and the number of staff. Use a RIGHT JOIN to ensure that the Engineering department is listed.Use CASE to show the name of each teacher followed by ‘Sci’ if the teacher is in dept 1 or 2 and ‘Art’ otherwise.Use CASE to show the name of each teacher followed by ‘Sci’ if the the teacher is in dept 1 or 2 show ‘Art’ if the dept is 3 and ‘None’ otherwise.",sql,https://medium.com/@WeiLi39939672/day-3-10-sql-challenge-question-4dfce146caa1?source=tag_archive---------5-----------------------
Ditch the Database,"David HurleyJun 3, 2020·5 min readHow to use AWS S3 Select to query smarter and maybe cheaperAmazon S3 Select, a feature of Amazon S3, became generally available in April of 2018. By using Amazon S3 Select it’s possible to run an application without a “classical” database making app architecture simpler.Anyone who works with data has probably used a cloud-based storage solution at some point in their data pipeline. Personally, Amazon S3 is my preferred choice as it’s relatively simple to use, easy to scale, couples with other AWS services, and stores objects of nearly any file type.While Amazon S3 is AWESOME for storing datasets it, like other traditional cloud-based object storage solutions, doesn't allow retrieving subsets of a dataset (i.e. the entire dataset must be retrieved). For example, if you need the first 1000 lines of a 1 million line dataset you have to retrieve the entire dataset and filter locally. This makes it impractical to use Amazon S3 as the sole database for applications, particularly apps that constantly need to retrieve subsets of a larger dataset (i.e. web app retrieving customer records).Enter Amazon S3 Select 🙌Amazon S3 Select lets you use simple SQL expressions to pull out only the data you need from an Amazon S3 object. This is HUGE!!!With S3 Select you no longer need to retrieve an entire dataset only to retain a few lines. Instead, you can use classic SQL expressions to query data in place and retrieve only a subset, such as data between two dates or above a certain price. Currently Amazon S3 Select works with objects in CSV, JSON, and Apache Parquet format.By reducing the volume of data that has to be loaded and processed by your applications, S3 Select can improve the performance of most applications that frequently access data from S3 by up to 400%. — Amazon Web ServicesThe best part of Amazon S3 Select is how it simplifies application structure. Prior to Amazon S3 Select, the typical structure of a web app may have looked like the following.With Amazon S3 Select the structure of the web app looks like the following.SO MUCH BETTER! Now you can build an end-to-end application using only Amazon S3. It’s fast, cheap, and simple to interface with! 👍Recently I built and deployed a web application to make downloading and visualizing historical Canadian weather data easier and faster. This was the first web application I built where I didn’t use a classic database and instead stored and queried static and dynamic data directly from Amazon S3. Using Amazon S3 Select I was able to quickly and easily take user inputs from my dynamic dashboard, pass them into a SELECT expression, and query and retrieve only the data requested.Below are some example data and a function to query a CSV in Amazon S3 and return a CSV format that can be read into a Pandas Dataframe. Amazon S3 SQL syntax for CSV can be a bit tricky. You can either query using column names or column index and based on which one you choose the SQL expression will be different.Let's query the above dataset between two dates using column names and return only the corresponding columns for date, wind speed, and wind direction.Let’s query the above dataset when the temperature is greater than 20 degrees Celsius using column index and return only the corresponding columns for date and temperature.Notice that the index starts at 1 (i.e. s._1) in S3 Select whereas Python would be 0. Make sure you are comparing common data types in the SQL expression by using the “cast” command to convert one data type to another. If my temperature data had been string format I would have needed “cast(s._4 as float) > 20”.Let’s query only the header names from the above dataset so that we can use them to append to other query results or as inputs to a dynamic dashboard dropdown. In this case, I set use_header to False so that column names are returned in the first row and use the S3 Select LIMIT feature to limit the number of rows returned to 1.Amazon S3 Select is CHEAP! Here is a breakdown of the starting costs for my application (current as of June 2020).I store approximately 37 GB of data in S3, perform nearly 100,000 SELECT commands a month, and transfer approximately 50 GB of data out of S3 a month. That brings my estimated monthly costs to….$1.39 USD!Although Amazon S3 Select has been available for more than 2-years (in code years that’s a millennium 😆) it’s a great tool for simplifying applications, particularly microservice frameworks.Happy querying!",sql,https://towardsdatascience.com/ditch-the-database-20a5a0a1fb72?source=tag_archive---------0-----------------------
Bucketing in SQL,"Vasanth SJun 1, 2020·2 min readStructured Query Language, commonly known as SQL, is a programming language which is used for handling and manipulating data in Relational Database Management Systems (RDBMS) and is particularly useful in efficiently handling structured data. SQL language has wide-spread popularity for its unique features and user-friendliness.Problem StatementRecently, I worked on a small problem and I wished to share my knowledge on the same. I was introduced to a data set which contained transactional data of a small company. The data contained basic columns like Transaction ID, Customer Name, Bill Amount, Mode of Payment etc. I was also presented with several analysis requests for which I had to build queries, execute them on the data and share the results.Most of the analysis required bucketing of the data i.e. segregate the records into various buckets based on certain conditions. For example, the transactions with bill amount between Rs.1 and Rs.100 fall into one bucket, Rs.101-Rs.200 fall into another bucket and so on. Initially, I wasn’t aware of how to create these buckets. After an extended search, I could find a solution.SQL solutionThe base table that we use for analysis is “Transactions_Sample”. The columns used for this analysis are Transaction_ID and Bill_Amount. Our goal is to partition the records into buckets based on the Bill_Amount field.In the above solution, the inner query does the job of creating a separate column which shows the bucket for each record. The case statement evaluates the condition based on the value in the Bill_Amount field like if the Bill_Amount is between 0 and 100, the bucket value “0–100” is assigned to that record and so on. This new column is given an alias name as “Bucket”.From the fields fetched from the inner query, the outer query groups the table based on the “Bucket” field, and the aggregate function SUM is used to calculate the total Bill_Amount for each bucket. Also, the number of transactions per each bucket is calculated using the “Count” function. The result table extracted in excel looks like:Using this subquery and case statements, I could do the task of bucketing the records based on different fields for different analysis requirements. This can still be shortened for MySQL by eliminating the subquery part. As I am using Microsoft SQL Server, this is the most efficient way of doing this.Hope you find this article helpful in doing SQL tasks.",sql,https://medium.com/@svasanth2308/bucketing-in-sql-c7726ce68cbd?source=tag_archive---------3-----------------------
[Text-to-SQL] Learning to query tables with natural language,"Aerin KimJun 5, 2020·10 min readThe views expressed on this post are mine alone and do not reflect the views of my employer, Microsoft.Text-to-SQL is a task to translate a user’s query spoken in natural language into SQL automatically. It is the project that I’m working on at Microsoft.If this problem is solved, it’s going to be widely useful because the vast majority of data in our lives is stored in relational databases. In fact, Healthcare, financial services, and sales industries exclusively use the relational database. This means the industries that can’t afford to lose transactions solely use the relational database. (You can risk losing social media comments here and there but you don’t want to risk losing transaction records of your credit card.) Also writing SQL queries can be prohibitive to non-technical users. Bill Gates noticed this problem and he himself(!) wrote down 105 questions (my team is working on 70 of them) that he wants a machine to be able to answer given enterprise databases. If we could enable people to directly interact with large-scale enterprise databases using natural language or voice, it’s going to be very useful.For the past two years, we have witnessed a major advancement in NLP downstream tasks thanks to contextualized embeddings on webscale data and transfer learning. This progress made me hopeful that we should be able to solve this relatively easy (?) problem. I mean, a translation should be easier than a chatbot! shouldn’t it?(when MongoDB is ‘webscale’)This Youtube video is one of my all-time favorite tech videos and it talks about the reasons why I’m working on Text-to-SQL, not Text-to-NoSQL. The relational database was blamed for the lack of scalability about 10 years ago and NoSQL came into the market as a promising solution. However, we’ve learned over time that NoSQL can be a wrong tool for many application use cases. Also, the relational database has evolved to be able to scale horizontally and serve data with low latency while meeting availability requirements. These newSQLs are Azure Data Warehouse, Google Spanner, MemSQL, CockroachDB, etc.I’m confident that the relational database is here to stay.Imagine a superstar recruiter, who can sell any job to anyone and also has extensive networks. But he has one drawback — he doesn’t know how to query the SQL database. He has experience and insights that he can draw from the data, but he just doesn’t have enough time to learn SQL.There are thousand different types of SQL queries that a user can ask, but we are focusing on solving the following cases only which we believe cover most of enterprise use cases:To successfully translate “What is Mark Zuckerberg’s salary?” into SQL, the model needs to know that it should look up “Mark Zuckerberg” in the ‘Name’ column. Also, it should infer that “salary” means the ‘BaseSalary’ column. Finally, the model should be able to construct the full SQ by correctly classifying those two columns as SELECT and WHERE, not the other way around.The model should match “who” with the ‘Name’ column, “software engineering” with the ‘Department’ column and “make … base” with the ‘Base Salary’ column. It also needs to predict “<” operator for “less than 100k”.The model needs to predict the aggregator SUM and also convert “hired after 2000” into ‘Hiring Date > 2000–01–01’.The example table “Compensation” has employees’ names and salary numbers altogether. However, that’s rarely the case in real life. The table will be divided into subject-based tables ‘Employee’ and ‘Compensation’ in order to reduce redundant columns. And it’s our job to join them using the EmployeeId. Then this problem becomes much harder.How do we build such a model to know these all?In research settings, text-to-SQL falls under the semantic parsing — the task of converting natural language to a logical form.One way to track the progress of research is through the benchmark. WikiSQL is one of the most popular benchmarks in semantic parsing. It is a supervised text-to-SQL dataset, beautifully hand-annotated by Amazon Mechanical Turk.The state-of-the-art (as of May. 2020) models on WikiSQL leaderboard frame SQL generation as a classification problem of predicting six SQL components. Some of the early works on WikiSQL modeled this as a sequence generation problem using seq2seq but we are moving away from it.Let’s say you are building this classification model from scratch. What are the challenges you need to tackle?For example:“What is Mark Zuckerberg’s salary?”“How much does Mark Zuckerberg make?”“mark zuckerburg base salaray” (with typos, no capitalization)Solution: Use the contextualized (language model pre-trained) word representation such as BERT, RoBERTa, XLNET, etc.Solution: Use the attention. Not only use a natural language query as an input, but also the column names of the table as well and take advantage of the attention mechanism.Attention helps the model focus on the query words (or subwords) that are most relevant to each label during the training.Solution: Use Execution-guided decoding. Notice that certain operators won’t be used with certain types. For example, you can’t sum over a column with a string type. You can’t apply > or < to a column of string type either. Execution-guided decoding detects and excludes faulty SQL during the decoding. Another thing that we can try is to use the type encoding as an input as well.The top three models on WikiSQL leaderboard as of May 2020 — HydraNet, X-SQL, SQLova — share a similar architecture.Typical architecture of Text-to-SQL modelsWe design the architecture so that it accepts the concatenation as an input. Some models take type embedding or positional embedding as an input as well. Column names can consist of several words.There are many such off the shelf encoders: BERT, RoBERTa, XLNet, ELMo, Glove, etc.The goal of this step is to capture a larger context of an input. Bi-LSTM or self-attention enables the model to access information about prior, current, and future input. The resulting embeddings don’t merely capture the local context of a word — they can, in principle, capture the entire semantic of the input.The final layer consists of classification modules over six different components of the final SQ. Each module has its own attention weight to align the hidden representation and the final label.During the training, we minimize the loss — a summation of six individual sub-task losses using cross-entropy or Kullback–Leibler divergence.Inference is pretty straightforward. The highest-scoring labels will be returned from each module. If the highest-scoring column is [None], we ignore the output from the WHERE-number predictor and return zero WHERE condition.The three models share a similar architecture but they are also different in detail: For example, while SQLova mainly uses LSTM (the question and table schema both are encoded through RNN), X-SQL completely removed LSTM and went self-attention all the way. SQLova uses BERT, X-SQL uses MT-DNN and HydraNet uses RoBERTa as an encoder.why can’t many research techniques be directly applied to production?While WikiSQL is an exceptionally well-made dataset, it has one pitfall — it assumes NO JOIN. It takes for granted that a single table will have all the columns you need for a query. However, you will need a JOIN in real life because enterprise databases are normalized into subject-base tables.Research datasets assume the values in natural language questions (e.g. how much did we pay Zuck last month?) would appear exactly the same way (last month) in the database. In production, the natural language ‘last month’ is stored as ‘2020–02–01’ in the database, ‘10k’ will be ‘10,000’ in the database, and ‘NY’ will be ‘New York’ in the database. The model should know how to match these NL slangs with the values in DB.For example, when ‘Employment Status’ is “Not active”, ‘Last Salary’ column should be returned, not the ‘Current Salary’.For example, “EstimatedRevenue” vs “ActualRevenue”, “CreatedOn” vs “ModifiedOn”, “CreatedOn” vs “CreatedBy”, ‘LastSalary’ vs ‘Salary’, etc.The number of columns in WikiSQL dataset is usually 5–7 and the number of rows is about 10. Meanwhile, a typical enterprise table has 30–40 columns and millions of rows in a single table.So how do we solve the challenges of Text-to-SQL in production?Surprisingly, it’s not the more complex model. It’s the training data that matters the most to the performance of the DL model.The main benefit of deep learning over conventional machine learning is that deep learning largely depends on the data with little or no features engineering. If we use more high-quality training data, the model almost always outperform. In other words, DL models are often constrained by the availability of large-scale high-quality training data.The details to generate training data are out of scope for this article (I’ll later talk about data generation/augmentation in a separate post) but here are the techniques that I’m using to generate accurate & cost-effective training data:CDM is a standard & extensible schema (entities, attributes, relationships) that represents concepts that are frequently used in enterprises. CDM exists in order to facilitate data interoperability. For example, every company regardless of its industry will have some kind of customer management database and have tables such as Account, Contact, Product, etc. Clients can name the columns differently as long as they match with CDM and as long as the database is CDM compliant, we can easily fine-tune the pre-trained Text-to-SQL model.In production, we have access to the cell content of the database. (If we do this in research, it’s cheating.) We build an index on the cell values so that it can identify the values in the NL query and finding the most appropriate columns.The key idea of execution-guided decoding is that a partially generated SQL can be executed and the results of that execution can be used to guide the rest of SQL generation. For example, the execution-guided decoder would eliminate an incorrect string-to-string inequality comparison “… WHERE department > ‘software engineering’ …” from the beam immediately after the token ‘software engineering’ is returned. Excluding erroneous candidates and not proceeding further during the beam search increases the accuracy of the model.Personally, I find translating a natural language into executable SQL a fascinating problem to work on and I see this project as more than a moon shot. Text-to-SQL is a project that has a potential to be solved, and once it’s solved, it is going to be widely applicable given how extensively relational databases are used in every sector. Yet surprisingly not many labs are working on this problem. As far as I know, the only teams that are actively working on Text-to-SQL are my team at Microsoft, Salesforce, and Naver (Korean Search Engine). If you are reading this and if you are working on Text-to-SQL problem, let me know in the comments!",sql,https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?source=tag_archive---------0-----------------------
XML Functionalities in Oracle,"Gaurav GoelJun 6, 2020·7 min readThere is always an extensive demand for exchanging data between different sources without worrying about how the receiver will use it or how it will be displayed. XML does that thing for us. It’s a W3C(World Wide Web Consortium) initiative that allows information to be encoded in meaningful structure and rules that humans and machines can understand easily.XML stands for eXtensible Markup Language. XML is not a replacement of HTML.While HTML is designed with focus on how to ‘display’ data, XML is designed how to ‘store’ and ‘transport’ it.XML, in itself, does not DO anything.Let’s look at a sample xml document:The above XML document contains a message with a body having a sender and receiver information. But it does not do anything on its own. It has just structured and stored information. Someone has to write a software code to receive, send or display it.The tags like <from> or <to> are not predefined. They are written by the author of this XML document.Thus, in short, we can say that XML is a software and hardware independent tool or way to structure, store and carry information.An XML document has a tree structure that starts at the “root” element. An example is as follows:The first line is the XML declaration. It defines the XML version. The next line declared the “root” element of this XML document. So, “Employees” is root element here. The other elements like “Empl”, “FirstName”, “lastName” and “Dept” are the child elements. Inside EMPL element, there is field id with value”1”. It is called attribute of that element. Attributes provide additional information about elements. Attribute values are always enclosed in quotes(single or double). A XML document with correct syntax is called “well-formed” XML.A Document Type definition (DTD) defines the legal building blocks of an XML document. It defines a document structure with a list of legal elements and attributes. A DTD document example is as follows:The DTD above is interpreted like this: The DTD above is interpreted like this:!DOCTYPE note defines that the root element of this document is message.!ELEMENT note defines that the note element contains four elements: “to,from,body”.!ELEMENT to defines the to element to be of the type “#PCDATA”.!ELEMENT from defines the from element to be of the type “#PCDATA”.!ELEMENT body defines the body element to be of the type “#PCDATA”.The description (#PCDATA) specifies parsed character data. Parsed data is the text between the start tag and the end tag of an XML element. Parsed character data is text without child elements.An XML Schema Definition(XSD) document is an XML based alternative to DTD.A “Valid” XML document is a “well-formed” XML document which also conforms to the rules of DTD or XSD.As discussed, HTML tags are predefined. In HTML <table> defines a table and a browser already knows how to display it. However, in XML <table> can mean anything. XML is focused on structuring and storing the data. So, we need a mechanism to define how this data should be displayed in browsers, cellphones etc. XSL (eXtensible Style Language) is the language to do that. It defines the rules to interpret the elements of the XML document.Let's take an XML document “CDCatalog.xml” defined as follows:In the second line, it's referring to an XSL document DisplayCD.xsl This document can be defined as below:Let's understand DisplayCD.xsl , line by line.The first line <?xml version=”1.0"" encoding=”ISO-8859–1""?>, defines the XML version and encoding used.The next line declared the XSL version and Namespace to be used. The xmlns:xsl=”http://www.w3.org/1999/XSL/Transform"" points to the official W3C XSLT namespace.The <xsl:template> element defines a template. The match=”/” attribute associates the template with the root of the XML source document.The content inside the <xsl:template> element defines some HTML to write to the output.The <xsl:value-of> element can be used to extract the value of an XML element. The select attribute in the example above, contains an XPath expression. An XPath expression works like navigating a file system; a forward slash (/) selects subdirectories. The XSL <xsl:for-each> element can be used to select every XML element of a specified node-set.When CDCatalog.xml is opened in a web browser, it will display the data as follows, as defined in DisplayCD.xsl:Oracle uses a new datatype, XML Type, to facilitate handling of XML data in the database. To view the data of an existing table in XML format, you can write the query as follows:The output will be as follows:XMLType can accept a Ref Cursor as a parameterTo insert the contents of an XML file into an Oracle table, follow the below steps:3. Execute the insert statement as follows:The value passed to nls_charset_id indicates that the encoding for the file to be read is UTF-8.4. This value can be selected using OBJECT_VALUE e.gXMLELEMENT is a function that returns an XML Type. It accepts 2 arguments: the first argument is the name of the tag, the second argument is the value and could be string,XMLTYPE, number or a date.This will return the employee number and name of the employee whose name contains letter ‘S’ as follows:Using XMLELEMENT, we can add xmltype with a userdefined tag name and value as employee name e.gIt generates an XMLType with tag NAME and the employee name as value:XMLELEMENT can be nested and can contain attributes:EXTRACT and EXTRACTVALUE:The EXTRACT function takes 2 arguments: an XMLTYPE element and an XPATH string and returns an XMLTYPE instance e.gTo select that data where country=”USA”, following query will be used on “catalog” table.EXTRACTVALUE is used to extract the particular value under a node. E.g if we have to find the artist name where title=’Still got the blues’, the query will be written as below:The output:EXISTSNODE:EXISTSNODE checks the existence of an XPATH expression e.g if we want to know that whether the title “Still got the blues” is present in catalog or not, we can write the query as follows:The output 1 indicates that it exists while 0 indicates that it is not present.XMLAGG:XMLAGG is used to aggregate multiple rows in a single XML document. E.g to aggregate employees in each department, the query can be written as:UPDATEXML:UPDATEXML searches for an XPATH expression and updates it. E.gThis will update all the titles to “changed” in the table catalog.This article was aimed to give the reader, a basic understanding of XML and related technologies and how XML data is handled in Oracle database. For more detailed knowledge of XML capabilities of Oracle, please refer to the below link:http://docs.oracle.com/cd/E11882_01/appdev.112/e23094/toc.htm",sql,https://towardsdatascience.com/xml-functionalities-in-oracle-4c4574942840?source=tag_archive---------1-----------------------
SQL or NoSQL?,"Let’s see how to resolve this conundrum…Let’s imagine a conversation between these two technologies:SQL: Hi ,buddy! I am table-based.NoSQL: Hey there! It seems you’re old-fashioned and too static to me. I, on the other hand, am document-based, plus I have a dynamic schema!SQL: Oh, such a Millennial! You probably can’t help yourself because you are young. Nevertheless, I already know something about JavaScript Object Notation. But despite your snark, you should be aware that I can run complex queries in a language that is widely used by almost every developer because, as you probably know, I have been around and I have a well-known predefined schema!NoSQL: That sounds impressive, Old Timer! But you are not as scalable as I am — I can easily scale horizontally.SQL: Um, sorry . . . do you know what’s the meaning of ACID? Atomicity, Consistency, Isolation and Durability. Surely you don’t . . .This conversation didn’t last forever, as they eventually decided to go together for a drink and laugh because they realized they can be very useful in different situations (and also because NoSQL just realized his name stands for “Not Only SQL”).So have we arrived at an answer? Nope, but read on! :-)From ancient times, when computers started roaming the planet, the primary model for database management was SQL (or relational databases). But nowadays, as technology has evolved and things have changed -we can’t always know in which direction that change may go- NoSQL databases are gaining prominence as an alternative to SQL. But why? Is it better? Why do we have to change something once it is stable?It is because of two things: big data, and the ability to manage tons of online activity.SQL databases are not equipped for handling “extremely” large volumes of data in the way they do (in a well-structured and organized manner). Hence, while data size goes up, its performance goes down. That’s where NoSQL comes in, as it easily scales the infrastructure for the handling a humongous amount of data (in an unstructured manner, of course!).Additionally, NoSQL is focused to be distributable. Documents may be stored on multiple nodes, and nodes can run across multiple servers and serving data in a concurrently way — as data keeps coming, just add more hardware to keep it flowing on low-cost hardware. In contrast, SQL databases require increasing the hardware’s horsepower, which can get quite expensive. Mind you, this doesn’t mean SQL is slow because, if you anticipate a scenario, SQL’s maturity has given it a lot of support with documented material. In that situation there’s probably no reason to use NoSQL.So we return to the question: SQL or NoSQL? I would say it depends on two things: your developer team skills and your budget. You may be thinking, “But could there be something else?” And I would say, “Sure!” In order to find you a better answer, let’s answer a few more questions:1) Are you working with complex queries and reports?2) Do you have a high transaction application?3) Do you need to ensure ACID compliance? (in simple terms: do you really need a 100% of data integrity?)If you answer “yes” to all three questions, then without a doubt you will need a SQL (relational database). But if you still feel unsure about how to answer the above questions plus you are thinking some of the below:1) You are uncertain how your data structure is going to look in the short-term2) Data consistency is not your top goal3) Your data needs will only grow over time. . . Then the answer is: You need a NoSQL solution. as its databases will ensure data won’t become the bottleneck when your applications are designed to be fast.Computers are binaries, while human beings are not. So, in many cases (and in my personal experience), sometimes we need both, as SQL databases can be the single point of truth in your platform, while NoSQL can be your data line of defense for serving the World Wide Web.",sql,https://medium.com/devborn/sql-or-nosql-70004c2bf69b?source=tag_archive---------4-----------------------
Magical Performance Improvements with Redis Caching in Django,"Prakhar Dev GuptaJun 1, 2020·5 min readNOTE: This post assumes that the reader has the basic understanding of Django models, views, starting the Django server, and how API endpoints are created for Class-based Views.In this article, I am going to share how you can use Django-Redis to improve your API performance in terms of DB calls and response time significantly.Often we experience that the page load speed is slow, one primary reason could be the number of queries is very high. This could be improved by using suitable SQL joins. However, we can also leverage the use of RAM in Redis to store the results of queries and then return those results from the cache in subsequent queries rather than doing the round trip to the database. This can be done when the update in the database is not very frequent. This is exactly what this article is going to cover.The idea is to reduce as much processing effort on the server as possible, which would lead to faster application performance.Redis is an in-memory data structure store that can be used as a caching engine. Since it keeps data in RAM, Redis can deliver it very quickly. There are several arguments for the superiority of Redis over Memcached. But Redis supports superior data structures that could easily fit in most common dev requirements. You can read more from HERE.To demonstrate the performance, I have created a basic Django project which has a model named Book. The image has been attached for the reference:To interact with Redis, we will install django and django-redis using pip. We will also be installing djangorestframework for building the API.pip install django django-redis djangorestframeworkThis command will install all the three requirements in one go.We need to make Django aware of the caches we will be using. Therefore this information is required to be put into settings.py file. The location should have your Redis endpoint, in my case, it is my localhost.For better maintenance, we should create a caches.py file which should maintain all the keys that are being used in your production, your custom get(), set(), and delete() functions where you can use the environment prefix to separate cache keys created in different environments. Attached here is a screenshot of the same file:In order to populate the Book table, we have used the Faker library which provides tons of utilities to generate fake data for the database.Now, we shall create two APIViews one of which will use caching and one will fetch purely from DB.For better demonstration, we have created a decorator, which adds the functionality of counting queries, displaying the involved SQL queries, and also the total time of execution of the function. Please refer to my previous article on how to create a Timing decorator.The next step is to create two endpoints, one uses the cached version and the other uses only DB as its source of information. We are using CBV for creating the API.We added two URL endpoints:Start your Django server using:python manage.py runserverOnce the server is up and running, we hit the second URL to get the uncached version of the information and it prints the following output from our decorator on the server console:Let us understand how the cached version is written for this API.“The cached version of this API first checks the data in the cache, if it finds the data, it no longer needs to perform computation on the server, and hence will return the response. If it is not present, it will go further to fetch the data from DB, then sets the response list against a cache key, and returns the data. Any further requests will be served from the cache if key is not expired or deleted.”Lets hit the cached API version, we get the following response on server console:As expected, the total time is not reduced as this is the first call, and data was not present in the cache, but this data is now set in Redis and we can also verify using redis-cli.Let’s hit the API again and check what is printed on our server console. This time the data should be served from cache only:As we can see, the overall API time reduced from ~120 ms to ~5 ms which is a huge improvement. This is because there is no computation on the server-side.The degree of improvement becomes more and more evident and essential with the increased number of DB calls, SQL joins, table size, and increased data processing logic on the app.I hope this article will help you integrate Redis caching into your existing Django project.HAPPY LEARNING!",sql,https://medium.com/@prakhar4edu/magical-performance-improvements-with-redis-caching-in-django-a31758e524ac?source=tag_archive---------5-----------------------
SQL for Beginners!,"Vududala Sai KiranJun 1, 2020·2 min readSQL is a language used for manipulating, storing and retrieving data in the database, people search through data to find insights to inform strategySome of the popular paid relational databases include Oracle, Microsoft SQL Server and Sybase. In this article, we will be using SQLite for executing Queries.To create the table, we need to run the following SQL command:To insert the data into the table:SELECT statement retrieves data from a databaseUPDATE is used to modify the existing data in the databaseDELETE is used to delete the existing data in the databaseDrop command is used to drop the existing table in the databaseThe TRUNCATE command is used to delete the data in the table but not the table itselfALTER command is used to add additional columns(data) to the tableI hope you found this article useful. It is supposed to help you get started in SQL!!Thanks for reading. I hope you found something interesting here :)",sql,https://medium.com/@saikiran37544/sql-for-beginners-9d681d95cfa9?source=tag_archive---------10-----------------------
[1]HackerRank [Weather Observation Station Solve],"Query all columns for all American cities in CITY with populations larger than 100000. The CountryCode for America is USA.Revising the Select Query IIQ.2 Query the names of all American cities in CITY with populations larger than 120000. The CountryCode for America is USA.Q.3 Select by IDQuery all columns for a city in CITY with the ID 1661.Q.4 Query a list of CITY names from STATION with even ID numbers only. You may print the results in any order but must exclude duplicates from your answer.Q.5 Weather Observation Station 4Let (N) be the number of CITY entries in STATION, and let N’ be the number of distinct CITY names in STATION; query the value of (N-N’) from STATION. In other words, find the difference between the total number of CITY entries in the table and the number of distinct CITY entries in the table.Q.7 Weather Observation Station 6Query the list of CITY names starting with vowels (i.e., a, e, i, o, or u) from STATION. Your result cannot contain duplicates.Q.8 Weather Observation Station 7Query the list of CITY names starting with vowels (i.e., a, e, i, o, or u) from STATION. Your result cannot contain duplicates.Q.9 Weather Observation Station 8Query the list of CITY names from STATION which has vowels (i.e., a, e, i, o, and u) as both their first and last characters. Your result cannot contain duplicates.Q.10 Weather Observation Station 9Query the list of CITY names from STATION that does not start with vowels. Your result cannot contain duplicatesSUBSTR(string, start, length)* https://blog.naver.com/hotzzang2004/221422400009Query all columns for all American cities in CITY with populations larger than 100000. The CountryCode for America is USA.Revising the Select Query IIQ.2 Query the names of all American cities in CITY with populations larger than 120000. The CountryCode for America is USA.Q.3 Select by IDQuery all columns for a city in CITY with the ID 1661.Q.4 Query a list of CITY names from STATION with even ID numbers only. You may print the results in any order but must exclude duplicates from your answer.Q.5 Weather Observation Station 4Let (N) be the number of CITY entries in STATION, and let N’ be the number of distinct CITY names in STATION; query the value of (N-N’) from STATION. In other words, find the difference between the total number of CITY entries in the table and the number of distinct CITY entries in the table.Q.7 Weather Observation Station 6Query the list of CITY names starting with vowels (i.e., a, e, i, o, or u) from STATION. Your result cannot contain duplicates.Q.8 Weather Observation Station 7Query the list of CITY names starting with vowels (i.e., a, e, i, o, or u) from STATION. Your result cannot contain duplicates.Q.9 Weather Observation Station 8Query the list of CITY names from STATION which has vowels (i.e., a, e, i, o, and u) as both their first and last characters. Your result cannot contain duplicates.Q.10 Weather Observation Station 9Query the list of CITY names from STATION that does not start with vowels. Your result cannot contain duplicatesSUBSTR(string, start, length)* https://blog.naver.com/hotzzang2004/221422400009",sql,https://medium.com/media-analyst-hyejin-jeon/1-hackerrank-weather-observation-station-solve-a448fdad107b?source=tag_archive---------9-----------------------
Which database should I choose for my serverless applications?,"Emrah ŞamdanJun 1, 2020·7 min readChoosing the right database has one of the most direct effects on the cost and performance of your application. A serverless environment is no exception to this principle. By considering parameters like access patterns, schema (data model), and expected performance, it’s possible to logically arrive at an optimal decision for your database type.Databases, in general, have two broad classifications: SQL relational or normalized databases and NoSQL or denormalized databases. A third classification has also arisen for specific use cases like modeling graphs, in-memory caching systems among others.But in this article we’ll explore certain aspects of SQL and NoSQL databases and take a look at the services offered by AWS for those respective types of databases.SQL, or relational databases, use Structured Query Language for working with data. It’s a powerful language and one of the most versatile options out there for handling data. You can make complicated queries for retrieval, which comes in quite handy if you are looking to perform data analysis aka OLAP (online analytical processing).SQL excels in handling multi-row transactions at scale. This type of database is the right choice if you don’t fully understand the access patterns your application may have over time and don’t expect exponential growth.However, SQL databases come with their share of drawbacks. The schema needs to be defined before you can start working with the database, requiring significant preparation and planning. Once the data model has been setup, you’ll need to maintain that structure throughout the existence of your application.Scalability can also be problematic as SQL systems have vertical scaling; adding more CPU and RAM to handle the load can become very expensive, and there is a limit to how much you can “scale up”. You can always add more servers, or “scale out,” but that requires extensive effort and comes at a steep cost.NoSQL databases unlike SQL don’t have a specific query language and champions the concept of working with denormalized data sets.Their types include:The elimination of complex joins across tables by simply putting everything in one collection with a systematic access pattern is the highlight of NoSQL databases. Their high performance, scalability, and extensibility makes NoSQL a perfect match for serverless development.NoSQL databases are known for their ability to scale out to handle tremendous OLTP (online transactional processing) work loads and manage disparate data sets. Also, changing the schema won’t disrupt your currently working application.Dynamic schemas + high performance at peak = best database option, right? Not quite so. If you are unsure of what data patterns your application entails then it’s probably best to avoid NoSQL or invest time into learning NoSQL first before shooting yourself in the foot.Think of it this way, a startup will probably benefit in getting out an MVP as soon as possible by choosing SQL databases because of the flexibility and general developer familiarity over NoSQL systems, but a NoSQL database can lead to future gains because of its very high scalability. The only hurdle is appropriate data modelling so as to service the future needs of data access effectively.Those who start with NoSQL from a SQL background without the proper understanding of access patterns may walk themselves into a corner or use NoSQL as if it was still SQL. This will also impact your application as it grows.With that being said, let’s explore Amazon’s managed versions of SQL and NoSQL services: RDS and DynamoDB, and their performance with Serverless applications.Amazon Relational Database Service (RDS) comes with six engines to pick from, MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, and their own implementation, Aurora.As explained already, relational databases follow a set pattern of structured data modeling and are governed by the SQL language for data handling. So if your decision is to go with Amazon RDS, here are a few key areas to be aware of.Your Lambda functions are pay-per-use but you will pay the hourly pricing for RDS instances unless your choice of engine is Aurora Serverless.Managing the connection limits with Lambda is important. Lambda does terminate connections on container expiration but the MAX CONNECTIONS issue still looms when the application comes under stress.Utilizing RDS Proxy, lowering connection timeouts for RDS, implementing a caching strategy, limiting concurrent connections, etc. are some ways to deal with this problem.Amazon’s Aurora database service (either MySQL or PostgreSQL) can prove to be a great option especially in terms of read performance (across regions too) owing to their ability to replicate quicker than RDS counterparts.Aurora does have clear differences in terms of performance and high availability when compared to RDS but it comes with an increased cost. Aurora should be chosen if your use case demands it. Aurora Serverless can be the next best thing for a fully serverless system but its performance isn’t quite there yet.Its main problem is dealing with cold starts. Aurora Serverless should ideally be chosen for testing or for small and inconsistent workloads.Now if DynamoDB is the choice for your application storage needs, the following is what you get.Unlike most NoSQL databases, DynamoDB is not server-based. It’s pay-per-request pricing and fits in perfectly with the Serverless way of things. This makes the idea of fully distributed serverless systems a close reality.DynamoDB can virtually have unlimited concurrent requests as long as the throughput limits have been defined.There is an additional model called PAY_PER_REQUEST which allows your DynamoDB database to scale to 0.With millisecond performance, DynamoDB can handle almost any load you throw at it.If you need even better read times, DAX (DynamoDB accelerator) is a fully managed, in-memory cache solution that Amazon offers. So performance and scalability are the least of your worries with DynamoDB.DynamoDB with Lambda seems like a great choice from a cost and performance perspective. The only hurdle is designing the table for effectively servicing the varied access patterns your application may have.Thundra’s platform for serverless observability and monitoring can further improve your costs and performance by giving actionable insights about your application. Traces with detailed information between various services of the application pinpoint areas of inefficiencies, which you can then fix. With regard to databases, especially with RDS instances, having insights about long-running queries tells you exactly which specific part of the application needs to be optimized.Let’s take an overview of what Thundra looks like. Once signed up, Thundra guides you through the onboarding process of instrumenting your Lambda functions. Your architecture and dashboard screens should look something like this.(Note: Both screens show details based on functions being grouped to a project. Unless specified, all instrumented functions will belong to “default project”)The dashboard at a glance will give you direct and important insights that need your attention.Clicking on the traces between the services in the architecture view will list out every single trace detail which on selecting will navigate to a page with detailed information about that invocation as below.As you can see in the image, we are able to gain additional insight into DynamoDB and our PostgreSQL database.You can also view the above invocation detail page from a list of Unique traces (traces that perform a specific operation in an application is regarded as a Unique trace) by navigating from the left panel. By filtering out Long-running unique traces you know which specific queries are increasing your cost and slowing down the application.A quick look at Thundra’s navigation panel on the left shows the different ways you can approach application monitoring. Thundra collects every trace between your function and other services to ensure that you have all the data needed for optimizing the application to its fullest potential.We have seen how SQL and NoSQL databases work with their strengths and weaknesses. We also took a quick look at Amazon’s database services and how monitoring them with a platform like Thundra can enhance your metric tracking capabilities for serverless optimizations.SQL databases will get the job done for most use cases, not to mention their robust data access and manipulation capabilities, provided you are not expecting millisecond performance at scale.On the other end, a NoSQL database like DynamoDB can do all the heavy lifting even at peak use without you having to worry about server management or failovers.It’s quite possible to achieve a result similar to SQL querying, but DynamoDB requires a learning curve in terms of data modeling and knowing how to index table attributes for efficient access patterns.Originally published at https://blog.thundra.io.",sql,https://medium.com/@emrahsamdan/which-database-should-i-choose-for-my-serverless-applications-a2fb1260ae98?source=tag_archive---------8-----------------------
SQL to SOQL,"Max ReynoldsJun 6, 2020·3 min readSOQL (Salesforce Object Query Language) is the language used to query data from your Salesforce Organization. Software engineers or analysts with a background in SQL may find SOQL to be syntactically familiar, but there are a few key differences to be aware of before getting started.Unlike SQL, SOQL is a language exclusively for querying the database rather than modifying data like in traditional SQL. There are no INSERT or UPDATE statements. Changing data is done using Salesforce’s UI or Apex DML, part of Salesforce’s proprietary programming language. In SOQL, Salesforce Objects are represented as SQL tables.The SELECT clause of a SOQL query is similar to SQL with one significant exception: SELECT * is not allowed; you must specify the fields to select. For example, the query inSQL:Would need to be more specific.SOQL:The most significant difference when using SOQL is its handling of JOINs. The JOIN keyword does not actually exist in SOQL. Additionally, only related objects can be queried together.Parent-to-child or child-to-parent relationships are the most common and straightforward relationships in Salesforce. We will demonstrate the concept of SOQL “joins” using the relationship between Account (parent) and Contact (child).Querying a child-to-parent relationship uses simple dot notation. Let’s compare these queries which perform the same action.SQL:SOQL:We can even traverse several relationships using this dot notation, the equivalent of joining more than two tables.SQL:SOQL:Parent-to-child relationships are queried using the child object’s plural form and a subquery within the SELECT clause.SQL:SOQL:Aggregate functions are used in SOQL similarly to traditional SQL. The available aggregate functions are COUNT() , MIN (), MAX (), AVG (), SUM (), and COUNT_DISTINCT().There is a slight difference in counting all records.SQL:SOQL:And the COUNT_DISTINCT function lets you easily identify the number of different values for a given field.SQL:SOQL:Other commonly used keywords in SQL — WHERE, LIMIT, OFFSET, GROUP BY, HAVING, LIKE, AND, OR, etc. — are supported and formatted as traditional SQL.SQL:SOQL:There are a few different options for writing SOQL queries. You can use the Developer Console Query Editor within Salesforce, Salesforce Workbench, or Inline SOQL within Apex code. I prefer Workbench over Query Editor because it displays all queryable objects and fields, and it allows you to quickly export results to a CSV file. Workbench also provides the option to write queries declaratively, which is helpful for beginners without much SQL experience.More helpful information on building SOQL queries can be found in the documentation and on Trailhead.",sql,https://towardsdatascience.com/sql-to-soql-a4b543474942?source=tag_archive---------0-----------------------
The Many Flavours Of SQL,"Kovid RatheeJun 4, 2020·12 min readWith SQL being the de facto query language for traditional databases, it has probably made its place in almost all tech stacks currently being used to deploy production grade applications. Even when traditional databases (read relational databases) are not in use, SQL inevitably shows itself in the system. Cassandra has SQL-like CQL, Spark has SparkSQL, even JIRA has JQL which resembles basic SQL clauses like WHERE, IN, BETWEEN , etc. Obviously, these clauses don’t just belong to SQL, they are also part of almost all programming languages and used in looping and conditional constructs. Not to get too far away from the point, I want to make a case that SQL is going to be around not just in the proximal future but in the distant future too.I was reading an essay by Paul Graham which talks about the languages of the future. He talks about how languages have evolved in the human timeline and how computer languages have evolved and how there is a similarity there. It’s an interesting question to ask, isn’t it?What kind of programming language will they use to write the software controlling those flying cars?— Paul Grahamwww.paulgraham.comWe never use the sentence — SQL is one of the programming languages — like we use the sentence — Python is one of the programming languages. SQL is the query language. Then there are flavours of SQL. It is the standard. But to think that it can’t change in the future would be an oversight.Having worked in this field for quite a bit now, I am aware of a lot of databases, traditional and non-traditional, that use SQL. I wanted to explore the different flavours of SQL currently in practice. Here’s a compilation of the most used, most popular flavours of SQL.towardsdatascience.comMost relational databases support the basic SQL functionality written down in the ANSI-92 SQL documentation. On top of that, every database has its own set of additional features which call for additional SQL statements to utilise those features. Let’s take a look into some of them.For the longest time, MySQL didn’t have support for analytic and window functions. It just supported basic aggregations along with GROUP BY. However, drop-in replacements of MySQL like Percona Server and MariaDB had released these features even before MySQL.Now, everything that you can do from a SQL client or a SQL terminal shall be considered SQL. For instance, in MySQL, you can run a SHOW SLAVE STATUS command to check the current status of the slave and if the replication is working or not and how far has it progressed and so on. Obviously this is not part of standard SQL and isn’t supported in the same way in the other databases. To get to know the SQL-92 standard, you can go through this 500-page document.Some of the features unique of MySQL aremedium.comUsually called the programmer’s database, PostgreSQL has established itself as a real competitor to MySQL in the open-source space with native support for advanced abstract data structures like arrays. For instance, to check replication status, you’d go and check this table — pg_stat_replication. Some of the features that make PostgreSQL unique areHere are a couple of examples of the PostGIS extension at work —What can be said about the Oracle database! It’s probably the most resilient and durable database — which is the reason why it continues to reign in time-critical businesses which require strongest ACIDity and the guarantees that come with that.Some of the unique features of Oracle databases areThe one reason why MySQL and PostgreSQL have become more popular over the years because they’re free to use — they’re open source. Oracle is the definition of closed-source. The licensing cost could kill a small company. Every business cannot afford proprietary databases like Oracle and SQL Server.The ultimate database from Microsoft which also boasts of similar features and levels of ACID support , etc., just like Oracle. Again, just like Oracle, it’s too costly for most companies. Some of the unique features of SQL Server areThese are the four main relational databases. I’m not counting DB/2, SQLite , etc., anymore because there are not enough new users for those databases. Wherever DB/2 is being used is probably a legacy system. SQLite is not feature rich and is unsuitable for production grade applications.Moving on from relational databases, let’s look into some of the databases that identify as NoSQL databases, NewSQL databases or non-relational data warehouses.CQL is Cassandra’s answer to SQL. It’s very much like SQL but again, like all the other databases you’ll see from now on, this one too has additional features pertaining to the architectural implementation which, in turn, necessitates additional features in SQL. For instance, CQL has something called ALLOW FILTERING which is explained very well hereVisit this page for the full list of features.Initially, Spark didn’t have support for SQL queries and the queries would have to be written with the Scala or Python APIs for DataFrames. SparkSQL provides another layer of abstraction to make it easy for the SQL-educated workforce to make full use of their knowledge. Here’s a pretty good introduction to SparkSQL by Databricksdatabricks-prod-cloudfront.cloud.databricks.comHere’s what a sample query would look like. You’ll see more and more of new keywords popping in with every other database for features specific to that database.Here’s an example from Rittman Mead on how to use SparkSQLwww.rittmanmead.comAlthough Spark has a lot to offer even without support for SQL, but since it has started supporting SparkSQL, Spark has gained more traction as it has opened up avenues for data analysts and data scientists who otherwise wouldn’t have had the chance to work on Spark (or would have had a steep learning curve).With the advent of the Hadoop ecosystem, it quickly got very difficult to analyze huge amounts of data because everyone wasn’t familiar with programming languages and one would have had to write a lot of lines of code to do analysis on the data. Facebook obviously had a lot of data to analyse and hence they developed this SQL-like query engine which ran on top of Hadoop’s storage system. Hive is also meant to query only structured distributed data. Here’s what a Hive query looks likeHere’s a Hive Cheat Sheet if you’re interested.InfluxDB is a time-series database for applications that require real-time querying. It’s a use case that traditional databases haven’t supported fully. Some of the new age streaming pipeline solutions do provide this feature with some limitations. InfluxDB, on the other hand, specialises in continuous queries like the followingThis query gives you the mean/average number of passengers over a period of one hour and that time period of one hour is continuously changing with every second so the metric we’re getting is live moving average of passengers over an hour. Isn’t it pretty cool?InfluxDB fits very well several high volume, realtime, critical data collection applications like IoT, stock market movement data, astronomical data and so on. Here’s a talk from David Simmons from InfluxDB who talks about IoT use cases with InfluxDB.In this talk, David has said that Inserts in InfluxDB are more than 50 times faster than on MongoDB using the same hardware. I don’t think that’s accurate. Here’s a better article to compare these two technologieswww.influxdata.comIt comes out that in comparison to MongoDB, InfluxDB is more than 2 times faster on the same hardware. Try it!Although most relational databases now support full text search but relational databases aren’t meant for extensive full text searching as it gets really costly to manage storage, memory, indexes after a while. Elasticsearch comes to the rescue as a dedicated full-text search engine.www.elastic.coHere’s an example POST call with a query payload to Elasticsearch.Other full-text search engines like Solr also support SQL syntax.The one graph database that matters, neo4j, also has great support for SQL-like querying. See for yourself. Here’s an example in regular ANSI SQLHere’s what it looks like in CypherYou can find out more about how to translate regular SQL to Cypher using the following guide loaded with simple to complex example conversions.neo4j.comGraph databases have picked up in the last couple of years with one of the main widespread applications being recommendation systems. With increasing social media based interactions, the use of graph databases is expected to increase in the future.Amazon’s key-value and document database offering currently competing mainly with MongoDB. Though DynamoDB doesn’t have native SQL support, there’s an open-source project called which provides aSQL-like external DSL for querying and scanning data in Amazon DynamoDB. You can check out the project here. Based on my experience, I have made some notes on DynamoDB, you can go and check them heremedium.comStackOverflow recently published their 2020 developer’s survey. It’s always interesting to know about what technologies people are working on around the world. Because DynamoDB is closed-source and proprietary, it doesn’t get so much attention. Open-source databases occupy the most space in the database world.insights.stackoverflow.comIt’s an open-source distributed query engine like Facebook’s Presto which was built to analyse large scale data (structural and semi-structural). Drill supports a large number of storage file formats and external systems. Drill was inspired by Dremel — Google’s closed-source large scale distributed query engine.On top of supporting regular SQL, Drill also supports language extensibility like PostgreSQL. Did you know that you can create user-defined functions using a jar file.Find more stuff about SQL support in the official documentation heredrill.apache.orgIt was one of the first proprietary in-memory columnar databases with additional support for document based and graph based data models. HANA is an ACID compliant database. It is also one of the few databases which has extensive native support for spatial, hierarchical and full-text search queries. Here’s an example of a query with hierarchical functionsThis query joins the hierarchy to the fact table and calculates various aggregate measures, originating both from the hierarchy itself and the fact table. The result set contains all nodes with a level less than or equal to 2 plus additional subtotal, balance, and total result rows.Is is one of the first players in the data warehousing business. Teradata has been active for more than 40 years now. Teradata operates mostly like a traditional relational database but it is meant for data warehousing. Like Redshift, Teradata is also a MPP system which boasts of being linearly scalable. Here’s a typical Teradata query from their documentationSnowflake is one of the most trusted cloud data warehousing platforms. Competing mainly with Amazon Redshift and Azure SQL DW, Snowflake has managed to separate storage and compute and that’s where it has an advantage. As the data analysis loads vary in a wide range, decoupling storage and compute seems like the right way to go.Here’s an example of a SQL query in snowflake. Notice that everything except the QUALIFY keyword is exactly you’d find in any other relational database with full SQL support.The documentation saysIn a SELECT statement, the QUALIFY clause filters the results of window functions.QUALIFY does with window functions what HAVING does with aggregate functions and GROUP BY clauses.One of the measures of how powerful your flavour of SQL is to go through the full feature set that the database has to offer — and find out whether your flavour of SQL is Turing Complete. But even a Turing complete SQL cannot guarantee handling specific use cases that the query DSL allows but the architecture doesn’t. For this reason, so many flavours of databases/SQL exist. Most companies use more than one of these systems to store and interact with their data at a given point in time.Applications use a mix of relational databases along with a document store but data analysts and data scientists cannot use these systems as they are. Data engineers transport data to systems like data lakes and data warehouses from the source systems so that the data can be then used for reporting and analysis. I have tried to cover most popular and loved flavours of SQL for they have all created their market because of the unique features they have to offer.Recently, I wrote about different technologies and concepts that a data engineer needs to be aware of. Some of the technologies discussed weren’t part of that piece. I have tried to incorporate the missing ones here. You can find the article heretowardsdatascience.comWe started with Paul Graham’s idea about how languages would look like in a hundred years. It is hard to say what would happen in 100 years but, with a pinch of salt, it is possible to say that SQL will probably be around, more efficient, advanced and widespread, in the 5-10 years timeframe. There’s no reason to believe otherwise. So while it might seem like a secondary thing, it’s not. SQL is very much at the centre of the software engineering and development process because, as I can’t stop saying, SQL is the language data speaks. Learn to speak SQL!",sql,https://towardsdatascience.com/the-many-flavours-of-sql-7b7da5d56c1e?source=tag_archive---------0-----------------------
Day0–10 SQL Questions Daily Challenge,"Mark LiJun 1, 2020·3 min readI started this challenge inspired by Jeffery Li to roll up my sleeves and get my hands dirty to share my SQL learning process.I have done some SQL questions before and have basic understanding of SQL, without cheating i will include those codes below as my day 0 learning materials. ( Sources from SQLZOO:https://sqlzoo.net/)-- Sum & Count is what we called as aggregate function, takes many values and delivers just one value.For example, when you codeSELECT SUM (Population) from WorldResult show asORSELECT SUM (population) From worldWhere name in ( ‘China’, ‘Japan’ , ‘Australia’)the figures show the total number in three countries.-- Incorporating Groupby such as SUM and Count are applied to groups of items sharing values. When you are specifying Groupby xxx (for example continen tin this case), the result is that you get only one row for each different value of continent, while all the other columns must be aggregated by one of SUM, COUNT.For example,SELECT continent, COUNT(name) FROM worldGROUPBY continentresult show: which tells in how many countries in each continentOR if you want to see any continent with total more than 500m population inSELECT continet, SUM(population) FROM worldGROUPBY continentHaving SUM(population) > = 500000000result shows:-- Like is similar to lookup(ctrl +F) function, helping user to locate specific keywords if you combine the proper usage of %% is what we called as the wild card and it can match any charactersSELECT name FROM worldWHERE name LIKE ‘%x%’Result show :Of Course it will not be that easy, what if you want to check the names have n has n as the second character, you can use underscore as a single character wildcard.concat is also frequently used in the SQL it adds two or more strings together.Example tells better story:That gives you:Also, replace function:That is pretty much it for the summary for my previous learning and Day0 Challenges.",sql,https://medium.com/@WeiLi39939672/day0-10-sql-questions-daily-challenge-1aa26067d06c?source=tag_archive---------11-----------------------
Distributed SQL with Dask,"Distributed SQL query execution is key in scaling the power of the RAPIDS community to the enterprise-level data challenges of today’s market.BlazingSQL makes it easy by utilizing Dask and Dask-cuDF (dask_cudf), which means you can effortlessly conduct multiple GPUs through an intuitive Python API.The easiest way to go distribute BlazingSQL is by leveraging the Dask CUDA project’s LocalCUDACluster() and Dask.distributed’s Client(), which are both downloaded with BlazingSQL by default (if you have BlazingSQL installed, then you have everything you need).Just import these packages alongside BlazingSQL, identify your cluster, and pass it’s client to the BlazingContext API upon initialization.That’s it. BlazingSQL will now distribute query execution across your GPUs and utilize the cluster’s combined GPU memory.We achieve this by calling Dask to run a BSQL Engine process on each available GPU (making a BlazingSQL cluster), and return your distributed results as dask_cudf.DataFrames (distributed GPU dataframes).Just like a dask.DataFrame, a dask_cudf.DataFrame is made up of partitions; each partition of a dask_cudf.DataFrame is essentially a cudf.DataFrame.Having that infrastructure means anything you do on a single GPU with BlazingSQL can be scaled out and done in parallel across multiple GPUs.Distributing DataFrames and computation with Dask lets you analyze datasets far larger than a single GPU’s memory without running into out of memory errors.With a dask_cudf.DataFrame being able to quickly .compute() to a cudf.DataFrame — which opens the door .to_pandas() and more — handing off your results, however they need to go, is straightforward and trouble-free.For this demo we’ll use BlazingSQL Distributed to preprocess 24 months of NYC Yellow Cab data, 342M rides in total, and handoff the results to Datashader for near-instant visualization of drop-off coordinates.Our data is stored as 24 Parquet files in a public AWS S3 bucket. Let’s register that bucket and create a table (taxi) to hold all our data;Here’s a sample of what the data looks like;All that remains now is running a query and passing the results to Datashader.Let’s define a query that will:And we’re ready to visualize. Thanks to Datashader’s support for cuDF and Dask-cuDF, we can plug bc.sql() directly in to .points(), and pass that agg to .shade() for near-instant visualization.Here’s the result:This demo can be run for free in BlazingSQL Notebooks, or in your own multi-GPU Data Science Workstation from AWS Marketplace.You’ll find the Notebook in the blog_posts folder. Just open it up & hit “Restart and Run All”.blog.blazingdb.comBecker, Nick, and Randy Gelhausen. “10 Minutes to RAPIDS CuDF and Dask CuDF.” Medium, RAPIDS AI, 19 June 2019, medium.com/rapids-ai/10-minutes-to-rapids-cudf-and-dask-cudf-3d16fcb84139.Mease, Jon. “Initial GPU Support (Holoviz/datashader#793).” GitHub, jonmmease, Oct. 2019, github.com/holoviz/datashader/pull/793.",sql,https://blog.blazingdb.com/distributed-sql-with-dask-2979262acc8a?source=tag_archive---------1-----------------------
Concurrent Operations in Databases,"Nipun JindalJun 1, 2020·10 min readThere are few features that Filesystem have that database can’t provide :But these are rudimentary features compared to features that DB provide :Concurrent access to data means more than one user is accessing the same data at the same time.With this concurrent operations, there are possibilities of Anomalies which may result in Database in in-consistent state.File system does not provide any procedure to stop anomalies. Whereas DBMS provides a locking system to stop anomalies to occur.Let’s try to get to all such cases from very basic. In as database we have two main category of operations :Database usually have many entities :Read operations are of various types, but essentially we are reading and not making any change. eg. :Write Operations are of much more concern as every change effect the other user as this is change in a shared resource i.e. database. They are of many types :Among these writes, you see there are few categories emerging, those are :If we start considering the other aspects that Database provide like : Data security then few more other types of operations come to picture, that are related to Access Control.All this leads to different sections of requests to a database :SQL (Structured Query Language) the most common format for using databases for day-to-day tasks easily is also structured in such Sections :The main idea of using database is to store our data and use it in required manner to carry out a logical unit of work. we have understood, the set of operations we can carry out, combining these operations in ordered manner constitutes “a logical unit of work”.This logical unit of work is what we commonly call TransactionTransaction is list of operations to be carried out, that’s it. there is no inherent constraint of transaction to be fulfilled without effecting other transactions. Only constraint is the Atomicity i.e. Either all operations went through OR none of them happened. This is though limited to write operations mostly, as they effect the DB for other concurrent operations. For “read operations”, we can’t un-read to put it simply.Enabling Transactions lead to new set of operations and new modules to be part of database system, that effect how query plan should be defined and how server will carry that out and how storage engine will act on it. That component for our understanding and for abstraction is “Transaction process Manager” on server (database) side. Similar “Transaction Manager” sits on the user/client side.How most “Transaction process Manager” work ? Well that be covered in later posts and its quite interesting and something I am still exploring more.How Transaction Manager work i.e. how it manages transaction : If we were to define an interface for transaction, it is only having 2 operations.To ensure the Server Side understands we are trying to do a transaction, we have a begin() & getStatus() method too as seen in JTA (javax.transaction.UserTransaction)From above, If we were able to define a Transaction, but IF we were to enable users to do transaction, then in a multi user environment where user is accessing Database concurrently, Atomicity alone can’t provide a consistent view of data.IF we desire Consistency i.e. changing database state from one valid state to other while maintaining database invariants, Let’s see what can be done to achieve it.Lets talk about what kind of Concurrency Issues we are dealing with :These concurrency issues are commonly called Anomalies.C > Commit by ith userA > Rollback/Abort by ith userWi[x] > write of data x by ith userRi[x] >read of data x by ith userX ~= pred : data based on predicate “pred”Dirty Write :Dirty Read :Non-Repeatable Reads :Lost Update :Phantom Reads :Phantom Reads says that Transaction actually read something and after few other operations, it tried to read the same data, but got either less no. of Rows [Due to deletion by other transaction] OR got more no. of Rows [Insertion by other Transaction]butNon-Repeatable Reads says that Transaction read some data and after few other operations, it tried to read same data but the rows were updated and have changed values.How can one solve given Anomaly with as limited requirements possible :Copy of data for each transaction is achieved using multi-version databases that allow given data to have a separate version for a given transaction.We see that, we are talking about putting some kind of limitations and in some way locking if required. these limitations are mostly trying to separate the data being operated upon from the original/current data in database.This separation is taken up as Isolations which help in achieving different levels/degree of Consistency.Most databases use ANSI SQL Isolation Levels :The idea behind each level is not only to limit a set of anomalies, but also to have a comparatively stronger degree of consistency guarantee too.Now that’s set most Databases are ACID compliant. What is ACID (Atomicity, Consistency, Isolation, Durability).These are properties of transaction that are intended to guarantee validity even in the event of errors, power failures, etc.(Note : Consistency here is different from what we read or it means in context of CAP theorem (Consistency, Availability, Partition Tolerance))In next articles, we will explore transaction process manager, undo/redo, maybe more of transaction data structure.",sql,https://medium.com/@nipunjindal/concurrent-operations-in-databases-4b8c3ef2ad08?source=tag_archive---------6-----------------------
How to integrate SQL Server and run SQL Script in Python?,"As a data scientist/ Analyst, most of the time we need to make connections with different tools to solve our business problem. most often we need to get our data from a relational database that is hosted either on your local server or in the central database that is on the cloud. so how we can extract data from the database, one way is we will be extracting the data by running the SQL script and save the data on excel and import that excel file into Python. However, the most efficient way to use SQL directly in Python.SQL stands for Structured Query Language, which is a database language used for extracting /manipulating or querying data stored in a relational database management system (RDBMS). There are other options for handling such data, but SQL has been the most popular, widely used language in the industry.As you know we need to get our data from the database so one way is we will be extracting the data by running the SQL script and save the data on excel and import that excel file into Python. another way which is the most efficient way to use SQL directly in Python.There are many libraries that have been developed for this purpose that can be used. In this tutorial, I will be using the Pyodbc library with integration with Pandas.To install pyodbc library run below code, this command automatically downloads and installs the libraryin this step, we will import important libraries that we will be using throughout this tutorial.Once the library is imported, we need to get the following details.Next, obtain the database name in which your desired table is stored.Now you’ll need to get the name of your desired table where the data is stored and then after gathering the details update the details in below formatCreating the connection and cursor.Now that you are connected to the database, you can submit data queries The two primary clauses that must be present in every query are SELECT, and FROM.SyntexSELECT column1, column2, …FROM table_name;After getting the response from the database we need to put that in the pandas data frame.The above statements cover the basics of SQL in Python. Similarly, you can write e many more SQL statements that can be used in python, for more SQL details you can refer to this linkThere are many libraries that can help us to interact SQL with Python, I believe the integration with multiple tools can be really productive and can give amazing results.I hope this article will help you and give a different perspective. Let me know if you have any suggestions.Happy Learning.Prabhat Pathak (Linkedin profile) is an Associate Analyst.",sql,https://blog.devgenius.io/how-to-integrate-sql-server-with-python-and-using-sql-script-in-python-a20bcbea6def?source=tag_archive---------1-----------------------
Climbing B-Tree Indexes in Postgres,"In computer science, a B-tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. Postgres has a few index types but B-trees are by far the most common. They’re good for sorting and matchingFull Article: http://www.entradasoft.com/blogs/climbing-b-tree-indexes-in-postgresHere is a quick overview of what types of index(s) available to use in your tables:· B-Tree: The default for Postgres.· GIN: For JSONB and arrays. As inverted indexes, they contain an index entry for each word, with a compressed list of matching locations.· GiST : For full text search and geospatial datatypes. A GiST index is lossy, meaning that the index might produce false matches.· SP-GiST : For larger datasets with natural but uneven clustering· BRIN (Block Range Index) : For really large datasets that line up sequentially. E.g. orders might have a date column, and most of the time the entries for earlier orders appear earlier in the table.· Hash: The query planner will consider using a hash index whenever an indexed column is involved in a comparison using the = operator.Basically, it’s a data structure that sorts itself. That’s why it’s self-balancing — it chooses it’s shape on it’s own. The B-tree is a generalization of a binary search tree in that a node can have more than two children. Unlike self-balancing binary search trees, the B- tree is optimized for systems that read and write large blocks of data.B-trees are “balanced” because the distance from the root to any leaf node is the same. A leaf node is a node without children. The root node is the node at the top. B-Tree is a tree where each node can have multiple children, or better said, a B-Tree can have N children. While in binary search trees each node can have one value, B-Trees have the concept of keys. Keys are like a list of values, that each of the nodes will hold.The most important thing about B-Trees is their balancing aspect. The concept revolves on the fact that each node has keys, like in the example above. The way B-Trees balance themselves is really interesting, and the keys are the most important aspect of this this functionality.Basically, whenever a new item (or, in our case, a number) is added, the B-Tree finds the appropriate place (or, node) for the item to go in. For example, if we want to add the number 6, the B-Tree will “ask the root node”, in what node should it push the number in. “Asking” is nothing more than comparing the new number with the keys of the node. Since the number 6 is larger then 5, but smaller then the number 10 (which are the root node keys), it will create a new node just below the root node:With this mechanism, the B-Tree is always ordered and looking up a value in it is rather cheap.Now if we wanted to look up the key, 6, we’d compare 6 to values in the root node and see that it’s between 5 and 10. So we’d use the pointer between 5 and 10 to find the node containing 6.This is an abstraction over Postgres’ implementation but you can already imagine why this is faster than iterating over every number in a table and checking if it equals 6.This is why B-trees can search, insert and delete in O(logN) time.Almost in all use cases, the power of indexes is noticeable on large amounts of data. This means that the indexes will have to be as big as the actual data tables. Or does it?Imagine if we are dealing with billions of records. This means that the index tables will have the billions of records stored in an ordered fashion. Okay, PostgreSQL could handle that. But, can you imagine how long would an INSERT command take? Adding the record in the data table will take really long, because the index will have to add the new record in the correct place, to keep the order of the indexes. Due to this limitation, the implementation of the B-Tree index keeps page files, which simply put, are nodes on a big B-Tree data structure.Syntax:— Create an index column using btree on the purchase table — CREATE INDEX <<index_name>> ON <<table_name>> USEING <<index_type>>Read More… http://www.entradasoft.com/blogs/climbing-b-tree-indexes-in-postgres",sql,https://blog.devgenius.io/climbing-b-tree-indexes-in-postgres-f82447a2d29c?source=tag_archive---------6-----------------------
Visualizing Police Violence,"AlexJun 9, 2020·2 min readSome time ago I wrote a brief article about a Washington Post dataset compiling all fatal police shootings since Jan 1, 2015. In that post I showed evidence of racial disproportionality in fatal shootings. Young Black men in particular were being shot and killed by police at higher rates than their White (and Asian) counterparts. In the wake of George Floyd’s murder in Minneapolis, I created an interactive dashboard so users can explore the data referenced in my post, which is hosted here. With this app, users select a state and are shown statistics about the rate, number, and demographics of fatal police…",r,https://medium.com/@gymnosophist/visualizing-police-violence-1b0f736fe2f9?source=tag_archive---------3-----------------------
The “confintr” package is on CRAN,"R is a fantastic statistical programming language. There is one tiny little thing however I never liked so much from a didactic point of view: In order to access confidence intervals provided in the stats package, you need to call a corresponding *.test function. Two examples:This is a bit unfair since confidence intervals bear strictly more information about a parameter than a hypothesis test…Fortunately, there is a new package confintr on CRAN where you can focus on confidence intervals rather than on tests: It offers both classic and bootstrap confidence intervals for a whole bunch of parameters:All functions follow the same interface, namely ci_mean(), ci_quantile() etc. Furthermore, the package uses as only dependency the famous boot package, so its is very light to install.Here a teaser.You can calculate two-sided (symmetric/asymmetric) or one-sided confidence intervals, and for most parameters, we offer different type of bootstrap confidence intervals.So if you are e.g. interested in a lower confidence limit for the population R squared, you can type:You can be about 95% certain that the true R-squared is at least 83%. This is greater than 0, so the corresponding F test would provide a p value below the magic 5% limit.Confidence intervals for quantiles, e.g. a 90% confidence interval for the upper 5% value at risk? No problem:For additional information, have a look at the vignette.In the future, the package will be extended to more parameters, so give it a try!For further sweet things you can do with R, check out R-bloggers.",r,https://medium.com/michael-mayer/the-confintr-package-is-on-cran-692973258e53?source=tag_archive---------5-----------------------
COVID-19 Data Acquisition in R,"Emanuele GuidottiJun 7, 2020·9 min readNote from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.Built with R, available in any language, COVID-19 Data Hub provides a worldwide, fine-grained, unified dataset helpful for a better understanding of COVID-19. The user can instantly download up-to-date, structured, historical daily data across several official sources. The data are hourly crunched and made available in csv format on a cloud storage, so to be easily accessible from Excel, R, Python… and any other software. All sources are properly documented, along with their citation.In this tutorial we explore the R Package COVID19: R Interface to COVID-19 Data Hub.The data are retrieved with the covid19 function. By default, it downloads worldwide data by country, and prints the corresponding data sources.To hide the data sources use verbose = FALSEA table with several columns is returned: cumulative number of confirmed cases, tests, recovered, deaths, daily number of hospitalized, intensive therapy, patients requiring ventilation, policy measures, geographic information, population, and external identifiers to easily extend the dataset with additional sources. Refer to the documentation for further details.By default, the raw data are cleaned by filling missing dates with NA values. This ensures that all locations share the same grid of dates and no single day is skipped. Then, NA values are replaced with the previous non-NA value or 0.Example: plot confirmed cases by country.Example: plot confirmed cases by country as fraction of total population.Filling the data with the previous non-missing data is not always recommended, especially when computing ratios or dealing with more sophisticated analysis other than data visualization. The raw argument allows to skip data cleaning and retrieve the raw data as-is, without any preprocessing.The package relies upon publicly available data from multiple sources that do not always agree, e.g. number of confirmed cases greater then number of tests, decreasing cumulative counts, etc. COVID-19 Data Hub can spot misalignments between data-sources and automatically inform authorities of possible errors. All logs are available here.Example: plot confirmed cases by country as fraction of tests.Example: plot mortality rates by country.Retrieve the snapshot of the dataset that was generated at the end date instead of using the latest version. This option ensures reproducibility of the results and keeps track of possible changes made by the data providers. Example: retrieve vintage data on 2020-06-02.Example: compare with the latest data for United Kingdom.The argument country specifies a vector of case-insensitive country names or ISO codes (alpha-2, alpha-3, numeric) to retrieve.Example: plot the mortality rate.The data are available at different levels of granularity: admin area level 1 (administrative area of top level, usually countries), admin area level 2 (usually states, regions, cantons), admin area level 3 (usually cities, municipalities). The granularity of the data is specified by the argument level. Example: retrieve data for Italian regions and plot the mortality rate.The package allows for cross-country comparison. Example: plot mortality rates for Italian regions and Swiss cantons. Depending on the country, city-level data (level 3) are also supported.National-level policies are obtained by Oxford Covid-19 Government Response Tracker. Policies for admin areas level 2 and 3 are inherited from national-level policies. See the documentation for further details.Example: load US data, detect changes in the testing policy, and plot them together with the mortality rate.The dataset can be extended with World Bank Open Data via the argument wb, a character vector of indicator codes. The codes can be found by inspecting the corresponding URL. For example, the code of the GDP indicator available here is NY.GDP.MKTP.CD.Example: extend the dataset with the World Bank indicators NY.GDP.MKTP.CD (GDP) and SH.MED.BEDS.ZS (hospital beds per 1,000 people).Example: plot the mortality rate in function of the number of hospital beds.The dataset can be extended with Google Mobility Reports via the argument gmr, the url to the Google CSV file.Example: detect changes in the Italian transport policy, and plot them together with the mortality rate, percentage of confirmed cases, and Google mobility indicators. Depending on the country, regional or city-level mobility data are also supported.The dataset can be extended with Apple Mobility Reports via the argument amr, the url to the Apple CSV file. Depending on the country, regional or city-level mobility data are also supported.Example: detect changes in the Italian transport policy, and plot them together with the mortality rate, percentage of confirmed cases, and Apple mobility indicators.The structure of the dataset makes it easy to replicate the anlysis for multiple countries, states, and cities, by using the dplyr package.Load the data.Define the function to apply to each group (e.g. country), must return a data frame. For example, the function could compute the R0 for each country or region. For the sake of simplicity, the following example is limited to extract the latest number of fatalities for each group.Define the groups, apply the function to each group, and bind the results.Print the first rows.The covid19 function uses an internal memory caching system so that the data are never downloaded twice. This is especially suited for interactive frameworks, such as Shiny. See How to Build COVID-19 Data-Driven Shiny Apps in 5mins.COVID-19 Data Hub is free software and comes with ABSOLUTELY NO WARRANTY. Please make sure to agree to the Terms of Use.[1] Guidotti, E., Ardia, D., (2020). COVID-19 Data Hub, Journal of Open Source Software, 5(51):2376",r,https://towardsdatascience.com/r-interface-to-covid-19-data-hub-244ae056af2c?source=tag_archive---------2-----------------------
Installing rtweet on Raspberry PI Zero W,"JoshuaJun 7, 2020·3 min readIf you have read my previous story on the setup of R and installing packages on the Raspberry PI Zero W you know that it is not so [as] easy and very time consuming. This is because of the limited ressources of the single-board computer. You will encounter with the same difficulties when installing rtweet on a Raspberry PI Zero W. Furthermore, some dependencies of the rtweet package need special treatment.This can take up to ~12min.The package ‘quanteda’ has dependencies to a lot of different packages which will be automatically installed. Therefore make sure your internet connection is stable and your swap file is set.This can take up to ~270min. So you can easily do something else.Installing the package ‘topicsmodels’ is a little bit more difficult. First, you have to install the GNU Scientific Library (GSL). GSL is a numerical library for C and C++ programmers. You can download the 1.6 GSL Version here. If you are using the Raspberry PI Zero W via SSH you can transfere the downloaded file with WinSCP.Before installing GSL make sure to close R (Ctrl + d). After downloading and transfering the GSL .tar.gz file you can navigate to the folder on your Raspberry PI Zero W:Furthermore, you have to install the following other packages:After successfully installing GSL you can now start installing 'topicmodels'. Also make sure that the swapfile is still in place.This whole process can take up to ~35min.This can take up to ~60min.There are several good ressources where you can find usefull information on how to get started with rtweet. I definitly can recommend you the following links:Good luck and have fun!",r,https://medium.com/@jobisch/installing-rtweet-on-raspberry-pi-zero-w-bd6226d5e74b?source=tag_archive---------4-----------------------
The Essential dplyr,"Jesse LibraJun 4, 2020·8 min readThis cryptically named package is a lifesaver for cleaning, organizing, and looking at data in R. Many professors wait to teach dplyr in favor of teaching basic R methods as a foundation, but that doesn’t mean you have to wait to learn it. In this article, we’ll look at some of the most time-saving dplyr functions and learn how to use them to manipulate datasets.Dplyr is a package written by Hadley Wickham and Romain Francois to help you manipulate raw tabular data into a neat dataset ready for analysis. In his 2014 paper on the package’s precursor, plyr, Wickham wrote:It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data.Prepping data still takes time, but we can spend significantly less than our predecessors thanks to dplyr and other tidyverse packages. In this article, I’ll use Mexico City’s air quality dataset as a base for showcasing the following key dplyr functions:We’re going to look at the functions in the dplyr package by performing some preliminary data manipulation on an air quality dataset for Mexico City. The code below will download data for particulate matter (PM) concentrations from Mexico City’s air quality monitoring website from January 2016 to April 2020. The for-loop allows you to specify the years included in your data by editing the z vector.Let’s start with the question. I want to know if April 2020 had better air quality than previous years due to the coronavirus lockdown. This is a pretty complex question, but to get a quick view of whether this is worth delving into I’ll compare 2020 monthly means to monthly means from previous years.Let’s look at the current state of the data by changing it into a tibble:The tibble looks better than a regular data frame: on typing its name, R gives you the dimensions, a 10-row preview of the data, and the classes of each column. With respect to the actual data in the dataset, we can see that it contains daily measurements of PM10 and PM2.5 in micrograms per cubic meter at a number of stations throughout Mexico City.To get at our question, we’re going to have to do some processing.Below, I used the mutate command to create a new date field which is in date-time (POSIXlt) format and to create separate fields for the month and year. I filtered the data via the filter command to get rid of any NAs, and then I reordered the fields using select. I used rename to rename my datetime field, datex, as “date”.Something else you may have noticed is the %>% operator. This operator is loaded with the dplyr package and roughly translates to “then”. It allows us to apply the code of the next line to the pm dataset, rather than having to rewrite the dataset name in every function and assign our changes to intermediate variables along the way. This process is called piping.Without it, the code would look like this:The results are the same, but using the %>% operator got us there in fewer keystrokes and required the creation of fewer intermediate variables, keeping our work space neat.Now that my dataset is a little cleaner, I want to look at some summary information for the value field.This summary doesn’t tell me much because it includes both PM 10 and PM 2.5 values. PM 10 and PM 2.5 have different behaviors, health impacts and regulatory thresholds, so it doesn’t make much sense to analyze them together.Fortunately, dplyr has an easy solution. To separate my data into groups I can use the group_by function, grouping by the id_parameter field. I also know that I want to compare the means of each month of 2020 with the means of the same months for past years, so I’ll also group by month and year. This won’t have any visible effect on my pmclean dataset, but if I create a new table using summarize, the functions listed will be calculated based on these groups.Here I summarize the value field by mean, median, min and max values, and assign it to the pmsummary data table. Notice that the assignment is placed at the top of the pipe.The result is the following tibble. You can see that the mean, median, min and max are calculated for every unique combination of month, year, and parameter.Arrange does essentially what sort does in Excel. Let’s use it to compare the mean contaminant concentrations for the first four months in 2020 with the same months in previous years. To do this, we’ll use filter to get the parameter and months we want, and then we’ll arrange the results by the month and then the mean. I don’t do this here, but you can use the desc function within arrange to arrange your data in descending order.Here we can see that April 2020 did indeed have a lower mean concentration of both PM 10 and PM 2.5, but so did January and February 2020, before Mexico City entered coronavirus lockdown. Let’s look at the information visually.This makes it much easier to see the trend from month to month. Interestingly, mean PM 10 concentration in March 2020 was pretty average when compared to other years, and then drastically dipped in April. For PM 2.5 we see a slight increase from March 2020 to April 2020. While far from definitive, I would say the question warrants further analysis.There are a number of join functions that come with dplyr. They include your standard inner, right, and left joins, as well as functions that let you filter your table based on other tables like anti_join(), semi_join(), and even nest_join, which lets you nest tibbles of data from one table within rows of another.Let’s say I want to look at my air quality data at the station level. My data at the moment is broken down by stations but I don’t know much about what these station labels mean. Lucky for us, Mexico City also publishes their station catalog:Their station data provides us with the latitude, longitude and elevation of each station. Let’s use one of the join dplyr functions to add this information to our main dataset.Every value in our pmclean dataset should have a station associated with it (the station where it was measured) and every station in our dataset should be in the Mexico City stations catalog. This means we can do an inner join. We want to join on station abbreviation, which is id_station in our pmclean dataset and cve_estac in our stations dataset. Let’s fix that and do the join.You can see that because I renamed my join field in the stations dataset to match the one in my pmclean dataset, the join operation doesn’t need me to specify which fields I was joining on; it automatically selects the matching fields.Cleaning and manipulating data isn’t the sexiest part of data analysis, but it is a critical process that doesn’t have to be painful. Play around with the dataset we’ve looked at today or use one of the datasets that comes with dplyr to get comfortable with these functions. Once you’ve got them in your toolkit, you’ll spend less time and tears figuring out how to manipulate your data and more time doing data science.All of the code used in this article can be found here.",r,https://towardsdatascience.com/the-essential-dplyr-cdf3057c1c6c?source=tag_archive---------0-----------------------
How to install ROracle on Windows 10,"For data analysis, it is convenient to have a big dataset. To handle data efficiently, it is best if the data remains inside the database. This article shows how you can set up RStudio together with ROracle and your Oracle Autonomous Database.To start with your installation, you have to know your local architecture — Either 32- or 64-bit. This guide uses the 64-bit installation files.First, download the basic oracle instant client that fits your architecture.You will also need to download the instant client SDK that matches your basic instant client version. Here we use version 19.6.0.📁Extract both downloaded ZIP files and put the SDK directory from the instant client SDK in a subdirectory of the extracted basic instant client directory at the same level as the vc14 folder. A screenshot of the target structure is shown below. ⏬Copy the assembled instant client directory to C:\instantclient or to another path you prefer.Download and install RTools.cran.r-project.orgDownload your wallet, unzip it and copy it inside the instant client or to another path you prefer.🧙‍♂️Set OCI_LIB depending on your architecture of 64- or 32-bit.🧙‍♂️Set OCI_INC to the SDK-include folder and ORACLE_HOME to your instantclient if you don’t have another local Oracle installation.🧙‍♂️Set TNS_ADMIN to the location of your extracted wallet.🧙‍♂️Set RTOOLS40_HOME to the location of your RTools installation.🧙‍♂️Set the PATH variable to include the location of oci.dll what is usually on the top level inside the instant client. For this example the path would be C:\instantclient .Now download and install RStudio. In this setup, the free version of RStudio Desktop 1.3.959 was used.rstudio.comWhen you start RStudio you can check with the following commands, if the environment variables are set correctly. ⏬If you have to change the environment variables, you need to restart RStudio before the changes are in effect. 💨Now install version 3.6.0 of R. Not all of the versions of R work with the ROracle versions. The compatible versions are listed at the ROracle download page under every ROracle version (see screenshot below).cran.r-project.orgDownload the ZIP file with version 1.3–2 of ROracle that is compatible with version 3.6.0 of R.www.oracle.comTo install ROracle go into RStudio and execute the following commands.💬 First set the current working directory of RStudio to the path where your ROracle ZIP file is stored.After changing to the right directory, install the ROracle package. If everything is set up correctly, you should see the following output in RStudio.To check if you have all the right credentials, check your database connection with SQLDeveloper or another IDE.📝Edit the sqlnet.ora file to contain your wallet location.📋 Go to your tnsnames.ora file inside your wallet and copy the connection string that you want to use (or the one with the _high ending) and replace the string between the ' 'in the following query:A second (simpler) solution would be to use the connection string alias from your tnsnames.ora.🔀 Open RStudio and enter the following commands to setup the connection.❔Then connect to your oracle database with the dbConnect command.❓ Create a simple query to test if the connection works:If you don’t have any data inside your database yet, you can test the connection also with dbReadTable.If you have problems executing ROracle commands, try to run RStudio as administrator.➡️ Installing ROracle and setting up the local environment to work with the cloud database in RStudion wasn’t easy. 💬 I hope this installation guide helps others to save a lot of time when they want to get startet with R and the Oracle Database.Please support this article with your claps👏👏👏 to help it spread to a broader audience. 💭 Don’t hesitate to ping me if you have any thoughts or questions on the subject! I’m on Twitter → @jasminflurihttps://cran.r-project.org/web/packages/ROracle/ROracle.pdfhttps://download.oracle.com/otn/nt/roracle/ROracle.pdftechnology.amis.nlcran.r-project.orgrstudio.comwww.oracle.comcran.r-project.org",r,https://medium.com/analytics-vidhya/how-to-install-roracle-on-windows-10-144b0b923dac?source=tag_archive---------0-----------------------
Interactive Power BI Custom Visuals with R,"Freddie TaskerJun 2, 2020·6 min readSometimes the right Power BI visual just isn’t there. Trying to create an executive summary of project management data recently I hit just this wall.The idea was a report page that would be a front page to users - a quick signpost before they got into the detail of the other report pages, and something that would easily translate to a slide in a presentation. Attempts at slimmed down Gantt charts were all messy and unwieldly.After a bit of hard thought (playing) a basic horizontal timeline seemed to work well. It would focus on the project milestones, use colour to describe the project status and try its best to get the project related text as readable as possible.There didn’t seem to be a ready made visual in Microsoft’s AppSource that would do this, so turned to the R Script Visual. With a bit of R knowledge, these are quick to develop and allow the use of the endlessly customisable ggplot to get it looking exactly how you want it (n.b. other R plotting packages are available).Here’s how it ended up. Project data has been replaced with the UK’s prime ministers and the project status colour now tells us what party they’re in. It might be handy in your next Zoom quiz. Code here. Thank you to Ben Alex Keen, who has a great article on the construction of this kind of timeline in R.The R visual dropped into the report as expected. Moving from accessing the JSON data directly in R to dealing with the data from the Power BI R Script Visual needed some variable renaming and formatting .The Power BI friendly code for the visual is here.The only thing these R plots are missing is a bit of interactivity. You can restrict the data going into them with Power BI’s slicers and filters but you can’t hover over and get a tooltip, or zoom in on an interesting bit.Thanks to the plotly package , we can turn our ggplot object into some html and add at least some of this interactivity.Plotly will also host these charts, and you can push them up from within R.chart-studio.plotly.comAs well as a host of other plotly features, we now have a hover over tooltip. Really useful in providing some more information in this fairly compact graph. This was defined using a dummy aesthetic in the geom_text call, named ‘text’ (ggplot ignores this aesthetic).A quick side note on using ggplotly for this task.geom_label: In our conversion to plotly from ggplot using ggplotly, geom_label isn’t supported, so the transparent rectangles behind each text label have been replaced with font coloring.Legend naming: There was also some strange behaviour with the legend, ‘Whig’ became ‘(Whig,1)’ etc. This was caused by the ggplot object having both a colour scale (for text and points) and a fill scale (not used, since no geom_label). The manual fill scale was removed, but it would be annoying if a fill scale was needed. The only solution seemed to be manually adjusting the plotly object json.The final step is getting this plotly object into Power BI.We can’t just paste our R code directly into the R Script Visual. The ggplotly function outputs an HTML object and Power BI is expecting an R plot object.Well it turns out that with not too much work, we can create our own custom visual to import into Power BI. Here’s a quick step by step.1 - Install node.js2 - Install the powerbi-visuals-tools package with Powershell3 - Create an rhtml template4 - Find the folder and files that this command creates (this depends on the working directory of your Powershell). Then find the script.r file, and replace the ggplot code with your ownOn my computer the rhtml files were output to C:/Users/FredTasker/fredTimelineVis. The script.R has an example ggplot chunk, and a ggplotly line, which I replaced with those above.A couple of notes:Here’s the script.R file used in this example.5 - Add author, description and support URL to the pbiviz.jsonThis is important, as package compiling will fail if you don’t. pbviz.json will be in the same directory as your scripts.R file.6 -Go back to PowerShell, navigate to your script.r directory and package it all up, ready for Power BI to consume.In my case:If you run into issues,here’s the Microsoft help page for R HTML.Now that the custom visual is packaged, we just need to import it into Power BI and drop our data fields into the visual.Well this was all very exciting. A really quick way to build interactive custom visuals in Power BI.Have a look at the resulting dashboard on the Power BI service, you’ll see that Power BI’s Publish to Web doesn’t currently support the static R visual. To see the static graph in the PBIX file and all other scripts mentioned here head to GitHub.",r,https://towardsdatascience.com/interactive-power-bi-custom-visuals-with-r-a6a4ac998710?source=tag_archive---------0-----------------------
"R programming: The Simplest, Universal Way for Importing Dataset by RStudio","Ho, Yu-FengJun 11, 2020·4 min readSince you have started learning R, you probably learned how to insert data using one line code like this:or some tutorials refer to insert the data only if you set in the right the working directory. For example:However, there are too much different types of code for inserting that is hard to grasp, such as read.table(), delim(), read_excel()….. and so on.Depending on the type of dataset, sometime you would have to library()a certain package. Moreover, some datasets need to note (header = FALSE), (sep = "","")or (skip = 1). It is quite difficult for R beginners to adjust code several times for a prefect data frame insertion. Nevertheless, using the convenient function in RStudio interface, you can insert a dataset simply and flexibly.The following steps would introduce you the simplest and universal way for importing dataset by R Studio”Note: the version might not influence much, so don’t worry you possess different version from mine(my version is 1.1.456) .Based on the file type that you want to insert, you can choose the options in menu for inserting.For example:.txt is [From text (base)].xlsx is [From Excel]Generally the file type would be .csv ,.txt or.xlsx. Of course, it also can import dataset from SPSS, SAS or the other statistical softwares.Next, I would use file type:.dat for instance.Scroll down the menu and click [From text (reader)], and the interface would become like this picture below.Then, click [Browse] to search the target file.After finding the file, open it.At this interactive interface, most commands could be added here. (col_types=...), (skip = ...) ,(header = FALSE) ,(sep = "","") as mentioning before could all be marked. Every change could be seen at [Code Preview].At this stage, click the right top copy button or highlight all the code in [Code Preview], use Ctrl+C (command+C) to copy, and paste it to the R script.Run the code, and the data would be inserted as “Dataset” in the R environment.Until now, the tutorial is done.The advantage of this method not only can insert the data easily and flexibly but also the code in R script can be further modified, combined with the other code. Which let the data insert process become more automatic. For instance, it could be written in a function like this:So that, it could be applied for inserting data frame in the same pattern at one time. Furthermore, if the other data frame is just slightly different, you can directly revise the code in the script.To sum up, rather than writing code for importing dataset in R, R Studio provides the R user with a user-oriented design method of data insertion. Since that, you don’t have to worry about remembering every code for inserting different kinds of data!",r,https://medium.com/@supermonk00/r-programming-the-simplest-universal-way-for-importing-dataset-by-rstudio-70261026b599?source=tag_archive---------2-----------------------
Time Series Forecasting in R,"François St-AmantJun 13, 2020·7 min readPython is great, but when it comes to forecasting, I personally think R still has the upper hand. The Forecast package is the most complete forecasting package available on R or Python, and it’s worth knowing about it.Here is what we will see in this article:For every method, we will build a model on a validation set, forecast with it for the duration of the validation set and compare the forecast with the real observations to obtain a Mean Absolute Percentage Error (MAPE).The data used will be coming from the Air Passengers Dataset, available on R.Any forecasting method should be evaluated by being compared to a naive method. This helps ensure that the efforts put in having a more complex model are worth it in terms of performance.The simplest of all methods is called simple naive. Extremely simple: the forecast for tomorrow is what we are observing today.Another approach, seasonal naive, is a little more “complex”: the forecast for tomorrow is what we observed the week/month/year (depending what horizon we are working with) before.Here is how to do a seasonal naive forecast:That gives us an MAPE of 27.04%. That’s the score to beat. By the way, remove the s from “snaive” and you have the code for simple naive. Here is how to plot the forecast:We see that what happened in the last year of data is repeated as a forecast for the entire validation set.There are many ways to do exponential smoothing. The idea is always to have a declining weight given to observations. The more recent an observation, the more importance it will have in our forecast.Parameters can also be added. You can for instance add a trend paramenter (Holt method) or add a seasonality (Holt-Winters).With the Forecast Package, smoothing methods can be placed within the structure of state space models. By using this structure, we can find the optimal exponential smoothing model, using the ets function.We see ETS (M, Md, M). This means we have an ets model with multiplicative errors, a multiplicative trend and a multiplicative seasonality. Basically, multiplicative means that the parameter is “amplified” over time. For instance the trend is getting bigger and bigger as time goes by (our case here).Here is how to forecast using the estimated optimal smoothing model:We see that the upward trend in demand is being capture a little bit (far from perfect, better than naive). It gives an MAPE of 12.6%.The ets function is good, but it only allows for one seasonality. Sometimes, the data we have can be composed of multiple seasonalities (monthly and yearly for instance).Double Seasonal Holt-Winters (DSHW) allows for two seasonalities: a smaller one repeated often and a bigger one repeated less often. For the method to work however, the seasonalities need to be nested, meaning one must be an integer multiple of the other (2 and 4, 24 and 168, etc.).The code here is a bit different since we need to specify the lenghts of our two seasonalities (which is not always something we know) and the forecast is computed directly when creating the model with the dshw function.We get a MAPE of 3.7% with this method!DSHW is good, but some processes have more complex seasonalities, which our previous functions cannot handle. Indeed, you could have both a weekly and yearly seasonality. You could even have more than 2!BATS and TBATS allow for multiple seasonalities. TBATS is a modification (an improvement really) of BATS that allows for multiple non-integer seasonality cycles.Here is how to build a TBATS model and forecast with it:We get a MAPE of 12.9% for this method.ARIMA models contain three things:That’s it for ARIMA but if you know the data you have is seasonal, then you need more. That’s where SARIMA comes into play.SARIMA adds a seasonal part to the model.P represents the seasonal AR order, D the seasonal differencing order, Q the seasonal MA order and m the number of observations per year.The auto.arima function can be used to return the best estimated model. Here is the code:The function returned the following model: ARIMA(0,1,1)(1,1,0)[12].To forecast a SARIMA model (which is what we have here since we have a seasonal part), we can use the sarima.for function from the astsa package.We get a MAPE of 6.5% with this SARIMA model.Just so you know, we could in theory complexify things even more by adding exogenous variables (explanatory variables) to an ARIMA/SARIMA model, which would make it SARIMAX.For this data, DSHW gave the best results. Keep in mind however that no model does best all the time.In all the previous examples, I forecasted 5 years into the future. However, if you want to forecast on a daily basis, why would you use a forecasted value from 5 years ago when you could use the real observations to predict tomorrow?The idea of setting up a one-step-ahead forecast is to evaluate how well a model would have done if you were forecasting for one day ahead, during 5 years, using latest observations to make your forecast.Simply put: instead of forecasting once for the 60 months ahead, we forecast 60 times for the upcoming month, using latest observations.Coding this is quite simple. All we need is to iteratively add the latest observation to the training dataset, forecast from there and repeat. Here is the code to do it using the SARIMA model we found earlier.As you see, for every iteration, the training set grows by one observation, a new forecast for n.ahead=1 is computed and results are stored in the one_step_ahead_sarima matrix.Here is how to get the plot:We get a MAPE of 4.1%, which is an improvement of more than 2% on what we had when using the same SARIMA model to forecast 5 years ahead.That’s not surprising considering that the shorter the forecasting horizon, the better the forecast should be. That is why you should always set up a one-step-ahead forecast if it is possible!I hope this helped. Thanks a lot for reading!To become a member: https://francoisstamant.medium.com/membership",r,https://towardsdatascience.com/a-guide-to-forecasting-in-r-6b0c9638c261?source=tag_archive---------0-----------------------
Portfolio analysis of Indian stocks in 10 minutes,"When I was new to the field of financial analytics, one of the biggest challenges I used to face while attempting to analyse a stock was finding a step-wise, learner-friendly approach which could be easily understood by a newcomer.But, finding such an approach is often time-consuming and the current content in the books may be difficult for new learners to grasp. The biggest hurdle isn’t the complexity of the code or the finance behind the approach; it is where to look at and what to look for. In this post, we shall look at the ways to overcome that challenge by using a hands-on approach rather than getting into too many equations and technicalities.Irrespective of whether you’re from an analytics background or a political science background, this post is for you. You don’t need knowledge about coding, mathematics, finance or even installation of necessary software. This post covers it all.“TELL ME AND I FORGET. TEACH ME AND I REMEMBER. INVOLVE ME AND I LEARN”— Benjamin FranklinKeeping in line with the above quote, let’s get this hands on activity started:Portfolio analysis will give you a clear picture of how your stock has been performing of late and how it is expected to perform in the near future as per the current trends. To keep things simple, we will only look at:1. Sharpe Ratio 2. Value at Risk(VaR)3. Expected shortfall(ES)Let us look at these metrics one by one:I gave you my word about keeping the explanation simple. So in layman terms, this is what each of the above phrases mean:Sharpe Ratio: Suppose there was a 10 year bond which has a fixed rate of return, fixed at 6.7%. Let us call this rate of interest a risk-free rate of return as it is free from fluctuations in the market. Now, what Sharpe ratio does is that it gives you back the difference between your stock/portfolio’s returns minus the risk free return. You can read more about Sharpe ratio here.Value at Risk(VaR): This refers to the maximum loss your portfolio is expected to take within the next quarter with a 95% probability. However, it is not the limit of the loss. It is just a measure which gives a 95% confidence over the loss’ limit. Read more about this here.Expected Shortfall(ES): Take all the losses above and equal to your VaR and call it ‘X’. The average of this value ‘X’ is your expected shortfall.The magic is about to begin soon. Are you with me so far ?Finally we’ve come to the good stuff! You will need the following tools to perform this analysis.Once in RStudio , run install.packages(“libraryname”) command (as shown below by typing on the console & then pressing enter) one by one for all the above libraries. Remember to keep the upper and lower cases same as above, R is case sensitive.NOTE: The console can either be on the right window or at the bottom. The leftmost pane is the ‘editor’ and you can type and run the commands on it using the ‘Run’ button.Once you have installed the packages, you can load the libraries as shown below:Now that the setup is done, let us start the analysis. We will be analyzing the following stocks1. Oil and natural gas corporation Ltd.(ONGC)2. Reliance Industries Ltd.(RIL)both of which trade on the National Stock Exchange(NSE).You can fetch the stock data and calculate the monthly returns(since 2008) of the stocks from the following code:Things are about to go colorful & dynamic from here on.Now that we have fetched the monthly log returns of the two stocks, we will merge the data from both the stocks and hopefully see some trends using the dygraph libraryUpon running the above code, you will get a nice dynamic graph as output which can compare the two stocks at each point of time by simply taking the mouse pointer anywhere over the trend line:Okay, comparison of two stocks is cool, but can we actually see the combined return of this portfolio ?The answer is yes, we can. To do so, we will have to allot weights to the portfolio. For being fair to both the stocks, I have assigned both of them a weight of 0.5 which means I own equal value of both the stocks in my portfolio.For simplicity, we will look at how a unit of currency has fared in this portfolio over time. The code snippet below can be used to do that:The output of the code shows how the portfolio’s unit currency has fared in the given time period of over a decade.We have finished the visual analysis of our stock. Now let us look at the S V E numbers as discussed in the initial portion of this post:This code snippet will calculate all the three numbers at once:The above code will give us results for each of the three metrics. Now the important question is, how do we interpret these results.Interpretation of some results:Conclusion: In general, similar values of VaR and ES show that ONGC and RIL are equally susceptible to market volatility. This makes sense since these two are among the biggest oil and natural gas companies in India. The market is in a state of turmoil due to covid-19 crisis which explains the negative trend in all the metrics.Well, that’s all for now! We have barely managed to scratch the surface here and there is a lot more to learn. However, this analysis will give you some insight into how stocks can be analysed and it may come in handy while taking re-balancing and investment decisions.Thank you again for taking the time out to read this post. I hope you will now be able to perform simple portfolio analysis on your own. To reproduce the results, simply copy and paste all the code from the github snippets in the same order and change the name of the stocks you wish to analyze.Do remember to check out this app made by me using RMarkdown and Shiny, which has the analysis of 10 such stocks from an actual portfolio.",r,https://medium.com/analytics-vidhya/portfolio-analysis-of-indian-stocks-in-10-minutes-6a50f722a63c?source=tag_archive---------3-----------------------
The Kenyan 6th June Case Using twitteR.,"Rohi AnonJun 8, 2020·8 min readOf the many Kenyan Twitter users, only a small fraction has realized the benefits of text analysis. Sitting down at my parlor and thinking of what to do during that rainy day of June 6th, 2020. I find my laptop and voila! All I could think of was the trending topics on twitter. Instead of jumping to twitter.com, I find my way out to twitter developers page only to realize that I was not actively using the token and keys.The path of a Data Scientist may not be easy as one may depict. It needs an outright mind that can be able to focus on the end goal rather than the turbulence during the process. Meeting Cyril was one of the most awesome things during my data science journey. That story for another day but at his guidance was this idea nourished to write this as my first article. I am not a perfect one anyway, but under the influence of bloggers Catherine Gitau, Cyril Michino, and Margret Nyawira(sorted alphabetically), I am becoming one.With this inspiration, here comes the first article regarding Natural Language processing. Having gained some experience in R and Python and what the two languages can do, R got an upper hand in this article though without bias on any of the languages. NLTK, a package in Python provides an awesome environment to work with NLP. I encourage you to delve deep into this article and that at a minimum should you also walk through the code herein on your PC. Social Media Intelligence is fun, and the hotter the topic the more fun you enjoy whilst gathering insight.Most of the R packages in this article would require you to do an installation. The code chunk snip shown is how one can install a new R package in their workflow.Furthering to what this analysis entailed, some of the required libraries are mentioned here. As a Data Scientist, it’s good practice to call the package where it’s needed.Our major module of interest now gets to “twitteR” which makes it easier to acquire social media data.Here is how to set up your Twitter Developer account in order to gain the OAuth Credentials.You can input your tokens and secrets here and run the code to start the mining process.A bit of explanation:At the bare minimum, the function readline() will require an input of your access_token, access_secret, consumer_key and consumer_secret. From your Twitter developer account so you can just copy and paste these credentials.The last line should yield the message '[1] ""Using direct authentication"".’ With the setup completed, let’s get the sweetness thereof — extracting the data.Let’s begin the course. We can now extract data from twitter. Having our API already set helps us easily access structured data from twitter. As noted previously, the format with which the data comes in is what we have learned from R data types eg vectors, lists, data frames just to mention but a few.A few elements discussed here include user timelines and trending topics. Many more functions can be found here so that you can play with twitter data and its functionality.Let’s see the code.As of June 06 2020 at 12:01, there were 50 trending topics in Kenya. That was elegant. Was it? R just told us everything happening in the background! Seek here to understand how:-Isn’t it cool? I hope you are enjoying the process.2. Extract tweets from a particular user.I won’t stay at this but for your friends and kin, you can always do as follows:Now that we already have our trending topics, the ‘#UhuruDontLiftLockdown’ featured among the trending topics during the article development process.See how the top 6trending topics of the day have an almost close relationship. The 6th Hashtag could probably be due to Kenya’s love for entertainment.Wisdom of the Crowds dictates that 80% of the time should be used for wrangling and cleansing while modeling takes only 20%. Thus the main objective here is to clean the dataset we acquired from twitter using the twitteR API.Remember, informal communication makes twitter data to be highly unstructured. Noise in the data is highly depicted eg the typos, poor grammar, stopwords, emoji usage, and even sometimes URLs.For starters, like I am, the following definitions give a hint of what we are going to do.This process requires more concentration and patience. To see how the whole process was accomplished, please visit my GitHub page here.A good instructor begins from the known to the unknown. For this article’s sake, let us begin from visualizations using histograms first before we move on to a…We can be able to build a Wordle-Esque word cloud. That’s a heavy term that can make one’s head burst yet easy on implementation.The first process is to have the required libraries such as “ggplot2” and “wordcloud” installed. The visual effect communicates a lot of sense if not much when it comes to any analytical process. I tend to think that ‘visualize before you analyze’ in itself is an analysis. This is how I did it. A few codes for you and then the outcome.Interest is always on the top. Let's see it in the plot below.Here is its code.To build a word cloud, the same process follows.Here are some of the word clouds that came after varying the ‘min.freq’ values till the topmost tweeted words. For the 10 and above frequencies:-Already, terms such as President, Uhuru, Kenyatta, and Kenyans can vividly be seen. Look at the following to see how as the threshold frequency increases the terms decreases.In the latter, at least you can see words such as ‘speech’, and ‘juneth’/’njuneth’. The ‘nth’ term in this case was a digit 6. Kenyans pleas were, that their able His Excellency The President, should give them a break when it comes to the ongoing lockdown, during the Covid-19 pandemic. For words over 150 frequencies:-Let us take a pause at that.As depicted by the whole twitter analysis process, the #uhurudontliftlockdown was the number one of Kenya's trending in twitter. The word cloud and the bar graph shows that #uhurudontliftlockdown was the most frequently used word. Many Kenyans tweeted about the speech that was to be made by President Uhuru Kenyatta on the 6th of June concerning the lockdown. An assignment for you is to try and figure out why the words time, address, today and wanjohi were used more than 200 times.Now that we’ve started, let’s keep going. Stay tuned and be prepared for deeper tweeter analytics using Machine learning methods such as Principal Component Analysis and Multidimensional Scaling. I believe that nothing in this world is difficult and in the same spirit, we will walk through this.Thank you for reading this article. In case of any comments, compliments, and criticism you are welcome. Find me on twitter @anon_rohi, pubs, or on linked in. You can also find my Github page. All the best as you continue coding",r,https://medium.com/@rohianon48/the-kenyan-6th-june-case-using-twitter-8d6fb422abd2?source=tag_archive---------3-----------------------
The missing R client for Sentry is finally here,"Billie approves a request for financing on average within 5 minutes. Such speed can only be achieved with quick teams, sound infrastructure, and great models.The data science team at Billie provides several models to different consumers of data inside the company. We are very hands-on, and go beyond just training models — we take care of our infrastructure, and implement REST APIs often as plumber packages.For us, in production means we thoroughly documented and tested a piece of software — whether that is a new model or the infrastructure to serve it. Sometimes though, things go wrong in ways we couldn’t predict 🤷‍♀. In such cases, real-time alerts are critical to keeping our microservices ecosystem running, and our customers happy.Our engineering teams at Billie are long time users of Sentry, a cross-platform application monitoring [service], with a focus on error reporting. Having Sentry means as soon as an error occurs, the team knows about it via e-mail and Slack.Until recently, the data-science team had to rely on simple log alerts or reports from other teams, because there was no Sentry client for R. We were mostly reacting to our other engineering team’s error reports, then looking through our logs to find the problem. Our alerts were not aggregated, so we couldn’t know if a new issue was the same as something we saw yesterday. This process was slow and inefficient. So, we used Sentry’s API and wrote our own client.We wanted sentryRto have good defaults and stay out of the way, but still be extensible to arbitrary situations. Waterproof and breathable. We achieved this by using lists and dot-dot-dot arguments. The user has full flexibility about what is sent to Sentry, starting with a basic capture(message = ""This is fine""). Want to include a list of all the installed packages and their versions? Sure, just add it as a list named modules. Don’t want the name of your app? NULLify it with app_name = NULL. Knowing which fields you can add is as simple as checking the Sentry API documentation.The defaults create a report with the commonly needed information:When the same error happens more than once, Sentry has the built-in capability to aggregate all its instances. This way you can track how often a specific error happened. When, or if, a regression occurs, you’ll know.In a production environment, you will likely want to pre-configure sentryR's settings like👇. In this example, we are adding some tags and skipping information about the runtime.Even though you can use sentryR in pretty much any R script/application (as shown above), its potential is greatest when integrated into an API. This is especially true when that API is part of a larger microservices ecosystem, as in our case at Billie. To integrate it into a plumber API you have to configure sentryR with at least the DSN, and set the error handler:here we use the default plumber error handler, pre-wrapped with sentryR. This is the way to go if you want plumber 's default behavior. It is also possible to use your own error handler and then wrap it with wrap_error_handler_with_sentry.Finally, in order to have access to the full error stack, you’ll have to wrap your Plumber endpoints with with_captured_callsWe had to write this function, because of how R's condition system is designed. Without it sentryR wouldn't capture a stack trace. Error handling in R is too long a topic for this blog post, but if you want to know more take a look here.After this, your API is ready to go. That’s it! Every time you explicitly stop() your code, or it explodes somewhere, plumber will pass the error to the error handler. The error handler then responds with 500, or your chosen HTTP code, and sends a report to Sentry. You will get an alert through the channels you configured (we get an email and a slack message in a dedicated channel).Errors sent to Sentry this way will invariably be of type simpleError. That’s base R for you. Using rlang you can make the errors more informative by creating custom conditions:This way Sentry will display a more informative title, and you can quickly filter and search for the type of issue you are interested.Using Sentry and sentryR lets us react quicker, and more precisely than before. Our APIs stay in top shape, and our customers get the fast service they’ve come to expect.Get sentryR from CRAN, or the latest development release directly from github. If you have a problem write an issue or reach out on Twitter. We definitely welcome PRs if you have ideas for improvements. Let’s keep #rinprod awesome!",r,https://medium.com/billie-finanzratgeber/the-missing-r-client-for-sentry-is-finally-here-13eccb05c237?source=tag_archive---------5-----------------------
Training Data and relevant packages,"Mikyung KimJun 1, 2020·27 min readWhat Determines the Housing Price?: Statistical Modeling for Real Estate PropertyAs a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.Use the code block below to load any necessary packagesWhen you first get your data, it’s very tempting to immediately begin fitting models and assessing how they perform. However, before you begin modeling, it’s absolutely essential to explore the structure of the data and the relationships between the variables in the data set.Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see.After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).Below are the three graphs that I found most informative. Graph 1 shows the relationship between price, area, and Zoning. As you can see, log(price) and log(area) has a very strong linear relationship (with a correlation coefficient of 0.74). Moreover, Zoning has an impact on the price particularly for RL and RM. Comparing homes with similar sizes (area), homes in an area zoned as Residential Low-density (RL) tend to have higher prices than those in area zoned as Residential Medium-density. The relationship between price, overall quality and sale condition is shown in Graph 2. There is a very strong linear relationship between log(price) and overall quality with a correlation coefficient of 0.83. In addition, homes sold in an abnormal condition or through family sales were sold at lower prices than homes with the same overall condition. Location matters when it comes to home sales. There are substantial differences in median home sales prices in different neighborhoods as you can see in Graph 3.In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is not to identify the “best” possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.Based on your EDA, select at most 10 predictor variables from “ames_train” and create a linear model for price (or a transformed version of price) using those variables. Provide the R code and the summary output table for your model, a brief justification for the variables you have chosen, and a brief discussion of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).Based on EDA, I chose 7 variables for an initial model, including Overall.Qual, area, Sale.Condition, age, Lot.Area, MS.Zoning and Bedroom.AbvGr. Three graphs above clearly show strong impacts that area, Overall.Qual, Sale.Condition and MS.Zoning have on the home price. I added Lot.Area because many people think having a big lot is a good investment. Number of bedrooms is important especially for a family with children as they demand rooms for themselves. People also care a lot about whether a home is old or not.Now either using BAS another stepwise selection procedure choose the “best” model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?I used three different method to choose the best model out of my initial model, including AIC,BIC and BMA, and they all agreed on the same model, which is the initial model with the lowest AIC (-3585.43), the lowest BIC (-3506.9) and the highest posterior probability of 0.6. This means that the EDA was conducted well and my initial model is pretty robust.One way to assess the performance of a model is to examine the model’s residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.There are two residual plots from the simple linear regression and bas linear regression. Both look very similar that the residuals are mostly randomly scattered around zero except for a few outliers. This means that the model fits the data pretty well except for three outliers. Normal QQ plot also confirms that the residuals are normally distributed in general except for three big outliers: Case 276, Case 310, Case 428. (All of them are sold much lower than their expected values.) Thus, I looked them up and found out that all three of them were sold under conditions that were NOT normal. From this, the model will fit better if we focus on the normal sale condition.You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).The initial model RMSE from the simple regression is 34162.53 and that from the bas regression is 34341.77. Their values are very close with each other.The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set ames_test.Use your model from above to generate predictions for the housing prices in the test data set. Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data? Why or why not? Briefly explain how you determined that (what steps or processes did you use)?I calculated the coverage probability of the initial model on the out-of-sample data frame (“ames_test”) and the result shows that 97.3 percent of the actual prices in “ames_test” fall in 95% confidence interval of the expected values of the data frame. I also calculated the RMSE, using initial bas regression model, and it is 27527.47, even lower than the RMSE for the original data frame. This implies that the predictions are very accurate. I believe the reason is the initial model covers most important predictors for the home price. In fact, the adjusted R square is very high, 0.85.Note to the learner: If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.Now that you have developed an initial model to use as a baseline, create a final model with at most 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.Carefully document the process that you used to come up with your final model, so that you can answer the questions below.This is the process that I used to arrive at my final (the best) model.Did you decide to transform any variables? Why or why not? Explain in a few sentences. I log transformed three variables (“price”, “area” and “Lot.Are”) that are highly skewed to the right in their distributions. All three variables resemble the normal distribution much better after their log transformation. This transformation is particularly important for “price” to meet the normal distribution assumption of the response variable in a linear regression.Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.No, I did not because we did not talk about variable interactions in the lecture or the quizzes.What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.As mentioned earlier, I used AIC, BIC and BMA model selection. I used them because it is relatively easier to do it using “stepAIC” and “bas.lm” functions in R. Also, they are quite different in their selection process and I wanted to see if they come up with different models. In my case, all three produced the same model.How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.I calculated the coverage probability of the final model on “ames_test” and it shows that 94.8 % of the actual home prices from “ames_test” fall in the 95% confidence interval of the predicted values produced by the final model.For your final model, create and briefly interpret an informative plot of the residuals.The residuals of the final model are randomly scattered around zero shown in the below plot and the normal QQ plot looks much better compated to initial model. This shows that the model fits the data better. Also, this plot does not show extreme cases as the residual plot of the initial model had even though it does have a few out liers, which requires further research.FMSE for the final model is 25767.59 and it is decreased from 27527.47 (RMSE of the initial model). This means that final model is more accurate than the initial model.What are some strengths and weaknesses of your model? I feel that my final model is pretty powerful in predicting the home prices. Unfortunately, it is only for those that are sold in a normal condition (even though I do not think it is a weakness of a model for an investor.) The bigger problem is that my model does not include “Neighborhood” as a predictor even though it has a very strong impact on home prices. I did not include “Neighborhood” as a predictor because there are 28 different neighborhoods and it means that there are 28 more variables in the model and it will complicate the model too much. It is important to remember that a good model is parsimonious.Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice.You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention: * What is the RMSE of your final model when applied to the validation data? * How does this value compare to that of the training data and/or testing data? * What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set? * From this result, does your final model properly reflect uncertainty?Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model.After exploring the data extensively, I began with a model with Overall.Qual, log(area), log (Lot.Area), MS.Zoning and Bedroom.AbvGr. To my surprise, it was a strong model with an adjusted R square of 0.85 and all three methods of selection (AIC, BIC and BMA) agreed that the initial model was the best model. (I learned the importance of good EDA from here.)To improve the model, I added a few more predictors, including “Lot.Shape”, Full.Bath“,”Garage.Cars“,”Bldg.Type“, and”BsmtFin.Type.1.” I used the same three methods of selection and decided on my final model: final.model <-lm (log(price)~Overall.Qual + log(area) + age + log(Lot.Area) + MS.Zoning + Bedroom.AbvGr + Garage.Cars + BsmtFin.Type.1, data = final_project)I evaluated the fitness of the final model using a residual plot. I also evaluated the accuracy of the model by applying the model to two different data sets (“ames_test” and “ames_validation”). The residual plot showed that the residuals are scattered randomly around zero, implying a good fitness to the data. The RMSE and coverage probability of the model for the true value of the prices in the two data sets came very strong. Thus, I’m confident that my final model is quite accurate in predicting homes prices.It would be interesting if this model holds up for housing markets in larger and denser cities in the U.S. such as NYC and L.A.",r,https://medium.com/@mikyungkim_2541/peer-assessment-ii-ca27221e6ad7?source=tag_archive---------1-----------------------
Analyzing activity trends and patterns using R and ggplot2,"Simon PicheJun 9, 2020·6 min readLast week, the gym I used to regularly frequent announced that they are starting to re-open their different locations in accordance with local public health officials guidelines. While no dates have been announced, I was pretty excited for this news — but it did not last.The more I thought about it, the more I realized that both my gym life and my work life had been closely intertwined in my routine for years. I’d go before or after work (sometimes both!) or if time permits, I’d go at lunch. The gym is two blocks from the office, but a ~30 mins drive from home. It is clear that my return to normalcy will not start with just gyms re-opening.However, I was interested in finding and visualizing how my physical activity patterns had been affected by the COVID-19 pandemic (and possibly other factors) in recent months. Not so coincidentally, I had been learning R since just before the pandemic and I thought this would be a neat project to work on to practice my newly acquired knowledge!My objectives were to:In a world where businesses are in a competition to collect more and more (and more) data about their users and thanks to the EU’s GDPR, it was fairly easy to obtain access to my own data. Actually, availability of data wasn’t a problem at all.As any good millennial, I use various apps including to track my workouts. One for my cardio activities and another for my weight resistance workouts. I’ve also been wearing a Xiaomi Mi Fit Band since 2017 to track my steps.And my phone (Samsung Galaxy) also came pre-loaded with Samsung Health and Google Fit, with the latter integrating all 3 sources of data above.I first chose to get data directly from my Mi Fit Band, as I thought it would have the most accurate data, since it was collected with a wearable sensor as opposed to a cellphone.A few buried menu options, a somewhat meaningless warning message and a confirmation that my data will be emailed to me in a few moments and I was set! Or not.The data never came and my account lost all synchronization capabilities with Mi Fit and their servers.I was quite disappointed by this turn of events. In addition to not being able to obtain my data, I was never able to sync my band again, and I lost my streak of more than 10,000 steps a day for the last 586 days. It more or less bricked my fitness tracker.Instead, I ended up exported data from Google Fit using Google Takeout, which worked beautifully and quickly, as it had in the past… and I was able to keep using my account.While I started learning R, I am far from having solid data literacy but I do know that one of the first steps is to explore the data! In the archive provided by Google were some data descriptions, which proved very useful to start poking around.P.S. If you’ve never looked at a Google Takeout archive, I encourage you to! At first you will be astounded by the volume of data, but it has made me much more privacy conscious.As you can see in the screenshot above, there’s almost 9000 .tcx (XML-based) and .csv files ready for consumption. Exploring this many files using R didn’t seem like the right approach, so I wrote a small C# application that transformed the thousands of files into two CSV files, ready for exploration and analysis in RStudio.Before anyone asks, yes I could’ve used python, but my background and whatever programming expertise I have left is in .NET/C# — and that was therefore quicker to write.Once the data loaded in RStudio, I had to do a few more data transformations for data types, calculated fields and display values but I was finally able to get to my objectives, which I’ve previously laid out. Below are the results of my analysis.In this image, I’m comparing my activity levels from the same period (January 1st to May 31st) between 2019 and 2020. We can see on the left, my activity trends downwards starting in April 2019 and this is due to lack of data. Google Fit collected no data between mid-March to July — for reasons I can’t explain.😒*N.B. Moving is time spent moving, whether running, walking, biking…On the right however, we can make a few interesting observations:I still wanted to have proper data to compare my general activity levels, so I did a quick comparison of Samsung Health data and it confirmed that the downwards trend in the Google data is indeed due to a lack of data — not a lack of activity. We can also observe the small dip in activity levels in mid-March 2020.Next, I wanted to visualize how the distribution of my physical activities compared between 2019 and 2020. If I wasn’t working out in the morning, how did I adapt my routine? Did I workout more in the afternoon vs. the morning? Let’s find out.So let me start by saying I like these charts less than I like the previous ones. I don’t find them as clear, but even these stretched my knowledge plenty. If anyone has any recommendations on how to better graph this data, I’m all ears!Some interesting observations can still be made with these graphs:This past weekend was lost to R analysis and dusting off C# skills, and I now feel a bit more comfortable with R, the ggplot2 library and RStudio. As well, practicing what I’d been learning through reading for a few months was super helpful.In the end, I was able to reach both my objectives of identifying differing activity trends and distribution between 2019 and 2020. It’s clear from the graphs above that many aspects of our routines and lives have changed and one wonders how they will change again in 2021.Next, I’d like to analyze driving patterns and how those have changed, as well as introduce geospatial data so I can practice that too!",r,https://medium.com/@spiche613/analyzing-activity-trends-and-patterns-using-r-and-ggplot2-87262fe24433?source=tag_archive---------2-----------------------
Bilingual Data Scientist: Python to R (Part 1),"Ahilan SrivishnumohanJun 8, 2020·8 min readAt college and university, Python is quite commonly taught and applied to a plethera of subjects; Mathematics, Physics, Engineering, etc. Hence, more coding beginners initially have a Python background as opposed to an R background. I am one of those people. I have experience using Python during University and I utilised Python again at Flatiron School; where I applied it to Data Science.There is always debate for the best programming languages but when it comes to Data Science there are two that take definitely take the podium, Python and R. Acquiring the knowledge that R as a programming language is the second most requested tool/language for Data Science in 2019 at 59.2%, I thought I might as well get that extra edge by learning to code in both Python and R.Let’s Begin! First let’s recap on Python.Python has been around since the late 1980s and was implemented by Guldo van Rossum in the Netherlands as a successor to another language, ABC. Python was named after the TV show Monty Python’s Flying Circus.What’s different about R?R is a modern implementation of the S programming language created by statisticians Ross Ihaka and Robert Gentlemen in the 90s. The name came from first inital of the two creators’ names, and play on the name of the S language.R is a programming lanauage made by statisitcians and data miners, typically used to analyze data, statistical analysis, and graphics. for statisical computing.Both Python and R are great in different ways. So, let’s start using R!Install R and RStudio. RStudio is a great IDE for using R. Here are links to both:R : https://www.r-project.org/RStudio : https://rstudio.com/products/rstudio/download/Here are some of the basic syntax and operations used in R to get started:Arithmetic Operations:Arithmetic is near identical to Python except for finding the remainder from division as we use %% instead of % and having to import ‘math’ or ‘numpy’ library in python for square root.Types of Data:Again, R’s data inputs are very similar to Python except for adding an ‘L’ after the numbers to differentiate between a numeric value and an integer value in R, and also TRUE/FALSE is capitalised in R.Assignment Operators:Most unique in R’s syntax is the choice for assignment operators. Python has a traditional ‘=’ for assigning the value on the right to the variable on the left, whereas R has a few different ways to approach assignment.Conditional Operators:Both Python and R are identical for conditional operators.If/Else Statements:When looking at more functional code the logic is the same between Python and R but the differences lie in syntax and indentation. Python indentation is key as it marks the block of code to be executed. R is more leniant when it comes to indentation but requires curly brackets to distinguish blocks of code.For Loops and While Loops:Both for and while loops take a similar approach as if/else statements between the two languages. Indentation and curly brackets are defining features of two languages syntax. Also both the for and while conditions are contained in parentheses in R, unlike Python which uses a colon to finish the condition.Data Structure:In Python we have different data structures for collections of data, such as, lists and dictionaries. R has data structures too, but a few different types and fairly different syntax to denote them.Vectors contain objects of the same class. If you mix objects of different classes the objects change into one particular class, this is called coercion.Unlike Python, R’s indexing does not start at the 0th element in the list but starts with the 1st element, so the location of the data in a list is the same as the index number used to call it from the list.Some of the benefits of R comes from the graphical capabilities, so I’ll expand on how a data scientist may use R to create some data visualisations.Importing and Manipulating Dataframes:Importing a csv file in R is similar to Python but you add two more parameters:I have listed a few dataframe functions below. There are plenty more but I wanted to show the approach and simplicity of the syntax for these functions with a few common examples.Plots:R has built in plotting functions without having to use external libraries that do a great job, and there are more specialised graphical packages available if needed. I won’t compare direct code between Python and R for plots, as there are a variety of libraries and packages both languages can use for plots, and also a wide range of graph types.I’ll use the in-built Iris dataset to show a simple boxplot and histogram.These graphs will appear in the graphical environment block of RStudio when you execute the above code in the console.In the graphical environment block of RStudio you can check the packages you would like to use. I am going to use the ggplot2 package here for some more advanced plots.I’ll use an in-built datset of the ggplot2 package.I will break the R guide into a multi-part series, so it is easier to digest…",r,https://towardsdatascience.com/bilingual-data-scientist-python-to-r-part-1-70438fcdc155?source=tag_archive---------1-----------------------
Crash Course in Bayesian Statistics with R (Part1),"Bayes’s Rule formulathis classical formula is the fundamental in the statistical Bayesian inference theorylet f() denote a probability or a probability density function thenUnder this formula we can see the first fundamental difference between the Bayesian approach and the frequentist approach, in Bayesian inference we treat the data x as fixed and the parameter of interest θ as random variable, that means θ has a distributionRemember that we have learned in beginner class of statistics and probability that there are different definitions of probability that have been suggested in the course of the development of probability theory two of these definitions are called relative frequency definition of probability and subjective definition of probability.So treating data as fixed and the parameter as random in Bayesian inference will radically change the intereprtation of very basics concepts in statisics like for example p value, hypothesis testing or confidence interval ..etc.Back to the formulaf(θ) is the prior distributionf(θ|x) is the posterior distributionL(θ;x) is the Likelihoodf(x) is the marginal distribution of the data xx or the the normalizing constant and it is less important in Bayesian inference since it is a function of the x which are fixed, dropping out this normalizing constant leads the posterior density f(θ|x) to lose some properties like integration to 1 (improper density ) over the domain of θ but this is not a big deal since we are usually not interested in integrating the function but ratherin maximising it, so multiplying this function with the constant does not change the θ that corresponds to the maximum point (MAP), in this case we can say that the posterior is proportional (not equal) to prior multiplyed with likelihood i.e f(θ|x)∝ f(θ)L(θ;x)Suppose we have a random variable X has a Poisson distribution with mean θ i.e.As we know the moments for such random variable areNow suppose that there is a sample x1,x2…,xn of n independent and identically distributed (iid) observations or realization of the random variable X∼pois(θ)X∼pois(θ), then joint density function pdf or the likelihood function isNow we will work with a class of conjugate prior distributions that will make posterior calculations simple.In general a class of prior densities is conjugate for a sampling model p(x1,x2,…,xn|θ) if the posterior distribution is also in that class. For the Poisson sampling model, our posterior distribution for θ has the following form:This means that whatever our conjugate class of densities is, it will have to include terms likeThe simplest class of such densities includes only these terms, and their corresponding probability distributions are known as the family of gamma distributions.Again we want to infer about the Posisson pdf parameter namely θ given the data at hand, this uncertain positive quantity θ has a gamma distribution with two parameters α and β ,i.ethis is anyway the prior distribution which reflects our prior information or belief about θ, this distribution can take diffrent shapes based on the two parameters α and β as we see belowNow we are done and we just have to calculate analytically the posterior distribution of θThis is evidently a gamma distribution i.eThe posterior expectation of θ is a convex combination of the prior expectation and the sample average:Now let us try to obtain a deeper understanding by taking an example.Suppose we have data about the number of children of of women for some ethnic group AThe appropriate model of this data is the Poisson model with the parameter θ if we suppose the our prior belief about the birth rate follows a gamma distribution with α=2 and β=1 thensince we know now the Posterior distribution then we can get inference about θ namely Posterior means, modes and 95% quantile-based confidence intervals, Bayesian HDI and Posterior predictive distribution (we will take about them later). Now let us work on with codeYou can download the used R code from my bayesstat repo :https://github.com/BahgatN/bayesstathttps://www.amazon.de/Mathematical-Statistics-Economics-Business-Mittelhammer/dp/0387945873https://cran.r-project.org/web/packages/BEST/vignettes/BEST.pdfhttps://stat.duke.edu/books/first-course-bayesian-statistical-methods",r,https://medium.com/analytics-vidhya/crash-course-in-bayesian-statistics-with-r-part1-1bb7df092d93?source=tag_archive---------1-----------------------
Calling R from Python | Magic of rpy2,"Two big friends of data science are Python and R. While many people are satisfied with either, it usually doesn’t come in our every day radar to use an R functionality in Python. Because when I started my path into Data Science, Python was THE language, and R was something that people were starting to forget. And R’s syntax like <-, %>%, variable$attribute were more than enough for me to say nope and stick with python.Long story short, some well written libraries forced me to get my ‘hands dirty’. Got to take a look at R and it wasn’t all bad, in fact time series analysis and certain data processing was really faster than python. That worked fine, till my project had both R and Python code in them. It was crazy, and for a lot of reasons you may end up with this scenario too. So I started asking the question, How do I call an R script from Python?If you’ve asked Google this question before, The first and obvious topic to come up would be the amazing python library rpy2. Now, this is not going to be a detailed tutorial on rpy2, but I will be explaining how you can call functions written in R from Python. Rpy2 provides a lot of functionality to use R libraries and functions from python itself, however it could be a real trouble fixing the data type inconsistency, so this is the best quick fix I could come up with. And if I was reading this article, I wouldn’t make it this far so congratulations! xD Let us jump straight into the code and look at what we are dealing over here.preprocess.RThis is an R script preprocess.R which contains a function filter_country which basically filters the dataframe you are passing with the country you are passing. Now this is purely for demonstration, you can make this function as complex as you can, with R’s advanced array of libraries. Now let us look at the python script which invokes the R script.preprocess.pyNow this can be a bit overwhelming at first, but I will explain it step by step.Voila, we are done interfacing R functions in Python. So by maintaining a basic dataframe input output operation, we will be easily able to interface R function inside Python! :)Also please note that in some of the forums it was advised that pandas version==0.23.x is the most desirable and I have personally tested it and found some bugs with integration for other higher versions.",r,https://medium.com/analytics-vidhya/calling-r-from-python-magic-of-rpy2-d8cbbf991571?source=tag_archive---------1-----------------------
A tool to estimate time and memory complexities of algorithms with minimum efforts,"Introduction to GuessCompx tool to estimate complexities empiricallyFeatures:Introduction to GuessCompx, an R packageA tool to empirically estimate the time and memory complexities of algorithmWhen an algorithm or a program runs on a computer, it requires some resources. The complexity of an algorithm is the measure of the resources, for some input. These resources are usually space and time. Thus, complexity is of two types: Space and Time Complexity. The time complexity defines the amount it takes for an algorithm to complete its execution. This may vary depending on the input given to the algorithm. For example, if the algorithm under consideration is Bubble Sort and the input is a sorted array, then the time complexity will be less. This is called the best-case complexity. When the input array is reversed, the algorithm takes much more time to sort the array. This is the worst-case complexity. When the input array is neither sorted nor reversed, the time complexity lies between the best-case and worst-case complexities. This is called average-case complexity. The exact time required for an algorithm to complete execution varies greatly from computer to computer. Thus, we actually focus on the behavior of the complexity when the input size increases. This is the asymptotic behavior of the complexity. There are various notations used to represent the asymptotic complexity of such as big O, big omega, big theta, etc. The complexity of the Bubble sort algorithm is O(n2) in the worst case. Since the running time cannot be greater than the worst-case scenario, the worst-case complexity is usually considered. The algorithm that has lower complexity is said to be more efficient than the algorithm having more complexity. So, when developing new algorithms, developers need to check its complexity. This process is quite time-consuming. Knowing the approximate execution time of a new piece of code, before running it in full scale, could save a lot of time. Calculating the complexity using various manual methods(such as master theorem) is a tedious process and more prone to errors. These methods are being challenged by the Empirical methods, which try to give an estimate of the complexity by observing the code being run several times.The package GuessCompx aims to estimate the computation time and memory required, before the final implementation of the algorithm. The complexity is affected by various factors such as dimensions of the data, the time horizon for recursive forecasting, number of features, parallel computing, etc. But this package only estimates the complexity with regards to the number of rows in the data. Other factors are out of the scope of the package. The algorithm is run on subsets of data, increasing the size in every run, using sampling or stratified sampling if needed. Various complexity functions are fitted and the best fit for the data is selected. The comparisons between different models are achieved through LOO (leave-one-out) routine using the Mean Squared Error as an indicator. The memory analysis uses memory.size() function, which can only be run on a windows machine. The package can be installed from CRAN repositories:install.packages(“GuessCompx”)CompEst()The main function and the only entry point for the package is the CompEst() function. The function accepts various input formats such as data frame, matrix, time series, etc. The user can configure the function using various arguments, according to their needs. The function creates a vector of data sizes, which are used as the input length. The argument max.time is used as a stopping condition while looping over the data sizes. max.time represents the maximum time for a single iteration to complete. The function works by evaluating the time and memory used by each iteration of the target function over the sample input. For this, system.time() and memory.size() functions are used before and after double garbage collection, gc(). These results along with data act as inputs to the complexity models. The cv.glm() function computes the LOO-error for each model and eventually, the best model is picked. The sampling of data is not included in the evaluation of time and memory. The function returns a list of the best model, computation time on the entire dataset, for both time and memory complexities, and two plots showing the best model curve highlighted. The complete function call is given below,CompEst(d, f, random.sampling=FALSE, max.time=30, start.size=NULL, replicates=4, strata=NULL, power.factor=2, alpha.value=0.005, plot.result=TRUE)d: Dataframe on which the algorithm is to be testedf: Function that implements the algorithmrandom.sampling: Boolean value to specify whether random samples are taken at each stepmax.time: The maximum allowed time(in seconds) for each step of the analysis.start.size: Size of the first sample to run the algorithm on. The default value is floor(log2(nrow(d)))replicates: Number of repeated runs for the same sample size.strata: String containing the name of the column that contains different categories.power.factor: The amount by which the sample size is increased. The default value is 2 meaning that the size is doubled in every step.alpha.value: The alpha risk of the test whether the model is significantly different from a constant relationplot.results: Boolean to indicate if the summary plot is to be displayed.CompEstBenchmark() and CompEstplot()This function provides a benchmark procedure, that fits all the complexity models. The function takes input a data frame(to.model), returned by the CompEst() function, and returns a list of all the fitted complexity models. The user also needs to specify whether the function deals with time or memory data(use).CompEstBenchmark(to.model, use=”time”)The CompEstplot() function plots the results of CompEst() function. The input to the function is a data frame produced by CompEst() (to.plot). Other arguments are element_title, a string that is added to the subtitle of the plot and use, a string to specify if the function deals with time or memory data. The function returns a ggplot object.CompEstPred()The function predicts the computation time of the whole dataset. The arguments to the function are:model.listmodel.list: a list containing the fitted complexity functions.benchmark: a vector of LOO errors of the complexity functions.N: Number of rows of the whole dataset use, A string indicating if the function deals “time” or “memory” data.GroupedSampleFracAtLeastOneSample() and head()The GroupedSampleFracAtLeastOneSample() samples a random proportion of the input data. The input is given as a data frame and the function returns a small sample of the data. The returned sample contains at least 1 observation. It is also possible to specify if the samples are to be drawn randomly.The rhead() function is used to generate small random samples from a given vector or a data frame. All functions other than CompEst() function are used internally to accomplish different tasks and are not accessible directly to the user.This package enables the R user to empirically estimate the computation time and memory usage of any algorithm before fully running it. The user’s algorithm is run on a set of increasing-sizes small portions of his dataset. Various models are then fitted to capture the computational complexity trend likely to best fit the algorithm (independent o(1), linear o(n), quadratic o(n2), etc.), one for the time, another for the memory. The model eventually predicts the time and memory usage for the full size of the data.Details on the subject of algorithmic complexity can be found on the wikipedia page. Note that the complexity is understood only with regard to the size of the data (number of rows), not other possible parameters such as the number of features, tuning parameters, etc. Interactions with those parameters could be investigated in future versions of the package.The complexity functions already implemented are the following: O(1), O(N), O(N²), O(N³), O(N^0.5), O(log(N)), O(N*log(N))Most algorithms out there have a complexity behavior that does not change in time: some are independent of the number of rows (think of the length function), some linear, some quadratic (typically a distance computation), etc. We track the computation time & memory of runs of the algorithm on increasing subsets of the data, using sampling or stratified sampling if needed. We fit the various complexity functions with a simple glm() procedure with a formula of the kind glm(time ~ log(nb_rows)), then find which is the best fit to the data. This comparison between the models is achieved through a LOO (leave-one-out) routine using Mean Squared Error as the indicator.The GuessCompx package has a single entry point: the CompEst() function that accepts diverse input formats (data.frame, matrix, time-series) and is fully configurable to fit most use cases: which size of data to start at, how much time you have to do the audit (usually 1 minute gives a good result), how many replicates you want for each tested size (in case of high variability), do you need a stratified sampling (in case each run must include all possible categories of one variable), by how much we increase the size at each run, etc.Note on the memory complexity: memory analysis relies on the memory.size() function to estimate the trend and this function only works on Windows machines. If another OS is detected, the algorithm will skip the memory part.Here, CompEst() is used to show the quadratic complexity of the dist() function:These results show that both empirically estimated time and memory complexities are Quadratic in nature. The validation of the GuessCompx for various well-known computing methods with known standard complexities is discussed in detail in the publication.This post is proposed a thorough description of the GuessCompx R package. The package is introduced to perform an empirical estimate on time and memory complexities of an algorithm or a function. It tests multiple, increasing-sizes samples of the user’s data and tries to fit one of seven complexity functions: O(N), O(Nˆ2), O(log(N)), etc. Based on a best-fit procedure using LOO-MSE (leave one out-mean square error), it also predicts the full computation time and memory usage on the whole dataset. The hereby suggested method and package are believed to be new to the R users community; however, there is a lot of room for improvement, both in terms of automation and a variety of complexity functions. The suggestions from readers are welcome and for further details, feel free to comment on this post.Author:Dr. Neeraj Dhanraj Bokde,Postdoctoral Researcher,Aarhus University, Denmarkhttps://www.researchgate.net/profile/Neeraj_Bokde",r,https://blog.devgenius.io/a-tool-to-estimate-time-and-space-complexities-of-algorithms-with-minimum-efforts-8d9a5c1e96ae?source=tag_archive---------1-----------------------
How to convert the R code to Python and reverse,"Muhammed TausifJun 3, 2020·1 min readDue to the versatile nature of both languages, and overlapping interested the need for code conversion between two languages is a natural question.However, it becomes too tidy and difficult when a language has a feature or command a work for which the other does not have or programmer have to trick the code to do so.My experience of converting the R code to Python and Python code to R was also a challenging job which took several days to complete the job.The procedure, I found, I would like to share.1. Use the same IDE for both programming languages when the conversion is concerned.2. As both languages are interpreted languages, the code compilation and debugger are not needed, even though helpful.3. Run the code line by line, and view the output. Try to execute the code in another language, and ensure that the code has the same output.Best of Luck,For more detailed, tips, email me at tausifasia@gmail.comor visit www.tausifasia.com",r,https://medium.com/@tausifasia/how-to-convert-the-r-code-to-python-and-reverse-234de276f901?source=tag_archive---------1-----------------------
100 coding challenge #1,"谷宜臻Jun 10, 2020·7 min readWant to challenge myself to code 100 consecutive day100 coding challenge day1下一階段想玩翻轉R教室 https://datascienceandr.org/# R-learning🌱向量的加減乘除當兩個不同長度的向量加減乘除的時候，R會直接把短的向量依序補到跟長的向量相同(無論有無整除)x <- c(1,2,3,4,5,6,7)y <- c(1,2,3)z <- x + yz就會是一個有七個元素的向量2 4 6 5 7 9 8具體拆解就是z會等於c(1,2,3,4,5,6,7)+c(1,2,3,1,2,3,1)同理x*2+100就會得到一個有七個元素的向量，每個向量都是對各x向量內的元素*2+100102 104 106 108 110 112 114這其實就是用到剛才的觀念，2跟100都是一個只有一個元素的向量，但是R會自動把他補齊到7個元素!看超多的R參考書都沒有講到這個XD今天終於在MIT的教材裡面看到了，覺得身心舒暢。🌱其他函數?函式名稱當你不懂這個函式用法時就可以求救?`運算符`當你不懂這個運算符(+-*/:……)用法時就可以求救，注意要加上反引號，通常在你鍵盤左上角的位置，TAB上面，但如果沒有你可以打引號就好args()可以了解一個函式裡面的引數(argument)預設值。你可以打args(list.files)試試看喔注意引數跟參數(parameter)的概念是不一樣的，這裡取材自維基百科，假設我們自定義一個add的函數def add(x, y): return x + y代表你之後只要使用add()裡面輸入任意的數值或是變數都可以得到相加的結果，舉例來說：狀況一：這裡7、8就是引數，x、y就是參數add(7,8)就會傳回15狀況二：這裡b、c就是引數，x、y就是參數b<-7c<-8add(b,c)就會傳回15參考資料 ：維基百科https://zh.wikipedia.org/wiki/%E5%8F%83%E6%95%B8_(%E7%A8%8B%E5%BC%8F%E8%A8%AD%E8%A8%88)🌱workspace and file 如何用R語言直接管理檔案(平常在window圖形化介面上創建資料夾、修改檔案名稱、創建文件...要如何用R語言來控制)除了跟著下面的步驟做以外，你還可以把當前的工作路徑打開(就是打開你圖形化介面的資料夾，你就可以看到你的代碼在電腦上運行的結果)這個會跟用的Operating system有關，我是用window，如果是mac或是Linux用的是Unix可能會有所不同。getwd() 檢視目前工作路徑(working directory)ls() 看有多少物件在local workspacelist.files()看有多少檔案在目前的工作路徑中(注意只會看到這一層的)dir.create(""testdir"")這會在你現有的路徑下創建一個新的資料夾(或是你也可以說是新的路徑)setwd(""testdir"")為了避免影響你現有的檔案我們把目前工作路徑轉到剛才創建的新路徑中file.create(""mytest.R"")現在來創個名叫mytest.R檔案吧file.exists(""mytest.R"")確定一下有沒有創成功，如果成功R會回復你TRUEfile.info(""mytest.R"")來檢視這個檔案的資訊file.info(""mytest.R"")$size因為上述的資訊有很多，你如果特別只想知道檔案大小就可以用$這個符號加上size，指定R回傳檔案的大小file.rename(""mytest.R"",""mytest2.R"") 重新命名檔案名稱（ “ 原檔名”, “新檔名” ）file.copy(""mytest2.R"",""mytest3.R"")複製檔案及重新命名複本（“原檔名”,“複本檔名”）file.path(""mytest3.R"")會給你這個檔案的相對路徑，另外這個函數也可以用來描述一個路徑，好處是因為window預設的路徑斜線跟R的斜線不一樣，window是\來描述檔案位子但是R是用/，這麼一來你就不用手動一個一個去調整/的符號。path <- file.path(""C:"", ""Users"", ""John"", ""Documents"", fsep=""\"")setwd(path)dir.create(file.path(""testdir2"",""testdir3""),recursive=TRUE)唯有加上recursive=TRUE創造出來的才會是巢狀的結構，也就是在testdir2的資料夾下面會有一個testdir3的資料夾unlink(""testdir2"",recursive=TRUE)這樣就會刪掉testdir2的資料夾，如果你忘記打recursive，R有防呆機制，怕你忘記這是個巢狀結構也就是裡面還有別的資料夾，他就不會刪掉了。🌱數列pi:10打打看他會呈現從3.14 4.14 5.14直到9.14，也就是說冒號就代表包含前面的數字並且以公差為+-1的方式到小於等於冒號的數字seq(pi,10,by=0.5)打打看，這個by就是可以調整公差的地方my_seq <- seq(pi,10,length=30)打打看，這個length就是指待會會創建出來的元素個數，所以從pi到10之間R會自動幫你等分成30個元素。seq(along.with=my_seq)複製my_seq的那串數字seq_along(my_seq)複製my_seq的那串數字rep(c(1,2,3),times=3)times就是把整個向量重複幾次的參數，3是你可以自由輸入的引數1 2 3 1 2 3 1 2 3rep(c(1,2,3),each=3)each就是把向量中各個元素重複幾次的參數，3是你可以自由輸入的引數1 1 1 2 2 2 3 3 3希望一百天挑戰能成功!附註：可以打出emoji的cheatsheethttps://www.webfx.com/tools/emoji-cheat-sheet/",r,https://medium.com/@flowing_tuscan_macaw_740/100-coding-challenge-1-f713cf3c1308?source=tag_archive---------2-----------------------
Use R Programming on “RStudio Cloud” Directly on Browser,"Korkrid Akepanidtaworn (Kyle)Jun 7, 2020·4 min readThe new era of cloud computing begins…I discovered RStudio Cloud (currently in beta release at the time I am writing) suitable for professionals, hobbyists, trainers, teachers and students to do, share, teach and learn data science using R. For those who don’t know, RStudio is a full-fledged IDE for R Programming. Here’s an exact view you are going to be using as the Desktop version designed to solve some pain points such as:Now, with the new cloud experience, I can simply log in and run the code from browser. How ridiculously easy!If the credential is valid, you will see the following landing page:I proceeded on creating a new project and given the project title as “AutoML H2O.ai in R”. It’s working amazingly on my end. I didn’t have to install R or RStudio at all. The navigation pane is the same experience as using RStudio on my local machine.Disclaimer: The following content is not officially endorsed by my employer. The views and opinions expressed in this article are those of the author’s and do not necessarily reflect the official policy or position of current or previous employer, organization, committee, other group or individual. Analysis performed within this article is based on limited dated open source information. Assumptions made within the analysis are not reflective of the position of any previous or current employer.",r,https://towardsdatascience.com/use-r-programming-on-rstudio-cloud-directly-on-browser-af19b9042751?source=tag_archive---------1-----------------------
Using a Neural Network to Predict Voter Preferences,"Gustavo CaffaroJun 2, 2020·7 min readWith presidential elections around the corner, political analysts, forecasters, and other interested parties are running to build their best estimate of election outcomes. Traditionally, polls have been used to gauge the level of popularity of political candidates, but increased computing power and the development of powerful statistical methods provide an interesting alternative to them. A good place to start forecasting elections is to first predict voters’ political preferences. This is is what we will do here.In this article we will build a simple neural network in R to predict voter preferences in the United States.We will do this using Keras, an amazing open-source API that allows you to run neural network models in a simple yet powerful way. Although it runs natively in Python, RStudio has developed a package that allows seamless integration with R.Before we start, please make sure you have the following R packages installed, as we will be using them to perform our predictions:DataThe data used to train the neural network comes from the 2018 Cooperative Congressional Election Study, administered by YouGov. It was compiled by Kuriwaki (2018), and was extracted from the Harvard Dataverse. You can download this data in .Rds file format for the years 2006–2018 here.Assuming you downloaded the file and placed it in your working directory, we can proceed to import the data and see its structure:d is a dataframe with 452,755 rows (observations) and 73 columns (features). These features include geographic, demographic, and economic variables, in addition to other interesting variables such as political approval and news interest levels. Of course, they also include the presidential vote choice of each individual. This last variable will be our dependent variable, e.g. what we will predict with our model. Finally, a detailed explanation of each variable is provided in the aforementioned dataset source link.Since we have data for the years 2006–2008, let’s filter d to select only the responses made in 2018, the year of the latest survey:Additionally, let’s select only the variables that are of interest to our model and exclude missing values:This is how this data frame looks like:The table below presents the number of respondents for each of the categories of voter preferences (variable voted_pres_16). It can be seen that around 88% of the respondents voted for either Donald Trump or Hilary Clinton, 9.91% voted for another candidate, and around 1.3% did not reveal their preferences or did not vote.Coming back to our dataset dd, excluding the variable age, all our features are categorical. Thus, we need to one-hot encode these variables into dummies. There are many packages and functions to do this, but here we will use the function dummy_cols from the fastDummies package.We also convert our dependent variable, voted_pres_16 into a numeric vector with integers (starting at zero) for each of the candidates, and we remove the variable voted_pres_16 from our dataframe:Finally, we separate our data into a training set (90%) and a test set (10%), so that after we train our model, we can test its performance on “new” data.Building the modelOur problem at hand is modeled as a classification problem, where each candidate on Table 1 represents a classification category (in total 5 categories). The input layer is formatted such that each of the 148 explanatory variables feeds a neuron of the input layer. These neurons are then connected to other neurons in the hidden layer. In this example, we use 100 neurons for the hidden layer. Finally, the output layer has 5 units, one for each category. Figure 1 contains a graphical description of this neural network.Thus, we define our model:The activation function for the first stage (input to hidden layer) is Rectified Linear Unit, or ReLu, while the activation function for the second stage (hidden to output layer) is softmax.We now proceed to compile and train the model. The optimizer algorithm that we will use here is the adam, an adaptative optimization algorithm usually used to train deep neural networks. The loss function used is the sparse categorical crossentropy. Finally, we will take around 20% of our training data for the model to iteratively calculate the validation error.The above algorithm will fit our neural network for 500 epochs, and it will stop before that if test model performance does not increase for 20 continuous epochs.Model PerformanceAfter training our model, we want to evaluate it using our test data by making predictions and looking at model performance:Test loss and accuracy of 0.4882 and 0.8461, respectively! Not bad!Even so, we would now like to take a look at where our model failed. A detailed presentation of the performance of the model is found in Figure 2.The image above contains a confusion matrix of the performance of our model. Correct classification rates (high accuracy on the diagonal and low values on the off-diagonal) are colored green, while incorrect classification rates (low values on the diagonal and high values on the off-diagonal) are colored red.A close look at the confusion matrix shows that the model made no correct predictions to the categories of “Did not vote” and “Not sure/Don’t recall”. This is due to the small number of observations that belonged to these categories relative to other categories: the number of respondents that answered “did not vote” or “not sure/don’t recall”, represent only 0.86% and 0.43% of the total sample, respectively (Table 1). Therefore, more information is required in order to accurately predict these categories: it is not enough to have information of the voter’s political and ideological preferences in order to know if the respondent did not vote or if they do not recall for whom they voted.Additionally, it may seem surprising that the model had a very poor performance assigning voters to the “others” category (a very poor 14.1% accuracy). This is especially true since about 9.91% of all observations belong to this category (see Table 1). Nonetheless, it is important to notice that this category includes a very diverse pool of presidential candidates, such as Gary Johnson (Libertarian Party) and Jill Stein (Green Party). These candidates have diverse political ideologies, and represent a heterogenous mix of voter preferences and demographics. Therefore, we may argue that it is actually expected that the model is unable to accurately predict any votes to presidential candidates that fall into this category.So, we built a model that predicts voter preferences. How do we forecast the outcome of an election?This is a significantly harder task and is out of the scope of this article, but a good start is training the model we developed here with poll data, and use data from an electoral roll to predict the political preferences of a given State or the whole US population.I hope you enjoyed this post and if you did, please let me know!Kuriwaki, Shiro, 2018, “Cumulative CCES Common Content (2006–2018)”, https://doi.org/10.7910/DVN/II2DB6, Harvard Dataverse, V4",r,https://towardsdatascience.com/using-a-neural-network-to-predict-voter-preferences-ccb9122a6df1?source=tag_archive---------2-----------------------
How Python Overtook R For Machine-Learning,"Emmett BoudreauJun 2, 2020·5 min readRoughly a decade ago, the idea of Pythonic machine-learning was only a glint and glimmer in the eye of many Data Scientists. Among the most popular of the languages used for statistics at the time was R, which was showing promise on the front of machine-learning as well. However, as you are most likely aware, this is no longer the case, and Python has taken the crown as not only the most popular language for machine learning, but also the most popular language in general.While Python and R are both approachable, high-level languages, they certainly have their differences. Some of these differences might have attributed to the uprising of Python as the Data Science language of the future.One factor that definitely contributed to the transition from R to Python for Data Science is probably the usability of both languages. R is a primarily functional language that was created almost exclusively for scientific computing. And while to some extent it is capable in many aspects, R is certainly not considered to be a general-purpose programming language.Python, on the other hand, is primarily an object-oriented programming language. This attributes to the conviviality of Python over R because in a lot of ways programmers might find using objects with easy functions a lot easier than methodized functionality with polymorphism. On top of that, Python wasn’t originally created with statistical computing in mind and as a result, is much more suited for general-purpose programming.While these differences might mean little to those who only do statistical computing, it likely means a lot to those who wish to approach machine-learning from a more technical and less scientific standpoint. In other words, Python is certainly a language that is more geared towards every programmer than just statisticians.The differences in usability don’t stop with uses and paradigms, either. Let’s face it,Pip is superior to R’s ‘package manager’In R’s defense, package managers certainly weren’t a flavor of its time. Despite this, even if R were to come out with a fantastic package manager right now it would likely not be able to compete with the convenience of Pip. Pip makes managing all of your Python libraries incredibly easy, which in turn makes learning how to use packages in Python a lot simpler. There have been attempts at creating a better package manager for R, like PackRat — which is a tool that you should probably pick up if you enjoy programming in R.rstudio.github.ioIn a way, Packrat is like the QuickLisp of R package managers. Certainly in this aspect, R falls victim to the time in which it was released.It’s incredibly difficult to talk about machine-learning without discussing performance. There is a reason that deep-learning computers are typically decked out with multiple graphics cards and server processors, after-all. Python might not have the greatest performance in the world, but certainly takes a step up from R. Here are some micro-benchmarks done by Julia computing that shows just how big the disparity between these languages can be in a lot of operations:Although there certainly are situations where R outpaces Python, for the most part, Python is certainly the better performer of the two. This, of course, attributes to less viability in terms of machine-learning inside of the R programming language. While R, like Python, can certainly draw benefits from calling on other languages like C, and C++, Python’s close integration and ease-of-access with the Python.h header makes it a clear choice for those wishing to create a high-level interface with C.As discussed earlier, Python is certainly a language that is more likely to appeal to the average programmer. That is likely why Python was the language in which packages like Scikit-Learn and Tensorflow were developed to be used by. The Pythonic ecosystem for machine-learning is absolutely stellar, and can’t be beaten!On the other side of the pond with R, though there certainly are some standout packages, when they are compared with all of the versatile options that Python has there is simply no question. Part of the reason for this is that many developers that already knew Python and wanted to get into machine-learning of course decided to just use that language rather than switching to R. And with Python’s partnership with C, it is easy to see why the packages were developed for Python instead of R.Although R has definitely taken the back-seat to Python at this point, R is still very widely used for statistical analysis, bioinformatics, and even machine-learning. However, it seems that R tends to be more popular among the scientific and medical community whereas Python tends to be used by the more technologically-influenced crowd.This is something I find very interesting. Why is it that R is more widely used among Biologists, Medical Doctors, and other disciplines than normal developers, Data Scientists, and computer-scientists dipping their toes into machine-learning? One plausible explanation for this is that R has had a foothold in those industries for such an extended period of time that it has become rather hard to replace. As I talked about before, computer-scientists and people from the programming world as opposed to the general science world would likely prefer to use Python over picking up a statistical language from the 70s. Of course, it wasn’t R but S that was created in the 70s, but S is widely considered to be the predecessor to R— so keep that in mind.I think the statistical computing’s shift from using R and MATLAB to Python is certainly justified. There are a host of advantages to using Python that you simply can’t get anywhere else. Python has superior performance, is more accessible for new developers, and is also the most popular programming language in the world. All of these factors certainly contributed to the rise of Python and its now fantastic foothold in the realm of analytics, statistics, and most of all, machine-learning.Although R is certainly less popular than Python, it seems to have found its home with the more scientific, less computer-centric crowd. It is pretty amazing to see how certain languages can be cycled out in a short period of a few years, and a new language’s capabilities can be realized and taken advantage of in such a short period of time as well.",r,https://towardsdatascience.com/how-python-overtook-r-for-machine-learning-46be67fc19af?source=tag_archive---------1-----------------------
Performance Optimization in R: Parallel Computing and Rcpp,"Emanuele GuidottiJun 7, 2020·5 min readMany computations in R can be made faster by the use of parallel computation. Generally, parallel computation is the simultaneous execution of different pieces of a larger computation across multiple computing processors or cores.The parallel package can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel.The mclapply() function essentially parallelizes calls to lapply(). The first two arguments to mclapply() are exactly the same as they are for lapply(). However, mclapply() has further arguments (that must be named), the most important of which is the mc.cores argument which you can use to specify the number of processors/cores you want to split the computation across. For example, if your machine has 4 cores on it, you might specify mc.cores = 4 to break your parallelize your operation across 4 cores (although this may not be the best idea if you are running other operations in the background besides R).The first thing you might want to check with the parallel package is if your computer in fact has multiple cores that you can take advantage of.The mclapply() function (and related mc* functions) works via the fork mechanism on Unix-style operating systems. Because of the use of the fork mechanism, the mc* functions are generally not available to users of the Windows operating system.Using the forking mechanism on your computer is one way to execute parallel computation but it’s not the only way that the parallel package offers. Another way to build a “cluster” using the multiple cores on your computer is via sockets.Building a socket cluster is simple to do in R with the makeCluster() function.The cl object is an abstraction of the entire cluster and is what we’ll use to indicate to the various cluster functions that we want to do parallel computation.To do a lapply() operation over a socket cluster we can use the parLapply() function.You’ll notice, unfortunately, that there’s an error in running this code. The reason is that while we have loaded the sulfate data into our R session, the data is not available to the independent child processes that have been spawned by the makeCluster() function. The data, and any other information that the child process will need to execute your code, needs to be exported to the child process from the parent process via the clusterExport() function. The need to export data is a key difference in behavior between the “multicore” approach and the “socket” approach.How long does it take?clusterEvalQ() evaluates a literal expression on each cluster node. It can be used to load packages into each node.Once you’ve finished working with your cluster, it’s good to clean up and stop the cluster child processes (quitting R will also stop all of the child processes).The Rcpp package provides C++ classes that greatly facilitate interfacing C or C++ code in R packages using the .Call() interface provided by R. It provides a powerful API on top of R, permitting direct interchange of rich R objects (including S3, S4 or Reference Class objects) between R and C++.Maintaining C++ code in it’s own source file provides several benefits (recommended). However, it’s also possible to do inline declaration and execution of C++ code, which will be used in the following example.Let’s implement the Fibonacci sequence both in R and C++:with F₀=1 and F₁=1.Compare the performance:[1] https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html[2] http://heather.cs.ucdavis.edu/~matloff/158/RcppTutorial.pdf",r,https://towardsdatascience.com/performance-optimization-in-r-parallel-computing-and-rcpp-6b541cd5ccf6?source=tag_archive---------3-----------------------
R Data Types and Data Structure,"Parbati BudhathokiJun 7, 2020·4 min readR is an open-source and popular programming language supported by the R Foundation for Statistical Computing and Data Visualization. This is compliant with a cross-platform such that it can run on a range of platforms, including Windows, Unix, and macOS. R provides an incredible forum for programming modern statistical approaches — in a simple and straightforward manner with state-of-the-art graphic capabilities. Let’s talk about R data types and Structures:Numbers that have a decimal value or are a fraction in nature have a data type as numeric. It has default double-precision i.e. 64-bit float representation.> num<-23.1> class(num)[1] “numeric”Numbers that do not contain decimal values have a data type as an integer. However, to construct an integer data type, you explicitly use as.integer() and pass the variable as an argument. It has a single-precision i.e. 32-bit integer.> num2<-as.integer(num)> class(num2)[1] “integer”A variable that may have a value of True and False like a boolean is called a logical variable.> bool<-TRUE> class(bool)[1] “logical”As the name indicates, a letter or a combination of letters enclosed by quotations may be a type of character data for R. This may be alphabets or numbers. Any list of characters, such as ‘Daniel,’ ‘ABC’, ‘1’, etc.> char<-’hello’> class(char)[1] “character”A vector is the most common and basic data structure, a series of data types of the same type defined by the function c(), a scalar is a vector of length 1. Examples of a vector> a<-4:8> a[1] 4 5 6 7 8> b<-seq(3,30,3)->b> b[1] 3 6 9 12 15 18 21 24 27 30if you would like to label the elements of a vector then,names(a)<-c(‘four’,’five’,’six’,’seven’,’eight’)A 2- dimensional structure containing identical data types stored internally as vectors.> M<-matrix(1:12, 3, 4, byrow=TRUE)> M[,1] [,2] [,3] [,4][1,] 1 2 3 4[2,] 5 6 7 8[3,] 9 10 11 12Elements of a matrix can be referenced by specifying the index along each dimension (e.g. “row” and “column”) in single square brackets.>M[2,3][1] 7It is the multidimensional extension of a vector. A two-dimensional array is the same thing as a matrix. An array is created using the array () function. We can use vectors as input and create an array using the below-mentioned values in the dim parameter.Array_NAME <- array (data, dim = (row_Size, column_Size, matrices, dimnames)data — Data is an input vector that is given to the array.matrices — Array in R consists of multi-dimensional matrices.row_Size — row_Size describes the number of row elements that an array can store.column_Size — Number of column elements that can be stored in an array.dimnames — Used to change the default names of rows and columns to the user’s preference.> vector1 <- c(20,9,1)> vector2 <- c(14,16,11)> result <- array(c(vector1,vector2),dim = c(2,2,2))> result, , 1[,1] [,2][1,] 20 1[2,] 9 14, , 2[,1] [,2][1,] 16 20[2,] 11 9In R lists act as containers. It is taken as generic vectors because that allow elements of different type of R object.> new_list<-list('a',123,TRUE,5.4)> new_list[[1]][1] ""a""[[2]][1] 123[[3]][1] TRUE[[4]][1] 5.4Data frames are useful to store data in matrix-like form while allowing for different modes. It is a table or array of mixed data types like pandas in python is based on this structure. Each column must have one mode(a type of data), however, the modes of each column do not have to be the same.> patientID <- c(1, 2, 3, 4)> age <- c(25, 34, 28, 52)> diabetes <- c(""Type1"", ""Type2"", ""Type1"", ""Type1"")> status <- c(""Poor"", ""Improved"", ""Excellent"", ""Poor"")> patientdata <- data.frame(patientID, age, diabetes, status)> patientdatapatientID age diabetes status1 1 25 Type1 Poor2 2 34 Type2 Improved3 3 28 Type1 Excellent4 4 52 Type1 PoorBecause data frames are rectangular, elements of the data frame can be referenced by specifying the row and the column index in single square brackets (similar to the matrix).> patientdata[3,4][1] ExcellentAs data frames are also lists, it is possible to refer to columns (which are elements of such list) using the list notation, i.e. either double square brackets or a $.> patientdata[[“age”]][1] 25 34 28 52> patientdata$age[1] 25 34 28 52Useful Data Frame FunctionsNominal and ordinal variables are called factors and are coded numerically internally, e.g. They are a data type that is used to refer to a qualitative relationship like colors, good & bad, course or movie ratings, etc. They are useful in statistical modeling. From the above example of patientdata> class(patientdata$diabetes)[1] “factor”Levels of factor can be checked using the keyword levels, and the type of level will be a character.> levels(patientdata$diabetes)[1] “Type1” “Type2”",r,https://medium.com/@parbatibudhathoki/r-data-types-and-data-structure-a688d90d9912?source=tag_archive---------6-----------------------
Array in JavaScript,"JaiJun 10, 2020·4 min readAn array is a data structure consisting of a collection of elements where each element is identified by an index in the array. An array can be single dimensional or multidimensional. Traditionally, a single dimensional array is represented as seen belowThe above array has eight elements with values 1, 2, 3, 4, 5, 6, 7, & 8. The indices of an array can be seen above the elements and the first item in the array is found at index 0. We say that the length of the array is 8 and the first index of the array starts at index 0 and if you have eight elements in the array then the last element will be found at index 7, because if the array starts at 0 then the last element will be found at length of the array - 1 and in our case that will be 8 -1 = 7.How do we declare an array?In the above code, both the variables, arr and arr2 are objects and you can find out their type by running the following commandBoth the variables, arr and arr2 are declared as constants because their type cannot be changed but the values in them can be changed.The two arrays we created, are two different ways of creating an array in JavaScript. The first one is with a literal notation and the second one is with a class and when we use the Array class we can specify how many elements we need at the time of creation. Check out what the MDN docs have to say here.There are many ways to add an element to an array and instead of decribing them here please checkout the MDN docs forWe populate our two arrays with dataThe push method on arr is used to add either one element or multiple elements separated by a comma and we have added eight elements.The push method on the second array, arr2 was created with having eight empty slots but we added nine elements to it and this is indeed possible in JavaScript as the size of the array will increase to accomodate the 9th element.Since arr and arr2 are arrays we will focus on arr moving forwardAfter declaring your array the next thing we can do with it is to access the elements.Remember, array starts at index 0 and the last element is found at the length of the array — 1. Accessing an element at index 9 will return undefined.Usually a for loop is used to iterate over elements in an array and this is how we do it in JavaScript.JavaScript also provides another way to iterate over elements, the forEach wayJavaScript also provides another for loop called for of which returns the element from the arrayA while loop can be used to iterate over elements but in a while loop or a do while loop you have to take care of the index and making sure it is incremented correctly and while comparision statement used in the while statement.An array is one of the most basic data structure and with an array we can implement algorithms like bubble sort, union find, & quick find to name some. The code in this article can be found in this gist.",arrays,https://medium.com/@iJKTen/array-bccda8c9893b?source=tag_archive---------3-----------------------
How to remove an element from an array in JavaScript,"So you want to remove an item from an array in JavaScript?Well, you’re not alone!It’s one of the most upvoted JavaScript questions on stack overflow and can feel a little unnatural considering how simple it is to array.push().If you want to remove an element while leaving the original array intact (unmutated) then filter() is a good choice!Removing a single element:Breaking that down.We define a variable newArray and set it equal to the return value of our array filter. Inside of our filter, we pass an ECMAScript 6 arrow function.This function tests each item in the array and returns:Removing multiple items:We follow the same method to filter our array, but this time, we test that each item is NOT included in our toDelete array.WARNING: there is no support in Internet Explorer or older web browsers for arrow functions and Array.includes(). If you have to support these browsers consider Method 2 OR take a look at BabelJS.The Array.splice() method allows us to remove any type of element from an array e.g. numbers, strings, objects, booleans etc.To remove something from an array with the splice() method, we need to provide two things:This means we must first figure out which position in the array our element is.For that, we’ll use the indexOf() method:Note thatindexOf() returns -1 for elements which don’t exist.Putting it all together:WARNING: this method removes the first instance of the item as indexOf() returns the index of the first matching item it finds.The splice() method does not return the original array but mutates it.This means that if we were to assign the return result to a variable, it would not be the original array with the object removed, but an array containing the removed object(s).When we console.log the original array, it has been mutated removing the element at index 2.We now have access to the removed element(s) array in our returnValue variable.The splice() method can do more than just remove items from an array.It also allows you to replace the item you’re removing with one (or more) items:This means that if we want to both remove AND replace an item, we simply:In summary, we can use Array.filter() or Array.splice() to remove items from our arrays.Array.filter() leaves the original array unmutated, while Array.splice() mutates the original array and returns the removed element(s).",arrays,https://javascript.plainenglish.io/how-to-remove-an-element-from-an-array-in-javascript-54612785295e?source=tag_archive---------0-----------------------
LeetCode 26. Remove Duplicates from Sorted Array,"Arpit ChoudharyJun 7, 2020·2 min readQuestion:Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.Example 1:Example 2:Solution:In this problem, the key point to focus on is the input array being sorted. As far as duplicate elements are concerned, what is their positioning in the array when the given array is sorted? Look at the image above for the answer. If we know the position of one of the elements, do we also know the positioning of all the duplicate elements?Hence, we will solve this problem by using two pointers sliding window pattern. We’ll keep two pointers (indexes) -Time Complexity: O(n) where n is the number of elements in the arrayI hope this post helped! Please do let me know if you have any feedback, comments, or suggestions by responding to this post.Happy Coding !!",arrays,https://medium.com/@choudharyarpit99/remove-duplicates-from-sorted-array-37a90a4c6dd8?source=tag_archive---------5-----------------------
LeetCode 1299. Replace Elements with Greatest Element on Right Side,"Arpit ChoudharyJun 7, 2020·2 min readQuestion:Given an array arr, replace every element in that array with the greatest element among the elements to its right, and replace the last element with -1.After doing so, return the array.Example:Constraints:Solution:A naive approach would be to run two loops where the outer loop will pick the array elements one by one from left to right and the inner loop will find the greatest element present in the array after the picked element. Finally, the outer loop will replace the picked element with the greatest element found by inner loop. The time complexity of this method will be O(n*n).A better approach would be to replace all elements with a single traversal of the array. The idea is to start from the rightmost element, move to the left side one by one, keeping a track of the maximum element. Replace every element with the maximum element.Time Complexity: O(n) where n is the number of elements in the arrayI hope this post helped! Please do let me know if you have any feedback, comments, or suggestions by responding to this post.Happy Coding !!",arrays,https://medium.com/@choudharyarpit99/replace-elements-with-greatest-element-on-right-side-fedec5bab280?source=tag_archive---------3-----------------------
Find the missing number,"Amar Jyoti KachariJun 7, 2020·1 min readYou are given an array of ‘K’ numbers which contain numbers from 1 to K+1. Only one number is missing and the array is not sorted. Your job is to find the missing number.Following are some ways to find the missing numberUsing XOR1.Do XOR of all numbers in the array and suppose you find ‘A’2.Do XOR of all natural numbers from 1 to K+1 suppose you find ‘B’Then missing number is => A xor B — — — — — — — — — — — — —Using Sum — I1.Expected Sum is (K+1)*(K+2)/2 = ‘A’2.Total sum of numbers in array is ‘B’Then missing number is => A — B — — — — — — — — -Using Sum — II3. Last remaining ‘A’ will be the actual missing number.",arrays,https://medium.com/@akachari/find-the-missing-number-90860db38b4f?source=tag_archive---------10-----------------------
LeetCode 26: Remove Duplicates from Sorted Array,"mktSuWinJun 10, 2020·5 min readI believe that learning by doing approach enables me to better understand the concepts, algorithms, detailed steps, solve coding challenge.This blog post focuses on LeetCode coding challenge no. 26 Remove Duplicates from Sorted Array. The topics covered are:2. LeetCode Logic3. Java ImplementationI have learnt the following concepts while doing this coding challenge.1.1 In-place sorting algorithmNOTE: Example images are from, baeldung website.1.2 Auxiliary Data StructureAuxiliary Data Structure is a helper data structure which might be used to solve a given problem and is terminated after the problem is solved. (Coming from the construction project background, this term reminds of scaffolding.Example: Find the count of each element in an array. Solution:1.3 Two Pointer TechniqueThis technique is used for sovling strings, arrays or lists where analysing the elements of the array or elements comparison is required. This technique does not require a temporary array creation and is used to loop over the given data structure therefor providing the space and time-efficient solution. This processes two elements per loop.The following variants of two pointer technique content are taken from plusralsight Algorithm templates: Two pointers Part 1 & 2. They are excellent and beginner friendly.Old and new state: old, new = new, cur_resultSlow and fast runner: slow-> fast->->Left and right boundary: |left-> ... <-right|Pointer-1 and pointer-2 from two sequences: p1-> p2->Start and end of sliding window: |start-> ... end->|1.4 Time Complexity O(1)O(1) definition from GeeksforGeeks — is the fastest possible running time for any algorithm and is commonly referred to as Constant Running Time. In this case, the algorithm always takes the same amount of time to execute, regardless of the input size. This is the ideal runtime for an algorithm, but it’s rarely achievable.NOTE: This image is from Analysis of Algorithms | Big-O analysis, geeksforgeeks.org site.NOTE: I was struggling with the logic and was confused about iterating through the array without using additional memory space to store the elements. I had to google stackoverflow and youtube to better understand the concepts and java implementation. GoodTeacher youtuber explained well on how to use two pointers technique and its java implementation. Therefore I have decided to write about his coding solution logic.",arrays,https://medium.com/@mktsuwin/leetcode-26-remove-duplicates-from-sorted-array-c57546030653?source=tag_archive---------0-----------------------
How to convert a Word DOCX to a PNG Array in Python,"CloudmersiveJun 13, 2020·2 min readToday’s post is all about proving that old adage “pictures speak louder than words.” To make this happen, we will be using Python to convert from a DOCX document file into a lovely array of PNG images. Instead of using the old fashioned way, which takes forever, we will be saving a boatload of time with the aid of an API. Let’s get started!Pip installation is our first order of business, like you see here:Now we are able to call our function, which is easy with this example code to follow.Boom! Done. Your resulting image array will be returned as part of a JSON object. Easy.",arrays,https://medium.com/@cloudmersive/how-to-convert-a-word-docx-to-a-png-array-in-python-7e80482e56dd?source=tag_archive---------2-----------------------
Quickly Learn How You Can Improve Your Java Coding,"In Instructions for Practical Living and Other Neo-Confucian Writings, the greater Chinese statesman and philosopher Wang Yangming wrote the following words:Lust grows day by day, like the dust on the floor. If you do not sweep the dust every day, it will accumulate. If you work hard, you will find that there is no end to the journey of life. The more you explore, the more there will be left to know. What is necessary is precision, clarity, and completeness.Wang Yangming’s wise words are still very much relevant to our daily lives today. Bad code, just like lust and dust, increases every day. If you do not constantly work to clean it, it will accumulate. But, if you work hard to clean bad code, you can improve your programming ability and make your code precise and clear, with no room for doubt. This article introduces three ways to improve your Java code based on the actual coding work of an Alibaba Cloud engineer, with bad code samples provided.www.datadriveninvestor.comYou should iterate entrySet() when the primary key and value are used. This is more efficient than iterating keySet() and then getting the value.Bad code:Good code:Compared with Collection.size(), Collection.isEmpty() is much more readable and provides better performance when it comes to detecting null values. The time complexity of Collection.isEmpty() is always O(1), but that of Collection.size() may be O(n).Bad code:Good code:To detect null values, you can use CollectionUtils.isEmpty(collection) and CollectionUtils.isNotEmpty(collection).Passing a collection as a parameter to the collection itself is an error or meaningless code.For methods that require unchanged parameters during execution, an error may occur when you pass a collection to itself.Bad code:The collection class of Java is easy to use, but the collection size is limited in source code. The time complexity of each scaling operation may be O(n). You can specify the predictable collection size whenever possible to reduce the occurrences of collection scaling.Bad code:Good code:In Java, concatenated strings are tuned during compilation. However, strings that are concatenated in a cycle are not concatenated during compilation. In this case, concatenate strings by using StringBuilder.Bad code:Good code:Random access to arrays is more efficient than that to linked lists. When a called method needs to randomly access data in the acquired List, without knowing whether an array or a linked list is internally implemented, you can check whether the RandomAccess operation is used.Good code:In the collection class library of Java, the time complexity of the contains method for List is O(n). If you need to frequently call the contains method in the code to search for data, you can convert List into HashSet to reduce the time complexity to O(1).Bad code:Good code:Append the uppercase letter L to long integer constants. Do not use the lowercase l, because it can easily be confused with the digit 1.Bad code:Good code:Magic numbers may make your code very clear, but it will be difficult to debug. Therefore, magic numbers must be defined as readable constants. However, -1, 0, and 1 are not considered magic numbers.Bad code:Good code:Assign values to static member variables of the collection type by using static code blocks rather than collection implementations.Bad code:Good code:Java 7 introduced the try-with-resources statement, which is used to close related resources and makes program code simpler and more secure. It is better than the original try-catch-finally statement.Bad code:Good code:Delete unused private methods and fields to make the code simpler and easier to maintain. You can recover deleted methods and fields from historical commits.Bad code:Good code:Delete unused local variables to make the code simpler and easier to maintain.Bad code:Good code:Unused method parameters are misleading. Delete them to make the code simpler and easier to maintain. However, do not delete unused parameters for override methods that are defined based on the methods of parent classes or interface methods.Bad code:Good code:The redundant brackets of expressions are considered unnecessary by some coders but helpful for code reading by other coders. For Java experts, these redundant brackets only make the code look complex.Bad code:Good code:The tool class is a collection of static fields and functions and must not be instantiated. In Java, an implicit public constructor is added to each class without constructor definition. If you are a Java novice, we recommend that you define an explicit private constructor to mask the implicit public constructor.Bad code:Good code:If exceptions caught by using the catch statement are thrown without processing, the result is the same as not catching the exceptions. To solve this problem, you can delete the related code block or add another processing method.Bad code:Good code:Though public static constants can be accessed through class instances, this may lead to the misunderstanding that the instances of each class have a public static constant. We recommend that you access public static constants through classes.Bad code:Good code:Prevent null pointer exceptions through coding (for example, no null values are detected) rather than through catching exceptionsBad code:Good code:Use String.valueOf(value) rather than """"+value to convert other objects or types into strings more efficiently.Bad code:Good code:If a segment of code is outdated but cannot be deleted for compatibility reasons, you can add the @Deprecated annotation to the code so that it will no longer be used. Add @deprecated to document comments to give an explanation and provide an alternative solution.Good code:BigDecimal(double) can cause accuracy losses and abnormal business logic during precise computation or value comparison.Bad code:Good code:To return null values, require the caller to detect null values. Otherwise, a null pointer exception may be thrown. Returning null arrays or collections can prevent null pointer exceptions from being thrown when the caller does not detect null values. To simplify the code, you can delete the statement that instructs the caller to detect null values.Bad code:Good code:The equals method for objects often throws null pointer exceptions. To solve this problem, call the equals method by using constants or objects with determined values. The best solution is to use the java.util.Objects.equals() method.Bad code:Good code:Enumeration is often used in the same way as constants. If enumeration contains public property fields or field setting methods, the properties of these enumerated constants are prone to modification. Ideally, enumerated property fields are private and assigned values in private constructors. We recommend that you add the final modifier due to the lack of the Setter method.Bad code:Good code:The string-specific split method passes a separator string which is a regular expression. Some keywords, such as .[]() \|, must be escaped.Bad code:Good code:And that’s it. I hope that you also learned a thing or two from Wang Yangming’s wise words. If nothing else, I hope that our Java coding guide helps you write more effective and elegant code.Are you eager to know the latest tech trends in Alibaba Cloud? Hear it from our top experts in our newly launched series, Tech Show!www.alibabacloud.comGain Access to Expert View — Subscribe to DDI Intel",arrays,https://medium.datadriveninvestor.com/quickly-learn-how-you-can-improve-your-java-coding-c0fc02d4bcaa?source=tag_archive---------0-----------------------
Are you sure to understand Java arrays?,"Java arrays are the most primitive way of aggregating elements. Yet, are you sure to really understand how they work and how to use them? You may be surprised…Java arrays are no new feature of the language. To review the basis, let’s recall that array-types are signalized using the brackets [ ] and created using the curly brackets { } , in a quite straightforward way:The above is syntactic sugar for the more verbose form (which we are going to stick on in the present text)Accessing elements in arrays is expected to be super fast, because of JVM optimization. In comparison with languages like Rust, array lengths is not part of the array type: the type of our array is String[] without information about its length. However, the length of an array is immutable: an array cannot change its size.In order to change the size of an array, you need to perform a two-steps operation: first create an array with the expected new size, then copy the source in the target:In the above example, the array of stringsis stored in a variable of type String[] . One could wonder in what kind of variable we can store our array of String.The good news is that array-types are covariant. This means that if String is a subtype of Object , then you can store some String[] in a variable of type Object[] :As usual, you should note two things:Why are arrays covariant? Well, this may be understood by looking at the arrayCopy method. If array types were invariant (as generic types in Java), it would mean that you would require as many versions of arrayCopy as array types.And since there exists one array-type for each type in the Java ecosystem, you could not code the arrayCopy method. You could argue by some generic-type argument but do not forget that at that time, Java had no generics! (They appeared in Java 5 only).Let’s inspect with more attention the following two codes:versusIn both cases, the variables myArray_OfStrings aretyped as Object[] . This means that an end-user receives some Object when accessing any entry of the array:But there is another, subtle difference. What happens if we know try to mutate our entry with another object:What the hell happened? Here is the trick: the variable myArray2OfStrings references an array that has been created as an array of Object , and this array contains String instances. The array as an object still knows many things: its length, but also its type. In other words, the instance referenced by myArray2OfStrings knows that it’s an array of Object and as such, it does not crash if you plug another general object in one of its entries.On the contrary, the instance referenced by myArray1OfStrings is an array that has been created as a String[] array. It also knows its length and its type, and it knows that the elements it should contain are expected to be String instances. Hence, trying to plug some general object in one of its entries results in a ArrayStoreException : you cannot store an Object in an array of String.You should be obfuscated to learn that: what about the covariance we have seen above? If String[] is a subtype of Object[] , we should be able to store Object[] in it.Well, in fact there is another subtle thing. The point is that String[] is assignable to Object[] , which means that you can reference an instance of String[] in a variable of type Object[] .The instanceof operator will also behaves nicely, because if mainly check assignability.But you should pay attention that String[] is no subclass of Object[] . A quick test shows that:In other words, String[] is no subclass of Object[] although it’s a subtype. In particular, you can store a String[] in an Object[] , but you cann’t perform any Object[] operation on it (in particular, you can mutate the entry as you want).You may think that the above examples are really weird, and no one would never do that. Well, don’t be so confident!Assume you have some interface Foo and one implementation FooImpl :You then expose a Foo array (and no FooImpl , off course!) but as you need to perform first custom operations on it, you write this:Now if a client also wants to mutate the array for achieving his goal, he would end in some ArrayStoreException !There is no good practice in the community: should you first copy the array for exposing a fair Foo[] or should it be the responsibility of the client to copy if necessary?The least we can say (and please, without entering in the debate of primitive arrays) is that everyone should be aware of those subtle rules governing the world of primitive arrays!",arrays,https://medium.com/javarevisited/are-you-sure-to-understand-java-arrays-99e979905999?source=tag_archive---------2-----------------------
Distributed subsegment sums,"We have a fixed array of integers of size N. There are also incoming requests that we need to process. Each request consists of two indices i and j. The response to that request should contain a sum of all elements of the array starting from index i to index j, that is, a[i] + a[i + 1] + … + a[j]The array is so large that it does not fit into memory of a single computer.Time complexity: O(1) per request.First, let’s understand how would we solve this task if the data fitted into the memory of a single machine.In that case, we can calculate the prefix sums array p.p[0] = 0 and p[i] = a[0] + a[1] + .. + a[i -1] if i >= 1.Then the answer to the request (i, j) would be p[j + 1] -p[i].For example, if the input array is a = [1, 2, 3, 4, 5], then we first construct the prefix size array [0, 1, 3, 6, 10, 15]. If we encounter a request with indices (2, 4), we should compute 15–3 = 12, which is the same as computing 3 + 4 + 5 = 12.So, to conclude, we can preprocess the array by constructing the prefix sums array. Then, on each request (i, j) we respond with p[j + 1] -p[i], which is O(1) time complexity per request.If we want to apply the same approach with multiple machines, we will immediately face a problem. We cannot simply preprocess data to compute prefix sums, because the data is randomly scattered across multiple machines.But even more fundamentally, here comes a question of how is data stored on the machines in the first place. When the data was stored on a single machine, it was obvious what was the order of the data. We could have easily understood what is the 1st element, what is the 2nd one and so on. But now, when data is stored across multiple machines, how do we even define the order of the elements in the array?There are many ways to define the order of elements. Let’s take the simplest one: each element will be stored together with its index.So if we want to store the array [3, 7, 9, 8] on 2 machines, one can store pairs (0, 3), (2, 9), on one machine and pairs(1, 7), (3, 8) on the other. The first element in a pair is the index of a number, and the second element in a pair is the number itself. Now at least we have a semantically clear idea of the order of elements.But, we still need to somehow compute prefix sums. Let’s try to do that with map-reduce.In map-reduce, we have key-value distributed data tables (k, v), and there are several operations allowed with this data:Coming back to our initial task, let’s assume data is stored initially in the table keyed by indices and values are numbers of the array. We will denote it like this: (i, a[i])Let’s compute the following tables for each k:T(k) = (i, a[max(i -2^k + 1, 0)] + … + a[i])T(0) is exactly (i, a[i]), which we already have.T(log(N) + 1) = (i, a[0] + … + a[i]), which is the one we want (prefix sums table).The only thing remaining is to explain how to compute T(k + 1) having computed T(k) before.T(k) = (i, a[max(i -2^k + 1, 0)] + … + a[i]).First we can map it by adding 2^k to all keys: M(k) = (i + 2^k, a[max(i -2^k + 1, 0)] + … + a[i]).Secondly, we notice that M(k) = (i, a[max(i -2^k -2^k + 1, 0)] + … + a[i -2^k]). It is the same, but mathematically expressed differently by substituting i with i -2^k.Now let’s join T(k) and M(k) and let’s sum up the values. Then we exactly get T(k+1), because a[i -2^k -2^k + 1] + … + a[i -2^k] + a[i -2^k + 1] + … + a[i] =a[i - 2^(k + 1) + 1] + .. + a[i].So we explained how to compute T(k + 1) having computed T(k) already.T(0) is our initial table. From it we compute T(1), then from T(1) we compute T(2), etc. until we compute T(log(N) + 1), which contains all the prefix sums of the initial array.We’ve learned how to compute prefix sums using map-reduce, now we should also learn how to answer the client request (i, j).We can put our map-reduce computed table (i, p[i]) into the distributed real-time key-value storage. After that, on each request (i, j), we will need to get value p[i] by key i, and p[j + 1] by key j + 1. After that, we compute p[j + 1] -p[i].So all the remaining complexity is hidden under the implementation of distributed real-time key-value storage, which we do not discuss in this solution, because it is a bit out of the scope of the problem being discussed.Despite that, distributed real-time key value storage is a very complicated piece of infrastructure if done properly to account for machine failures, network splits and hot keys.",arrays,https://medium.com/verbetceteratech/distributed-subsegment-sums-7c58e3eb22d3?source=tag_archive---------1-----------------------
HOW WELL DO YOU KNOW ARRAYS!,"Akshit GuptaJun 11, 2020·5 min readAn array is a data structure that contains a group of elements. Typically these elements are all of the same data type, such as an integer or string. Arrays are commonly used in computer programs to organize data so that a related set of values can be easily sorted or searched.To initialize an array in java, we use the following syntax —int[] arr = new int[10];Here,Arrays use contiguous memory allocation.As depicted above, we have an array of integers of size 12. The array is allocated memory from 200 to 244.Accessing a specific element in an array is an operation of O(1) time.Question is, how is it possible? Well, simple approach is to traverse the array from the initial position and reach to the index where the element is stored. It will take O(n) time.But, it does not work this way. The magic is related to the address where the element is stored.The formula used is —Address of element = initial_address + (M x N)Initial_address refers to the address of first element of the array, N refers to the index of array and M refers to the size of the data type of the elements stored in the array.For instance, we want to access 1 in our array.1 is stored at index 4, N=4Array contains integers, M=4Initial_address that is address of first element is 200.=> address of 1 = 200 + (4 x 4) = 216The compiler calculates the address of the element and retrieves it in O(1) time.Insertion in an array can be divided into three cases —Insertion at the beginningIf the array is empty, then inserting an element in the beginning will take O(1) time. If the array is not empty and we insert an element at the beginning of the array, it will take O(n) time because all other elements in the array will be shifted to the right.Insertion in the middleInserting an element in the middle, if the array is not full, will be an operation of O(n) time because all other elements in the array after that index will be shifted to the right.Insertion at the endInserting an element at the end of an array is an operation of O(1) time given the array is not full.Deletion in an array can be divided into three cases —If the array contains only 1 element, then deleting the element will take O(1) time.Deletion from beginningDeleting an element from beginning of an array will take O(n) time because all other elements will be shifted towards left.Deletion from middleDeleting an element from middle of an array will take O(n) time because all other elements after that index will be shifted towards left.Deletion from endDeleting an element from end of an array is an operation of O(1) time.After the array is full, we cannot insert any more elements in the array. Array is a static data structure i.e. we need to define the size of the array at the time of initialization of the array.To overcome this drawback, we can use dynamic arrays.In C++, we have vectors and in Java we have Arraylist.Inserting an element in an array takes O(1) time. Same is in the case of dynamic array i.e. arraylist.Question is, what happens when the arraylist is full? How the size of arraylist increases?Suppose we have an arraylist of size 10. When the arraylist is full, the compiler will initialize a new arraylist of size = (2*size) and copy the previous elements in the new arraylist.This implies, inserting an element in a dynamic array when the array is full will take O(n) time as we need to copy all the elements in the new array and then insert the element. But for other indices, the insertion time is O(1).We analysed the arrays. Outcomes are —",arrays,https://medium.com/@akshit.gupta98/how-well-do-you-know-arrays-586d825b79dd?source=tag_archive---------0-----------------------
Apartment Hunting Problem,"Roshan PandeyJun 7, 2020·2 min readHi, let's encounter a problem from the Algoexpert array section. The problem is -Problem Statement — You’re looking to move into a new apartment, and you’re given a list of blocks where each block contains some services that it offers. In order to pick your apartment, you want to optimize its location in such a way that the maximum distance to any services that you care for is minimized.Example —blocks = [ { “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ]requirements = [ “gym”, “school”, “store” ]Solution Approach — There are multiple possible answers to that question, where we can basically establish a trade-off between space and time complexities.Suggested approach — Initialize a vector that stores the farthest distance of a particular service for every block with INT_MIN. Now for every block, go through every requirement and find the closest block that satisfies that requirements and change vector value for that block with a minimum of the current value and maximum distance for any requirement. At the end return the index the minimum value in the vector.I hope it helps :-)",arrays,https://medium.com/@pandeyroshan556/apartment-hunting-problem-85c6acee8741?source=tag_archive---------1-----------------------
Two Pointers Technique,"Gaurav GuptaJun 5, 2020·3 min readLet’s discuss one of the most important techniques of Array named Two Pointers Technique.what does this mean?Let's forget the computer jargon and try to relate it with a real-world scenario.for example-Let’s take there are n shops and in one of the shop crime scenes happened but don't know in which it happens?Now Sherlock is in a hurry and with the help of Watson, He wants to find the shop quickly.Now, what will Sherlock do?Sherlock is smart and he finds a certain way of solving this problemwow, Sherlock is now able to go to his date quickly.So, I hope you will now get the idea of the Two Pointers Technique.Let’s Elaborate this now.The idea here is to iterate two different parts of the array simultaneously to get the answer faster.Let’s take one simple code example to illustrate this concept.Reverse the String.we will swap the last and first character of string using two pointers technique.Let’s go through this example.swapTheCharacters() function will swap the characters so that easy part is up to you.what we have done is that we are iterating over the string and keeping two pointers I and J in the opposite direction.“i” will move in forwarding direction and “j” will move in a backward direction and hence we will keep going like this until a situation came where “i<j”.This code shows the opposite directional Two pointers.I hope you like this article. Share and feel free to clap and Don’t forget to subscribe.",arrays,https://medium.com/@gauravit1996/two-pointers-technique-2f5ac7cd7190?source=tag_archive---------3-----------------------
Find two repeating elements in array,"Amar Jyoti KachariJun 7, 2020·1 min readYou are given an array of size n+2 where you have number from 1 to n. Two elements are repeated in the array. Find those repeating elements in the array.SolutionA good solution is to create to mathematical equation. Maximum possible number in array is ‘N’. Suppose your missing numbers are ‘X’ and ‘Y’. And sum of all numbers in the array is ‘S’. And product of all numbers in the array is ‘P’. Now we can make following equations.Equation 1: X+Y = S — (N*(N+1)/2)Equation 2: X*Y = P/N!Why above equations are true? Because Sum of all natural number from 1 to N is (N*(N+1)/2)Product of all natural number from 1 to N is N! (N factorial)Now just solve the equations to find X and Y",arrays,https://medium.com/@akachari/find-two-repeating-elements-in-array-e1964d7b49c2?source=tag_archive---------9-----------------------
PostgreSQL Data Deduplication Methods,"Alibaba CloudJun 9, 2020·11 min readDeduplication is a common process with many variants. For example:1) Remove duplicates from a single column.Retention Rule: Retain the latest or oldest record or the record with the largest value of a specified field.2) Remove duplicates from multiple columns.Retention Rule: Retain the latest or oldest record or the record with the largest value of a specified field.3) Remove duplicates by rows.Retention Rule: Retain the latest or oldest record or the record with the largest value of a specified field.4) Multi-column hybrid deduplication (for example, ROW1: col1=1, col2=2; ROW2: col1=2, col2=1).Retention Rule: Retain the latest or oldest record or the record with the largest value of a specified field.5) If multiple versions of a record are generated due to pg_resetwal, determine the records to be deleted or retain the record with the largest XMIN value.The following examples show how to use different methods to remove duplicates. Choose the one that works best for you.Consider the following test data.Requirement: Remove repeated rows in c1 and retain the rows with the largest ID.Deduplication per record only involves the overhead of one sort operation and the comparison of each record.Index sorting can be used even if some driving columns exist. For example, index(c1) is used for order by c1,id.Consider the following test data.Requirement: Remove repeated rows in c1 and c2 and retain the rows with the largest ID.Using ctid = any(array(select ctid from ...)); to remove duplicates is the fastest method.Consider the following test data.Requirement: Remove repeated rows and keep one record.Reserve data by row number when no primary key exists.The NOT IN method of ctid causes a loop, which affects performance. We do not recommend using the NOT IN method.To remove duplicates in multiple columns, use arrays. However, if the element orders in two arrays are inconsistent, the elements in the arrays are different.Therefore, sort and store the elements and use the columns that require deduplication as array elements.Create a sorting function that supports any columns and output sorted arrays.Consider the following test data.Requirement: Remove records repeated in c1 and c2 (for example, 1,2 and 2,1 are repeated records) and retain any record.Use sort_vals to sort and reorganize arrays. This method is simple and easy to understand.Use SUBQUERY to sort and reorganize arrays. This method is efficient but complicated.The performance of the three methods varies greatly depending on whether index acceleration is applied. However, the most stable method is to use window functions. Therefore, we recommend using window functions. When the element orders in two arrays are inconsistent, the elements in the two arrays are different and you can customize the shuffling function.www.alibabacloud.com",arrays,https://medium.com/@alibaba-cloud/postgresql-data-deduplication-methods-1bf54eb267d5?source=tag_archive---------2-----------------------
Prefix Sum Algorithm,"Gaurav GuptaJun 8, 2020·5 min readHey folks, As you can see today’s topic is Prefix Sum Algorithm.Another most important topic of Array through which range query problems are being solved.Let’s see an example,There is n number of kids, and these kids love to eat chocolates. well, what’s so special about these kids, as each kid love to eat chocolate. But again, Mother didn’t allow them to eat the chocolate 😅. So at the end of the day, she went to the shop and asked the shopkeeper from how many chocolates did this range of kids ate in a day?So, let's say there are 10 kids.1, 2, 3, 4, 5, 6, 7, 8, 9, 10At 10:00 a.m, some of the kids brought chocolates for themselves.let's say 1, 2, 3, 4, 5 kids or we can say kids from 1 to 5 have brought 3 chocolates, and ate them.again at 1:30 p.m, kids range from 4 to 8 brought 7 chocolates and enjoyed.Now kids at night, ranged from 6 to 9 brought 1 chocolate, and ate them.Now what? It’s their sleep time 🤔. and they slept.But her mother is very worried and as her responsibility, she wants to know which kids have eaten how many chocolates the whole day.So, she went to the shopkeeper and asked her how many chocolates did the kids ate?The shopkeeper is smart and always keeps updating the account book and he knew but there is a long queue of the customer and he also wants to attend the other customer. So how can he respond back to his mother and handle all customers as quickly as possible?I hope you got the problem 🙎 ‍with the mother and shopkeeper problem.Let’s help them.again we will see two solutions, a basic and then an efficient way.so, as the shopkeeper is maintaining the record of kid’s visit.what he can do is to update the cost each time from the beginning of the day.So,shopkeeper= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]; Don’ sneak in the book please. 😋so, as we know at 10:00 a.m 1 to 5 children came and took 3 chocolates.So, shopkeeper= [3, 3, 3, 3, 3, 0, 0, 0, 0, 0]; updates his bookagain those cute kids came at 1:30 p.m and took 7 chocolates.shopkeeper= [3, 3, 3, 10, 10, 7, 7, 7, 0, 0];and the last group at night came and took 1 chocolate and now final time the shopkeeper updates his records as shopkeeper= [3, 3, 3, 10, 10, 7, 8, 8, 1, 0];Let’s say we have this list and mother asked random questions.tell me, Ramesh, kids from range 1 to 4 ate how many chocolates and now shopkeeper is started adding the numbers, and gave the result as 19.again she asked kids from range 5 to 7 and again the shopkeeper start counting and gave result as 25.and more and more.But this took time and some of his customers went away.The shopkeeper is sad as this approach is taking too much time.So, he wants some other technique. Let’s help this man.So, now what the shopkeeper will do instead of adding up again and again he will only update some entries and in the add apply the algorithm as this man is smart. “Prefix Sum Algorithm”let’s see how, as we now know the problem so, let's see the shopkeeper way…so the shopkeeper is having this result-shopkeeper= [3, 3, 3, 10, 10, 7, 8, 8, 1, 0];but now before the mother starts asking the question. The Shopkeeper knows the algorithm and starts managing it.can you see the pattern? The Shopkeeper have the register and I hope you also have it.So, why wait? Take out your pen and match the result.shopkeeper= [3, 6, 9, 19, 29, 36, 44, 52, 53, 53];Did you also get the same?If yes! 🥳 congrats you have learned the first part of the algorithm and those who are just reading please enjoy the reading.Now, the mother has many questions and she starts with her query.tell me, Ramesh, kids from range 1 to 4 ate how many chocolates and now the shopkeeper instead of adding directly check the number 4 kid and gave the result as 19 which is correct as above mentioned.again she again asked kids from range 5 to 7? and by applying some calculationTotal chocolates = kid at 7 subtract by kid at 4.but why this equation?ok, let’s again break thiscan you tell me kids from 1 to 7 ate how many and yes result is 44.but we want to know the result from 5 to 7. Please don't afraid. It’s easy peasyTotal(1, 7)= chocolate(1,4)+chocolate(5, 7).let’s get back to the 9th class, x= y+z. students, please find z?Easy? let's look at above equation now.chocolate(5, 7)= Total(1, 7)- chocolate(1,4) = 25and her mother gets all the answers and the shopkeeper is happy too as he can now attend more customers now.Pretty long 🤯?But the concept will always remain in your mind.Let’s look at the code of shopkeeper which he contains in his brilliant mind.Time Complexity- for searching the particular query is O(1). and for n query, we have O(n) time complexity.So, that’s it. The shopkeeper is now happy and after this prolonged reading, I think you should also grab some chocolate 😃.I hope you like the article. Please clap, share, and follow me for more amazing posts like this.Please ignore grammar errors, I am a coder, not an English teacher. 😁For the next time, bye-bye.",arrays,https://medium.com/@gauravit1996/prefix-sum-algorithm-cffdde0ccdb4?source=tag_archive---------1-----------------------
Dive deep in the ocean of JS — 2,"Tejas BontadkaJun 15, 2020·4 min readHey Scripties(one who code Javascript… Hahahahahaha 🤣).In the previous blog we understood, “why everything is Object in Javascript”.Please find the below link to visit the previous blog and understand “why everything is Object in Javascript”.https://medium.com/@tejas.techo/dive-deep-in-the-ocean-of-js-1-466df2d93d5bIn this blog we will look in to properties and methods of function objects we frequently use.Lets look in to properties and methods of Array.Below are the properties of Array:1. constructor : This property returns the constructor function of an Object. i.e, constructor property will return the function from where the array you created is instantiated, not the function where you declared an array.example:In the above snippet, we are printing the constructor of array which has been declared in the function called functionWhichHasArray().Okay here we think in two different ways regarding the output.First output we might think about: 1. Array arrayOfNumbers=[1,2,3,4,5] is declared with in function functionWhichHasArray(). 2. If we create a object of function functionWhichHasArray(), then functionWhichHasArray() is the constructor which will invoke while instantiating the object. So arrayOfNumbers.constructor is nothing but the function from where it is instantiated. Output will be signature of functionWhichHasArray().Second output we might think about: 1. Irrespective of the function where you declare the Array, It will be instantiated from Array() function object.like let arrayOfNumbers=new Array(1,2,3,4,5); 2. Then obviously the constructor of array created will be signature of Array().If you are thinking Second Output, Then you are in right track.Yes, Output will be the signature of Array function object, not the signature of function where you declare the array.Output will be:2.length : This property will return count of elements in the array.example:Output for the above snippet is 5.3.prototype: This property lets you add your own properties or methods to Array() function object.Prototype is basically a global object which holds good for every Javascript function objects.**And keep in mind, you are adding properties/methods to Array(), not the variable you use to declare array.example:Output will be :Now lets look in to methods we have in Array():Output will be:2. copyWithin() : This method copies the elements with in the array to elements of array itself.Confused????????…Even I was…(Don’t mind, I use lot of ‘…’ in my blog. Because I’m a fan of Spread Operator 😆… Will discuss about this guy later…).Okay lets come back to copyWithin().Will revisit the definition once “This method copies the elements with in the array to elements of array itself.”copyWithin() function looks like array.copyWithin(index from where pasting of elements need to start, index from where copying of elements need to start, number of elements need to copy from the index specified in second parameter).Third parameter is optional.Explanation:We have array [1,2,3,4,5] & we want to copy the elements of array to the another index of the same array.To achieve that we have done something like array.copyWithin(2,0,2)It means: Copy 2 (third parameter) elements from 0th (second parameter) index and paste the copied two elements from index 2(first parameter) of an array.[1,2,3,4,5] is an array, We copy 2 elements from 0th index i.e., [1,2], Then we paste the copied two elements([1,2]) from index 2 i.e.,[1,2,1,2,5].Note: copyWithin() will copies the elements till the length of the array, Not more that that.i.e., if array.copyWithin(3,0,3) , Then it wont exceed the length of the array to paste the copied values, It will only paste until the length of the array occurs. Output will be [1,2,3,1,2].Okay Scripties… Still there are lot of methods to discuss in Arrays.We will cover in next blog.Don’t mind… We need to dive deep.Till then,Happy scripting.",arrays,https://medium.com/@tejas.techo/dive-deep-in-the-ocean-of-js-2-253b9f5591dd?source=tag_archive---------1-----------------------
Interview Programming Questions and Answers in C — Part 3,"Naveen SharmaJun 14, 2020·13 min readQ. W.A.P to find the maximum/largest element without using an array concept.A.Q. W.A.P to find the maximum/largest element in an array.A.Q. W.A.P to find the minimum/smallest element in an array.A.Q. W.A.P to insert an element in an array.A.Q. W.A.P to delete an element from an array.A.Q. W.A.P to remove duplicate elements from an array.A.Q. W.A.P to merge two sorted arrays.A. It is assumed that the user will enter arrays in ascending order.Q. W.A.P to merge two unsorted arrays.A.Q. W.A.P to merge elements of two arrays alternatively without using the third array.A.Q. W.A.P to print the largest and the second largest element of the array.A.Q. W.A.P to print the first largest, second largest, and third largest element of the array.A.Q. W.A.P to reverse an array without using the second array.A.orQ. W.A.P to arrange even and odd numbers in the random array.A. Segregate even and odd numbers in the array such that all the even numbers should be present first, and then the odd numbers.Q. W.A.P to sort elements of an array through Bubble Sort.A.Algorithm:Step 1: Compare the first and the second element of the array and swap them if they are in the wrong order.Step 2: Compare the second and the third element of the array and swap them if they are in the wrong order.Step 3: Proceed until the last element of the array in a similar fashion.Step 4: Repeat all of the above steps until the array is sorted.Q. W.A.P to sort elements of an array through Selection Sort.A.Algorithm:Step 1: Set min to the first location.Step 2: Search the minimum element in the array.Step 3: Swap the first location with the minimum value in the array.Step 4: Assign the second element as min.Step 5: Repeat the process until we get a sorted array.Q. W.A.P to sort elements of an array through Insertion Sort.A.Q. W.A.P to sort elements of an array through Quick Sort.A. Quicksort is based on the ‘Divide & Conquer’ algorithm. In this sorting technique, an element is picked as a pivot and the array is partitioned around the pivot element. The target of each partition is, to put all smaller elements before pivot & put all greater elements after the pivot.Now, what is ‘Divide & Conquer’? In the Divide & Conquer algorithm design paradigm, we divide the problems into sub-problems recursively then solve the sub-problems, & at last combine the solutions to find the final result.One thing to keep in mind while dividing the problems into sub-problems is that the structure of sub-problems should not change as of the original problem.Divide & Conquer algorithm has 3 steps:1. Divide: Breaking the problem into subproblems2. Conquer: Recursively solving the subproblems3. Combine: Combining the solutions to get the final resultNow first, we look into the pseudocode for recursive QuickSort function :A program for the above Pseudocode is given below.Q. W.A.P to sort elements of an array through Merge Sort.A. Mergesort is also based on the ‘Divide & Conquer’ algorithm. In Merge sort, we divide the array recursively in two halves, until each sub-array contains a single element, and then we merge the sub-array in a way that it results in a sorted array.Merge sort is one of the efficient & fastest sorting algorithms with the following time complexity:Worst Case Time Complexity: O(n*log n)Best Case Time Complexity: O(n*log n)Average Time Complexity: O(n*log n)Q. W.A.P to find an element from an array through Linear/Sequential Search.A.Q. W.A.P to find an element from a sorted array through Binary Search.A..“The best way to learn a new programming language is by writing programs in it.” — Dennis RitchieThank you for reading! If you liked this article, please clap to get this article seen by more people.Please follow me on Medium by clicking Follow.I’m also active on LinkedIn, Twitter, and GitHub.",arrays,https://medium.com/@imnaveensharma/interview-programming-questions-and-answers-in-c-part-3-68c38df0047c?source=tag_archive---------1-----------------------
"“Minimum Swaps 2”, An Array Manipulation Algorithm in JavaScript","John SpeckJun 9, 2020·3 min readThis Algorithm is in the beginner/medium series level, but can get Overly- confusing if you look at the wrong tutorial.The problem: Given an unsorted Array of consecutive integers, how do we find the minimum number of swaps to sort the Array.To visualize that, consider:arr =[1,3,2]This clearly takes 1 Swap (swapping 3 and 2) and returnarr = [1,2,3]As usual, that solution is very easy to see and reason through. However, when scaled for a larger array, (like test case arr = [7,2,3,4,6,2,5]) it quickly becomes overwhelming.1.) One thing is certain, we are going to have to analyze every element in the given array to determine if it’s in the appropriate location. A simple for loop with an “if” statement will suffice.Consider the pseudocode:We can ignore elements in the correct place, because we don’t want to swap those. So our “if” statement allows us to look at each element in the array and compare it to “i+1”.But why “i+1” you may ask? Well, Array’s are indexed from “0"", while our elements in the Array is a set of numbers starting with the value “1”. So, when it’s sorted, the Array Element at index “0"" is going to be the same as our first element, (ie. arr[0] == 0+1)2.) So what is “all that jazz”, and how do we count the quantity of swaps we need to “do”??Why don’t we initialize a count? Say:and each time we make a swap, we can just add 1 to it:Done.3.) Here’s where the logic begins to get a bit confusing. This is what we will do inside of the “if” statement when it evaluates to “!==”. We are going to need a variable, “temp”.Whenever we find an element that is not where it should be, let’s assign it to to temp.We then subtract 1 from the value represented by temp and assign that to the spot we just removed the element from. Yeap! That’s our “count++”.The third magical step is to take temp and put it in the place designated by “arr[temp -1]”.Clear as mud, yes?Basically this allows us to cycle through all the values that have been swapped and place them one place closer to where they should be, every time recording the swap. This, ultimately leads to a sorted array in the least swaps necessary.The code:Elegant, clean and ES6, baby! Voila!Happy Coding!",arrays,https://medium.com/@eltocino/minimum-swaps-2-an-array-manipulation-algorithm-in-javascript-41ab4f9ce3ce?source=tag_archive---------0-----------------------
JavaScript spread operator: To infinity and beyond!,"Three dots can make a huge difference in a sentence. In JavaScript, this is not the exception. In JavaScript, the spread operator is identified by using three dots. This syntax is very powerful because it allows you to perform plenty of different tasks in a very simple and easy way.Let’s go over some examples of how to use this syntax on JavaScript.The spread operator lets you grab an array, and decompose it in such a way that now you can have each one of the values out of it and can be passed as arguments to a function.Let’s start by assuming you have the following code. A function that takes in three arguments and logs them separated by commas and an array with three elements.You could do this in a simple but not ideal way. This is, to take each one of the elements out of the array by their indexes and passing them as arguments.By using the spread syntax, there’s a more elegant (and safe) way of doing this. As you can see, when using the spread operator, now it passes in all the three elements as arguments to the fun function and no need to worry about indexes anymore.Another useful use case for the spread operator is that you can use it to copy objects. Let’s say you want to make a copy of obj1 and copy it to obj2. Finally, you will like to change the property a’s value on obj2 to be 3.Your first thought might be to do the following…In this case, even though you think you are changing the value of obj2’s a to 3, when you instantiated obj2, you are assigning a reference to obj1 and not making a copy.You might have also thought about doing copying the properties manually. While this example will work, it doesn’t scale. This is because if you end up adding more properties to obj1, this might lead to bugs in the future because of the manual effort of copying the object.There are other ways in which we could dot his safely, but we’ll focus for now on how to do it using the spread operator. In this example, you can instantiate obj2 to be equal obj1 but you will surround it with curly braces and use the spread operator in front of it.In this last code snippet, as you can see, what we do now is to leverage the spread operator to build obj2. This will be creating a new object identical to obj1.Let’s now assume you want to take two arrays and join them into a single one. There are multiple ways in which you can do this.To do this using the spread operator is once again… very simple. You will first start by creating a new array where you insert both arrays and prefix them with the spread operator.As you’ve seen the spread operator shines in multiple areas where it makes writing code in JavaScript simpler and some cases less prone to errors. Enjoy!",arrays,https://blog.devgenius.io/javascript-spread-operator-to-infinity-and-beyond-9a3222bd944e?source=tag_archive---------0-----------------------
Asynchronous array behaviour in JavaScript,"Varchasvi PandeyJun 5, 2020·4 min readWhile working with any API, you might have come across a situation where your response returns some data that’s when pushed into an array returns a weird thing! This looks like an array and you might also see your data there, but have you ever tried checking it’s “length” or maybe applying some cool array functions like “map”?Okay, so that’s not a weird array 😂!Let’s talk about some asynchronous behaviour of JavaScript. For this, I have written a very simple JS code. The whole purpose of this blog is to make you guys aware of this behaviour. So let’s just dive right into it!The function created below is making a call to JSON Placeholder API. We could have called this API once to get all the data (200 entries) at once. But here we have to create a real-life situation. Think of it like fetching data of top scorers of a class.Let’s consider array “users” to be a list of top scorers. So this is a statically generated array. This array is passed as an argument (will also work without even passing) to our “fetchData()” function.This function is applying our almighty “map” function to users array and for each user, a get request to the API is made. The response is then pushed inside a local array called “data”.“Ignore the try-catch statement. I’ve used them just to promote good coding habits when working with APIs while using async-await”Let’s update this code and pass the data array to a function to generate result.This function is responsible for the output that you saw earlier in this blog. So basically, this function is handling two expected arrays but is logging some different behaviour. Let’s click on the array data array output that looked empty from outside.So, the data is there. Hmm… 🤔, so let’s try using our almighty “map” function to fill another array with titles.“I’ll remove comments now and will reload the page again.”And we did all that just to get this! Another empty array! I’ve never used my favourite map() just to get nothing.If you know why that happened, then do share your experience in the response! And if you don’t know why this happened, here’s my small and simple explanation. But first, try clicking on the data array output which looks empty but shows data when clicked. (index.js: 7)On hovering, we see a message that explains the complete situation in just one line. “Value below was evaluated just now”. And this makes a lot of sense!!When working with asynchronous JS, which is most common while working with the back-end using NodeJS, it is desired that our application should be able to handle requests from multiple users without blocking the code. That is exactly what’s going on here. The flow of the code is going great! The thing is when our data array is passed to our “generateResult()” function, the values inside data are yet to be evaluated. So you can see the logging data output with a message that says “Value below was evaluated just now”, and therefore our map function does not apply on this array. Hence, we are getting an empty array on logging “ourArray”.Thank you, everyone, for reading this blog. If you found this helpful then do give it a nice clap. And do add a response down below if you think something can be added to this. Also, share your workarounds for this situation. I’ll be back with another amazing topic which you won’t be able to find anywhere else, just like this one. #look_im_so_confident.PEACE ✌",arrays,https://medium.com/@varchasvipandey/asynchronous-array-behaviour-in-javascript-7e86ff3638eb?source=tag_archive---------2-----------------------
LeetCode 905. Sort Array By Parity,"Arpit ChoudharyJun 8, 2020·2 min readQuestion:Given an array A of non-negative integers, return an array consisting of all the even elements of A, followed by all the odd elements of A.You may return any answer array that satisfies this condition.Example:Note:Solution:Two List Implementation: Here, we can have two lists, one to store the odd numbers and the other to store the even numbers. By iterating the original array of numbers, we push each number to its corresponding list.At last, we just need to append all the elements of the even list to the output array followed by elements of the odd list.Two Pointer Implementation: We maintain two pointers start and end, start will point to the first element of the array and end will point to the last element of the array.We keep incrementing the start index until we encounter an odd element. If we encounter an odd element then we will start decrementing the end index until we find an even element.Once an even element is found we will swap the odd and even element. The process is continued until the value of start is less than or equal to the end.I hope this post helped! Please do let me know if you have any feedback, comments, or suggestions by responding to this post.Happy Coding !!",arrays,https://medium.com/@choudharyarpit99/leetcode-905-sort-array-by-parity-ec3420eddbde?source=tag_archive---------0-----------------------
Return unique of duplicate values array,"Mike SunJun 12, 2020·2 min readGiven a sorted array, return n unique counts of elements in it, while also rearranging the array such that the first n element is unique. Diagram illustration as belowIf you think the problem is confusing, you haven’t seen its crude form. Initially the problem was phrased as removing duplicates which is hella confusing.Here is my code to this problem.First we check if the nums array is empty, if it is, terminate the function by returning 0. If not, we initialize 2 variables, i and j. We then loop through the length of nums array. Let’s go over an example with our given array [0,0,1,1,2]. Here is what happens in the for loop:i=0, j=1. Check if nums[0] is equal to nums[1]. True.nums = [0,0,1,1,2]i=0,j=2. Check if nums[0] is equal to nums[2]. False. =>increment i by 1. nums[1] = nums[2]. Note lists are mutable.nums = [0,1,1,1,2]i=1,j=3. Check if nums[1] is equal to nums[3]. True.nums = [0,1,1,1,2]i=1,j=4. Check if nums[1] is equal to nums[4]. False. =>increment i by 1. nums[2] = nums[4].nums = [0,1,2,1,2]We then return the count of i+1, since i started from 0. This is how we return the number of unique elements while reordering the array with no added memory.Learn together, happy coding!",arrays,https://medium.com/@mightnent/return-unique-from-duplicate-values-in-array-21c6c5f2f09f?source=tag_archive---------0-----------------------
JavaScript Arrays: The Basics,"Betty BadaJun 16, 2020·4 min readWhat happens when you want to work with a lot of data at once, wouldn’t it be nice if there was a way to keep track of all this information, a way to store it all in one place or in one variable where we could keep a list. Well, this is what Arrays are for, arrays are data structures that can hold multiple data values like a list, and we’ll explore how to create arrays, how to use arrays, and how arrays are structured.2. Accessing Array Elements: We can access elements in our array by referencing its position or location using an index. Each array element is numbered starting from zero and we can use these numbers as indices to access whatever value we want. One thing to be aware of is if you try to access an element at an index that does not exist, a value of undefined will be returned back.Arrays have a range of properties and methods that make them powerful data structures. Properties are special pieces of information about a data structure, for example, length which is used to get the number of elements in an array. To access the length property, type the name of the array, followed by a period . , and the word length. The length property will then return the number of elements in the array.Methods are special predefined functions that a data structure can call, here’s an overview of common array methods you should know:2. Pop: This removes the last element of an array. You don't need to pass a value with the pop() method; instead, pop() will always remove the last element from the end of the array. Also, pop() returns the element that has been removed in case you need to use it.3. Unshift: This adds an item to the beginning of an array.4. Shift: This removes an item from the beginning of an array.5. Splice: This can be used to add and remove items by its index position.6. Slice: This copies parts of an array (It slices out a piece of an array into a new array).The slice() method can also take two arguments, it selects elements from the start argument, and up to (but not including) the end argument.7. Sort: Sorts the elements of an array in place and returns the array alphabetically. The sort method is ideal for strings and not numerical values as it is prone to make mistakes.8. Concat(): It creates a new array by merging existing arrays to become one.These are a few Array methods you should know as you’ll be making use of them frequently. For more information visit the MDN documentation or Freecodecamp to learn more. I hope this article was insightful, keep practicing and you’ll be an expert before you know it. Never stop learning.",arrays,https://medium.com/@badabee/javascript-arrays-the-basics-6385cb9e0d83?source=tag_archive---------1-----------------------
How to map PostgreSQL ARRAY type to List using JPA and EclipseLink,"In PostgreSQL database, you can have columns in a table as variable-length multidimensional arrays in any types (built-in or user-defined base type, enum type, composite type, range type, or domain):This feature is similar Oracle VARRAY types. You can insert data into a column with this type in this format:And you can use available functions and operators for PostgreSQL Array type to fetch data:You have to use PostgreSQL Extensions to the JDBC API for array data types to work with arrays as column types, function arguments, and criteria in where clauses in PostgreSQL. You can read about this PostgreSQL data type here.Unfortunately, JPA does not support mapping array data type and we have three alternatives:EclipseLink and hibernate support mapping this datatype in a different way. In the following link, you can find a good article about how to map array data types in PostgreSQL using the Hibernate Types project.vladmihalcea.comJust like Hibernate, there is a specific approach to map array data type in EclipseLink. To achieve this you have to use @Struct to introduce a class to map to a database Struct type and use @Array on a collection attribute that is mapped to an array data type:You can also use @Struct to map Oracle VARRAY datatype.I have to mention that you should use the @Column and @Array properties to introduce the type of your array.Now you can store your array of images easily using EclipseLink:And also you can fetch data using specific functions and operators for PostgreSQL Array type by using JPA native query:",arrays,https://itnext.io/how-to-map-postgresql-array-type-to-list-using-jpa-and-eclipselink-b4e25ca13490?source=tag_archive---------0-----------------------
LeetCode Weekly Contest 192,"Shubham AgarwalJun 7, 2020·2 min read1471. The k Strongest Values in an Array !!!Problem :Given an array of integers arr and an integer k.A value arr[i] is said to be stronger than a value arr[j] if |arr[i] - m| > |arr[j] - m| where m is the median of the array.If |arr[i] - m| == |arr[j] - m|, then arr[i] is said to be stronger than arr[j] if arr[i] > arr[j].Return a list of the strongest k values in the array. return the answer in any arbitrary order.Median is the middle value in an ordered integer list. More formally, if the length of the list is n, the median is the element in position ((n - 1) / 2) in the sorted list (0-indexed).Example :Trick: Using two pointers patternThe solution is pretty straightforward and simple.If the difference between the median and arr[i] is greater than the difference between the median and arr[j] then we will push arr[i] in the resultant vector else we will push arr[j] and increase and decrease the index respectively.Solution :",arrays,https://medium.com/@shubhamagarwal./leetcode-weekly-contest-192-4cedbe9433dd?source=tag_archive---------6-----------------------
Array Methods In JavaScript,"Mayank VirmaniJun 7, 2020·3 min readIn JavaScript, array is a single variable which can hold more than one value at a time. It is often used when we want to store list of elements and access them by a single variable. It’s a more convenient way to store and structure information than is defining of lots of variables with different names.JavaScript arrays are “dynamic” entities in that they can change size after they are created2. Array.pop() : Removing elements from the end of an array.3. Array.splice() : method changes the contents of an array by removing existing elements and/or adding new elements.Syntax:Start-Index at which to start changing the array (with origin 0).DeleteCount-(Optional)An integer indicating the number of old array elements to remove.item1,item2…-(Optional)The elements to add to the array, beginning at the start index. If you don't specify any elements, splice() will only remove elements from the array.4. Array.slice() :method returns a shallow copy of a portion of an array into a new array object selected from begin to end(not included). The original array will not be modified.SyntaxParametersBegin (Optional)Zero-based index at which to begin extraction.end (Optional)Zero-based index before which to end extraction. Slice extracts up to but not including end5. Array.reverse() : method reverses an array in place . The first array element becomes the last, and the last array element becomes the first.6. Array.shift(): method removes the first element from an array and returns that removed element. This method changes the length of the array.7. Array.toString(): method returns a string representing the specified array and its elements.8. Array.sort():method sorts the elements of an array in place, and returns the array. The default sort order is built upon converting the elements into strings, then comparing their sequences of UTF-16 code units values.9. Array.length(): The length property of an object which is an instance of type array sets or returns the number of elements in that array.10. Array.unshift(): The unshift() method adds one or more elements to the beginning of an array and returns the new length of the array.",arrays,https://medium.com/@virmanimayank19/array-methods-in-javascript-8a6a924ec9a?source=tag_archive---------7-----------------------
New task for your interview prep,"We are given an array of integers of size N and also a number X. We need to find such a number Y that the expressionmin(a[0], Y) + min(a[1], Y) + … + min(a[N-1], Y)is as close to X as possible.Time complexity: O(N)Let’s digress a bit and talk in general about linear reduction. It’s when you can reduce the task in O(N) time with input of size N to the same task, but with input of smaller size N / 2. In that case, you can solve the whole task in O(N) time. Let’s figure out why.If you can reduce in O(N) time, that means there exists such constant number k that the reduction time is < k * N. Let’s spend k * N time and perform one reduction. Now we have the same task, but with the input size N / 2. Now let’s perform another reduction, spending k * (N / 2) time. Now we have input of size N / 4. And we continue going on like that. In the end, we reduce the initial task to size 1 and solve it.The overall amount of time spent isk*N + k * N / 2 + k * N / 4 + … + k = k * (2N -1) < 2k * NThat means that the whole solution is linear.This fact greatly simplifies all linear tasks. Now we do not have to solve the entire task, we just need to be able to reduce it to the same smaller task in linear time.If we want to use reduction, then we must be aware that not all problems are actually reducible, i.e. not all problems can be reduced to the same problem but with smaller input size.To make a problem reducible, sometimes we need to make it more complicated.For instance, instead of solving our initial task, we will solve more complicated task: given the array and the numbers X and Z, we need to find such Y thatabs(min(a[0], Y) + min(a[1], Y) + … + min(a[N-1], Y) + Y * Z-X) -> min.If we can solve this task, we can solve the original task by simply putting Z = 0 for the original task.Now we need to solve the task of finding Y such thatabs(min(a[0], Y) + min(a[1], Y) + … + min(a[N-1], Y) + Y * Z -X) -> min.But we will not go into fully solving it, but instead we will just reduce it to the same task of size N / 2.Let’s find a median M of the array a. Median is such a number that exactly N / 2 elements of the array are smaller than M, and exactly N / 2 elements are bigger than M. This could be done with O(N) time (you can read how to do this, for example, here).After that, we need to question if the potential best Y is > M or < M. For this, we are going to computemin(a[0], M) + min(a[1], M) + … + min(a[N -1], M) + M * Z and compare it with X.Let’s understand in detail what we do in each subcase.Reduction in subcase 1If the expression above is > X, we need to find ideal Y and we know it is < M. What do we do about it? We notice that if some a[k] >= M, then a[k] >= M > Y, therefore for the ideal Y we havemin(a[k], Y) = Y.In that case, let’s in O(N) time compute indices k_i such that a[k_i] < M. Let’s also compute the count of all other indices K such that a[k] >= M.Then min(a[0], Y) + min(a[1], Y) + … + min(a[N-1], Y) + Y * Z =min(a[k_1], Y) + min(a[k_2], Y) + … + min(a[k_(N/2)], Y) + K * Y + Y * Z =min(a[k_1], Y) + min(a[k_2], Y) + … + min(a[k_(N/2)], Y) + Y * (K + Z)So now we need to solve the taskabs(min(a[k_1], Y) + min(a[k_2], Y) + … + min(a[k_(N/2)], Y) ++ Y * (K + Z)-X) — > min.This is the same task as before, but with K + Z instead of Z, and the size of the array is now N / 2. So we successfully reduced the task to the same task but of size N / 2.Reduction in subcase 2We need to analyze another case when the expressionmin(a[0], M) + min(a[1], M) + … + min(a[N-1], M) + M * Z < XIf the expression is < X, that means that ideal Y should be > M.That means that for all elements for which a[k] <= M it is true that a[k] <= M < Y, somin(a[k], Y) = a[k].Let’s in linear time O(N) find all such indices d_i, such that a[d_i] > M. Let’s find the sum S of all elements a[k] that are <= M.Then we havemin(a[0], Y) + min(a[1], Y) + … + min(a[N-1], Y) + Y * Z =min(a[d_1], Y) + min(a[d_2], Y) + … + min(a[d_(N/2)], Y) + S + Y * ZSo now we have to solve the taskabs(min(a[d_1], Y) + min(a[d_2], Y) + … + min(a[d_(N/2)], Y) + S + Y * Z -X) — > minIt is the same asabs(min(a[d_1], Y) + min(a[d_2], Y) + … + min(a[d_(N/2)], Y) + Y * Z -(X-S)) — > minSo we again successfully reduced the task to the same task, but with size N / 2 and number X -S instead of number X.In all the cases, we were able to spend O(N) time and reduce the task to the same task but of size N / 2. That means that we are able to solve the whole task in O(N) time.",arrays,https://medium.com/verbetceteratech/task-6e940c134e94?source=tag_archive---------1-----------------------
Swift Challenge: Find the majority element [Amazon],"VarunJun 13, 2020·3 min readYou can read the full story along with many more on The Swift Nerd Blog:theswiftnerd.comOur aim to find the majority element from a list of elements. A majority element is that element which occurs more than n/2 times in the array (length = n).We are given an array of unsorted integers.Return the majority element if exists or return -1One way of solving the problem is to iterate the array and maintain a map of the frequency count of every element. We then check for all values in the map and if any exceeds n/2 then we return the element.Want to read this story later? Save it in Journal.Since we iterate over the array once, the time complexity is O(n).We maintain the count of elements and in the worst case, all elements can be distinct. Thus space complexity is also the order of n.There can be only one majority element in the array (No other element can occur more than n/2 times if the majority element exists). So we can tweak our approach to find the element with the max frequency since it would have the best chances to become the majority element and later validate the frequency.In its simplest form, the algorithm finds a majority element, if there is one: that is, an element that occurs repeatedly for more than half of the elements of the input. However, if there is no majority, the algorithm will not detect that fact, and will still output one of the elements. A version of the algorithm that makes a second pass through the data can be used to verify that the element found in the first pass really is a majority. Source — wiki.The algorithm is a two-step process:Since the majority element would occur the maximum number of times, even if other elements cancel out the occurrence we would still have some count left. Let's say, you have 10 elements and one element occurs 6times. So then count of the rest elements would be 4. Even if we cancel out the counts for the rest of the elements, we would still have the remaining count of majority element (6-4 = 2 ). We need to validate the frequency because there could be a clash of votes( there are only two elements with equal frequency).Since we iterate over the arrayFor extra variableYou can find the code on Github in my Leet Code repo.Thanks for reading this article, happy coding!!📝 Save this story in Journal.👩‍💻 Wake up every Sunday morning to the week’s most noteworthy stories in Tech waiting in your inbox. Read the Noteworthy in Tech newsletter.",arrays,https://medium.com/@vrat28/swift-challenge-find-the-majority-element-e2b4b33d9bb0?source=tag_archive---------0-----------------------
Now I can .c you,"Gustavo Adolfo MejíaJun 9, 2020·1 min readls is a command Written by Richard M. Stallman and David MacKenzie. Yes, our favorite Hippe creator of the GNU project.The philosophy of Unix is that “everything is a file”, and the ls command applies it. This command searches in the directory and list the files founded in a special file named standard output (stdout). Normally this output goes to the screen where we can see it.ls [OPTIONS] ... [FILE] ...ls use can use one or more options to customize the selected file(s).ls use wildcard to customize the output ls *c list all the files with .c extension for exampleIf the input use the * , ls will select all the files in the directory and if it use *.c it will select all the files that ends with *.c. The next line will show an example of the output of ls vs ls *clsls *cls ignores the lib.h, make, o.outand usr.h files",c,https://medium.com/@gamejia/now-i-can-c-you-6c9d20885377?source=tag_archive---------5-----------------------
What happens when you type gcc main.c?,"Firas gratiJun 10, 2020·1 min readWhat is gcc:GCC stands for GNU Compiler Collections which is used to compile mainly C and C++ languageThe 4 steps to execute a c program:1. Preprocessing:During compilation of a C program the compilation is started off with preprocessing the directives The preprocessor is a separate program in reality, but it is invoked automatically by the compiler2.Compilation: in this phase compilation proper takes place. the compiler translates the file into another file that contains assembly code. 3.Assembly:the assembler translates the file into machine language instructions,and generates an object file. 4.Linking:This is the final stage in compilation of “Hello World!” program. This phase links object files to produce final executable fileso basically when you type gcc main.c the file main.c will be executedand the output wil be displayed on the console",c,https://medium.com/@1921/what-happens-when-you-type-gcc-main-c-d5a424b187de?source=tag_archive---------8-----------------------
What happens when you type ls *.c?,"Deyber CastañedaJun 11, 2020·2 min readIf you are starting to use Linux or Unix-like operating sistems maybe you want to know what are the things that you have in your machine.The terminal allow us to know that through a command and their options, This command is ls This list the content of the current directory. Some options of this command are the following.-l Show the content in long format-a This allow us to see the hidden filesLike these are a lot of option to list the content of directory.Also there is a concept that is nessesary to undestand the answer to the tittle of this publication and are the wilcard characteres.Wildcard characters:The three main wildcard characters are,Star or Asterisk (*)Question mark (?)Square brackets ([])Asterisk (*) is used to search for particular character(s) for zero or more times. Question mark (?) is used to search for a fixed number of characters where each question mark (?) indicates each character. Square brackets are used to match with the characters of a defined range or a group of characters.So, If we combine the ls command with the * wildcard character we will have an excellent way to list the files or directories that have a .c extension.In this case this extension means that the file is a c program.I hope that you get undestand what does the ls *.c command and you can apply it in your projects. Good luck.",c,https://medium.com/@garzondeiber6/what-happens-when-you-type-ls-c-ef7b28d50d9f?source=tag_archive---------12-----------------------
What is the Pisano Period? How is it applied to coding problems?,"Paul AririJun 10, 2020·3 min readA week ago, I was watching various videos from my favorite “math” YouTuber — Numberphile, and I stumbled upon a video he made about the Pisano Period. Not knowing what this was, I was intrigued as to what mysteries were yet to unfold.Before breaking the ice, let’s talk about Fibonacci, well, “Leonardo Fibonacci of Pisa.” He was an Italian mathematician who invoked the Fibonacci Series — 1, 1, 2, 3, 5, 8. The pattern is clear from the series and can be represented as Fn = Fn-1 + Fn-2; although it seems like a basic, hardly used series, it was used widely to calculate the growth of rabbit populations.The existence of this periodic occurrence called the Pisano Period was discovered by “Joseph Louis Lagrange,” he noticed that if every number in the Fibonacci sequence were to be divided by a number, there would be a point where the remainders would start repeating themself. For example, the Pisano Period of 10 is 60 and that of 4 is 6 as seen in the diagram above.Now we know a little bit about the Fibonacci sequence and the Pisano period, what coding problem we can encounter that may require us to use this Lagrange-found logic?Answer: Find the remainder when the nth Fibonacci number is divided by m:Sample Cases:i. Input: n = 5, m = 3; Output: F(5) % 3 = 5 % 3 = 2ii. Input: n = 6, m = 2; Output: F(6) % 2 = 8 % 2 = 0iii. Input: n = 115, m = 1000; Output: F(115) % 1000 = 483162952612010163284885 % 1000 = 885;Now the normal naive approach in C++is shown in the image below:This approach would work for some cases, but could take days if not months to successfully give a solution for big/large numbers. This is where the Pisano Period comes in.Efficient Approach: Now, the thing about the Pisano Period is that a new period starts when F(n) % m= 0 and F(n+1) % m = 1. In the case of 4, each Pisano period holds the numbers 1, 1, 2, 3, 1, 0…, so with intuition, if we are able to find the Pisano Period, we can find the remainder of any large Fibonacci number without actually finding or reaching that number.If our input was n = 8 and m = 4, F(8) = 21, and 21 % 4 = 1, since we know the Pisano period when divided by 4 is 6 and 8 is 2 after that, that means F(2) % 4 == F(8) % 4. This explains the whole algorithm and the solution in C++ is below;There are many more ways to use the Pisano period, and if you still find yourself lost in my explanation or implementation, there are many videos on YouTube that talk in-depth about it. But, I will recommend you watch this video by Numberphile → https://www.youtube.com/watch?v=Nu-lW-IfyecCheck out my GitHub: https://www.github.com/paulAriri and my website: https://www.paulariri.com. Thank you!",c,https://medium.com/@ariripaulcoc/what-is-the-pisano-period-how-is-it-applied-to-coding-problems-5e70590fb3da?source=tag_archive---------0-----------------------
What happens when you type GCC main.c,"jgra007Jun 10, 2020·3 min readGCC is a set of compilers for various languages (ada, C, C++, fortran, ObjC, ObjC++, at one point java). It provides all of the infrastructure for building software in those languages from source code to assembly.It is responsible for the conversion of the “high level” source code in the respective language and ensuring that it is semantically valid, performing well formed optimizations, and converting it to assembly code (which is then handed off to the assembler).It also provides the general “driver” to invoke the various tools in the toolchain (e.g. invoking the assembler or linker) so that you do not need to worry about the exact ordering of a large number of implementation details about the object file format and underlying runtime library.When you want to execute code to create an object file, the computer needs to be communicated with in machine language, which is (base-2) binary code. Unfortunately (and fortunately), humans communicate in language higher than binary. Hence, why we use a higher-level language like Python, Ruby, and in this case, the C programming language. But, in order for computers to execute our C code, we have to compile the code using the Unix command:Here’s what happens when we GCC the file main.cThree main steps happen when we compile code:1. Reads the source file2. Processes it3. Links it with a runtime libraryA lot happens when the code is being processed. Let’s unpack.A compiler has multiple modules: preprocessor, compiler, assembler and linker.When we write the file main.c, the preprocessor generates some intermediate file, that file is given to the compiler. The role of the compiler is that it compiles files generated by the preprocessor as input, and that generates assembly code, so it can convert our C program file into the assembly language. Computers can only generate binary code, which is why assembly language is the format it needs to be in.Though, assembler code is still not understood by the machine — it needs to be converted into machine code. The converter that does this job: the assembler. The assembler module will convert the assembly code into the object code.Lastly, the linker, the last module, links the object code (created by the assembler) with library functions code that we use (when we write our code). From that linkage, txt files are generated.Let’s go in depth about each module.The preprocessor does 3 tasks: removes comments from the code, includes the header file (standard in C files) into the generated file itself, and if any macros were used, will replace the macro name with code.The compiler will take the file (created by the preprocessor) code and create the assembly code. The assembly code are comprised of mnemonics, instructions defined by english words.The assembler converts the assemble code into the object code.Lastly, the linker can play one of two roles:1. Can merge multiple C files by compiling them, into one executable file.2. Links our code (generated from the binary code of the assembler output) with the library function code.There are two types of linking: static and dynamic. The linker decides what type of linking it will use.The linker will pack all the code into a single file, which is famously known as the .exe file.",c,https://medium.com/@jr047x/what-happens-when-you-type-gcc-main-c-abc9037da0b8?source=tag_archive---------2-----------------------
What happens when you type gcc main.c?,"SantiagopinzonJun 11, 2020·4 min readwhen we start a topic we have to try to know all its termslet’s start with the root to understand that is compiled in gcc let’s start with c.C is a general-purpose programming language that offers a syntactic economy, simple flow control and structures, and a good set of operators. It is not a very high level of language and rather a small, simple language and is not specialized in any type of application. This makes it a powerful language, with an unlimited field of application and above all, it is quickly learned. In a short time, a programmer can use the entire language.Now we know that it’s c how we can create a c file in our terminal?commonly the extension of our c files are going to be .cLittle by little, we are approaching the main question but we still have some unknowns like which editors can run a .c file and the truth is that we can run it in most text editors like emacs, vi, or atom.ready now when we create a .c file it is called a program, to be able to see its result we have to use a compiler here enters gcc.GCC is an integrated compiler of the GNU Project for C, C++, Objective C, and Fortran; it is capable of receiving a source program in any of these languages and generating an executable binary program in the language of the machine where it is to be run.The syntax is this:these are some of the options that can be used with gcc.-E: Preprocess only; do not compile, assemble or link-S: Compile only; do not assemble or link.-c: Compile and assemble, but do not link.-o: Place the output into <file>.The compilation process involves four successive stages. To go from a source program written by a human to an executable file, it is necessary to perform these four stages in succession. The Gcc and G++ commands are capable of performing the entire process at once.At this stage, the directives to the Preprocessor are interpreted. Among other things, Variables initialized with #define are replaced in the code by their value wherever their name appears.The Compilation transforms the C Code into the Processor’s own Assembly Language of our machine.Assembly transforms a program written in Assembly Language into Object Code, a binary file in Machine Language executable by the Processor.The C/C++ functions included in our code, such as printf() in the example, are already compiled and assembled in existing libraries in the system. It is necessary to incorporate somehow the binary code of these functions to our executable. This is the linking stage, where one or more modules are assembled in object code with the existing code in the libraries.When we type gcc “name of file” it creates an executable file called a.out if the compiler doesn’t catch any errors it creates.when we type gcc “name of file” it creates an executable file called a.out if the compiler doesn’t catch any errors it creates in this example, we are going to see how gcc is executed and creates the a.outhere we can appreciate how the a.out executable is created now we execute it,we simply put this ./a.out and it will run.At this point, you could conclude that by writing gcc name-of-file, an executable file is created and compiled, remember that every time you make an update to your file you must write the gcc again so that the a.out is updated.Linux is the base of everything, don’t ever stop learning.",c,https://medium.com/@santiagopinzond/what-happens-when-you-type-gcc-main-c-d12258518d04?source=tag_archive---------7-----------------------
Apartment Hunting Problem,"Roshan PandeyJun 7, 2020·2 min readHi, let's encounter a problem from the Algoexpert array section. The problem is -Problem Statement — You’re looking to move into a new apartment, and you’re given a list of blocks where each block contains some services that it offers. In order to pick your apartment, you want to optimize its location in such a way that the maximum distance to any services that you care for is minimized.Example —blocks = [ { “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ,{ “gym” : true, “school” : true, “store” : false } ]requirements = [ “gym”, “school”, “store” ]Solution Approach — There are multiple possible answers to that question, where we can basically establish a trade-off between space and time complexities.Suggested approach — Initialize a vector that stores the farthest distance of a particular service for every block with INT_MIN. Now for every block, go through every requirement and find the closest block that satisfies that requirements and change vector value for that block with a minimum of the current value and maximum distance for any requirement. At the end return the index the minimum value in the vector.I hope it helps :-)",c,https://medium.com/@pandeyroshan556/apartment-hunting-problem-85c6acee8741?source=tag_archive---------1-----------------------
Introduction to Secure Coding in C and C++,"Both languages (C and C++) were designed to be general purpose, structured programming languages that suited systems and embedded programming.C++ was designed from the start to be an object oriented language with support for encapsulation, data hiding, inheritance, and polymorphism.A stack overflow occurs when the execution stack of an application grows beyond the memory that is reserved for it. A common example of when this occurs is when recursive functions are used. Recursive functions generally call themselves until some terminal condition is reached. Each recursive call to the function results in a new Stack Frame being created and placed on the Stack. Once the terminal condition is met, the function returns a value and the Stack is unwound, shrinking to its original size. However, if the terminal condition is never met, new Stack Frames will continue to be created and eventually the Stack will consume more memory than is reserved for it. As a result, it is highly recommended that recursive functions be thoroughly tested.A Buffer Overflow or Buffer Overrun is an anomaly where a program, while writing data to a Buffer, overruns the Buffer’s boundary and overwrites adjacent memory locations.Buffers are areas of memory set aside to hold data. Buffers ate useful to move data from one section of a program to another, or between programs. Buffer Overflows can often be triggered by malformed inputs. If one assumes all inputs will be smaller than a certain size and the Buffer is created to be that size, then an anomalous transaction which produces more data could cause it to write past the end of the Buffer.On the other hand, Heap Overflows are a specific type of Buffer Overflows. A Buffer Overflow occurs when an application writes data beyond the memory allocated for any buffer or data structure. If this structure or buffer is located on the Heap, the overflow is called Heap Overflow. However, keep in mind that the buffer or data structure could also be allocated on the Stack. In this case, the vulnerability is still a Buffer overflow, but it is called a Stack-based Buffer Overflow. Older C and C++ applications were especially vulnerable to Buffer Overflows when accepting and processing string inputs from an end user although many systems have also experienced this issue when accepting and processing input from the network.Let us take a quick look at some simple Heap-based and Stack-based buffer overflows before we move on to the next topic.The code we see here, courtesy of StackOverflow.com, is vulnerable to classic Heap-based and Stack-based Buffer Overflows. At the top, we see the definition of the heap_overflow function. What makes this function vulnerable is that no matter how we call it, it will always ask for one too few characters from the heap and then writes beyond the end of the memory allocation. In the example we have here, we send in four bytes of data: the characters: a, b, c, and a null character, but the amount of memory returned by malloc will only be three bytes.One of the things to realize about both of these vulnerabilities as that ultimately, they can lead to arbitrary code execution if exploited by a skilled attacker. For more in-depth analysis and examples of Buffer Overflows view module 180, “Buffer Overflows”.Based on previous example, we can see that strcpy can potentially be a very dangerous function to use if we do not verify that the space receiving the data is at least as large as the data we are copying.At a minimum, avoid following functions:However, as Sam discovered, there are other similar functions that we should also avoid using for the exact same reason. These include strcat(), strcmp(), and gets(). For a more comprehensive list of functions we should avoid, and a list of suitable replacements, checkout the list of additional resources listed here.gets(), strcmp() and strcat() are standard C library functions that fail to do bounds checking, allowing the input buffer to be written to the destination even if the size of the destination buffer is smaller resulting in a potential buffer overflow.Beyond Insecure Memory Handling, the next item on Sam’s list of concerns is Concurrency and Race Conditions. Concurrency describes the situation where multiple threads or processes are executing on the same core. In most systems, this is possible through context switching. Where concurrency becomes dangerous is when you have multiple threads or processes attempting to access and update a shared resource or piece of data.Here we see a single C++ application that attempts to square the numbers 1 through 20. If we compile and run this application, we are likely to get the correct answer 2,870. However, in certain conditions, we will get a different answer. This is because the updating of the accumulator variable within the square function is not an atomic operation. As a result, as multiple threads all contend to call the square function, there is a chance, called a Race Condition, that one thread will overwrite the changes to the accumulator variable made by another thread. In these cases, we get an incorrect answer.While this example is rather trivial, there are other situations where this could have a detrimental effect. Think about if your banking website used a multi-thread C++ back-end application to process debits and credits to your account. Depending on the order of the operations, there is a possibility that you could wind up.To fix our vulnerable code and solve this issue, we can use one of three different techniques. The first is known as Mutex, which is short for Mutual Exclusion, allows us to ensure that certain blocks of code are always executed in order and cannot be interrupted. A safe version of the relevant of the previous code using Mutex might look like this.Most of the code is the same, but the changes are that we first include the mutex library and declare a mutex object named account_mutex. Then in the square function, we calculate the value we want to add to the accumulated total, lock the mutex, add the new value to the total, and unlock the mutex. The way the Mutex protect us is it ensures that all accounts performed between the lock and unlock.The next option is to use what is known as an Atomic, as long as we are using C++11 or newer.By declaring the accumulated total as an atomic integer, our compiler will ensure that all modifications to the total are done in a thread-safe fashion.One last option we could use, which is also part of C++11 and newer, is Tasks. Tasks works based on the concepts of promises and futures. Promises are a concept that is used with asynchronous programming where we want to continue processing, while some complex or time intensive operation completes, but we still want to guarantee that we will eventually get an answer. A promise is this contract that we will eventually get an answer. A promise is this contract that we will eventually get an answer. To check for the answer, we use a future, which is linked to the promise. If a promise has not been fulfilled by the time that we need the value, the future will block until the promise is fulfilled.We can see here what a version of our code looks like fulfilled by the time that we need the value, the future will block until the promise is fulfilled.We can see here what a version of our code looks like using Tasks. First, we declare a vector to hold all of our futures which promise to eventually give us a real value. We don’t need the values immediately, so we just store the future away in our vector. Later on, we attempt to retrieve that actual calculated values by calling the get method on each one of our individual futures. Since all of the calculations are already complete, there are no threading issues so we simply add the individual values to the accumulated total and print the result. As expected, we always get the correct result.Overall, we have quite a number of ways that we can solve the Concurrency and Race Condition problem. Even so, there are some things that we need to keep in mind as we use any of these solutions.First, we need to be careful and not destroy any of our Mutexes while they are locked. If we do this, we are almost guaranteed to have our application enter a state known as deadlock.Second, we need to be careful of exceptions in our multithread code. Especially when using Mutexes. To avoid deadlocks resulting from exceptions, we must always release any Mutexes we have while resolving exceptions, unless we can guarantee that future code will release the Mutex.Third, we must validate that any libraries we use inside of critical sections are thread-safe. Many Standard Template Library functions are not considered to be thread-safe, so we need to be mindful when using them in multi-threaded code.For more information on Source Concurrency Coding Guidelines, see the CERT guidelines reference here.https://www.securecoding.cert.org/confluence/pages/viewpage.action?pageId=158237219Another common issue with Race Condition that Sam identified is called time-of-check, time-of-use. In general, these type of Race Conditions occur when software first checks the state of a resource, but before actually using the resource, the state of the resource changes. As a result, the software could perform invalid actions or enter into an undefined state. This issue become security relevant when an attacker is able to purposefully influence the sate of the resource between the time its state is checked and the time that it is used.Looking at the example C code we have here, modified from MITRE, the code first gets the name of the file we want to access from the user. We store this in the filename parameter. Next, the code uses the access method in unistd.h to ensure that the program can read the file. It then obtains the FILE pointer via a call to fopen() and processes the file’s contents as necessary. If the program can’t access the FILE, a message is written to standard error.The issue with this code is that both: access() and fopen() rely on filenames rather that file handlers. If an attacker can change the location of the file referenced by the path between the path to access() and the call to fopen(), the attacker can cause the program to read a file other that the specified.Changing the file can happen by manipulating the file system and replacing the file with a symbolic or hard link to a different file, or by replacing one of the directories in the file path with a link. If this program had the setuid bit set, an attacker could possibly manipulate the system to read a file only accessible by root, thus elevating their privileges.https://cwe.mitre.org/data/definitions/367.htmlUnfortunately, unlike the Race Conditions presented before, time-of-check, time-of-use (TACTOU) issues ate difficult to prevent, if at all. In general, our advice is to limit the number of CPU cycles between the check and the use of the resource as we did here. While this does not resolve the issue, it does make it more difficult for an attacker to exploit.Other solution include performing operations on resources atomically or using an environmental locking mechanism to protect the resource. Of course, these solutions can incur a performance penalty which may be unacceptable to the business and its needs.Fortunately, C++11 has added some new modes that can help us prevent these issues in certain situations.The code we see here makes use of a new mode available to fopen(). The X mode of operation combines the checks to determine if the file exists and the operation to open it for writing into one operation. As a result, we do not have to perform both checks in our code, thus reducing the likelihood of an attacker exploiting a time-of-check, time-of-use (TACTOU) bug.Unfortunately, this mode does not apply to read operations, so we ate still responsible for mitigating this risk in these situations.Below safe solution invokes fopen() at a single location and uses the X mode of the method, which was added in C11. This mode causes fopen() to fail if the file exists. This check and subsequent is performed without creating a race window. Please bear in mind that the X mode provides exclusive access to the file only if the host environment provides this support.While these types of Race Conditions can occur in places other than the file handling code, it is nonetheless important to keep few things in mind.Be mindful of any functions that use file names instead of file handles.Second, canonize all path names that can be influenced by outside sources of input to ensure that the files being accessed are the intended ones. If possible, restrict the files that can be read or processed to a specific directory that only the program has access to. These types of directories are commonly referred to as secure directories.Third, make user of the X mode of operation in C++11 if it is available to you.Finally, user the CERT secure coding standards referenced here for more detailed information and examples on secure file I/O.https://www.securecoding.cert.org/confluence/pages/viewpage.action?pageId=3575File handlers guarantee that the program is operating on the correct file, even if an attacker is able to modify the file system to have the file name point to a different file. As a result, file handles are often more secure options than file names.Through various assessments conduced in the past, Sam remembers that exceptions can expose internal implementation details of the application that can aid an attacker. In certain circumstances, error or exception messages can give an attacker a better understanding of the application, which can allow them to craft better exploits. To overcome this issue, we must not only catch and handle all exceptions, but we must filter any exception messages that may get returned to an end user.For example, look at the following code. Here, we accept a file name from the user and print the contents of that file to standard out. However, if there are any exceptions or errors thrown during this process, those errors could be returned to the user. Worst yet, a core dump could be triggered which could also contain sensitive information about how the application works.To prevent this accidental disclosure of information, what we need to do is implement basic error and exception handling.This other sample code included below shows a much more robust implementation of our file printing code.What we have done here is extracted the code into its own function and wrapped the parts of the code that could throw an exception in a try catch block.The catch block of the code allows us to handle the various exception cases in a deterministic manner. We start by catching any file related exceptions that could be thrown. We then take a highly pragmatic approach and catch all other exceptions that could be thrown that we might not expect.When implementing exception handling code, it is recommended to always have a generic catch block to handle any unexpected exception that could be thrown. In this example, file related exceptions should caught before the generic catch block.For more examples and best practices on good exception handling, go to the references provided here.https://www.securecoding.cert.org/confluence/pages/viewpage.action?pageId=146440597http://www.stroustrup.com/except89.pdfSam notices that many of his company’s applications communicate with each other over the network. To facilitate this, the applications use heavy use of sockets.Whenever we begin to receive input from network connection, one of the main issues that we need to address is Denial of Service. By default, the send() and recv() methods of raw socket library block a thread’s execution until all of the data is processed. However, an attacker that can connect to our socket can either send a larger amount of data causing all of the memory on our system to be consumed, or the attacker can open multiple connections and send data very slowly, eventually causing our system to run out of available connections to serve our legitimate clients. To solve this issue, we can try three primary solutions.The first possible solution is to have our code multithreaded. By using one of the multithreading solutions we talked about earlier, the send() and recv() functions will only block a child thread which will allow the main application thread to continue processing.The drawback of this solution is that proper implementation of multithread socked code is nontrivial and should only be left to highly experienced developers.The second solution, which is much simpler, is to implement time limited connections for all networking code. The general approach to this solution is that for each connection that is opened, we need to keep track of how long the connection has been open and whether we have received all of the information that we expected in a reasonable amount of time.The general outline of what this solution would be is something like this: First, we would open our socket and establish a connection. At this point, we would also capture the current system time. If we were sending data, we would inspect that data and our network connection speed, and calculate a threshold of how long it would take to send all data. Then, we would begin looping over the data, sending it in chunks. With each data chunk sent, we would also check to see if the current system time, minus the threshold we calculated earlier, was greater than our initial start time. If so, we could choose to terminate the connection. If we were receiving data, we could take a similar approach by calculating a reasonable amount of time for the other system to send us all data we expected. If the time threshold is exceeded, we close the connection.This approach is quite simple and can easily prevent common Denial of Service conditions, especially when combined with reasonable caps on the data size being transmitted between systems using a single connection. However, keep in mind that network transmission can occur and available bandwidth can fluctuate. If these are encountered, be certain to adjust the calculated threshold appropriately.The last potential solution is to use asynchronous or non-blocking I/O operations. A good example of this is the boost ASIO or asynchronous I/O library. Boost’s ASIO library provides a number of functions and classes that can assist us in complementing all kinds of non-blocking I/O. Of course, it does require the use of boost, which may be unacceptable in some organizations.In any case, Boost provides us with a simple API for non-blocking I/O that abstracts away the complexities of having to implement different techniques for the various systems we want to support.For more detail information on non-blocking I/O checkout these resources:http://www.boost.org/doc/libs/1_63_0/doc/html/boost_asio.htmlhttp://beej.us/guide/bgnet/output/print/bgnet_USLetter_2.pdfOrganizations must also be intimately aware of the cloud provider’s disaster recovery process. An understanding of how those processes impact the organization, and what tasks are left for the organization to perform in addition to the cloud provider’s processes and actions is important.Data protection beyond encryption is also important, especially in the areas of data deletion and data mining. Strict controls should be put into place by an organization to ensure the complete removal of all data in the event that a contract needs to be terminated. In addition, restrictions should be placed on the cloud provider by the organization such that the provider is not allowed to mine or use any of the organization’s data without express written permission.Lastly, organizations should ensure that the cloud provider is accustomed to supporting regulated applications, and that their data centers are properly certified.",c,https://levelup.gitconnected.com/introduction-to-secure-coding-in-c-and-c-d8ece627facb?source=tag_archive---------0-----------------------
Compilation stages,"Julian TorresJun 10, 2020·3 min readThe compile process involves four successive stages: preprocessing, compilation, assembly, and binding. To move from a human-written source program to an executable file it is necessary to perform these four stages successively. The gcc and g++ commands are able to perform the whole process at once, in this we will use gcc, and the four stages are:Preprocessed:At this stage, directives are interpreted to the preprocessor. Among other things, variables initialized with #define are replaced in code with their value in all places where their name appears.We will use this simple program as an example:Preprocessing can be ordered with any of the following commands; cpp specifically refers to the preprocessor.If we examine circle.pp it can be seen that the PI variable has been replaced by its value, 3.1416, as set out in the judgment #define.Compilation:What compilation does is transform the C code into the assembly language of our machine’s processor.Assembly:What the assembly does is transform the program written in assembly language to an object code, a binary file in executable machine language by the processor.The assembler is called as:creates the file in circle.o object code from the circle.s assembly language file. It is not common to perform only the assembly; it is usual to perform all the above stages until you get the object code like this:This creates the circle.o file from circle.c.In large programs, where many source files are written in C code, it is very common to use gcc or g++ with the -c option to compile each source file separately, and then bind all the created object modules. These operations are automated by placing them in a file called make file, interpretable by the make command, which takes care of making the necessary minimum updates whenever any portion of code is modified in any of the source files.4. LinkedThe last stage is to gather one or more modules in object code with the code that exists in the libraries.To bind we use the ld command, to get an executable we should use something like this:How to do it all in one step?In program with a single source file the whole above process can be done in one step:The circle.o file is not created; the intermediate object code is created and destroyed without seeing it by the operator, but the executable program appears there and works.It is instructive to use the -v option of gcc to get a detailed report of all build steps:",c,https://medium.com/@juliantorresr/compilation-stages-1ea1fe4e6bdd?source=tag_archive---------5-----------------------
Compile with gcc,"ArquimedesJun 11, 2020·4 min readIt is a software that translates a program written in a high-level programming language (C / C ++, COBOL, etc.) in machine language. A compiler generally generates assembly language first and then translates the assembly language into the machine language. A utility known as a ""linker"" combines all the necessary machine language modules into an executable program that can be run on the computer.GCC is the GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, Go, and D, as well as libraries for these languages (libstdc++,…). GCC was originally written as the compiler for the GNU operating system.The compilation process involves four successive stages: preprocessing, compilation, assembly, and linking. To go from a human-written source program to an executable file, it is necessary to carry out these four stages in succession. The gcc and g ++ commands are capable of performing the entire process at once.At this stage the directives to the preprocessor are interpreted. Among other things, variables initialized with #define are replaced in the code by their value wherever their name appears.Star with the compilate process, creating the file main.c, .c is the file extension for the process written in c.The source code is the code which is written in a text editor and the source code file is given an extension “.c”. This source code is first passed to the preprocessor, and then the preprocessor expands this code. After expanding the code, the expanded code is passed to the compiler.You can type “-E” with the gcc command on the source file, and press enter.The code which is expanded by the preprocessor is passed to the compiler. The compiler converts this code into assembly code. Or we can say that the C compiler converts the pre-processed code into assembly code.In the compiler process, you can type “-S” with the gcc command on the source file, and press enter.After you do this, the file main.s is created and looks like this:The assembly code is converted into object code by using an assembler. The name of the object file generated by the assembler is the same as the source file. The extension of the object file in DOS is ‘.obj,’ and in UNIX, the extension is ‘o’. If the name of the source file is ‘hello.c’, then the name of the object file would be ‘hello.obj’.After this the file main.o is created:Mainly, all the programs written in C use library functions. These library functions are pre-compiled, and the object code of these library files is stored with ‘.lib’ (or ‘.a’) extension. The main working of the linker is to combine the object code of library files with the object code of our program. Sometimes the situation arises when our program refers to the functions defined in other files; then linker plays a very important role in this. It links the object code of these files to our program. You can type this to create the executable file:In DOS, the extension of the executable file is ‘.exe’, and in UNIX, the executable file can be named as ‘a.out’. For example, if we are using printf() function in a program, then the linker adds its associated code in an output file.So now we could either type “./a.out” if you didn’t use the -o option or “./my_program” to execute the compiled code, the output will be “Hello, World”, and following it the shell prompt will appear again.After this process, we can already type “./a.out” or -o or “./example”, and thus execute the compilation, the output will be “Hello, World”.So you ready to do your first gcc main.c ?Go ahead and try!",c,https://medium.com/@richardcqt28/compile-with-gcc-800377b837ad?source=tag_archive---------1-----------------------
What happens when you type gcc file.c,"Esteban Martinez FuentesJun 11, 2020·2 min readIt’s really fascinating to see how computers can understand what we write, considering that they only understand binary language.Today I want to show you how the code that we write in a programming language such as C, for example, goes through different processes until it becomes 0 and 1 and thus can be interpreted by our machines.The first thing we have to understand is that C is a compiled language, that is, it needs a translator to convert its content to a binary language. We know this translator or compiler as gcc.GCC is an integrated compiler of the GNU project for C, C ++, Objective C and Fortran; it is capable of receiving a source program in any of these languages and generating a binary executable program in the language of the machine where it has to run.There are several ways to execute our code, one of them is to run gcc <file.c> in our terminal.The compilation process involves four successive stages: preprocessing, compilation, assembly, and linking.At this stage the variables initialized with #define are replaced in the code by their value in all the places where their name appears.2. Compilation.The compilation transforms the C code into the assembly language of our machine’s processor.3. Assembled.The assembly transforms the program written in assembly language to object code, a binary file in machine language executable by the processor.4. LinkedNow it is necessary to somehow incorporate the binary code of these functions into our executable. This is the link stage, where one or more modules in object code are brought together with the existing code in the libraries.This is a good illustration of the above process.Well my dear coders are encouraged to print the stages through which your code passes, if they do they will see more in depth all this fantastic process.",c,https://medium.com/@esteban.tvop/its-really-fascinating-to-see-how-computers-can-understand-what-we-write-considering-that-they-f23fa300f0f6?source=tag_archive---------10-----------------------
Curiosity Strikes! The Intricacies Before main() in C,"In this article, I’m going to talk about how a C program in a generic embedded device gets into main(). I wrote this because a myriad of explanations online are very device-specific. They use a lot of architecture-specific terms that clutter the basic principles behind the reset mechanism of an embedded device. My goal is to explain to you in simple terms how a device prepares itself so that the main() can do its job. I hope this would lay a solid foundation to your embedded systems toolbox.Ready? Let’s pop what’s under the hood 😉Do you ever notice that every time you attempt to turn on a device (from a totally powered-off state) there’s a fair amount of waiting time before you see the main screen? In your phone for instance, before you see that phone manufacturer’s logo, it takes a few seconds (or even minutes) for it to boot up. During this period of waiting time, you may or may not wonder why it does that. If you do, the discussion below outlines a very simplified explanation of a device reset mechanism.When you turn on a digital-enabled device (assuming from its reset state like totally powered off state), hundreds of instructions are performed to gear up the device to be ready to serve what the end-user commands it to do. There’s a well-hidden callback function, typically known as the Reset Handler, to execute these hundreds of steps before handing over the system to the main(). A good analogy that I’ve used from time to time when explaining this is when you make someone an instant coffee, it may be as easy as:In reality, even as simple as serving an instant coffee, you still did some preparation before handing over the coffee to that person. You can think of it as steps 1 to 5 being the preliminary steps performed by the Reset Handler and step 6 is done by the main().So how does the device know how to find the Reset Handler and do the preliminary steps?Well, typically devices these days have built-in CPU that is preprogrammed to contain the memory address of the Reset Handler’s entry point. Consider the figure below for a better understanding.When you perform a device reset, the CPU jumps to a specific memory address (addr_x) containing the information (another memory address addr_y). The CPU will load this information(addr_y) to its own internal register PC (program counter) that points to the next instruction to be executed, in this case, the Reset Handler() function entry point. From this point, a lot of initialization is performed sequentially to enable the entire system to be ready to execute high-level application tasks. At the last step, after the prerequisite steps are done, the handler jumps to the memory address (addr_z), the entry point of the main().In summary, I’ve discussed how a generic embedded device sets up itself before invoking main() without delving into device-specific architecture such as ARM, PIC, AVR, or even 8051. That is, during the pre-boot phase, the CPU’s program counter (PC) initially contains the address of the Reset Handler. The CPU jumps to that address and executes whatever is inside the handler (usually initialization routines). The last step of the Reset Handler will make the CPU jump to the main() entry point where the application code resides to serve the end-user.There you have it. You now have the basic technical knowledge of an embedded device reset mechanism. Feel free to tweak around the concept and implement your own reset mechanism if you are really curious. I hope this article has been useful to you!",c,https://levelup.gitconnected.com/curiosity-strikes-the-intricacies-before-main-in-c-d4327714f8ee?source=tag_archive---------2-----------------------
Code Motion Sensor with Different Programming Languages,"Ravindra RanaJun 11, 2020·2 min readHello Internet, today I am going to post about C and C++ VS JavaScript in Embedded System and Internet of Things(IoT).I am passionate about the Internet of Things(IoT) and Embedded Programming, it feels me happy while I am doing some stuff with hardware components.If you are wondering to start learning IoT then you may start with Arduino Technology, it provides you some basic concepts about sensor and actuator after that you would hands dirty with ESP32 or ESP8266 and Raspberry Pi and so on.I have done some projects like automation with ESP8266 and some projects like Digital Signage and making Web Server with Raspberry Pi.C and C++ are the foundation of any programming language and mostly used by Embedded System.But JavaScript has a huge area of development, it is everywhere nowadays from Web Development to the Internet of things(IoT).In this post, I will show how C and C++ control the motion sensor and How JavaScript controls the motion sensor.You will see the difference and analyze which one is best for you.So before this, if you have not run any blink program using C and C++ or JavaScript then it’s a little bit difficult to understand so try to read this post if you want.medium.commedium.comAfter reading these posts you have some basic concepts then let’s get started…C and C++ RecipeStep 1: SetupWe have to install the Arduino IDE. This video demonstrates how to install it.Step 2: CodeCode for Motion Sensor, you may type or copy from here.Step 3: WiringJavaScript RecipeStep 1: SetupYou need to install some components before start Coding so follow this posts or google it for setup Johnny-Five.medium.comStep 2: CodeStep 3: WiringOutput",c,https://medium.com/@ravindra-sombati/code-motion-sensor-with-different-programming-languages-1638bdf8419c?source=tag_archive---------2-----------------------
GREET PLANET TO HELLO WORLD!,"TharunravuriJun 7, 2020·2 min readHola! 😄If you have read my earlier post then you will know what this post is. As I mentioned in the earlier post the following is the article that I submitted for the Geeks for Geeks and rejected. 😅Let’s talk about Hello World!So, if you are wondering what is this post. Then just hold the same excitement and read the entire article. If you are a CSE student then you should come across the term called Hello World!In the programming realm Hello World! is a paradigm and most of the programmers will start their programming carrier with printing Hello World!But have you ever wondered is there any way to write Hello World?Yes, there is another way to print “Hello World!”Today I will explain how to write Hello World! in a completely different way and I will explain the code. The following code may look a bit awkward but still, hold the excitement. 😜Try solving the below code or run the code in your compiler.#include<stdio.h>int main() {long long p=1,e=2,t=5,a=61,l=251,n=3659,r=271173410, g=1479296389; long x[]={g*r*e*e*t,p*l*a*n*e*t}; puts((char*)x);}Output: Hello World!Well if you have solved the question and understood the logic 🎉 Hurray!! 🎉According to the above code, all the letters in the character array were initialized with a value.The statement g*r*e*e*t gets multiplied with the initialized values (1479296389*271173410*2*2*5) and produces the result as 8022916924116329800 and the statement p*l*a*n*e*t gets multiplied with the initialized values(1*251*61*3659*2*5) and produces the result as 560229490.The result g*r*e*e*t = 8022916924116329800 and p*l*a*n*e*t = 560229490 needs to be converted in to hex.Upon converting the values into hex you will get the values as:g*r*e*e*t = 0x6f57206f6c6c6548p*l*a*n*e*t = 0x21646c72Now arrange the values according to little-endian memory organization then you will get 48 65 6c 6c 6f 20 57 6f 72 6c 64 21 00 00 00 00Now convert each number from hex to decimal and compare the decimal value with the ASCII table.You will get the output as “Hello World!”That’s the mystery behind this programming question. I am not a fan of programming but after solving this question I understood why programming is so addictive and fun 😄Well, that’s what I wrote for the article and named it as “GREET PLANET TO HELLO WORLD!”",c,https://medium.com/@tharunravuri/greet-planet-to-hello-world-a842e5c7f9e0?source=tag_archive---------3-----------------------
Compilation Processes in GCC,"Juan DelgadoJun 11, 2020·2 min readGCC, the GNU Project’s integrated compiler, is not only a C compiler, we can also compile in C++, Objective C and Fortran. GCC generates an executable binary program in the language of the machine where it will be executed.The compilation process consists of 4 stages: pre-processing, compilation, assembly and linking. The gcc command is capable of performing all 4 processes at once.It handles directives like #Include and #define.Pre-processing can be requested with any of the following commands :In the age.pp file the value of YEAR will be replaced by its value.The compilation transforms the C code into the assembly language of our machine’s processor.The above command will perform the 2 stages.Assembly transforms a program written in assembly language into object code, a binary file in machine language executable by the processor.the file cirulo.o is created from cirulo.c. Using the -c option is useful when we want each source file to be compiled separately.At this stage, the binary code that was generated in our executable will be incorporated.And finally he will create the executable and our program is ready to fulfill its tasks.If we only have one source file we can execute all the steps with just one step:2. To perform the compilation : $ gcc -S age.c3. To perform the assembly : $ gcc -c age.c4. To perform the linkage : $ gcc -o age age.o5. To perform all the above steps in one step : $ gcc -o age age.c",c,https://medium.com/@josedelgadozambrano30/compilation-processes-in-gcc-2af2fa71f560?source=tag_archive---------9-----------------------
Understanding Linux — GCC compiler,"Chris DJun 10, 2020·2 min readAnytime you want to compile your C code into an executable, you will find yourself using the gcc command. This gcc command initialized the C compiler, taking your human readable code and giving you an executable binary file that executes commands and functions on your machine. The gcc compiler does this in 4 different steps.First the code goes through the preprocessor. The preprocessor preps your C code before going into the actual compiler. It’s like the seasoning in your recipe. One function of the preprocessor is finding the #include files you have added, grabs their code, and actually puts it into your program instead of just referencing it. At this point it’s still human readable but slightly more confusing!Secondly the code is compiled. This step takes your human readable code and translates it into Assembly. Assembly is still technically human readable. In fact many early machines depended on Assembly and it’s just one step up from Binary!Third, the assembler stage takes the Assembly code and translates it into object code. This object code contains machine instructions.And lastly, the linker. The linker takes all the machine instructions and blocks of object code and orders it into the correct place — providing you with a fluid executable program!An interesting way to try to observe the compiling process is using these 3 arguments. Take your program (we’ll use main.c for this example) and run it through the gcc compiler in chronological order: -E, -S, and -cYou can then view each file individually and see how they work!",c,https://medium.com/@1840/understanding-linux-gcc-compiler-d9a452e70eb?source=tag_archive---------6-----------------------
How to Interact with Nessus API to automate the scans,"Adarsh LalJun 3, 2020·12 min readNessus was first developed as an open-source vulnerability scanner, but it became a closed source in 2005 after being purchased by Tenable Network Security. As of this writing, Tenable offers a seven-day trial of Nessus Professional and a limited version called Nessus Home. The biggest difference between the two is that Nessus Home allows you to scan only 16 IP addresses at once, but Home should be sufficient for you to run the examples in this chapter and become familiar with the program. Nessus is particularly popular with professionals who help scan and manage other companies’ networks.The advent of web applications and APIs has given rise to an architecture of APIs called REST APIs. REST (representational state transfer) is a way of accessing and interacting with resources (such as user accounts or vulnerability scans) on the server, usually over HTTP, using a variety of HTTP methods (GET, POST, DELETE, and PUT). HTTP methods describe our intent in making the HTTP request (for example, do we want to create a resource or modify a resource?), kind of like CRUD (Create, Read, Update, Delete) operations in databases.For instance, take a look at the following simple GET HTTP request, which is like a read operation for a database (like SELECT * FROM users WHERE id = 1):GET /users/➊1 HTTP/1.0Host: 192.168.0.11In this example, we’re requesting information for the user with an ID of 1. To get the information for another user’s ID, you could replace the 1 ➊ at the end of the URI with that user’s ID.To update the information for the first user, the HTTP request might look like this:POST /users/1 HTTP/1.0Host: 192.168.0.11Content-Type: application/jsonContent-Length: 24{“name”: “Brandon Perry”}In our hypothetical RESTful API, the preceding POST request would update the first user’s name to Brandon Perry. Commonly, POST requests are used to update a resource on the web server.To delete the account entirely, use DELETE, like so:DELETE /users/1 HTTP/1.0Host: 192.168.0.11The Nessus API will behave similarly. When consuming the API, we’ll send JSON to and receive JSON from the server, as in these examples. The classes we’ll write in this chapter are designed to handle the ways that we communicate and interact with the REST API.Once you have Nessus installed, you can find the Nessus REST API documentation at https://<IP address>:8834/api. We’ll cover only a few of the core API calls used to drive Nessus to perform vulnerability scans.To automate sending commands and receiving responses from Nessus, we’ll create a session with the NessusSession class and execute API commands, as shown in Listing 5–1.public class NessusSession : ➊IDisposable{ public ➋NessusSession(string host, string username, string password) {ServicePointManager.ServerCertificateValidationCallback = (Object obj, X509Certificate certificate, X509Chain chain, SslPolicyErrors errors) => true;this.Host = ➌host;if (➍!Authenticate(username, password)) throw new Exception(“Authentication failed”); }public bool ➎Authenticate(string username, string password) { JObject obj = ➏new JObject(); obj[“username”] = username; obj[“password”] = password;JObject ret = ➐MakeRequest(WebRequestMethods.Http.Post, “/session”, obj);if (ret [“token”] == null) return false;this.➑Token = ret[“token”].Value<string>(); this.Authenticated = true;return true; }As you can see in Listing 5–1, this class implements the IDisposable interface ➊ so that we can use the NessusSession class within a using statement. As you may recall from earlier chapters, the IDisposable interface allows us to automatically clean up our session with Nessus by calling Dispose(), which we’ll implement shortly, when the currently instantiated class in the using statement is disposed during garbage collection.At ➌, we assign the Host property to the value of the host parameter passed to the NessusSession constructor ➋, and then we try to authenticate ➍ since any subsequent API calls will require an authenticated session. If authentication fails, we throw an exception and print the alert “Authentication failed”. If authentication succeeds, we store the API key for later use.In the Authenticate() method ➎, we create a JObject ➏ to hold the credentials passed in as arguments. We’ll use these to attempt to authenticate, and then we’ll call the MakeRequest() method ➐ (discussed next) and pass the HTTP method, the URI of the target host, and the JObject. If authentication succeeds, MakeRequest() should return a JObject with an authentication token; if authentication fails, it should return an empty JObject.When we receive the authentication token, we assign its value to the Token property ➑, assign the Authenticated property to true, and return true to the caller method to tell the programmer that authentication succeeded. If authentication fails, we return false.The MakeRequest() method makes the actual HTTP requests and then returns the responses, as shown in Listing 5–2.public JObject MakeRequest(string method, string uri, ➊JObject data = null, string token = null){ string url = ➋”https://” + this.Host + “:8834” + uri; HttpWebRequest request = (HttpWebRequest)WebRequest.Create(url); request.➌Method = method;if (!string.IsNullOrEmpty(token)) request.Headers [“X-Cookie”] = ➍”token=” + token;request.➎ContentType = “application/json”;if (data != null) { byte[] bytes = System.Text.Encoding.ASCII.➏GetBytes(data.ToString()); request.ContentLength = bytes.Length; using (Stream requestStream = request.GetRequestStream()) requestStream.➐Write(bytes, 0, bytes.Length); } else request.ContentLength = 0;string response = string.Empty; try ➑ { using (StreamReader reader = new ➒StreamReader(request.GetResponse().GetResponseStream())) response = reader.ReadToEnd(); } catch { return new JObject(); }if (string.IsNullOrEmpty(response)) return new JObject(); return JObject.➓Parse(response);}Listing 5–2: The MakeRequest() method from the NessusSession classThe MakeRequest() method has two required parameters (HTTP and URI) and two optional ones (the JObject and the authentication token). The default value for each is null.To create MakeRequest(), we first create the base URL for the API calls ➋ by combining the host and URI parameters and passing in the result as the second argument; then we use HttpWebRequest to build the HTTP request and set the property of HttpWebRequest Method ➌ to the value of the method variable passed into MakeRequest() method. Next, we test whether the user supplied an authentication token in JObject. If so, we assign the HTTP request header X-Cookie to the value of the token parameter ➍, which Nessus will look for when we authenticate. We set the ContentType property ➎ of the HTTP request to application/json to ensure that the API server knows how to deal with the data we are sending in the body of the request (otherwise, it will refuse to accept the request).If a JObject is passed to MakeRequest() in the third argument ➊, we convert it to a byte array using GetBytes() ➏, because the Write() method can only write bytes. We assign the ContentLength property to the size of the array and then use Write() ➐ to write the JSON to the request stream. If the JObject passed to MakeRequest() is null, we simply assign the value 0 to ContentLength and move on, since we will not be putting any data in the request body.Having declared an empty string to hold the response from the server, we begin a try/catch block at ➑ to receive the response. Within a using statement, we create a StreamReader ➒ to read the HTTP response by passing the server’s HTTP response stream to the StreamReader constructor; then we call ReadToEnd() to read the full response body into our empty string. If reading the response causes an exception, we can expect that the response body is empty, so we catch the exception and return an empty JObject to ReadToEnd(). Otherwise, we pass the response to Parse() ➓ and return the resulting JObject.To finish the NessusSession class, we’ll create LogOut() to log us out of the server and Dispose() to implement the IDisposable interface, as shown in Listing 5–3.public void ➊LogOut() { if (this.Authenticated) { MakeRequest(“DELETE”, “/session”, null, this.Token); this.Authenticated = false; } } public void ➋Dispose() { if (this.Authenticated) this.LogOut(); }public string Host { get; set; } public bool Authenticated { get; private set; } public string Token { get; private set; }}Listing 5–3: The last two methods of the NessusSession class, as well as the Host, Authenticated, and Token propertiesThe LogOut() method ➊ tests whether we’re authenticated with the Nessus server. If so, we call MakeRequest() by passing DELETE as the HTTP method; /session as the URI; and the authentication token, which sends a DELETE HTTP request to the Nessus server, effectively logging us out. Once the request is complete, we set the Authenticated property to false. In order to implement the IDisposable interface, we create Dispose() ➋ to log us out if we are authenticated.We can easily test the NessusSession class with a small Main() method, as shown in Listing 5–4.public static void ➊Main(string[] args){➋using (NessusSession session = new ➌NessusSession(“192.168.1.14”, “admin”, “password”)) { Console.➍WriteLine(“Your authentication token is: “ + session.Token); }}Listing 5–4: Testing the NessusSession class to authenticate with NessusManagerIn the Main() method ➊, we create a new NessusSession ➌ and pass the IP address of the Nessus host, the username, and the Nessus password as the arguments. With the authenticated session, we print the authentication token ➍ Nessus gave us on successful authentication and then exit.NOTEThe NessusSession is created in the context of a using statement ➋, so the Dispose() method we implemented in the NessusSession class will be automatically called when the using block ends. This logs out the NessusSession, invalidating the authentication token we were given by Nessus.Running this code should print an authentication token similar to the one in Listing 5–5.$ mono ./ch5_automating_nessus.exeYour authentication token is: 19daad2f2fca99b2a2d48febb2424966a99727c19252966a$Listing 5–5: Running the NessusSession test code to print the authentication tokenListing 5–6 shows the methods we need to implement in the NessusManager class, which will wrap common API calls and functionality for Nessus in easy-to-use methods we can call later.public class NessusManager : ➊IDisposable{ NessusSession _session; public NessusManager(NessusSession session) { _session = ➋session; }public JObject GetScanPolicies() { return _session.➌MakeRequest(“GET”, “/editor/policy/templates”, null, _session.Token); }public JObject CreateScan(string policyID, string cidr, string name, string description) { JObject data = ➍new JObject(); data[“uuid”] = policyID; data[“settings”] = new JObject(); data[“settings”][“name”] = name; data[“settings”][“text_targets”] = cidr; data[“settings”][“description”] = description;return _session.➎MakeRequest(“POST”, “/scans”, data, _session.Token); }public JObject StartScan(int scanID) { return _session.MakeRequest(“POST”, “/scans/” + scanID + “/launch”, null, _session.Token); }public JObject ➏GetScan(int scanID) { return _session.MakeRequest(“GET”, “/scans/” + scanID, null, _session.Token); }public void Dispose() { if (_session.Authenticated) _session.➐LogOut(); _session = null; }}Listing 5–6: The NessusManager classThe NessusManager class implements IDisposable ➊ so that we can use NessusSession to interact with the Nessus API and log out automatically if necessary. The NessusManager constructor takes one argument, a NessusSession, and assigns it to the private _session variable ➋, which any method in NessusManager can access.Nessus is preconfigured with a few different scan policies. We’ll sort through these policies using GetScanPolicies() and MakeRequest() ➌ to retrieve a list of policies and their IDs from the /editor/policy/templates URI. The first argument to CreateScan() is the scan policy ID, and the second is the CIDR range to scan. (You can also enter a newline-delimited string of IP addresses in this argument.)The third and fourth arguments can be used to hold a name and description of the scan, respectively. We’ll use a unique Guid (globally unique ID, long strings of unique letters and numbers) for each names since our scan is only for testing purposes, but as you build more sophisticated automation, you may want to adopt a system of naming scans in order to make them easier to track. We use the arguments passed to CreateScan() to create a new JObject ➍ containing the settings for the scan to create. We then pass this JObject to MakeRequest() ➎, which will send a POST request to the /scans URI and return all relevant information about the particular scan, showing that we successfully created (but did not start!) a scan. We can use the scan ID to report the status of a scan.Once we’ve created the scan with CreateScan(), we’ll pass its ID to the StartScan() method, which will create a POST request to the /scans/<scanID>/launch URI and return the JSON response telling us whether the scan was launched. We can use GetScan() ➏ to monitor the scan.To complete NessusManager, we implement Dispose() to log out of the session ➐ and then clean up by setting the _session variable to null.Listing 5–7 shows how to begin using NessusSession and NessusManager to run a scan and print the results.public static void Main(string[] args){ ServicePointManager.➊ServerCertificateValidationCallback = (Object obj, X509Certificate certificate, X509Chain chain, SslPolicyErrors errors) => true;using (NessusSession session = ➋new NessusSession(“192.168.1.14”, “admin”, “password”)) { using (NessusManager manager = new NessusManager(session)) { JObject policies = manager.➌GetScanPolicies(); string discoveryPolicyID = string.Empty; foreach (JObject template in policies[“templates”]) { if (template [“name”].Value<string>() == ➍”basic”) discoveryPolicyID = template [“uuid”].Value<string>(); }Listing 5–7: Retrieving the list of scan policies so we can start a scan with the correct scan policyWe begin our automation by first disabling SSL certificate verification (because the Nessus server’s SSL keys are self-signed, they will fail verification) by assigning an anonymous method that only returns true to the ServerCertificateValidationCallback ➊. This callback is used by the HTTP networking libraries to verify an SSL certificate. Simply returning true causes any SSL certificate to be accepted. Next, we create a NessusSession ➋ and pass it the IP address of the Nessus server as well as the username and password for the Nessus API. If authentication succeeds, we pass the new session to another NessusManager.Once we have an authenticated session and a manager, we can begin interacting with the Nessus server. We first get a list of the scan policies available with GetScanPolicies() ➌ and then create an empty string with string.Empty to hold the scan policy ID for the basic scan policy and iterate over the scan policy templates. As we iterate over the scan policies, we check whether the name of the current scan policy equals the string basic ➍; this is a good starting point for a scan policy that allows us to perform a small set of unauthenticated checks against hosts on the network. We store the ID for the basic scan policy for later use.Now to create and start the scan with the basic scan policy ID, as shown in Listing 5–8.JObject scan = manager.➊CreateScan(discoveryPolicyID, “192.168.1.31”, “Network Scan”, “A simple scan of a single IP address.”); int scanID = ➋scan[“scan”][“id”].Value<int>(); manager.➌StartScan(scanID); JObject scanStatus = manager.GetScan(scanID);while (scanStatus[“info”][“status”].Value<string>() != ➍”completed”) { Console.WriteLine(“Scan status: “ + scanStatus[“info”] [“status”].Value<string>()); Thread.Sleep(5000); scanStatus = manager.➎GetScan(scanID); }foreach (JObject vuln in scanStatus[“vulnerabilities”]) Console.WriteLine(vuln.ToString()); }}Listing 5–8: The second half of the Nessus automation Main() methodAt ➊, we call CreateScan(), passing in a policy ID, IP address, name, and description of the method, and we store its response in a JObject. We then pull the scan ID out of the JObject ➋ so that we can pass the scan ID to StartScan() ➌ to start the scan.We use GetScan() to monitor the scan by passing it the scan ID, storing the result in a JObject and using a while loop to continually check whether the current scan status has completed ➍. If the scan has not completed, we print its status, sleep for five seconds, and call GetScan() ➎ again. The loop repeats until the scan reports completed, at which point we iterate over and print each vulnerability returned by GetScan() in a foreach loop, which may look something like Listing 5–9. A scan might take several minutes to complete, depending on your computer and network speed.$ mono ch5_automating_nessus.exeScan status: runningScan status: runningScan status: running — snip — { “count”: 1, “plugin_name”: ➊”SSL Version 2 and 3 Protocol Detection”, “vuln_index”: 62, “severity”: 2, “plugin_id”: 20007, “severity_index”: 30, “plugin_family”: “Service detection”}{ “count”: 1, “plugin_name”: ➋”SSL Self-Signed Certificate”, “vuln_index”: 61, “severity”: 2, “plugin_id”: 57582, “severity_index”: 31, “plugin_family”: “General”}{ “count”: 1, “plugin_name”: “SSL Certificate Cannot Be Trusted”, “vuln_index”: 56, “severity”: 2, “plugin_id”: 51192, “severity_index”: 32, “plugin_family”: “General”}Listing 5–9: Partial output from an automated scan using the Nessus vulnerability scannerThe scan results tell us that the target is using weak SSL modes (protocols 2 and 3) ➊ and a self-signed SSL certificate on an open port ➋. We can now ensure that the server’s SSL configurations are using fully up-to-date SSL modes and then disable the weak modes (or disable the service altogether). Once finished, we can rerun our automated scan to ensure that Nessus no longer reports any weak SSL modes in use.Wrapping UpIn this writeup, we did automation of Nessus through API in order to complete an unauthenticated scan of a network-attached device. In order to achieve this, we needed to be able to send API requests to the Nessus HTTP server. To do so, we created the NessusSession class; then, once we were able to authenticate with Nessus, we created the NessusManager class to create, run, and report the results of a scan. We wrapped everything with code that used these classes to drive the Nessus API automatically based on user-provided information.This isn’t the extent of the features Nessus provides, and you’ll find more detail in the Nessus API documentation. Many organizations require performing authenticated scans against hosts on the network in order to get full patch listings to determine host health, and upgrading our automation to handle this would be a good exercise.",c,https://medium.com/@b4by/how-to-interact-with-nessus-api-to-automate-the-scans-eb935fb7952a?source=tag_archive---------1-----------------------
What happens when you type ‘gcc main.c’?,"In this blog, I will explain the whole process that a C file goes through to become executable. First, you must start by defining basic concepts such as what a compiler is and what process a .c file must go through.A compiler is a computer program that translates a program written in one programming language into another programming language, generating an equivalent program that the machine will be able to interpret. Usually, the second language is machine language, but it can also be an intermediate code (bytecode), or simply text. This translation process is known as compilation.The translation process is composed internally of several stages or phases, which perform different logical operations. It is useful to think of these phases as separate pieces within the translator, and they can actually be written as separately coded operations although in practice they are often integrated together.For a C file to become executable, it goes through four stages:1. Pre-processing2. Compilation3. Assembly4. LinkingThis is the very first stage through which a source code passes. In this stage the following tasks are done:Comments are removedMacro names are replaced with codeHeader files are included in the source codeThe preprocessing step creates a file with .i extension. If you run the command gcc -E main.c you can see the preprocessed source code, which is sent to the standard output.After completing the pre-processing stage, the next step the compiler takes is to take print.i as input, compile it, and produce an intermediate compiled output or assembly code.The compiling stage creates a “main.s” file.At this stage the print.s file is taken as an input and an intermediate file print.o is produced. This file is also known as the object file and contains machine-level instructions. At this stage, only the existing code is converted into machine language, and function calls like printf() are not resolved.Since the output of this stage is a machine-level file, as you can see in the image below, it’s completely unreadable.This is the final stage at which all the linking of function calls with their definitions are done. It creates an out file, which is a compiled executable file. With the out file, you can then run the program.And that would be all in the compilation process. See you at the next blob. Don’t stop learning!",c,https://blog.devgenius.io/what-happens-when-you-type-gcc-main-c-4e9ec816829a?source=tag_archive---------3-----------------------
C/C++/Python — locks,"Abu ObaidaJun 5, 2020·3 min readstackoverflow.comAfter a thread that owns a robust mutex terminates without unlocking it, the next thread that attempts to lock it will get EOWNERDEAD and become the new owner. This signals that it's responsible for cleaning up the state the mutex protects, and marking it consistent again with the pthread_mutex_consistent function before unlocking it. Unlocking it without marking it consistent puts the mutex in a permanently unrecoverable state.Note that with robust mutexes, all code that locks the mutex must be aware of the possibility that EOWNERDEAD could be returned.14A robust mutex can be used to handle the case where the owner of the mutex is terminated while holding the mutex lock, so that a deadlock does not occur. These have more overhead than a regular mutex, and require that all clients locking the mutex be prepared to handle the error code EOWNERDEAD. This indicates that the former owner has died and that the client receiving this error code is the new owner and is responsible for cleaning up any inconsistent state.A robust mutex is a mutex with the robust attribute set. On Linux this can be set using pthread_mutexattr_setrobust_np(&attr, PTHREAD_MUTEX_ROBUST_NP), or using the POSIX standard function pthread_mutexattr_setrobust(&attr, PTHREAD_MUTEX_ROBUST) if you have glibc 2.12 or later (this function was standardized in POSIX.1-2008).stackoverflow.comsays, std::unique_lock use the RAII pattern.When you want to lock a mutex, you create a local variable of type std::unique_lock passing the mutex as parameter. When the unique_lock is constructed it will lock the mutex, and it gets destructed it will unlock the mutex. More importantly: If a exceptions is thrown, the std::unique_lock destructer will be called and so the mutex will be unlocked.Example:C++ locks, another exaple",c,https://medium.com/@ijcode/c-c-python-locks-da12512348b?source=tag_archive---------1-----------------------
Bertown roads,"AasthaJun 8, 2020·2 min readProblem:Bertown has n junctions and m bidirectional roads. We know that one can get from any junction to any other one by the existing roads.To deal with traffic the government decided to make the traffic one-directional on all the roads, thus easing down the traffic. Our task is to determine whether there is a way to make the traffic one-directional so that there still is the possibility to get from any junction to any other one.Observations:Here we can consider all the junction as nodes and the roads as edges. So, for every node to be reachable from another node the graph should be STRONGLY CONNECTED.Hence, the problem sums up to finding whether the graph is strongly connected or not and finding the relevant path if they are strongly connected.Keys:Strongly Connected Graph: A directed graph is strongly connected if there is a path between any two pairs of vertices.Bridges: An edge in an undirected connected graph is a bridge if removing it disconnects the graph.Span-edges: These are the edges that form a spanning tree of Graph G, rooted at any vertex 1. i.e, They are the edges responsible for creating the DFS tree.Back-edges: Edges other than span edges are called back-edge. Or simply, they are the edges that are not a part of the DFS tree.Implementation:To implement the same, we can use the concept of DFS Tree.Reason :We will now print these oriented edges as our output.Code:",c,https://medium.com/@aastharanjan7/bertown-roads-b5d9a78db790?source=tag_archive---------1-----------------------
How GCC do it’s job,"Jiseth PeñaJun 11, 2020·4 min readIt’s a very easy job use a command like gcc and execute your file but do you know how actually this command works?. I will try to explain in a lit bit deeper detail how this job is performed.The process is composed by 4 steps, that four steps perform changes on our code, that allows our machines to understand how to execute the program.So let’s get started.You could think that your “hello world” program written on C its pretty basic with a “little” amount of lines, but when the preprocessing process is executed this will change your “little” to a very big amount of lines that you don’t even know what are they.Well on this process the compiler is trying to complete the code that is contained on the #include and #define hashtags, because they act as a shortcut in your file, but actually they are part of every file on C thats the reason that you need to add at least (#include <stdio.h>).The command to perform this action can be something like this:On the image, I create a file called test.c with a very simple program that prints something to the console, next to that you can see the command that i use on this first stage of the process (first line of the image).The file called test_expanded.c actually contains our code on the last part of the file, but above our code is “expanded” the rest of the code that was included with the #include tag. As a comment, you can find interesting the fact that the file have an .c extension thats because the code maintain C code on it (feel free to check it “im going to avoid that explanation”).Now we have a file with all the C code on it, now we need to compile the C code to Assembly code , this process translate your C code to another language that is part of a lower level language called Assembly, if the expanded file was a little bit unreadable don’t try to understand this Assembly code, its pretty weird.So lets see the command that you can use to do this compilation:On this image, I use the expanded file as an input for gcc with the -S option, as you can see i do a cat on the file generated (test_expanded.c). The code showed only the computers can understand it, but you can give a try to understand it, let me know in the comments if you know Assembly.Now we have our expanded.s file that is written on Assembly code, so in this stage we need to use that file and translate it again to Object Code, basically 1s and 0s.We can do that task with the following command:If the last 2 codes was really hard to understand, i don’t need to say that this one is unreadable (at least for me).On this stage we are ready to link our files with another files that actually were C files but now that they are compiled too we need to merge them together, in our case we are just working with one file so the next step will work only for our file.So as we said, the rest of the process is the linking between our file and other compiled and assembled files on our project , or with libraries that we are using, but in this case our file is an independent project.The command for accomplish this task is:On this case the final result is an executable file that looks pretty weird, but believe me it runs…As i said the program was a printf statement, really simple but for educational purposes:The result expected is “explaining gcc process :)”, I owe you the break line i promise the next post will have two break lines at the end.",c,https://medium.com/@jisethpenarias/how-gcc-do-its-job-81bdb48f9490?source=tag_archive---------8-----------------------
Template Metaprogramming: A C++ walkthrough,"Update: I was told on Reddit that this is an old perspective of C++ programming and compiler optimizations. If you are familiar with template metaprogramming and want to update on the latest, head over to the comments on the Reddit post.“Metaprogramming refers to a variety of ways a program has knowledge of itself or can manipulate itself.” reads Wikipedia. The first time I had read it as a novice graduate student, my mind was boggled with the possibility of AI programs capable of writing other pieces of code and taking over the world. While that’s the Hollywood version, here’s a realistic kind that’s used in the industry for compile time optimisations using C++ templates.I encountered this concept while researching for a side project involving optimisations. While it’s not a concept far off from the basics a CS101 course covers, it’s not quite well known and hence the blog to break it down.Let’s look at what templates are, for starters:Templates are the foundation of generic programming, which involves writing code in a way that is independent of any particular variable type.Take the following code for instance:The template tMax provides a generic function which can be used by all data types.Templates are compiled to real types by the assemblerThe templates themselves do not exist as generics after the compilation of the program i.e. in the assembly code.As in the tMax template example we saw above, the template tMax is transformed by the assembler into different functions for each of the types called in the code. ref: Assembly Code (line 38 defines the function int tMax<int>(int, int) which is tMax for type int)You might want to branch off to Tutorials Point (C++ Templates) to spend some time on templates.The equivalent construct to Templates provided by Java is Generics.Since templates ‘unroll’ into functions during compile time, and the compiler also performs computations where variables are not involved at the compile time itself, some computations can be completed at compile time and avoided during run time as a result.Computations can be completed at compile time and avoided during run timeLet’s say you are writing a piece of code and you want to use the value of 15! (15 factorial) as part of the code. The recursive way to do it is with a code as below:This compiles into a function and a call to that function with argument n=15 will be executed at a run time. ref: Assembly Code (you can see the factorial function and a call factorial(int) on line 17)Let us now convert into a program using templates.During compile time, the compiler unrolls the Factorial template for argument N=15 and encounters the template for argument N=14 and so on going on till N=0. Since no variables are encountered during these computations, they are resolved at compile time, the Factorial<15> is eventually resolved to the value of 15! i.e. 1307674368000 at compile time itself avoiding any runtime computations. You can see in the assembled code that the value 1307674368000 is used directly than a call to a function. ref: Assembly Code (There is no function definition as the value of 15! has already been calculated by the assembler and used in line 4)Executing both the implementations with time profiling (and too many iterations) shows us the impact of the optimisation. The templated code runs in ~0.03ms while the recursive version runs for at least 30 times longer, as expected, calculating the factorial and going down the recursion during run time.A deeper (more complicated, more rewarding) example comes from Template Metaprograms (Todd Veldhuizen). Using templates for Bubble Sort does not seem intuitive as the input is not a single number, but the idea is to create a template that unrolls into a specialised function for bubble sort catering to a certain number of elements. I strongly suggests going through the example and working out the details.The first thought that crossed my mind was why we would need to execute a function like call to a template if there are no variables involved. Isn’t that equivalent to writing code with the value itself (say 1307674368000 in place of `Factorial<15>`). The counter-argument to this is the requirement to abstract the functions instead of strewing random values around code.Hence, the libraries at Boost leverage templates to provide libraries for complicated functions like factorials, sine, cosine etc. to ensure the values are not computed at run time wherever they can be done at compile time.All in all, templates make a neat trick to ensure compile time computation and save some run time in time critical projects (High Frequency Trading, possibly air & space travel).Check out SO for community discussions on template metaprogramming. Comment below if you’ve used this in your code and how.",c,https://levelup.gitconnected.com/template-metaprogramming-a-c-walkthrough-a7c6db0b4148?source=tag_archive---------0-----------------------
Talking about (home~: gcc main.c) in human language,"JossvegaJun 11, 2020·2 min readWell, this command ”gcc main.c” comes from language C, is a general-purpose programming language that was developed by Dennis Ritchie. It is a medium-level, weakly typed, static data type language (in a few words the father of all languages). The other languages like Java, C++ or Go, they have C foundations.When you invoke GCC (I know, this word “invoke” sounds like spells but get used to it is very common in programming), It is divided into four stages: preprocessing, compilation, assembly, and linking. once you invoke the GCC command these steps unfold like a domino effect, one token after another (finishing preprocessing goes to the compilation and so on). We have options to stop that domino effect in some of the steps (preprocessing or assembly) that the GCC command has.The compilation process is roughly that of converting one or more source code files into executable binary code for a given hardware/software architecture. And I say “roughly”, because it involves several stages, particularly in C language, that “convert” source code into an executable one.First and foremost, let’s define source code. Source code is the program we write as programmers, the plain text that “tells” the computer how to do things.It is still not very clear, is it? Well, I’m going to give you an example of what it would be like in everyday life. Suppose you want to make a recipe and we invoke “recipe finder that ends with the last name .c” (command gcc)and the recipe egg_with_ham.c (name file)comes out. The next steps are*(PREPROCESSING) Prepare the ingredients*(COMPILATION) Beat eggs and ham*(ASSEMBLY) Cooking*(LINKING) Serve and carry from kitchen to dining roomFinally, we have a file executable (edible or cooked eggs with ham) egg_with_hamFor example, the -c option stops between assembly and does not execute a linking. By default, the object file name for a source file is made by replacing the suffix .c, .i, .s, etc. (further name), with .o. Other options are passed on to one stage of processing. Some options control the preprocessor and others the compiler itself.Easy, right? Now go ahead and type “man gcc” you’ll be able to see the file listing options and mixes with different ways, you will love it (Maybe).Welcome to the world in code!Source",c,https://medium.com/@jossvega/talking-about-home-gcc-main-c-in-human-language-65faa831b141?source=tag_archive---------0-----------------------
Sudoku solver. C recursive implementation (backtracking technique),"Ivan ShelomentsevJun 8, 2020·2 min readSudoku programming problem is well known. There are several algorithms that can be implemented. In this article I’ll show one that you’ve probably seen already — recursive solving (with backtracking). This algorithm is Depth-First Search, attempts to solve the branch completely, hence, has exponential time complexity (O(exp(n)) and, in current implementation, will return unsolved puzzle when unsolvable is given.First, we need a solvable puzzle. Ideally, one that you can solve by hand in order to write tests properly:Second, we need to introduce vocabulary and rules:Row — an array of 9 elements of the puzzle located horizontally. (E.g. first row is [1,7,4,0,9,0,6,0,0]).Column — an array of 9 elements of the puzzle located vertically.(E.g. first column is [1,0,5,0,8,3,2,0,0])Square — an array of 9 elements of the puzzle located in shape of 3x3 square stating from first row and column.(E.g. first square is [1,7,4,0,0,0,5,3,0])According to rules of traditional Sudoku: element in row | column | square should be unique.Third, lets pseudocode the solution using backtracking technique:Current implementation will explore every branch until (a) finds a solution or (b) finds that there is no solutions. Every empty cell is a node, every guess is the branch.Since this is my first C program, I don’t know any other way to test, but using printf function :)First, declare functions:Each function will return 1 as `success` and 0 as `failure` signal.Next, implement find_empty_cell :In order to use it we need to declare two variables in caller-function. This function will use pointers to this variables and modify them.Now, implement valid function:And now, solve function:And, finally, complete solution:Thanks for reading this .Cheers!",c,https://medium.com/@qwerty2323/sudoku-solver-c-recursive-implementation-backtracking-technique-415b42f9a24c?source=tag_archive---------0-----------------------
Computer’s Memory Underneath the Hood,"Khanittha KrajangjaemJun 8, 2020·3 min readAs we program, we are typing and seeing things on the monitor, there’s something we are also associating with that works behind the scene, it’s called computer’s memory, where things are stored and every byte has its own identifier.Who would know, when we create something like int n = 20; in C. What computer does underneath the hood is storing the value inside of the memory. While below is just an example and how the value can be stored anywhere within the memory, and variable n turns out to be just an address of the value 20 in hexadecimal format.in C we can see the address by using an & operator:let’s break it down a little — printf(""%p\n"", &n); this line of code tells the computer to find the address of n wherever it exists in memory:%p is a special format code in C which will give us the address of that specific value and &n represents the address of n (& — the AddressOf operator).The above code returns something like 0x2fe07adb which is an address that tells you where the value is inside of the computer’s memory, in another word, that address is a “pointer” (%p) to that value.If we want to create something like string s = ""STRING""; this is what the computer’s memory will look like:Apparently s from string s is just a pointer that points to “STRING” or the first character of a string “STRING”.In C string is just an array of characters which means there is no such thing as string in C — In fact, string in C is just a pointer to the sequence of characters as described above.So instead of using string s = ""STRING"";we use char *s = ""STRING"";char * represents a pointer to a character:: will print out STRING: will output a pointer or an address (of the first character in the string) e.g. 0x123: will output the addresses of each character of the string, starting from the first character s[0] e.g. 0x123 0x124 0x125 0x126 0x127…%c is a placeholder for a character: will prints out S: prints out T: prints out RAnd so on …While we are not getting addresses as return values. Blob.size returns the size, in bytes, of the data contained in the Blob object.The similarity is when the variable got declared and assigned value to, JavaScript underneath the hood creates a unique identifier or an address for the variable, and store that value next to it. — Apparently string “hello” contains 5 bytes in memory which would be the same with a sequence of characters in C.While this might not be something important to learn, it’s really interesting how things work in the lower level, sometimes having things working like magic makes us a little bit blind, you never know how many things are working beyond (behind) what we can see.",c,https://medium.com/@khanitthakk/computers-memory-underneath-the-hood-6043f0606520?source=tag_archive---------3-----------------------
What happens when you type gcc main.c?,"Deyber CastañedaJun 11, 2020·3 min readTo undertand the answer to the question in the tittle of this blog, first we need to know what is compilation and know about gcc command.C is a high-level language and it needs a compiler to convert it into an executable code so that the program can be run on our machine.Below are the steps we use on an Ubuntu machine with gcc compiler.Compiler converts a C program into an executable. There are four phases for a C program to become an executable:-Preprocessing means that the program will remove anything that is not needed for the program to actually run, such as comments.-Compilation means the program will now take your preprocessed code and translate it to assembly which then gets converted to binary.-Assembly basically is the first step to translating code from what humans can read to what machines can read ( 1’s and 0's).So, after look at the concepts that allow us to understand the compilation we can coclude that what the command gcc main.c does is take the main.c program and remove the comments in the code and translate from human readable formatt to machine readable formatt (Ones and zeros) to after link every piece of code and make a executable file.I hope that you get understand what is compilation and what does gcc main.c command.",c,https://medium.com/@garzondeiber6/what-happens-when-you-type-gcc-main-c-cf504785c57b?source=tag_archive---------6-----------------------
Creating My First Julia Executable,"Emmett BoudreauJun 2, 2020·8 min readIn my past, I have complained a lot about a few issues that persist in the Julia programming language right now. Julia is a great language, but no language is without its problems, and Julia’s short-comings are impressively rare and certainly don’t destroy the experience of using the language. One of these complaints that I have made is that Julia’s support for compiled executables is definitely not optimal at this moment in time.While this isn’t to say that creating executables in Julia is not possible, there is definitely a lot more hoop-jumping involved to get a proper executable out of Julia. I approached this very issue sometime earlier this year (or late last year, I don’t remember) , and remember spending many hours struggling to get any result whatsoever. Eventually, I gave in and put the keyboard down, and it was a very depressing defeat for me. Since then, there have been numerous updates to Julia’s compiler and I am very optimistic that there might now be a way to get a compiled executable. From what I have been told, it still certainly isn’t optimal, but now it is possible.For our application, I wanted to make a very basic GUI that would both let us know if our executable worked with visual feedback, as well as test the how Julia’s compiled executables are going to handle system-wide dependencies. To do this I chose GTK 3+. I chose GTK becauseAll of these are massive benefits and are going to make the actual writing part of this experience fairly simple. Though I am familiar with Gtk, as well as its classes and functions, the transfer of the library into Julia is certainly an interesting one.The first step to making a graphical user interface with GTK is to make a window.(predictably)We can do this the same way we would in Python or Vala. Though in Python the classes aren’t exported of course so you’d have to call Gtk, for example:Obviously, they all share their differences in syntax, but where the code really becomes Julianized is with the removal of functions like add(), and the replacement of those functions with the classic Julia Base functions we all know and love.Additionally, we can use the usual showall() method to show our creation!:I’ve never seen better UI/UX design.In order to compile our code, we’re going to have to save it as a .jl file. I opened up G-edit and jotted down the code I just demonstrated, however now running that file through bash yields this result:nothingWell maybe we see nothing, but perhaps we should ask Julia herself if she has any input on this situation?It seems we have come to an understanding, nothing happened. Of course, I have no clue what to actually do, but luckily we can consult my good friend and partner-in-crime:Google.After a bit of Google Fu, I ran into a Stack Overflow post where the original poster came back claiming that a Pkg update was what he needed for his to work.stackoverflow.comSo with that in mind, I realized it might be about time to create a virtual environment anyway.And now i’m going to add Gtk to that new environment:This time, I also decided to instead try running the code by including it from the REPL.So that definitely works, but how exactly is it that we are going to package this application into its own executable?In terms of packaging and compiling binaries with Julia, there are two main approaches that we can have to dependencies. Our first option involves saving loaded packages and compiled functions into a file that we can make Julia execute on startup. The second, and more viable for this use-case option is to compile the entire project into a relocatable compiled application. This generates a bundle of other files along with an executable that can be run on a computer that might not even have Julia installed (and we will test that out.)In order for package compilation to actually work, we are first going to need to restructure these files “ Julia style,” which involves creating both a src directory, as well as a Project.toml. We’ll start with the Project.toml file.Now we need to make a new directory called src and move our source file into there.Now, of course, we’re going to have to go get Julia Gtk’s UUID and add it to the [deps] section in our project file (yes I almost forgot we had a dependency.) After that, we will also need to add a “ main” function to our Julia file, for a final result that looks something like this:For our next step, we’ll reactivate our Pkg environment and add Julia’s package compiler to it.Now we will create a systemimage using this command:This will create a precompile file called app_precompile.jl in our working directory:We probably shouldn’t edit this directly, but let’s look at what the file looks like inside.This Julia code is precompiling our system image in order to create an executable. Something that should be noted is that there will be a few parameters to add to compile for Windows. Of course, I use GNU+Linux, so I just set it up to compile for X11. Here are the parameters you would add for Windows:Finally, we’re going to need to write some C to build a little ‘ compiler’ for our particular Julia package. So let’s touch a C file real quick:Now let’s open that file in our text editor of choice, I like Nano.The first thing we will want to do is include some standard C headers that the Julia compiler will need, these are string and stdin.Next we will also include the headers needed by Julia. There are two headers that are vital for using Julia, but if you’re using a C-ported package you might want to include its headers as well. As this is the case with GTK, I went ahead and included it as well with Julia.h and uv.h:For a final result that looks a little like this:Next, we’re going to run this from the Julia header:Then declare the prototype of the C entry point in our application:Next, we’ll create a new class called main which will take two parameters provided by the Julia main operation. We’ll also call uv_setup_args() which will tell UV the arguments that are provided by Julia:Next, we’ll initialize libsupport. This is an important step because we want to be able to use dynamic linking in our executable.Then we’ll adjust the Julia options provided by the Julia.h header:Note that JULIAC_PROGRAM_LIBNAME will be a parameter that we can add to the C compiler after we finish creating this file. Next, we’ll set the Julia arguments the same way that we did with uv:Now we need to set the PROGRAM_FILE parameter to argv[0]. This is also, of course, going to be a parameter so that we can reuse this compiler with any Julia file.Now we’re going to set Julia’s Base arguments.And with that we can close off this class and call the work function:Then we’ll add an exit hook with the Julia header:Here is my final file:(So not TOO much C)Here is my final compilation command:And now we’ve done it!We have a compiled binary, and when clicked, we get:I can’t even lie to you,that was kind-of a pain.Julia compilation is definitely not quite there yet. It certainly wasn’t fun to figure this out, and the struggle was compounded by the lack of documentation on how to do this, but from what I’ve seen it really doesn’t have that far to go. I am excited for the potential future that compiling Julia files could have! I think there are so many uses for Julia as a general purpose high-level programming language that has yet to be explored due to its lack of easy-to-access compilation. It makes sense why something like this would be so complicated, certainly, but I am hoping that in my perfect beautiful future we can have an experience more akin to compiling an executable from a different language.",c,https://towardsdatascience.com/creating-my-first-julia-executable-53e0f6bd0db5?source=tag_archive---------0-----------------------
What Happens When You Type ls *.c?,"jgra007Jun 8, 2020·2 min readYou may be new to coding. You crack open a coding book with exercises, or even enroll in a coding school, and the first thing you see is shell exercises. You ask yourself, “What? Why do I need to know this when I just want to code? Why can’t I use the GUI instead?” Well, this little example will show you the usefulness of knowing how to use the shell.Let’s consider the command: ls *.cWe will start by breaking this down step by step. When you open your terminal application, type ls, and hit enter, you are greeted with a list of the files and directories in your current directory. If you want to know more, type man ls for the manual page.Next we have the asterisk, which represents anything and everything. If you enter ls *, you will see all the files and directories in the current directory, along with the files and directories the next level down, which are inside the directories that are shown with just ls. For more information, see the Wildcards section of learning the shell.That lists too much information to be helpful, so we will add .c onto the asterisk. *.c represents every filename that ends with a .c. Anything can come before the .c because the asterisk represents anything. In the below picture, I’ve navigated to a directory on my computer that has a lot of C files. As you can see, entering ls *.c brings up all my C files in this directory, and none of my other files (header files). You can try this exercising by navigating to the directory where you keep your code files, and entering ls *[extension] with the filename extension of your choice.",c,https://medium.com/@jr047x/what-happens-when-you-type-ls-c-967dd30d4ef1?source=tag_archive---------6-----------------------
Secure C Coding through Binary Exploitation — Reading Assembly,"Ragnar SecurityJun 9, 2020·3 min readIn the previous article, we discussed the origins of vulnerabilities and how having an offensive mindset can be a great tool for defense. Now, we are going to learn the fundamentals of assembly code because it is the foundation for reverse engineering. In addition, most binary disassembly software are not able to translate bytes into C-like code (except for Ghidra, and there are a few exceptions to this that we will talk about later).As mentioned before, assembly languages can be thought of as a list of instructions that describes exactly what the computer is doing. In other words, it is almost a direct translation from bytes to human readable instructions.If you look at the example above, you can see that the bytes from the binary are on the left, and assembly representation of the bytes are on the right. We can often find patterns to determine whether certain bytes call a specific instruction (e.g. 0xe8 is call) or accessing a specific place in memory. Understanding these patterns can help construct shell code (as we will see later, most exploits are trying to gain shell access or higher level (root) privileges).More important than understanding how bytes correlate with instructions, it is essential to understand how to read assembly code. There are many manuals and cheat sheets to help you learn how to read assembly (e.g. https://cs.brown.edu/courses/cs033/docs/guides/x64_cheatsheet.pdf). It will take time to be able to become fluent in assembly; however, it is well worth it because most disassembled binaries will be in assembly format.In addition to learning instructions, you need to become familiar with the different syntax related to assembly. The two that are most used are Intel and AT&T (http://staffwww.fullcoll.edu/aclifton/courses/cs241/syntax.html). You could come across both during your journey in binary exploitation and reverse engineering. Intel is generally more common; however, we have noticed that academia uses AT&T syntax more frequently.The most useful tool that we use when learning/teaching reverse engineering Is GDB. It is the most effective way in teaching me how binary exploitation works because it forces you to have an understanding about the program is doing. It also allows you to step through the code and view memory in the middle of execution. Several important concepts that you should learn prior to reverse engineering with GDB is disassembling a function (disass <function name>), reading data from a register or the stack (x/#xw $reg), and being able to step through a program/set break points. It is also important to be able to understand how to find what functions are in the binary. A good cheat sheet that we recommend is from the University of Texas (https://users.ece.utexas.edu/~adnan/gdb-refcard.pdf); however, we recommend googling tutorials, as they will teach you tricks to make it more effective.To get started, we created a list of tasks that you should complete before reading the next article:1) Create a hello world program, compile it, then disassemble it using GDB. (gcc helloworld.c -o helloworld)2) Write down a list of instructions that you do not know and google them.3) Do steps two and three again; however, have the program’s main function call another function, include variables, and have a return value (in the second function).4) Use gdb to set breakpoints, step through, see values in registers, examine the stack.Now it is your turn to start your reverse engineering course. Be patient, Google everything that you do not know, and be creative. If you have any questions, contact us at contact@ragnarsecurity.com. We look forward to teaching you, so stay tuned for the next blog post. The next post will be the first one covering buffer overflows.",c,https://medium.com/@ragnarsecurity/secure-c-coding-through-binary-exploitation-reading-assembly-e4cea5047f7f?source=tag_archive---------4-----------------------
What is “ls *.c” and what does?,"Jorge OtalvaroJun 8, 2020·2 min readI bet you guys want to know what this command does? But to start we must first know what is the ls command?The command ls:It’s short for the word list “ls”,ls lists the files in the current working directory when you typing in “ls” and hitting in your shell or terminal “Enter”, you get:As we can see when using ls it shows me all the files I have inside the current directory where I am.* = Asterisk. The Asterisk is a symbol that we call a wildcard in the Command Line for of the Shell. A wildcard can specify a number of characters. By itself, it specifies every single file and directory in the current directory(the folder that we’re currently working in):As you can see, “ls *” produces the same results as “ls” does on its own. This is because when you type “ls” by itself your command line assumes you’re referring to all files and directories in the current directory.Some examples:When we write the command “ls (letter) *” It will show us the files that start their name with that letter(Note: It is very specific between upper and lower case)But it also happens otherwise when we write in the following way:ls * (letter)In this case it looks for the files that end with the written letter.“ls *.c” Prints(displays) all files in the current directory that end with a “.c”. If a file ends with a “.c”, that means that it’s written for the C Programming Language.",c,https://medium.com/@jorge.otalvaro00/what-is-ls-c-and-what-does-b5272dab16a9?source=tag_archive---------2-----------------------
Secure C Coding through Binary Exploitation — Introduction,"Ragnar SecurityJun 9, 2020·3 min readWhen people think about binary exploitation, they might think of Mr. Robot, hacking things quickly, and being able to gain access to some secret E-Corp server. The truth is, exploitation and hacking are slow, meticulous, and requires a lot of patience. By learning it, we will learn how to secure our software. It’s a puzzle that evolves continuously and there is always something to learn from it. In this series of blog posts, we will be learning secure coding through the eyes of an attacker. By understanding the fundamentals of how an adversary finds and uses vulnerabilities, we can understand how to secure our software and systems.Just as a disclaimer: Please DO NOT exploit a device without expressed permission from the owner. There are CTF Competitions (ctftime.org), many binaries that you can run on your own computer, or even programs that you can write yourself.To begin let us start off with the most fundamental reason why there are so many vulnerabilities in C programming: bits and bytes. The combination of various bytes is how a computer understands what to do. You would not be able to read this article without a computer translating the bytes. A bit is just a 1 or a 0, and a byte is a collection of eight bits. It is very tedious for us humans to read through; thus, we created the assembly level language and higher-level programming languages like C. The difference between the two is that assembly level languages are bytes converted to human readable instructions whereas higher level languages allows us to write programs in a logical way. These are not perfect because of the limitations of data sizes and human error. For example, if you look at the chart below, data types need to be standardized in the machine; hence there are byte limitations. For example, a character is one byte (8 bits with max value of 255), and you need four bytes to represent an integer (max value = 255^4). If you assign values greater than their respective max values, an integer overflow will occur. For example, if you have an unsigned character with the value of 255, and you add 1 to it, rather than getting 256, you get 0 becausae 256 = 0001 0000 0000 in bits; however, the last byte gets cut off since a character can only be one byte of data; thus, it turns into (0000 0000 = 0).Another fundamental C issue is related to memory. In most programming languages, we need to assign a specific size to a variable so that we know how much memory we need. This applies to C as well; however, the language allows us to assign a value to a location we are not supposed to. For example, in older versions of C you could create an array that is 64 elements long, but you decided that you needed to write the letter A at the 65th element; thus, resulting in memory corruption. The most prominent exploit that has come from this is called buffer overflow; which allows users to take advantage of functions like fgets() to modify program behavior.Having the basic understanding of where vulnerabilities come from can help you understand where to look for potential issues. This will allow you to be able to test code to ensure security and write code that has no flaws to begin with. In the next article, we will be going over the basics of using gdb for disassembly.",c,https://medium.com/@ragnarsecurity/secure-c-coding-through-binary-exploitation-introduction-e9c802376d01?source=tag_archive---------3-----------------------
What happens when you type gcc main.c,"Israel DiazJun 10, 2020·2 min readGCC is a compiler for a lot of programming languages!, GCC was originally written as the compiler for the GNU operating system.Here in this article we’ll exploring how is the Compilation Process for the C programming language.GCC normally does this C Compilation Process:Example:We have this C program:If we only want to see code generated by the preprocessor we use the gcc compiler and the -E option.2. Compilation: takes the code generated by the preprocessor and generate Assembly code.3. Assembly: takes the Assembly code generated by the compiler and convert it to Object code (Machine Code).4. Linking: the linker merges all the Object Code into a single one, that file is the executable file, by default is called a.out.Now if we are using a function from a library like in our previous example (#include<stdio.h>). The linker will ‘link’ our code with the code of the library.As we can see the C Compilation Process it is composed by internal steps that generates a executable file.Keep exploring, coding and learning 💛",c,https://medium.com/@1777/what-happens-when-you-type-gcc-main-c-87f4683da55?source=tag_archive---------1-----------------------
How to easily deploy a React app,"KishoCodesJun 1, 2020·4 min readDeploying a React app (and even apps built on it) should not be a drag. I use the word drag here because I first attempted to deploy my react apps to GitHub pages to showcase them on my portfolio and it was very very tiring due to the many steps I had to go through. The good news is that you do not have to do that. There is a far easier way to do this and I am here to show you how.If you would like to watch the YouTube tutorial on this, you can check it down below. Or you can keep on reading as I explain the same here.In order to deploy your React app online, all you need is 2 different things.Considering that you are on this blog post looking to learn about deploying it, I assume that you already have your React app ready and open on your IDE (I use Visual Studio Code). If you haven’t, open it, stop the serve and type in your the React Build commandPS- remember that the build command for frameworks such as Gatsby might differ, so make sure you type in the build command to match your framework.Once you have run the build command, React will start doing its magic and start bundling all the files together, minifying them and just everything it needs to make sure your app is now production-ready. After the building is done, you will see that your project folder has a new folder with the name Build. This is where all your build data lies and is the only important folder for us at the moment.If you have the build folder ready, congratulation! You have now completed step 1 and is now ready to move onto step 2 with Netlify.Before I jump into deployment, I wanna talk about what Netlify is. The folks at Netlify like to identify themselves as‘An all-in-one platform for automating modern web projects. Replace your hosting infrastructure, continuous integration, and deployment pipeline with a single workflow’And to be honest, that is exactly what they are. They are (in plain English) the providers of a platform that lets us host our web development projects for free (they really have a great free tier).To get started with Netlify all you have to is go and create an account here. When you do, you will be asked to confirm your email address as usual. Once that is done, you will be taken to a page which looks the one belowGreat! So the only thing that is left for us to do is to drag our React build folder from step 1 onto this empty space in Netlify (highlighted in yellow). Once you Netlify will take some time process the files you just gave it and after a bit of time (depending on how large your app is), it will deploy your React app with a funny name like the one I have here. To see your deployed React app that was just deployed, all you have to do is click that link with the funny name.Woohoo! Congratulations! You just deployed a React app in just 2 steps!Ah, but wait. What if you want to change the funny name of the deploy link in which your React project is hosted? Easy! All you have to do is look for Site Settings and then choose Change Site Name. Just remember that you will not be able to change the .netlify.app of your link as it is hosted on a Netlify domain. But hey, if you have a domain that you own, you can easily connect it here. I will cover how to do that in a future blog post.If you found what you read informative and helpful, please let me know and follow me on my social media channels for more! If not you can just sign up to my email newsletter from my website below to be notified of any blog post I publish.See you around! 😊Originally published at https://www.kishokanth.com.",reactjs,https://medium.com/@kishokanthjeganathan/how-to-easily-deploy-a-react-app-36c3ddabc28f?source=tag_archive---------15-----------------------
On the power of Hooks: Part 3,"We’ve come to the final instalment of a series of articles dedicated to React Hooks. In Part 1 we’ve learned how to use a hook to let our component play audio files. In Part 2 we’ve learned how to enhance a component to always be aware of the current time. Now it’s the time to piece all of this together and explore how we can leverage what we’ve learned in a real-life scenario.So, let’s imagine that a firm wants to encourage its employees to regularly wash their hands while on shift. The executives have decided to roll out a very simple app that will nudge the employees to wash their hands every once in a while. We’re now assigned to the development of such app, and on the first day of development these user stories land on our desk:Let’s see how we can leverage hooks to complete this piece of work.The very first thing we want to do is to have a model for our shift data. We’ll leverage redux-toolkit to create all the necessary code to store and manipulate our user shift’s state.If you’re not familiar with slices, they’re super handy to declare state and it’s mutations from a single place. The code above gives us access to a reducer and its actions through shiftSlice.reducer and shiftSlice.actions respectively.Let’s use this into our first component, a Button-like view that lets the user check-in and out from a shift while changing color accordingly.Let’s break it down.We’re leveraging useCurrentTime from the previous article to have a reference to the current updated every second. We’re then getting the latest shift in memory using lastShiftSelector through redux’s useSelector.The juicy bit is in useEffect, which is being invoked every time currentTime changes and then checks if currentTime is before last shift’s endTime. If yes, we’re inside the shift and the state is updated accordingly through setIsOnShift.Our TouchableOpacity then takes care of updating the shift state according to user input. Checking out if we’re currently on shift, or checking in if we’re out. This is done through the slice’s action dispatched through useDispatch.So far we have taken care of stories 1 and 3, now we want to have a separate text reminder to show if the user is on shift. As you can imagine, we’d have to copy most of the above code in another component to check if the user is on shift. So why don’t we use what we learned and extract a custom hook from the above component? Let’s try.There you go, we have abstracted away useIsOnShift and we can now use our custom hook across our codebase. We can now refactor ShiftButton to use our hook.Then, we can leverage useIsOnShift to complete our second assignment.We have now completed all user stories that were assigned to us.Go celebrate!This is the third part of my journey through the land of React Hooks. You can find Part 1 here and Part 2 there.",reactjs,https://blog.devgenius.io/on-the-power-of-hooks-part-3-78d79cc2a110?source=tag_archive---------8-----------------------
Multilingual site by react and redux,"Abobaker HilalJun 3, 2020·7 min readCreate a site that supports multiple languages ​​in a simple and easy way without librariesThe idea is to store the language variables in a JavaScript file and then control the change of those variables according to the language chosen by the user. For this reason, I used the redux state manager to save and change based on a certain action that comes from the user who runs a specific reducer which in turn stores the variables that come from Action and then I call the variables that were stored in the store inside the site. In order not to forget, I also pass the action directions and language of the site and reuse it from the store inside the site. You also used the redux-persist library to save the language redox in local storage, and even if you make a reload of the site, the language you chose will be saved in your browser.First, we create a React application by way of writing the command, and we enter the application folder and run it. Follow these instructionsInstallationAfter creating and running the application, we will create the components of the front end of the site, as follows :navbarcontentfooterlanguagesBefore that we will follow a hierarchy of ordering our ingredients create a folder called componentsWhy use this hierarchy? You will see this in the files, as I separate the JavaScript codes that deal with the store and action and reducer (redux) from the jsx codes.Inside the components folder, create the previous folders. We’ll start with the languages folderThe first thing was set up a file to store all the translations. I wanted it all in a single place to make it easy to update or add new languages to. In languages.js is this basic code — a language variable holding each individual translation within in, and in each translation a collection of key-value pair consisting of a shared variable name and it’s translated value.Inside Folder languages create a folder languages.jsInside Folder navbar create a folder components and file index.jsin side components create a folder Navbar.jsIt receives the changeLang()function that comes from the index component and accepts three parameters, which are language, direction, and abbreviation of the languageNavbar.jsindex.jsThis component receives a function selectLanguage() from action, and it also receives the language, direction, and acronym for the language from state via redux .In this component there is a function changeLang()which just by clicking on the user will change the parameters and then pass it to the function selectLanguage()There is also a function direction()that changes the direction based on the direction that comes from the statenavbar.cssstyle navbarInside Folder content create a folder content.jsIn this component, use language variables that come from the statecontent.cssInside Folder footer create a folder footer.jsNow we go to the redux files inside the src folder create a redux folder that will includefolder actionfolder reducerfile storeInside Folder action create a file languages.jsInside Folder reducer create a file languages.jsInside Folder reducer create a file index.jsInside Folder reducer create a file store.jsHere I used the redux-persist library to storage reducer to local storage, and it turns out that if you reload the page again, you will find that the language you chose is deleted and remains the default language. Watch the video below.In the App.js file, we call the components to see their instructionsUpdateI added a function defaultLang()in action file languages.jsand its function is to put the primary language in the state when the site is requested for the first time, you will notice that there is no content, so I added an action and a function that adds the language to reducer When the site is requested for the first time I will rewrite the action and reducer and then add the function to the navbaraction file languages.jsreducer file languages.jsin navbar.js add function defaultLang()All these modifications and development processes will be found on github DevelopmentThis open source project is available on GitHubYou can also see demo",reactjs,https://medium.com/@abobakerhlal09/multilingual-site-by-react-and-redux-d8a8dc961817?source=tag_archive---------8-----------------------
No-Config React With Parcel,"When I was starting with React, one thing that bugged me was the complexity of configuration files that came with building React applications. Want to import SVG files? You need to install another dependency and edit your Webpack config file again. For many toy projects, the time required to create and configure a new project made it hard to get started actually making stuff. When I finally learned about Parcel, a no-config bundler for React, I was sold. Here’s a short guide on getting started with Parcel.At a glance, Parcel just requires you to install one dependency, parcel-bundler and you’re good to go. Let’s start by creating a new folder for our project. You can use NPM, but I prefer to use Yarn as my package manager.Next, add Parcel and React as project dependencies.You’ll need a couple of files to start with, but it mainly boils down to having an entry point (index.html) and your React entry point (index.js)Add an index.html to the root of the folder and put this in the content of the file:The meta tag makes your page responsive, so mobile users have a better experience. If you’re new to React, the div element with the root ID is conventionally used to mount the application.Next, we create the index.js which allows our React application to mount.Now we just need a way to run the program! Since parcel is only installed as a dependency of this project, we’re only able to use parcel commands when they’re run through a command in the package.json file.Let’s add a start command that will run the Parcel bundler when we run yarn start:Now, if you run yarn start, you should see the bundler doing its magic, and your application will be served on localhost:1234. It’s looking a bit plain right now, so let’s add some more content to it.Let’s make a new folder, components, in the root folder, then create another folder in it, MyPage, and finally, create anindex.js file in that folder. Ideally, we want to separate the mounting of the application from the actual application, so let’s move the content from index.js to components/MyPage/index.js .You should also notice that if you change the content of MyPage from Hello World! to something else, the bundler will automatically reload the page with the new text. This is a convenience that Parcel brings to you without any configuration. Isn’t that cool?Let’s add some more content to our page and a little styling. First, save this CSS file as sakura.css and place it in the MyPage folder. This CSS file is a no-config stylesheet — all you need to do is to import it.Much better! No additional configuration required to handle CSS files, and everything just works — it’s very conducive to a beginner’s learning. Later on, when you need more fine-grained control, you can use Webpack, or other bundlers, to fine-tune your application. As for learning, I believe that using Parcel is the way to go.I have come to the end of my tutorial. I hope that you’ve learned how to use Parcel to speed up your learning and development. If you’re interested, check out Parcel’s awesome documentation here.",reactjs,https://betterprogramming.pub/no-config-react-with-parcel-503babdd3992?source=tag_archive---------5-----------------------
React: Class Components vs Functional Components,"Michael KofronJun 2, 2020·3 min readIn this article I will explain the differences between Class Components and Functional Components after React 16.8, and explain why you should probably start using Functional Components more often.The most visible difference between Class Components and Functional Components in react is their syntax. Class Components extend from React.Component while Functional Components do not.The Functional Component is obviously a much more simplistic way of achieving the same result, but this simplicity offers some difference in the way a Functional Component operates.Functional Components and Class Components handle state differently. Functional Components actually used to be known as “stateless components” because they didn’t offer the ability to set state at all. But, with the React 16.8 introduction of “Hooks” this changed.Hooks allow developers to use state and other React features within Functional Components.This is how state is handled in a Class Component:In this example we use setState to update our counter’s value.This is how state is handled in a Functional Component:In this example we use “useState”, a new Hook provided by React. useState’s only argument in this example is the initial state you want “count” to have, which is (0).In a Functional Component you can call state directly by its name, but in a Class Component you have to call state by using “this.state”. So, even with new updates Functional Components are known for their simplicity!With React 16.8 Functional Components can also react to lifecycle events using the “useEffect” Hook. Read more here.Before React 16.8 use cases between Functional Components and Class Components were very clear — Class Components were stateful and Functional Components were presentational. But with the advent of Hooks there is a budding preference for Functional Components.They’re plain JavaScript, they’re simple, require less coding, and potentially may be even faster than Functional Components. With larger React apps the amount of lines saved could potentially be in the thousands using Functional Components, which would no doubt speed up your app and decrease its file size. Along with less lines of code comes easier readability, which is great for future-proofing any app and is the main feature of great code. Because of these features I will definitely give preferential treatment to Functional Components in my future React projects!",reactjs,https://medium.com/@kofronmichael/react-class-components-vs-functional-components-35134219063?source=tag_archive---------3-----------------------
What is ReactJs?,"In modern world when Intranet is one of the necessity next to water,food and shelter. JavaScript is one of the major programming language governing the web. Web application constitutes to more than 90% of the total web engagement out there since its the only programming language that runs in the web browser thus allowing us to create, great experience for the end user by creating highly reactive and interactive web applications. The react ecosystem allows you to create and achieve mobile like app user experience in the web.As using JavaScript alone can be challenges at times and with long lines of code it becomes hard to be managed in projects where we have to write and manage thousand of lines of code, it becomes quiet easy to create a web app using we application using libraries like Jquery or and ReactJs.React being a UI library as the reactjs.org suggests is all about components. Components are small modules of code divided so as to achieve modularity across the process. Using which we can easily, create, re-create amazing web applications. All it need is create a react component once, customize and code it to match your needs, dynamically pass it data or listen to user events and create a workaround as as to achieve the end requirements.A JavaScript library for building user interfacesFor instance while we want to create a Navigation menu for a website. It’s as simple as creating a Navigation component, configuring it to your niche needs and re-using it as and when needed.React being a UI library it is important to know few things beforehand to use it to greatest of its potential.Few of the things you should know and will help you have a better understanding of the React Library. If you are all set to learn React, please make sure you have know the following points:React is being used by a lot of big players across the web to name a few, Facebook, Instagram, WhatsApp, Netflix, Airbnb and a lot more. Due to its robust structure and re-usability it allows to create code, leverage and maintain in long run.Learning a UI library have its own benefits, allowing you to maintain large code and create beautiful user interface with the least of the challenges. Learn one now.",reactjs,https://blog.devgenius.io/what-is-reactjs-af983f62df27?source=tag_archive---------6-----------------------
Displaying PDF in React app,"I want to show you an easy and simple way to display PDF files using React in the browser. I will be using a library called react-pdf. This library is capable of rendering PDF files given an URL or a local file inside the project or base64 encoded version.To get started, create a simple React application using Create React App. To do this you can run the following commands:After this, add the react-pdf library to the project using the following command:To render the PDF file, react-pdf has a Document component where you need to pass a file prop. Then inside the Document component, you need to create a Page component. Simple isn’t it? Sample code:The final project structure will look like this:Let’s start with creating single-page.js and all-pages.jsinside the components -pdf folder with the following contents:Now navigate to App.js and replace the content with the following code:As you have noticed we have to create buttons to change the page of the document in single-page.js component. This is because react-pdf does not provide a user interface.The full project code can be found here github and you can see the demo below:If you get the following error in console:then follow the steps below:Referenceskilled.dev",reactjs,https://levelup.gitconnected.com/displaying-pdf-in-react-app-6e9d1fffa1a9?source=tag_archive---------0-----------------------
How to Build a To-Do List With React,"Original article published on my blogThis article was originally published using React’s class-based components. But given the fact that functional-based components are now the norm, I would be providing the functional equivalent of the App component with the useState React hook. Enjoy!😊Building a To-do list application is a great way to challenge yourself to get going after grasping the fundamentals of React Js. It is a starter project that would help solidify basic concepts like State, Components, Virtual DOM, and what have you.I would be walking you through the steps in creating a simple To-do application and would encourage you to build on it and add any features you may like. Before we get started, here are few things you must know:For those who would like to check out the real code, here’s the link to that.You could also access the live app here.The repo for the functional-components version of this app has been added to this article.Since this is just a React tutorial, I am just going to drop the links to the CSS files for each component:For the sake of this example, we would be using the create-react-app method to set up our app. This is to make everything easy for us.If you do not have create-react-app set up on your computer, you can get started by checking out the Create React App Documentation.The following steps will get you started setting up your app:Note: In this article, I will be using only the term terminal.2. Change path to the directory you want to app to be created on, using the cd command:3. Type create-react-app <name of the app> in your terminal and hit Enter. This installs and the app and voila: We can start coding our app.Note: If you get this kind of error in the message below, try using the sudo command to switch to root.4. Go into the app folder and open in VS Code or any other IDE that you use:Before we dive into coding our app, we need to have a visual representation or structure of what we want to achieve. I have a quick sketch of what we want the app to look like :Most especially in React, we want to know the following:Looking at the image, there are three basic components we will need to come up with:Just as these components have been set out, this is how they would look in our app. We will have to include a to-do component for each task, and finally, our App.js file will contain all of these components (this is already provided to us by create-react-app.)After the whole setup, we have our app’s src structure looking like this:As you can see here, we created a component folder under the src folder to house all individual and reusable components. This is one of the major advantages of React; it helps to break down our entire app into chunks (reusable components). We tend to see that advantage more when building larger applications.Finally, it’s time to code our app. With all our files in place, let’s start with the App component since it will house all of our remaining components.This particular App component will be a class component so that we can have access to React’s state.I recommend we clear the whole content inside our App.js file and start coding it from scratch. However, you could just modify it to the code below:Okay, let’s explain what we’ve done so far. The first image just describes what we have to import. As you know, before writing any functional or class component in React, we have to import React. Since this is a class component, we also have to import React’s component class as well.We then import the TaskIndicator function from its respective file and also the CSS file for the App component.After this, we start writing our App class components, which will extend the Component class of React and has access to its methods (by calling super()). This gives us access to a state property where we can declare what we want to change.In the case of our app, what we want to change is our tasks (lists) property, with object values containing each task and its respective id.The render method is then called, which will return the components we want in HTML-like format (JSX). The first thing we render for now is the TaskIndicator component, which we pass the number props into, indicating the number of tasks left in our to-do list.Let’s see how our TaskIndicator component will look:We make it a functional component(since it doesn’t contain any state) that just returns text with the number of tasks we have (using the number props we passed to it).After applying the component’s CSS file, we should see this in our browser:As you can see, it indicates that we have three tasks, which is the number of objects present in our lists array.That was pretty easy, wasn’t it? Now let’s move on to adding our ToDoList component. which will take the lists array as its props and also have a handleDelete method that will help delete any task we are done with. The handleDelete method is as follows:This method takes in an index argument that would help to filter out a task from the list with id not equal to the index. It then modifies the list’s state to the filtered form, using the this.setState() method.We then add the ToDoList component to the render method of our App.js file:Don’t forget to import the ToDoList function from its file and then go on to write the ToDoList function:This functional component iterates through the lists array given as props and returns a ToDo component for each list. We then give each ToDo component the following props:Of course, we import our ToDo component and write the function in our ToDo.component.jsx file.This part of our app finally returns a div with each task and a Font Awesome minus (-) icon that has the onDelete props passed to it. When this icon is clicked, the corresponding task is removed. The component is then exported.To access the Font Awesome icon, we add the script tag below to the end of our body tag in our index.html file, which is located in the public folder of our app.After all this, our app comes out looking like this:Now for the final stage of our app, which is creating the input form that will take in new tasks and add them to the list. From our sketch, this form will contain an input element (of type=text) and another Font Awesome icon with the plus (+) sign.To help us achieve this, we need the following methods:First, we add an input state in our App component:After that, we proceed to write our handleChange and handleSubmit methods after the constructor.The handleChange method, as mentioned earlier, just takes note of the user types and sets our input to that value.Our handleSubmit method, on the other hand, does nothing if the user’s input is empty. If the user eventually does write something, it creates an object with the task and id property and adds this object to our to-do list. When the task is added, the form is cleared or reset.As we have done for the previous components, we import the InputForm component from its file and render it in our App component:Remember that we have to pass in the two methods we just wrote, as props into the InputForm component.Here is the InputForm function:We make use of all necessary props and export our component.As you would notice, switching our App component from Class-based to functional-based comes with a few modifications.The first and obvious thing we do is to change from a Class to function.Doing this, we lose the this context; hence, we initialize all handle functions using const. We also no longer need the constructor, and can go on to remove it.Lastly, we use the useState hook to initialize our state in place of this.state object in the constructor. useState allows you to initialize a state and also pass in a function to change the state later on — just like this.state and this.setState would. The difference is, it is used in conjunction with functional components and can be used as many times as possible to specify individual states just like we did for lists and input.Every other thing is pretty much straightforward from here.And that’s it. We have our to-do list app ready. Just add the necessary styles, and we have our app looking exactly like the finished product at the beginning.Understanding a little project like this can help you grab some basic concepts in React and prepare you to learn more challenging concepts as you proceed.",reactjs,https://betterprogramming.pub/how-to-build-a-todo-list-with-react-d2d5dd9f6630?source=tag_archive---------2-----------------------
React Hooks-Calling Child Component Function From Parent Component,"Nugen I.T. ServicesJun 5, 2020·4 min readIn this article we’re are assuming that you are somewhat familiar with React HooksFor calling Child Component function from parent component in hooks we are using React.forwardRef and React.useImperativeHandle hooks from React. Now let us know briefly about these two. What are they? How are they used? And the most important question, how are we going to use them? So let’s get started!React.forwardRef creates a React component that forwards the ref attribute, which it receives, to another component below in the tree. So, in simple words forwardRef creates such a component which receives a ref attribute from its parent component and forwards it to the components down in the hierarchy.Arguments and return type: React calls this function with two arguments one is props and the other is ref. This returns a react node something like this.When we use useRef then instance value of component is generated with which ref is used, that instance value can be used to manipulate the DOM element, now React.useImperativeHandle customizes the instance value that is exposed to parent components. It is used with forward ref otherwise it will throw an error Cannot add property current, object is not extensible.So let us create a parent component first So we have created a component with a button to call the child function later on.Now, lets create a child component using forwardRef .A child component is ready along with a function in which it alerts a message “Child function called”, this function is named as showAlert with no arguments, using useImperativeHandle as we have discussed above about the useImperativeHandle and the forwardRef hooks in React.Our output is still the same because the child component is not rendered yet.Here comes the important part to call the child function.Now we will just render the child component in the parent and create a ref using React.useRef with that the showAlert() function will be called.Child component is rendered and a ref is created using React.useRef named childRef. The button we created in parent component is now used to call the child component function childRef.current.showAlert();As we know useRef() returns a mutable ref object whose .current property is initialized to the passed argument. The returned object will persist for the full lifetime of the component. Now, the function showAlert() is initialized in .current property of ref, which can be used anywhere in the component.reactjs.orghttps://stackoverflow.com/",reactjs,https://medium.com/@nugen/react-hooks-calling-child-component-function-from-parent-component-4ea249d00740?source=tag_archive---------1-----------------------
Node vs. Deno in Layman Terms,"Worried that dinosaur or sock shaped “deno” is going to kill our favorite Node? Let me help you to get out of this dilemma.Before starting, the very first thing to address is “Why Deno is launched?”. Deno is created as a convalescent version of Node, both are created by Ryan Dhal. While releasing Node, he thought he could do better with Node, but it was way too famous before he could do anything. So now he chose the chance to release a new avatar of its kind.So, Let’s know what new Deno has that Node doesn’t?Security : Deno is secure by default. A script run with Deno cannot access the file system, network or environment. Thus the third party services/ modules cannot exploit if malicious in case. Thus the power to distinguish remains in hold of the developer.— allow-read=/tmp — allow-write — allow-net — allow-envTypescript Support: No transpilation needed. As typescript is built into deno, Painless execution of ‘.ts’ files. Below is the example:No More NPM Modules… OMG!So the folder which makes the entire application heavier, wont be there. similarly No HTTP modules will be required. Dependencies will be accessed on the run via urls/remote scripts.The More Powerful “Await”No more exclusive coupling of async-await, Await is more powerful doing exorbitant things on plain vanilla promises, like following.With all main features understood, few more things to go before going to conclusion.Version 0.0.1 is stable, but it is still working on the unstable features. Also, there are so many projects working on Node, it is tough for a 2 year old new language to replace the ongoing tech in market this easily. Also, its not compatible with NPM packages.So, with new features like decentralized dependencies ,built-in-tooling etc, in general simultaneous coexistence is a great possibility for these techs but for now Node still has a stronghold.",reactjs,https://medium.com/thatiitgirl/node-vs-deno-in-layman-terms-97fa2a5fe1c?source=tag_archive---------9-----------------------
My Journey from XR to Java Script,"Nipun DavidJun 5, 2020·7 min readDisclaimer: This is not a tutorial“Take the first step in faith. You don’t have to see the whole staircase, just take the first step.” — Martin Luther King JrThis post is about my experience with java-script how I started with it and what challenges I faced from a perspective of a Game/AR/VR developer who had very little or no exposure on how the web works and was only acquainted with type-safe object-oriented language i.e. C#.In my defence yes I have studied web technologies in my bachelors and I know what HTML/CSS/JS and HTTP etc. are, but I never wrote a single piece of code on it in my professional career that can run on the web.So here how it all started, I wanted to get my hands dirty with web-services for a long time and my manager can tell how much I used to pick his brains to give me a problem statement or task that can give me some exposure on web technologies i.e. microservices, REST, web essentials like ReatJS, NodeJS, MongoDB, etc.Okay, I think before we proceed we need backstory — So I started as a game-developer in 2012 at that time I used to make games for good old Nokia phones and my games used to go on OVI store, then gradually I moved into games for smartphones using game-engines like AndEngine, LibGDX, but in the end I fall in love and decided to get settled in 2014 with Unity3D.In years ahead I started experimenting with AR/VR using Unity3D when this tech was in a very early stage and did found good opportunities which helped me landing in Nagarro i.e. my current employer where I am part of the CoE-IoT team in which I am responsible for everything related to AR/VR and I gradually shifted my entire expertise in the area of computer graphics.But like all marriages, things started getting monotonous with Unity3D and I wanted to try out new things to expand my horizon and acquire few new skills.A few weeks ago we received a request to create a 3D platform which should have capabilities of AR and VR as well, this has to be a web platform which means no app, no SDK installations. The AR and VR view in the mobile browser itself if there is AR/VR support on mobile.When I first read the email, I was like- “God is there! And he has heard me…” as this is what can let me work on technologies that will give me exposure to the picture on the other side.It has all the components which I wanted to work on for a long time!Gameplan — It was agreed among the senior folks that I will work on this lead to make sure that we give the impression of wow. And we started doing our homework. So after a lot of research and scrolling through the web, we decided to either propose ThreeJS or BabylonJS.Okay so what next, what else we will need? Oh yes! a framework to create UI and backend services to handle requests. In my head, there were already fireworks as I knew what we will choose. ReactJS for UI and NodeJS for backend services.So I had a little over a week to understand the key services we would require in the solution which we are going to pitch and how we are going to build it — Note: Not a complete solution but at least what and where are the risks and how much time it would require to deliver this at a high-level, we also evaluated a lot of 3rd party licenses, paid and open-source that could make our life easy:-I started evaluating ThreeJS and BabylonJS but they required me to have JS knowledge so that I can try out their examples and sample code. There is a huge list of examples provided by both the libraries but there was no point without first learning some JS first. So I started reading and watching crash courses and fired up the VSCode to run these code snippets.But things started to go south when I first saw the syntax of JS —” I am doomed…” first thought came into my mind. We can assign functions to variables! I know we can do the same in C# using delegates but you WHAT THE HELL! we can define functions in other functions! No way, this can’t be true…I took a deep breath and told myself —” You yourself got into this now deal with it”, So I started watching videos on JS on youtube.One channel that helped me a lot was Dev Ed, I learned a lot about ES6 from one of the videos by him. But that was not enough as time was running out fast so we delegated a few of the research we were doing to one other team member in my team — Surabhi Arora she had the experience of the web development and we achieved the momentum to meet the deadline.I then started looking into ThreeJS and youtube again came to save the day and while doing so I discovered few awesome channels who have great tutorials on front-end and other web technologies — DesignCourse, Traversy Media, Coding Tech, Programming With Mosh.Finally, we had a point of view(PoV) on ThreeJS and my colleague took care of BabylonJS.Now we need a PoV on the convertors we had identified and evaluating them was a mammoth task as for that it required me to download the boiler-plates they have in their development portal and run it on my machine, yeah it may not sound a mammoth to most of the people who are reading this but for me, it was as to do so I had to learn ReactJS to create a dummy client app on which I can consume those APIs which were provided by Autodesk Forge.Again back to youtube…I saw a lot of videos and tried a lot of code snippets but every time I discovered new things and got myself confused with what to use with what. After a day I thought let’s again begin from the beginning and I opened up the home page of ReactJS and after few clicks here and there I reached Intro To React page. Here I learned some of the key concepts like States, Function Components, Immutability, etc. I thought okay I know some of the concepts lets start working on the UI which I have to create to consume the APIs. Again all hell broke loose as then came the demon called CSS! to haunt my layout. Nothing was making sense my entire layout was crap. I was already missing my first love i.e. Unity3D but it was too late T-minus-3 days in the meeting and a lot of ground to cover.Then I thought “….crash courses won’t do any help. I need to learn how to create an end to end solution in ReactJS then only my boat will sail” so I referred one of the complete series on youtube. This laid the foundation of the basics of ReactJS finally in my head. But that was not enough how to transfer the data from one React component to another — and then I made a grave mistake which led me to burn the light oil for the whole night aka Redux, I mean…. What the F@#%, why this is so complex?? At this time I had started to question my capabilities. The next morning I decided to ring a few phones to check if I am doing what I should be doing or is this thing broke?Around noon the next day, I was introduced to Hooks and Contexts — Finally, the day was saved. I referred to a video series on youtube to understand the core conceptIn the end, I was happy with my progress and finally, I was able to visualize all the important concepts of ReactJS — How to create function components, pass props, transfer data from one component to another, and Bootstrap. Phew…So now we had everything ready — Surabhi integrated the AdobeForge dev API with the react page which was created and we were able to have a PoV on it.It was one hell of a week, and very exciting I was able to learn JS, ReactJS, and basics of routing through ExpressJS all in one week.A lot was at stake and I didn’t know if I will come out from the other end alive or not as it was something I had never worked on but it was worth it. Every stupid syntax of JS and bugs in my ReactJS code which were making me pull out my hairs they all were worth it.In the end, I want to thanks heroes who saved me from drowning.The video series which I followed were:-",reactjs,https://medium.com/@nipundavid/my-journey-from-xr-to-java-script-ea6c24d7a711?source=tag_archive---------5-----------------------
React-Select Makes Creating Selectable Menus Easy,"HTML’s select isn’t easy. It takes two types of tags to define:React’s select is similar, except it uses JSX (JavaScript + XML), which is an extension of JavaScript that writes HTML directly within JavaScript. In many ways, JSX is similar to HTML, but it needs to be transpiled into standard ECMAScript and then converted to HTML.I would recommend using React Select, which makes selection easy. This open-source code started in August, 2014, and now has hundreds of contributors and tens of thousands of stars. It provides a single component, Select, with commonly used select capabilities, easy styling, async loading, dynamically created options, fixed options, and many other features.As always, we use the Create React App as a starting point. First, install React Select with the command npm i react-select. Then the package becomes part of dependencies in package.json.Change src/App.css to this for minimal styling:Out of the box, we can easily code the following user interface:This is the code for src/App.js:How do we style the Select component? First, we need to know how the Select component is converted into HTML elements.This line of code, <Select options={options} />, generates the following HTML elements:At the top level, it’s a container, which contains a control, which is composed of a valueContainer and a indicatorsContainer.The valueContainer has a placeholder and an input.The indicatorsContainer is the right side’s dropdown arrow, etc.The Select component can take a className for the Select element, and classNamePrefix for inner elements with the given prefix.We set className to “top”, and classNamePrefix to “inner” inside <Select options={options} className=”top” classNamePrefix=”inner” />. These are the converted HTML elements:With specific class names, elements can be styled using CSS rules.However, the Select component provides a better way to overwrite the built-in styles for each part. It uses the styleFn function, which has two parameters:The following code customizes a Select component:Remember to spread the original provided style. Otherwise, you will be surprised by the look and feel of the missing basic styling.This is the provided object:This is the state object:In the above code:Here’s the styled Select component:These parts can be styled:We can make the Select component searchable by setting isSearchable:What if the options list is remote?React Select provides a component, AsyncSelect, to dynamically load options based on the user input:Line 26 calls loadOptions, which is defined in Lines 12-21. This function has two parameters, inputValue and callback. In the above example, a promise is returned. Alternatively, the retrieved options can be passed into callback. Here, a three-second delay shows the loading process clearly:After three seconds, options with a letter a display in the open menu:Typing aa, there are no options:Backspace to have one a in the Select component, we can see the effect of cacheOptions at line 26.With cacheOptions, it immediately shows the previous options. Otherwise, it takes another three seconds to retrieve them.What if the choice is not in Select’s options?React Select provides a component, CreatableSelect, to dynamically create an option based on user input:Lines 27-33 construct the CreatableSelect component. When a new item is created, it calls handleCreate, which is defined in Lines 16-23. This function adds the newly created value to the option list.In addition, CreatableSelect can be used in conjunction with Async. React Select provides a component, AsyncCreatableSelect, to dynamically load options and allow creating an option based on user’s input.Now the AsyncCreatableSelect component filters the existing options and provides a choice to create.React Select with multiple values can have some values fixed, i.e. they’re always selected.This user interface shows Banana and Oranges as fixed options. Apple and Berry can be deleted and added.The following code shows how it’s accomplished:Lines 74 - 82 define a Select component with fixed options.Line 76 makes the component animated. When a value is deleted, the item disappears in animated mode.Line 77 defines the clear button that shows only when there are some delete-able options.Line 78 defines the styles that differentiate fixed and delete-able options.Lines 52 - 70 show the algorithm to make fixed options not delete-able. handleChange is passed in with two parameters:These are available values for action:Do you feel the power of React Select? It seems easy to compose a Select component with a few lines of code. Is it time for you to give it a try?To make things more interesting, you can also check out Select in the Ant Design System.Thanks for reading. I hope this was helpful. You can see my other Medium publications here.",reactjs,https://betterprogramming.pub/react-select-makes-creating-selectable-menus-easy-5d9dffc7e0d9?source=tag_archive---------1-----------------------
"Vue vs. React: Performance, Speed, Use Cases","Sasha AndrieievJun 3, 2020·2 min readVue and React are two frontend JavaScript frameworks popular in the developer world, with React taking the top spot. Both tools offer developers a productive approach to building various web products, but each has its own best use cases and responds to different business needs.What developers love about React: Elegant programming style and patterns, rich package ecosystem, widespread usage.What developers love about Vue.js: Easy learning curve, elegant programming style, and patterns, good documentation.Vue.js and React are both great tools for building interactive user interfaces. To choose which one is better for your next project, you have to take a number of factors into account. We have prepared a full guide, profoundly introducing the performance, speed, and use cases of both Vue and React.https://jelvix.com/blog/js-frameworks-is-vuejs-better-than-react",reactjs,https://medium.com/@jelvix/vue-vs-react-performance-speed-use-cases-def7559175ac?source=tag_archive---------6-----------------------
Working with ReactJS and Redux,"Vidyashree hkJun 1, 2020·5 min readReactJS being one of the trending technologies, is present in most of the developers up-skilling checklist.If you have attended a session or have read some articles explaining what react is, you will have a rough picture of what it is. If not, it is okayLet’s brief it out in simple words:With all the advantages it has, it is considered to be the most used library in 2019 (Refer the image above).With this textual definition, let’s get into the hurdles you face while getting started with a project in react.Yes,It gets tricky once you finish installation and proxy configurations. Why?? Go on readingDecide the project structure:Basic structure comes using create-react-app. You can edit it if needed.It is similar to any other typical javascript project.But when you go into main file, index.js, you get to see code, where syntax is neither a string nor HTML. So what is it?? It is JSX. JSX produces react elements.When you get started with an idea, you will know what are the different entities needed for the project.For example, if you are developing some application related to collecting users’ information, you will haveAll these entities become components in react. Simple. isn’t it?Well, let’s understand it better with an example :In the above example:We haveYou can observe, there is some data going from form component to user details component.Here, we have to know one more concept, before starting to work with components. It is state.State object is where you store property values that belongs to a component.UserDetails is a component , your local state for the component might look like:Any changes in state, will update virtual DOM of the component.When update happens in a component’s state (suppose a parent component), children must update too.Not only child components, but also other components which are not in sub-tree relation.These updates happen only through interactions.Now comes the question. How do these entities(components) interact?A simple method, which you might know, is by parent-child , sibling-sibling or any-any interaction .SimpleYetComplex1. Props — By passing props while calling child component. Props in simple terms, are named arguments passed while calling component.In the above example of formData :Form component sends data to user details component. So, form component is parent and user details is child. Data passed here is called props.UserDetails is a component, it is called as shown below:Child will access it as ,2. Instance methods — Instance methods on the child component can be called by the parent using refs (https://reactjs.org/docs/refs-and-the-dom.html) .Please refer the link for refs usage.3. Callback functions — Typical javascript callback4. Event bubbling — It is not a react concept, it is old concept, just like callback functions, it is a way to send data up from child to parent.Some other least used methods: (Observer pattern, global variables, context and many more)Having these many options, choose the convenient one based on your requirement.Wait, it is not done yet.Lets give food for your thought now! What if you want to communicate with a component, which is not in the sub-tree, not a parent, or ancestor , but in some other level in tree?Then comes the concept of data-flow management (nothing but state management).Before, checking out the process, lets see what is a state , and how is it related to component.If you have heard about react, you must have heard about redux too. What is it then?Redux is a predictable state container for javascript apps. (Which means it is not limited only for react).Looks complex initially, but once you get set with one reducer and store, it is pretty simple to extend with any number of them.Lets see different entities of redux state management:1. Store — All data will be kept here.2. Actions — They let you send data to store. They are typically, simple javascript objects, which has type , and key-value pairs.Something like this -type : “LOGIN”payload : ""user data here..""}3. Dispatch — It is the redux API that runs our actions.4. Reducers — They are pure javascript functions. They accept actions as their argument and decide what to save in store based on the action sent to them.Now, refer to the above image, and relate to the definitions. Able to relate? Hope its a “Yes”Well, now let us see what are the challenges you might face once these concepts are implemented in a project.Now, we are in a stage to define advantages of using react:Below are some of them :Explore more of them while you code.",reactjs,https://medium.com/@vidyashreeraichut/working-with-reactjs-and-redux-d6d198116ae?source=tag_archive---------11-----------------------
GitHub Commit Booster using Node.js and React.js,"Gowtham ParuchuruJun 2, 2020·3 min readThere comes a certain time when one comes to your GitHub page and view your profile, repositories and your commit history. And more green the commit history, more cooler your profile looks. So, boosting your commits make your profile look professional and cool.So, for making our GitHub profile look cool i have made an automation project using Node.js and React.js which increases the number of commits in your profile with just a click of a button. Now, let’s dive into the steps of installation and running this project in your own system.Now, we have to make a GitHub SSH key pair to authenticate your computer terminal with GitHub account, This tutorial can be used to do that.If your console output “server listening at port 8082…” then your node server is successfully running.Now, everything is set and you must be good to go. Just select the number of commits you want to add to your profile, check the estimates time and click on start. The process begins and will be completed in estimated time.This get function basically does the following steps when it gets a request from front-end:1. Create a random number.2. Write this line This is ${rand} line to the data.txt file, which means it is changing the contents of the data.txt file which makes it modified file and untracked by git.3. Then adds untracked files to the staging area using git add . 4. Then commits all files using git commit -m ""commit_msg"" 5. Then pushed all files to the remote repository using git push origin master All these steps are made async using the setTimeout function.When the user type in number of commits he wanted to add and click on start button, this startProcess function is triggered and this function basically uses a for loop to call the end-point “/commit” and that get call initiates the committing process in backend.Note:* If any issues occurs in the installation process or running the project, feel free to rise a issue in my GitHub repository and i will try to solve it ASAP.* My Repository link here.* And also feel free to contribute to this project by forking it and making pull requests.Hope this project helps you or atleast gives you basic understanding of Node.js and React.js .",reactjs,https://medium.com/@pvsg737/github-commit-booster-using-node-js-and-react-js-8ccc656b5dd5?source=tag_archive---------4-----------------------
Deploying React.js + Express.js App on Alibaba Cloud ECS Server,"BrianWJun 3, 2020·4 min readThe article will briefly introduce the steps taken to deploy a react app with an express back end on an Alibaba ECS.Some prerequisites for this guide:You don’t have to take this step but using ssh makes it much easier and faster to operate on the ubuntu server. If you prefer connecting on Alibaba cloud console using the VNC password to log in, it is totally fine. Note that you have to reset the VNC password if you still want to log in through Alibaba console after you set up an ssh connection.Create SSH Key PairsThe SSH key pair gives a permit for your machine to access ECS. To get the SSH key pair:Then the browser will download the key pair for you. Note that you can’t retrieve the key pair again once you lost it. So save it somewhere safe.Alibaba official document for creating SSH key pairs: https://help.aliyun.com/document_detail/51793.html?spm=a2c4g.11186623.2.12.71017394xJvZoB#concept-wy4-th1-ydbBind the SSH Key pair to your ECSFind the entry for the ECS you just created, choose the bind option, and add your ECS to bind to the SSH key pair.After the step, you have to restart your ECS to make the ssh session work.ConnectYou should see a welcome message look like this notifying your connect succeeded.Note that if you find the node version for Ubuntu is outdated even after updating to the latest version, try running following command:It is possible that you have installed the newer version of node but the system still remembers the path to the old version. Use hash -r command to forget the remembered location for the outdated node.You don’t need Nginx when you test your react+express app in the local environment because you directly run on the port of your localhost. When you host your React app on another IP address, you need to listen to the HTTP request from other addresses. We can use Nginx as our HTTP server.To install Nginx:After setup the environment for ECS, we can start to import our application.Next, we will deploy the application on the ECS server, so that when you access the ECS IP address on the designated port, you can see your react app content.Setup Nginx config filedirect to Nginx folder:you can see there is a default.conf already created. It listens to request from port 80(HTTP). The only thing you need to modify in this file is proxy_pass. You need to change it to the same port as the one your React app uses. It is the port number that follows the localhost when you run it locally.If you have a domain name you can change the server_name variable to your own domain name.Just for your reference, my .conf file looks like this:Note that each time you change the .conf file, you need to run the following code to restart the Nginx service.Import React and Expressimport your app through GitHub:Start your react app:Now you should be able to see the static react app page on any web browser on http://your_ECS_IP_address:your_port_numberNext, connect the React app to the Express backend. To open another terminal for the ubuntu system, you can simply start another ssh session while your react app is running.Now you have already deployed the react app with an express backend to your Alibaba Cloud ECS server.Thanks for reading!",reactjs,https://medium.com/@lichengwei96/deploying-react-js-express-js-app-on-alibaba-cloud-ecs-server-c2ae0b848bd?source=tag_archive---------5-----------------------
Learn to build websites with GatsbyJS,"Nabendu BiswasJun 3, 2020·1 min readAs many of my readers know from my previous post that, after investing in learning GatsbyJS for the past 1 year and publishing blog series and creating production websites, I am releasing a book on GatsbyJS.The book is called Gatsby Cookbook and the content of the book is below.It will also launch all the major platform like leanpub, amazon, iBooks, gumroad once it is complete in July 2020.The book is currently 80% complete and available on leanpub to purchase at this url. So, please show your support and purchase the book or tell it to someone who is interested in developing blazing fast production websites in GatsbyJS.leanpub.com",reactjs,https://medium.com/@nabendu82/learn-to-build-websites-with-gatsbyjs-42690762a906?source=tag_archive---------10-----------------------
ReactJS: auto save feature for any input field,"Minh ReigenJun 1, 2020·2 min readI picked up ReactJS recently after a few years away from coding in Javascript at all. Since ReactJS is a powerful and useful framework with a lot of bells and whisles, in the midst of so many new things to learn, I lost sight of it being just a *javascript* framework. Yes, I can still use vanilla JS in my ReactJS app! (duh)Here is one example:My team wanted to implement a simple debounced auto-save function for our text box input. At first we used lodash's debounce to make it work. It seemed to be working and we launched it. But customers came back with a complaint that typing in the textbox appeared to be jittery. So, here is the old code:This works when it doesn’t :) e.g. when a user is typing with an interval of about half a second between letters. The app would save the text, and re-render the textbox with the saved text while discarding what they have typed *after* the saving.To fix the bug, I opted to using vanilla JS’s setTimeout as follow:Here we go. Auto-save works now! The AutoSaveDisplay component you saw above is only a cherry of top bonus to display “saving…” and “saved” when it’s saving your text.",reactjs,https://medium.com/@mreigen/reactjs-auto-save-feature-for-any-input-field-a0d2ca0c0a7?source=tag_archive---------1-----------------------
Building OTP Authentication with ReactJS and AWS Amplify,"Today, primarily due to their convenience and simplicity, passwords are still widely used in our everyday life for many authentications on the Internet.The use of passwords has been shown to be plagued by various security problems, especially in recent times. Password theft is becoming a common occurrence, and for these security reasons, many business companies and organizations are adopting alternative user authentication solutions.This has led to the increasing popularity of One-Time Password (OTP) where the generated password is only valid for one login session. How do we build it securely in a scalable manner? How do we make use of AWS to build and embed OTP within our applications? In this project, let’s explore AWS and implement the OTP authentication with ReactJS and AWS Amplify.AWS Amplify is a development platform for building secure, scalable, mobile and web applications. It hides the actual implementation details required to be done in AWS while providing a good developer experience you expected from your favorite terminal.It covers the complete mobile application development workflow from version control, code testing, to production deployment, and it easily scales with your business from thousands of users to tens of millions. The Amplify libraries and CLI, part of the Amplify Framework, are open source and offer a pluggable interface that enables you to customize and create your own plugins.Let’s start with a new React application via create-react-app and you will need at least node >= 8.10 on your local development machine. I am currently using node v13.14.0 (npm v6.14.4)Once the app is freshly brewed, go to the project directory by entering the following command and open it up with your favorite IDE.Okay now, we are ready to get our hands dirty.AWS Amplify is a development platform for building secure, scalable mobile, and web applications.This is completely optional. In this project, we are going to use the Bootstrap UI library with react-bootstrap to make our React app looks slightly nicer.From the default React JS home page, I will be adding some UI components to work with later. The new react app comes with some basic CSS stylings and we are going to reuse some of that in this guide. These are the components that I am adding:The javascript codes for UI rendering will now look like this.You can now run the React app locally to see the new UI components with the following command.We will now add four key functions to be triggered by the UI components we have setup earlier.Notice that we do not have signUp function and that is because we can handle that part of the logic under the signIn function. Later on, we can do some code cleaning if the functions get messy. Let's now tie the function to each button allocated in the render function. Now, your javascript codes should look the following.Before we deep-dive into AWS Amplify and configure its auth component, we have to first understand how Amazon Cognito works.Amazon Cognito serves as a managed Auth service that provides the user authentication and authorization capabilities to control access to your web and mobile apps. You also have access to Advanced Security features which includes risk-based adaptive authentication and compromised credentials protection.Amazon Cognito provides you the capability to better manage your users with User Groups and Custom Lambda Triggers that can be triggered during the user pool authentication such as user sign-up, confirmation, and post-confirmation. We are going to explore these triggers in the Amplify CLI to tweak the way we are going to authenticate the users.Modern authentication flows incorporate new challenge types, such as Captcha and OTP, to verify the identity of the user on top of the existing passsword verifier. Amazon Cognito provides the ability to customize your authentication flow with AWS Lambda triggers as well.Let’s now take a look below at the authentication flow that we need to configure in this project.By default, the newly-created user has an unconfirmed status in the Cognito User Pool and the user can verify their account via either email or phone number. Since we are using OTP for user authentication as well, we do not need to confirm and verify the phone number (again). To auto-confirm the user during the sign-up process, we need to update the user’s status in the pre sign-up stage during the user pool authentication. In total, in order for us complete this whole authentication flow for this project, we need to add and update the following 4 Cognito lambda triggers.Before we begin to amplify-ing, you have to make sure that you are using at least node version 10 and above. You can enter the following command to verify your node version.If you realize that you are not using the latest node/npm, you can use the Node Version Manager (NVM) to install and select the node version you need. You can enter the following command to install and use node version 13.The Amplify Command Line Interface (CLI) is a unified toolchain to create AWS cloud services for your app. Let’s go ahead and install the Amplify CLI.Now, we can proceed to initialize our new Amplify project within the React app folder. This should kick start a new Amazon CloudFormation stack within the our AWS environment.Let’s go back to our main agenda that is to use AWS Amplify to provision the Auth features in AWS. The Amplify CLI supports configuring many different Authentication and Authorization workflows, including simple and advanced configurations of the login options, triggering Lambda functions during different lifecycle events, and administrative actions which you can optionally expose to your applications. And that is why we do not actually need to go back to the AWS Console to click and setup manually in the browser. Let’s now add auth features by selecting Manual Configuration.We will now give a friendly name for our AWS resources, user pool and identity pool so that (if you have multiple projects) we can easily locate them in future. In this project, I named my resources as amplifyreactotp.As you have stepped through the amplify add auth setup, you will need to update the 4 functions. Firstly, let’s go through the javascript code for create-challenge lambda trigger. You should also take note that in this function, we are going to perform a few key steps to make sure this OTP process works:Since we need to have the ability to send out OTP via SMS, we will make use of the Amazon Simple Notification Service (SNS) to send out SMS. By default, the lambda function does not have the permission to use SNS because using the best practices, the function is only granted with least privileged permission via AWS Identity and Access Management (IAM). Therefore, we will need to update the CloudFormation json file located in the amplify folder that is auto-generated by the amplify-cli.Under the lambdaexecutionpolicy, you can paste in the following (as shown above) to add the permission to send SMS via SNS.Proceeding to the next function, you can update the define challenge function to the following javascript codes. We are going to limit the option of number of ways to only authenticate and use CUSTOM_CHALLENGE, which inform Amazon Cognito to proceed to thecreate-challenge function.Next, we can update the code for pre sign-up function. You can see that in the code, we also auto-verified the email and phone number if it is in the request. This is mainly done for reusability and you can use and customize this code to your need in future. In this project, the email condition does not matter.And lastly, these are the codes for verify function.Lastly, let’s check that we have added the auth and function correctly by entering the following command and as shown in the screenshot below, we should be able to see all the new four functions and the auth resources.And now, let’s push the changes to AWS and let amplify does its magic.We will need two npm libraries from @aws-amplify to configure and add auth to the React app.Once the packages are added, we can go to the App.tsx to begin by importing and adding the following to the top of the file.We will need some constants and variables to store certain messages and values respectively for us to show the status of the authentication process, as well as to process and implement the authentication functions in the React app.First, we will definitely need to display relevant messages first to tell the user at what state they are at. We can put these constants at the top of the codes for future easy reference and iterations.Next, we will update the following four functions to begin using amplify auth functionalities.We will also need to capture the user inputs for number as the user's phone number and otp for the OTP value needed to verify the challenge.Let’s take a look at the signIn function below. We will going to use the exception code to tell if the user exists in Cognito User Pool or not. If it exists, it will trigger the create-challenge lambda trigger and we should be able to receive the OTP.For better UX, you can set a different message when the “Login” button is pressed and show another message when OTP SMS is sent. This way, to the user’s point of view, the React app is “working in process” while we are waiting for Cognito to trigger the lambda functions. To complete the signIn function, you can update the code to the following.In this case, since the new signUp process is asynchronous, we will await for the signUp process to complete before we signIn again.Next, let’s take a look at verifyOtp function. Did you notice the new variable session in the earlier code? We will need to pass the session variable at the signIn function earlier? Once the OTP is verified and the challenge is accepted, you will be able to receive the useras Cognito user (as shown in the following).It looks like we are more or less done. Let’s implement the signOut function. Do you know that you can also trigger a signOut globally if you want the current user to sign out all of its existing sessions in other browsers? We will empty or null the values for the variables as we can make use of them to indicate the UI components if they should show or hide.The last function verifyAuth is optional but it is great for you to auto-trigger this function when the page is loaded. We can make use of useEffect to achieve this.There are some considerations we can do here for the user to know where they should focus their attention. For example, we will only show the OTP input form only when the user entered their phone number and pressed the “Login” button. If you have noticed that we have previously declared a few variables to work with, we will need to put conditions with a few variables to hold certain values as we hide and show certain UI elements.When user is not logged in and have not attempted to login, you can make use of the this condition !user && !session. And when user is not logged in and attempting to login, we can then make use of the following condition !user && session.The Amplify library makes it easy to integrate your code with your backend using declarative interfaces and simple UI components.Okay, let’s take a look at the whole React app in App.tsx. You can find the the source code from my Github repository.With Amplify CLI, we can easily add hosting to any applications. I already pushed my codes and published them at otp.bryanchua.io. As steps are exactly the same, you can refer to my other article and see how I published a Vue JS app with Amplify Console.Well, once you understood the concepts explained earlier with Amazon Cognito and AWS Amplify, the implementations and codes written in Vue should be more or less the same. Nonetheless, if you are interested to see how to build the same app in VueJS, you can refer the source code at this Github repository.I have gone through the capabilities and functionalities with Amazon Cognito, how to use AWS Amplify, and how to provision the AWS resources with custom authentication flow with Amplify CLI. I’ve also gone through the codes to add authentication within the React JS app. You can easily replicate this whole setup in your own apps yourself and refer to this github as a working example.Any thoughts about it? I’m excited to know what you have built with AWS Amplify. Feel free to reach out if you have any questions and I am available on LinkedIn and Twitter.Bit makes it easy to publish reusable React components from any codebase to Bit’s component hub.Need to update a published component? bit import it into your project, change it, and push it back with a bumped version.Share components with your team to maximize code reuse, speed up delivery, and build apps that scale.bit.dev",reactjs,https://blog.bitsrc.io/building-otp-authentication-with-reactjs-and-aws-amplify-c5fd2e517fac?source=tag_archive---------0-----------------------
React vs. Angular 5: Choosing the Right Front-end Technology for your Project,"React and Angular as front-end development framework may look similar but a comparative study helps to understand what is better for particular scenarios and further aid in choosing the right one.Front-end technologies enable web development companies to build applications that provide a rich client experience. Among the most prominent of these front-end technologies are Angular and React, JavaScript frameworks that have taken the web by storm. Angular is from Google and React is from Facebook.Both Angular and React technologies are used for development of SPA pages. Both are open sourced, progressive and leading technologies across the world and have well-supported tools; however, they take two different approaches in HTML rendering.Thus, It is necessary to understand the unique points of each one in order to choose one of them for developing specific web applications.There are numerous pros and cons between React and Angular in terms of use, performance, applicability, and support. In this post, I’ll take you through everything you need to know about these two frameworks. Let’s begin with understanding a bit about Angular and React, shall we?The following features characterize ReactJS as a front-end development tool:The following points highlight the salient points about Angular as a front-end development framework:Here are some statistics to back the facts!(Source: W3techs.com)There are hundreds of JavaScript frameworks available and used across the world. The overall measure of Angular and React is indicated with the usage percentage and the market share of (0.6%, 0.4%) and (0.2%, 0.1%) respectively- according to a survey by W3Techs.With millions of websites in use, the relative spread of usage of these two technologies is indicated with the percentage splits between Angular and React for all websites — for the top 1 million, for the top hundred thousand, top ten thousand and top thousand. This provides a relative density of usage within the different websites.While the use among websites is lead by Angular compared to React, React seems to be more popular among the developers. Technical aspects such as migration paths, flexibility, and ease of development may have contributed to this. You find a number of Angular development companies in India using the latest version of Angular for different products and projects.Also Read: Angular 9 To Release Soon: What You Need To KnowThe two technologies are popular in different countries in varying measures. The relative use in different countries between Angular and React is indicated in terms of the number of programmers pursuing each one. It must be noted that React is in some cases used as library and used in conjunction with other architectural components such as Flux from Facebook.(Source: HackerNoon)In comparison from application size point of view, the usage of Angular is more compared to React especially for the Enterprise web applications. Angular may have been more widely used at Google sites but React is used in many popular social media sites such as Facebook, Twitter, Instagram, Apple and others.Another interesting statistic can be viewed from the classification of loved, dreaded and wanted between Angular and React. The plots about this tell us that whereas React is the most loved and least dreaded JavaScript framework, Angular is preferred over React.Angular and React are aptly described as Powerful JavaScript Framework and Popular JavaScript Library to strike a very concise comparison. The following table provides a little more details and a quick comparison between these two seemingly similar choices:AspectAngularReactRemarksOwnerGoogleFacebookBoth are open source toolsArchitectureJS based MVC (or more recently MVVM) architectureWorks via JS library for view part in MVC; requires Flux to implement the architecture Data Binding2-Way data binding1-way data binding Dependency ManagementAutomaticAdditional tools required LanguageTS + HTMLJS + JSX Available VersionAngular 5React 15Angular 6 and React 16 are recently releasedUI suitabilitySPA with single view at a timeSPA with multiple views at a time SupportDetailed documentation with larger learning effortLesser official documentations but easier for a basic knowledgeAngular 5, as well as React, use a server-side rendering (the initial version AngularJS was adopting client rendering), so they are at par from this comparison. In short, both of these like other leading JS tools, are progressive and provide regular updates and upgrades.The comparisons between Angular and React are many but it is their differences that must be considered to enlist one of them. For an end user, some aspects are important whereas for a developer certain others may be favorable. We can see how the technical aspects can favor each of these aspects:The angular community has seen the Angular platform progress from AngularJS to Angular 5 (Angular 6 is in its initial stages) with several feature and technology-related enhancements.Similarly, React community witnessed progress with the React with several enhancements and additions to the library to the latest React 15 (React 16 is in the initial release stages). Adoption of any of these requires a careful application of the differences between them for any given scenario.",reactjs,https://medium.com/classic-informatics/react-vs-angular-5-choosing-the-right-front-end-technology-for-your-project-34b80790c4fd?source=tag_archive---------10-----------------------
5 Popular JavaScript Frameworks for Front-End Development,"Tech & Marketing blogs by TechAffinityJun 4, 2020·7 min readECMAScript or ES was introduced as a web technology 25 years ago. But, over time, it was renamed as JavaScript and in the past decade, the JavaScript community saw a growing competition between frameworks. Almost all of the frameworks are open-source and as a result, you can use them for free under an open-source license and also contribute to communities. If your question is “What’s the most popular JavaScript framework?” let’s have a deep look into some of the popular open-source JavaScript frameworks.1. AngularJSAngularJS is one of the most popular JavaScript frameworks opted by the front-end community. Though React has a lead when it comes to the adoption of JavaScript frameworks among the front-end development community, large enterprises wish to go ahead with AngularJS because of its strict structure whereas. On the flip side, startups prefer to go ahead with ReactJS. Despite being more complicated than ReactJS, AngularJS has secured its own position and has incredible support from Google. Below are some of the strengths of AngularJS:Excellent Command Line Interface: You can easily get started with AngularJS as it comes with a comprehensive Command Line Interface (CLI). ReactJS also provides a CLI, but AngularJS has extensive documentation and support.One Way Data Binding: Since it implements one-way data binding, your framework is free from unwanted threats, vulnerabilities, and bugs.Offers Support for TypeScript: AngularJS understands the codes written using Microsoft’s TypeScript, which is a lot similar to JavaScript. Also, AngularJS transcompiles TypeScript codes to JavaScript codes to get outputs efficiently without bugs.Having said the strengths of AngularJS, let’s answer the question: What is AngularJS used for? AngularJS is used to develop dynamic web apps that use HTML as the template language. Some of the renowned apps that use AngularJS are YouTube, IBM, Walmart, Netflix, and more. Before using AngularJS, it is recommended to get your hands dirty with JavaScript so that you can understand the framework even better. As a result, your efficiency and productivity improve dramatically.Not to mention, the active AngularJS community is quite huge and you can get support to almost all of your queries quickly. AngularJS’ GitHub repository has 1119 contributors and 18029 commits. You can start to make use of them in your projects or even contribute to the source. These are also open-source under MIT license.Suggested Read: AngularJS vs jQuery2. ReactJSReactJS was developed and maintained by Facebook and entered the front-end development space after Google’s AngularJS. It rose to popularity in a very short span because of its virtual DOM concept. The virtual DOM is an abstract copy with which developers can apply ReactJS features for the elements of their choice without rewriting the project from scratch within the framework. Also, the ReactJS community is ideal in supporting the community. Below are some of the strengths of ReactJS:Relatively Shorter Learning Curve: JavaScript developers can quickly create React components without having to write JavaScript codes from scratch. You will know the benefits of adding ReactJS to your tech stack when you visit ReactJS’ home page.Enhanced Performance: As said earlier, the implementation of virtual DOM & other similar features enhances app rendering performance. You can learn more about ReactJS’ performance by taking a look at its description and how ReactJS can be benchmarked and measured by monitoring your app.Sufficient Supporting Tools: Some of the best tools for building well-structured and debuggable codes are Thunk, Reselct, and Redux.The model used in ReactJS flows from owner to child and it makes it easy to trace the cause and effect in codes. As a result, your codes are easy to comprehend. Having said the strengths of ReactJS, let’s answer the question: Who is using ReactJS? As the creator of the framework, Facebook uses ReactJS for Instagram and its front page. Some of the other renowned brands that use ReactJS in their tech stack are Netflix, New York Times, Khan Academy, and more.Another interesting part about ReactJS is that, a research from Stackoverflow shows that there are specific job roles for ReactJS Developers in the job market. So, you will be getting paid for working on an open-source project. ReactJS’ GitHub 13k+ commits and 1.3k+ contributors and has been an open-source project under MIT license.Also Read: 5 Reasons to Choose ReactJS for Front-End Development3. Vue.jsVue.js is a relatively new framework considering the lot and has managed to rise to popularity in a short span. Evan Yu, a former Google Engineer who worked on AngularJS developed Vue.js on top of JavaScript. It is one of the most preferred JavaScript frameworks among the front-end development community. Below are some of the strengths of Vue.js:Shorter Learning Curve: Most of the front-end developers found the learning curve to be shorter than AngularJS and even ReactJS.Lightweight: Vue.js is a lightweight framework when you compare it with AngularJS. Also, the official documentation of Vue.js explains that it is somewhere around 30kb, whereas; the project generated by AngularJS is over 65kb.Comprehensive Documentation: Vue.js comes with a comprehensive and clear documentation. Here’s a to Vue.js’ official documentation.Vue.js’ GitHub repository has 3.1k+ commits and 293 contributors.4. MeteorJSMeteorJS is an isomorphic JavaScript framework that is capable of running both client and server-side codes similar to Node.js. You can use MeteorJS along with other frameworks such as AngularJS, ReactJS, Vue.js, etc. Popular brands that make use of MeteorJs are Mazda, Idea, Qualcomm, Rocket.Chat, Dispatch, and so on. Below are some of the strengths of MeteorJS:Data on the Wire: In this process, the server sends the data and not the HTML file. As a result, client-side rendering should happen by means of WebSocket. Data on the wire refers to the approach with which MeteorJS establishes a WebSocket connection to the server during page load and then transfers the necessary data over the connection.Develop Everything in JavaScript: Right from webpage to application server, mobile interface, and client-side can be developed using JavaScript.Supports Most of the Major Frameworks: You can combine popular frameworks such as ReactJS, Vue.js, & AngularJS, and use them in conjunction with MeteorJS. As a result, you can leverage the advantages of MeteorJS as well as another popular framework that meets your project needs.As of now, Meteor’s GitHub repository has 22k+ commits and 430 contributors.5. BackboneJSBackboneJS follows the Model-View-Presenter (MVP) architecture and is lightweight. It comes with a RESTful JSON interface and secured a place of its own in the tech stacks of popular brands such as Airbnb, Hulu, SoundCloud, Trello, and more. The BackboneJS GitHub repository has 3.3k+ commits and 288 contributors.So, all the above-mentioned frameworks help explore and take the front-end design landscape to the next level. You can also collaborate with certain frameworks and take advantage of the features of another framework as well. Build comprehensive web apps with the help of frameworks that meet project specifications.We, at TechAffinity, have an adept team of JavaScript developers who are proficient in working on various JavaScript frameworks. Feel free to shoot your queries regarding JavaScript frameworks to media@techaffinity.com or schedule a meeting with our JavaScript team.Get in TouchOriginally published at https://techaffinity.com on June 4, 2020.",reactjs,https://medium.com/@techaffinity/5-popular-javascript-frameworks-for-front-end-development-d5158cb7875f?source=tag_archive---------12-----------------------
How to Share Components Between React Apps and Gatsby Websites,"UPDATE: Bit has released a new version that radically changes the development process of independent components, learn more about it, here.It’s quite common to see single-page apps (SPA) complemented by static documentation/blog/marketing sites. The job of serving rarely-changing-content is perfect for static sites as they’re more SEO friendly and quick to load.When both the app and its static sites are built with React they can — and should — share their components. It's not only a way to build faster by maximizing code reuse, but also a way to maintain a consistent UI across all products (as they’re all related to the same brand).Bit is a tool and component hub that makes it easy to publish and document components from any codebase. It offers both a CLI tool for isolating and publishing components and a place to host, document, and display them.In this tutorial, I’ll be publishing components from my uber-popular “Bad Jokes App” and install them in my “Bad Jokes Blog”. As I’ve mentioned earlier, the two should naturally look the same as they are part of the same “Bad Jokes” brand.github.comI’ve used CSS Modules to make sure “class name collisions” never happen in future consuming apps. I’ve also placed each component’s files under the same directory to keep things simpler both to the publisher of the components (yours truly) and future maintainers of it.My app has four reusable components — check them out in my collection.To get started, install Bit’s CLI tool (globally):Head over to the project's root directory and initialize a Bit workspace:Let’s start tracking all components under the ‘components’ directory:Install and configure a compiler for these components (this way they’re not coupled to this app’s build setup):It’s time to ‘tag’ these components:Open an account on Bit.dev and create a new collection (a scope for the soon to be published components).Then, we’ll just log-in using our terminal:It’s finally time to publish or “export” our app’s components:ExampleThis Button component is typed with prop-types. The children and onClickMethod have each a description written in JSDocs. The disabled prop is set to false as default.Bit will generate this out of the above code:2. Make sure to provide your component with an example (seen in the first of the two screenshots above) — otherwise, it would not render in Bit’s playground.To set up a blog as quickly as possible, I’ve used Gatsby with the gatsby-starter-blog starter:github.comWe’ll then head over to the blog’s root folder and install two components published from the “Bad Jokes App”:Notice how each component is installed individually (as you would expect from independent components that are not coupled to a library).NPM is used here to install our components but it is important to note it is not used as the component registry (that would be Bit’s registry).Let’s use the installed components in the post-page.js file:And that is the end result:And, the Bad Jokes components doing their thing in the index.tsx page:Thanks to React’s great eco-system, it is used nowadays to build almost anything. A greater diversity of products, built by the same framework, means a greater potential for component reuse. That’s especially true when multiple products are built as complementary pieces (e.g, an app and its doc site) or are under the same brand.That’s true not only in theory but in practice thanks to tools like Bit that make it simple to publish and document individual components from any codebase.Learn more about authoring, versioning and collaborating on independent components here:blog.bitsrc.ioblog.bitsrc.io",reactjs,https://levelup.gitconnected.com/how-to-share-components-between-react-apps-and-gatsby-websites-21c0541f8c78?source=tag_archive---------1-----------------------
"Publish and Reuse React Components to Build Gatsby Sites, Faster","Reusing components between Gatsby sites is a great way to deliver faster and provide your users with a consistent look and feel at every touchpoint.In this demo, I’ll use Bit to publish React components from a marketing site built with Gatsby. I’ll then use my published components to quickly compose a brand new marketing page.These are the steps we’ll go through:When we share and reuse our components across projects — we can build sites faster and safer. We don’t have to waste time reinventing the wheel and can make sure we create a consistent experience for our users.Bit is a platform that makes it easy to publish and document components from any project. It provides both a CLI tool for isolating and publishing components and a cloud hub to host and display them. Pretty useful when building different Gatsby websites with the same components.bit.devI’ll build a landing page for my up-and-coming goldfish business. I’m positive this will draw interest from many goldfish enthusiasts around the world.github.comThe styling in this project is done using styled-components. Every component is themed using the theme object (in the theme.js file) which will be published as well.I’ve also classified my components into four groups: common, elements, blocks, and sections.In this demo, I decided to publish my blocks, elements, and common components — and leave out the more concrete ‘sections’.In general, there’s no absolute right or wrong to which components should be published. It all depends on the way you intend to use your collection. For example, it wouldn’t make much sense to publish very large compositions to a collection that serves as a design system but it does make sense for us since we’re using our collection to compose other, similar, websites.As mentioned earlier, there’s no reason why you shouldn’t share anything that you might find useful. It doesn't have to be solely pure and “dumb” UI components. It should be anything that’s useful for you and your team.For example, in this demo, I’ve published the SEO component which queries data from the site metadata and tags the website accordingly. It may not seem like a very reusable component but in the context of Gatsby, it actually is.The query should be structured the same across our different Gatsby projects. That’s also the case if we were to use the same headless CMS for multiple projects. If a specific logic is something we often use, then it should be a reusable component.To get started I’ll first install Bit on my machine:I’ll then head over to my project’s root directory and initialize a Bit workspace:It’s time to start tracking my components. I’ll add all three groups and tag each of them with the corresponding namespace to make them easier to find and manage both in my local project and in their shared collection. I’ve used the * sign to select all components under each directory.I’ll then set up a compiler for the tracked components. That would essentially decouple them from my build setup and make sure they run in other projects.I’ll then ‘tag’ them (the -a or --all flag is used to select all tracked components that have been changed since the last tag).It’s time to head over to Bit.dev and create a new collection for my soon-to-be-published components. It’s free and takes about 1 minute.I can now publish (“export”) my tagged components:And here’s the end result (notice I’ve also added example code so that I can now visually see and try-out the components):bit.devThe response to my marketing page was good — I’ve received many emails that show me, without a doubt, people are looking for a convenient golden pet 🙂So, I've decided to launch a new marketing page with a simple “Buy Now” button and a different message.To make things as simple as possible, I’ll start a new Gatsby project using the gatsby-starter-default starter.I’ll use a few of my recently published components to create a simpler version of my previous header:We can install Bit components using NPM or Yarn (we can also clone them into our repository using bit import so that we could modify them and push them back to their collection with a bumped version — but that’s beyond the scope of this tutorial).As you can see, for the demo, I’ve used NPM.I’ll then use these components in the index.js page.I’d like to show some features on my new marketing page. For that, I'll install my Feature component. As you can see, it was built as a compound component so that it would be easy to add and customize new features listed on it (using JSX).That’s great but that also creates much boilerplate. To make it easy on me and save me some time I’ve copied the example code presented on its page on Bit.dev.I now have a new marketing page, composed in no time 🎉github.comGatsby used to be thought of only as a static site generator but lately, it’s become obvious that it’s much more than that. Gatsby makes our dev life simpler and easier with its great eco-system of plugins, starters, etc. Thanks to it, we build with optimized code that not only makes good apps and websites but also allows us to deliver faster than we would otherwise.It is in the same spirit that we look for solutions that would make our code more reusable across projects. We’d love to be able to write good code and never have to do it again.That’s where Bit comes in. It does most of the “dirty job” of publishing and sharing components, for us. It helps in component isolation, auto-generates documentation, renders our components in its playground and notifies us of any change, both in the component itself and its usage (for example, if a new project has recently installed one of our components).Put together, Gatsby and Bit create a great development experience where I can build components and use them to compose new websites faster.Thanks for reading, I hope you enjoyed it!blog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.io",reactjs,https://blog.bitsrc.io/publish-and-reuse-react-components-to-build-gatsby-sites-faster-7c08c63e6198?source=tag_archive---------0-----------------------
Tools and Practices for Micro Frontends,"Today, Microfrontends are no longer a proof of concept. If we search on the internet, we can find many case studies of adopting Microfrontends. Besides, most of these web apps are already in production, proving its robustness.However, it is essential to follow the best practices and choose the right tools to succeed with Microfrontends. Otherwise, Microfrontends could quickly become an overhead for your teams, risking the overall growth of the project.In this article, I’m going to share some of the best practices around Microfrontends and recommend several tools to establish these practices in your projects.When adopting Microfrontends, one of the critical decisions you have to make is to choose the approach used to structure the code.The two conventional methods used out there are the Distributedrepo and the Monorepo. Other than these two are hybrid approaches that fall somewhere in between.In the Distributedrepo approach, the Microfrontends are divided into multiple repositories with their lifecycle managed separately. In the Monorepo, all the Microfrontends will reside in a single repository.The Distributed Repo approach is the more flexible one but it has the great challenge of sharing UI components, to maintain a consistent UI across MFs (which usually means maintaining UI components as NPM libraries, which creates the overhead of maintaining different build pipelines, different repositories, and version mismatches).One good example of both implementing this Micro Frontends approach and solving the problem of sharing UI components between them, can be seen in Bit.dev.The Bit.dev marketing website is composed using two groups of React components published and managed by the Bit platform. The two groups were built and delivered separately. The “moment of integration” happens on build time, in a codebase that consumes components from both collections. Whenever a new component version is delivered, a new integration happens.Hover over different components on Bit’s landing page to see each component’s “scope” or “collection”. Click on the component name (on top) to inspect the component and/or to install it in your project.As mentioned earlier, the above page is built from components developed in two different codebases, on two different GitHub repositories. Components of each codebase are published to their respective Bit collections.The base-ui collection serves as Bit’s design system. Its components are also published to Bit. It’s worth mentioning that each component is published, versioned, and consumed, individually.Components published to the evangelist collection (used for Bit’s marketing pages) use some of the components available in the base-ui collection, to maintain a uniform look-and-feel.If you follow the Monorepo approach, you won’t get into the same challenges of managing shared components as previously. But as the name suggests, all the teams will work on the same repository, where ownership of code segments needs to be clearly defined. Also, it is crucial to establish governance across code and dependencies between Microfrontends.One of the best tools out there for the Monorepo approach is NX, Extensible Dev Tools for Monorepos. Currently, NX supports both Angular and React out of the box. The main advantage of NX is that it will provide the structure for Monorepo and the underlying toolset for DevOps and governance.You can find more information on how NX works in practice referring to the video on Nx: Extensible Dev Tools for Monorepos.When building Microfronends, dependency management impacts the project in two ways. One common requirement is to manage third-party dependencies. The other element is to handle shared code like utilities and frameworks which fit well into NPM libraries (applicable for the Distributedrepo approach). These libraries, we need to publish to a repository to access from each Microfrontends, similar to third-party dependencies.One of the best practices around managing both internal and external libraries is to use a private artifact repository. We can publish the internal library code to the artifact repository as well as to proxy the third-party libraries (also works as a backup) and deliver to Microfrontends from one place. Besides enabling access to libraries, using an artifact store will help to enforce governance around the versions used across Microfrontends.One of the tools I would recommend for this is to use Azure DevOps Artifacts. If you are not using Azure DevOps for any reason, you can find several other tools that do the same job.One of the standard practices around UI theming for web apps is to use a UI framework like Material, Bootstrap (or a specific implementation like NGX Bootstrap) &, etc. These UI frameworks provide the standard UI elements, styles, and the possibility of customizing the themes. In Microfrontends, it’s essential to keep the constancy of the UI components across to make the overall application theme look consistent.Therefore, if you use the Distributedrepo approach, it is a best practice to export the theming, and standard UI elements to a private NPM library with CSS shared across Microfrontends. You can use Azure DevOps Artifacts for the same purpose.Using the right tools and practices in DevOps will show immediate results. Also, it is an area that has a diversity of tools and methods which you need to choose based on your underlying technologies and platforms. In this article, I will be touching mainly on CI/CD automation, Code Quality, Telemetry, and Monitoring aspects for Microfrontends.Using automation is essential both for Microfrontends and Microservices. With the additional complexity in place with Microfrontends, automation is the key that keeps the accuracy and speed of delivery.In the context of Microfrontends, we need to set up automation for code builds, integrations (depending on the Microfrontend approach), and carry out the unit, and integration tests. One crucial step with Microfrontends is to validate the application works as a whole after deployment. You can use an end to end automation tool like Cypress to write the automation scripts to verify the web app functionality.Since CI/CD is a commonly used practice, I won’t be highlighting any specific tools. But you can use any other means to automate, addressing the concerns mentioned above.When we use Microfrontends, it’s essential to maintain the consistency in code and govern its quality. Unlike Microservices, you will likely use the same technology stack across Microfrontends. Therefore, it’s important to enforce rules to maintain code quality across Microfrontends.One tool I could highly recommend is the SonarCloud, where it will help to establish a quality gate for continuous enforcement of code quality as well as to capture the overall quality matrices of the application.For Microfrontends, I would recommend tracking the behavior of the Frontend itself. It is good practice to monitor the frontends as a practice since the risk of failure of Microfrontends is higher than a single frontend for apparent reasons. One of the tools I could suggest is to use Azure AppInsights using their JavaScript SDK to trace from Frontend to the entire Backend infrastructure. It suits best if your backend also uses .NET.If you are on a different platform, you could probably find similar tools that do the same job. The most crucial factor is to have the tools in place to monitor the entire application without ignoring the Frontend part.Overall, I hope you went through the best practices around Microfrontends discussed in this article. However, it’s important to note that these practices are continuously evolving, and it is vital to adopt these practices based on your application, team structure, and organization.Also, there are various existing tools we can use for Frontend development, which are still applicable to Microfrontends. Therefore, don’t limit the tools mentioned in this article and try to investigate the possibility of using the tools you are familiar with in supporting the best practices discussed in this article.At last, its essential to understand that Microfrontends is not the best choice for everyone. Therefore, before adopting Microfrontends, do your background research whether you are at the right stage and having the right match towards selecting Microfrontend architecture for your application.blog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.io",reactjs,https://blog.bitsrc.io/tools-and-practices-for-microfrontends-dab0283393f2?source=tag_archive---------1-----------------------
How to design UI components?,"Niraj ChauhanJun 4, 2020·3 min readAt Social Aider, when we interview any Frontend developer, we begin the interview by asking the developer to design a UI component on the whiteboard. By starting with this we get a lot of visibility to understand the developer. This looks simple, but trust me, we have received questions such as “Component designing using which framework, Angular or React?”. A lot of times we have found that devs generally lack the basic understanding of the UI components. So I thought of writing it down on what should be the correct approach to design a UI component.We generally follow the Atomic Design Methodology for our UI development. The approach is really simple, once you have the UX ready, try to identify the smallest reusable/common components, those components are your atoms. Then you combine your atoms to form molecules and this goes further down to form a compound.To explain this more in detail, lets try to breakdown a page in components.I will be using following opensource design:Let’s start with the first step i.e. to identify the atoms.Above, we combined HTML & Atoms, now we can easily identify molecules & compounds and extract those as a separate components. One thing to remember here is that after creating atoms, we didn’t created any molecules or compounds instead we created the page, this approach helps us to identify molecules & compounds based on our HTML code.Now lets integrate functionality of registration for which we will require state management. There is a rule to follow here, atoms/molecules/compound should only manage UI state, and the page/template should manage the business state. So in RegisterForm compound component, we will have state to manage the field values using v-model and the parent component will manage the business state by having onRegister functionality.So lets add the changes to the Page component:It’s also important to know that the basic validation of the fields will be done by the RegisterForm and business validation will be taken care by the backend.A developer’s responsibility is to design a good UI component, this is a very important stage in frontend development because UI keeps on updating very frequently and this makes a requirement to make sure that the UI code is structured in a more flexible & robust way. One more key aspect to these components is the testing and that’s the next interview question we ask. May be in our next blog post we will demystify it.",reactjs,https://medium.com/@socialaider/how-to-design-ui-components-a33477cc52c7?source=tag_archive---------7-----------------------
API’s with React,"Hassan RashidJun 3, 2020·3 min readThis week I learned about implementing fetch with React. We used the random user API and implemented a contact list of sorts. It seems as though many things are similar, in regards to what we learned before. The JavaScript and HTML aspects come in handy and are still used, but you have to wrap them in React’s library of code. I can see why React is a popular library and tool, it’s familiar and uses existing convention.In React we have props and state. Props are variables passed to a component by its parent component. State is the condition of a component, in terms of variables and data, and it’s initialized and managed by the component. Props are used for access, such as accessing methods in the parent component. We can also manage state in a parent component, so we don’t need to have state in children components. State can initialized and then set via setState.ReactDOM helps to render React on the web. It also lets you manipulate DOM. React, is a JavaScript library that enables you to build UI’s. You can think of React as the main piece, and ReactDOM enables React to be used on the web. These are split into two different libraries, because React can be used on more than the browser.React.createClass allows us to generate component classes. This is given to us by React. You can also now use extends React.Component. There are some differences. For example, you can’t getInitialState() with React.Component. And you can’t do a constructor() with React.createClass. There are also some syntax differences. With ES6, you can use JavaScript classes to create React component classes.Event delegation, in JavaScript, gives us the ability to avoid adding event listeners to certain nodes. Instead, we can add the event listener to the parent. The event listener on the parent essentially looks for bubbled events, events that ‘come up’ to the parent, and finds a match on child elements. For example, in a ul you may have several li elements. You can check the target of the event when it bubbles up to the ul element. This can save code, and make things cleaner.Technology is changing constantly. Companies churn out feature after feature to enhance products and capabilities. A few things in JavaScript I’m looking forward too are private fields, WeakRef, and Object.fromEntries(). Private fields will enable us to hide fields inside the class block. You can use a # to designate a private field. WeakRef is a weak reference to an object, that is not enough to keep it alive. This will allow the garbage collector to remove that weak reference object. This will help with not having to store everything long-term. Object.fromEntries() will allow you to create an object from an array. This gives us more flexibility and options when dealing with objects and arrays.",reactjs,https://medium.com/@hbrashid/apis-with-react-b892ac44967b?source=tag_archive---------11-----------------------
5 Lessons learned from re-writing end-to-end tests with Cypress,"For anyone with experience writing interactive tests, they will know all too well these types of tests are temperamental at best. So many factors can affect them, that when they fail the first reaction is the test is faulty, not the code it’s testing. This was exactly my experience with our original Cypress test suite, not the fault of the tools (honest) but the way the tests were written.As a bit of perspective, the test suite we had was very small, written over a Christmas break period, and poorly maintained. Not a great start! However, after a post-release bug was found by a user, not us, and realizing we could have easily caught this if we had better test coverage. We committed to invest more in our test suite.On top of this, the tests we had were flakey and failing often because of timing issues. Adding the quick dirty hack cy.wait() all over the place to try and keep them alive was a naive approach to solving the problem and bound to fall over eventually.After some heavy persuasion from my colleague, we talked through a few key aspects these tests needed to meet for us to continue to support them. Part of this was to persuade me you can write reliable end-to-end tests as well. We came up with:This was a major cause of our initial set of tests unreliability so solving this was fundamental to us continuing this investment with Cypress. If you’ve never seen this error before, it occurs when you select an element on the page and try to perform an action or assertion against it but in-between time the DOM has changed and the node no longer exists.A really common problem for apps that use the hydration technique, documented in many issues on the Cypress GitHub account. There are a lot of suggestions on how to overcome these but the one we found most reliable is cy.waitUntil() , a simple retry function to get your element if it's not been found, or anything you want to wait on, not just DOM nodes. I have no affiliation with the team that made this but I’m incredibly thankful I could regain some sanity after using this helper. I highly recommend you take a look (https://github.com/NoriSte/cypress-wait-until).Our key functionality, ordering a product, relies on a bunch of API calls for different aspects of this process. Fetching a list of products, buyers, suppliers, favorited products. The list goes on and will grow as we enhance the app. Because of this, we need to assert on a DOM element when we know it’s available, not just when assume it is. Listening to the API calls in a sequence meant we could step through a complex process with confidence the data is there when we want to assert on the UI. Something this simple took me a wee while to realize and has enhanced the stability of our tests tenfold.Taking inspirations from a Cypress blog post around testing a vuex data store, it was obvious there was a lot of benefit in testing our redux store alongside the UI. Namely:These are just a select few common examples, but I have no doubt there will be many many more.Closely related to point three, but I think it’s a valid point on its own. Being able to dispatch events from our store has been incredibly useful. It’s meant we can fetch data from the API to set up the state in our app if we need to, or clean up data after a test which we do a lot.The last thing we wanted the tests to do is to navigate through the UI for these tasks. It’s slow and unnecessary. It also means we don’t have to do separate API calls specific to the test saving time and code duplication. Sure, we do need to write our API calls outside the app for certain tasks but not every time, which is fantastic.Something we should have considered from the beginning but worth noting a lesson learned from not doing this. We’ve decided to approach our tests based on a top-level of user role. For example an unauthenticated user, user acting as a buyer or user acting as a supplier. They all have different workflows and experiences in the app dictated by this so made sense to make it our top-level structure. From here we split out into more specific flows such as ‘ordering’ or ‘adding products’ etc. We can share common functionality outside of this or within each role. It works well for us as a simple solution to keeping our Cypress house in order.We were able to start building a robust, fast, and reliable test suite with Cypress by following a few key principles and learning a few lessons along the way. I hope you find some of these useful in your test suite and looking forward to seeing more in the comments.Cheers 👋",reactjs,https://codeburst.io/5-lessons-learnt-from-re-writing-end-to-end-tests-with-cypress-470fba675f45?source=tag_archive---------3-----------------------
Medium Like Progressive Image Loading In React,"Ever noticed the way Medium uses to smoothly load its images providing a great user experience. This seemingly pleasing animation, however, is quite easy and straightforward to implement in any React project with some basic understandings of the environment. Here’s what our final component will look like and here’s its demo. So without further delays lets start with understanding how to implement this and then implementing the code to achieve this.The idea here is quite simple explained by the following steps:And that’s all we need to do!If you have followed up to here I assume that you have a basic React App setup. With that being said let’s begin with creating a components folder inside our src folder. I will be using scss for styling purposes, you can however implement the same using plain css, so inside of components folder create a Image folder, its where we are going to place our Image Component, so create two files inside it namely: Image.js and Image.module.scss, now open Image.js and paste the following code snippet there.Now with just that we are done with the JavaScript part of things, with some styling, we will be good to go. So now open Image.module.scss and paste the following code snippet there.Now with these things in place our component is ready for use.To use this in App.js, simply render the component like this:Where originalImageSrc is the source(path) to original image and smallImageSrc is the source to small(resized) image.",reactjs,https://javascript.plainenglish.io/medium-like-progressive-image-loading-in-react-eefa49f5c550?source=tag_archive---------2-----------------------
What Is DangerouslySetInnerHTML?,"In this article, we will be discussing what dangerouslySetInnerHTML is, how it is used, what the differences are between dangerouslySetInnerHTML and innerHTML, and a real project where it proves to be useful.dangerouslySetInnerHTML is an attribute under DOM elements in React.According to the official documentation, dangerouslySetInnerHTML is React’s replacement for using innerHTML in the browser DOM.This means that if in React if you have to set HTML programmatically or from an external source, you would have to use dangerouslySetInnerHTML instead of traditional innerHTML in Javascript.In simple words, using dangerouslySetInnerHTML, you can set HTML directly from React.First, let’s take a look at how innerHTML works.(https://codepen.io/lelouchb/pen/MWaMYde)Here is the Javascript code:Using innerHTML is simple. You just have to use it using dot notation with the DOM element and pass the HTML string using quotes or template literals.Now let’s see how the same is achieved using dangerouslySetInnerHTML.(https://codepen.io/lelouchb/pen/PoPrqEw)Code:While using dangerouslySetInnerHTML, you will have to pass an object with a __html key. (Note that the key consists of two underscores). The object dangerouslySetInnerHTML can be passed via different methods.As a variable:As a function:The immediate effect of using innerHTML versus dangerouslySetInnerHTML is identical: The DOM node will update with the injected HTML. However, behind the scenes, when you use it, it lets React know that the HTML inside of that component is not something it cares about.Because React uses a virtual DOM, when it goes to compare the difference against the actual DOM, it can straight-up bypass checking the children of that node because it knows the HTML is coming from another source. So there are performance gains.More importantly, if you simply use innerHTML, React has no way to know the DOM node has been modified. The next time the render function is called, React will overwrite the content that was manually injected with what it thinks the correct state of that DOM node should be.Improper use of the innerHTML can open you up to a cross-site scripting (XSS) attack. The prop name dangerouslySetInnerHTML is intentionally chosen to be frightening, and the prop value (an object instead of a string) can be used to indicate sanitized data. You need to make sure your HTML is structured properly and sanitized before inserting it into your page. You can use libraries like dompurify to do so.The prop name was intentionally chosen to be frightening so when it should be used? Let’s discuss a real-life React project where a developer will have to use dangerouslySetInnerHTML.Projects made using HackerNews API are very common, simple to make, and fun, but most of the API requests made in them are for the id’s of the items of topstories, beststories, etc. Here is the response JSON data of one of the items of topstories:https://hacker-news.firebaseio.com/v0/item/23331287.json?print=prettyBut the request made to items in askstories returns something interesting. Let’s take a look at it.https://hacker-news.firebaseio.com/v0/item/23325385.json?print=prettyAs you can see, the textfield contains HTML tags <i>live</i>, and you can’t simply show them in the HTML as compared to other fields containing string, numbers, etc. This is just one example. There are response data that contains numbers of HTML tags.Here is the actual use case of dangerouslySetInnerHTML, and responses such as discussed above are rendered in the HTML using dangerouslySetInnerHTML.Thanks for reading!",reactjs,https://betterprogramming.pub/what-is-dangerouslysetinnerhtml-6d6a98cbc187?source=tag_archive---------0-----------------------
Making a Todo app from a beginners perspective.Intro to FastAPI:,"kavii suriJun 4, 2020·6 min readYou obviously might be thinking, “Why another framework? what’s wrong with the one which already exist? How is this any better?”Firstly, It’s fast both in development time and in speed. I mean it has fast in the name, it has to justify its name, right? How fast? It is Very high performance, on par with NodeJS and Go (thanks to Starlette and Pydantic). Such speeds are not imaginable for flask and django.Independent TechEmpower benchmarks show FastAPI applications running under Uvicorn as one of the fastest Python frameworks available, only below Starlette and Uvicorn themselves (used internally by FastAPI). To get more info, go to the benchmark section of the docs.Well, to answer that, we can just look at the excellent FastAPI docs.Still not convinced? I’ll let Sebastián Ramírez (the developer behind this awesome framework) do the work, just take a look.Now that you are convinced, Let’s discuss what this article is?This article is one where I am writing in which we will make a simple Todo List app with FastAPI for backend.Note: I am in no way an expert and this series of articles are not intended as a complete guide to either FastAPI, I am a student and thus we will learn together here and that’s the tone in which this article is gonna be.All the code of this tutorial series can be found here.We are gonna use SQLite for the database so that we aren’t caught up with the setup work.First, let’s install FastAPI, uvicorn (we need this to host the server) and sqlalchemy (this is the ORM we are going to use)Note: I would recommend using a virtual environment for this.Next, let’s just make a simple Hello World App in order to get startedTo run this, we use the following commandLet’s break this down. First, we import FastAPI class from fastapi and create an instance of it called app. Next, we define a route using the app.get(“/”) decorator. Then we define an asynchronous python function called index and return a python dictionary.On running the command in terminal and accessing PORT 8000 on the machine, I get the following result.FastAPI also auto-generates docs, just go to http://localhost:8000/docs to look at them.Now make a folder and make the file structure as shown.First, we need to make a db.py folder in order to handle sessions and import the base classHere, we import the create_engine function and use it to create an instance of the engine which will manage all sessions and connections. We also import the declaritive_base class which will act as the base class for all the models in the database. Now we import the sessionmaker class in order to create connection sessions to the database.Now, let’s define the model and make the table for the todos in the models.pyHere, we create a Todo class which is subclass of the Base class, this defines the tables in the database. Each todo will have an id, content, a boolean called done and a created field which contains the date created.Now comes the fun part, FastAPI uses this package called pydantic in order to lots of stuff like type checking and auto-generating docs etc. This is a really cool package, I recommend having a look at its docs.Initially, I was really confused about this package, so I just conceptualized that theses classes are like gatekeepers in this scenario, they define what data is allowed to go out in the responses and what data is allowed to come in. I can’t say that this is the best and exact explanation but this surely helped me grasp it.Breakdown time!!We import datetime. Next, we import BaseModel from pydantic. This class is the base class for all the pydantic models.Now you might be thinking, why are there 3 classes for a single table and why is there a TodoCreate if it is exactly the same as TodoBase.So, we generally have multiple classes as shown in fastapi in order to define what is allowed as input and what is allowed as output. And I just made the TodoCreate class to future proof the code.And that “orm_mode=True” you see, it’s a pydantic feature that makes it easy to use with ORM’s (Object Relational Mapping). You can read more about it here.Ok ok, I know you wanna move on, but there is one more small detail,you see the “done: bool = False”, the “= False” here basically gives default value.Moving On (finally!), we will create some basic crud functions that will help us doing operations on the data.Here, we make 3 functions, get_todos which will we used to get all the todos from the database. This function also has skip and limit parameters which will be used to implement pagination. The other function create_todo is used to add todo into the database. the db.commit() commit changes (obviously) and db.refresh() refreshes the value in the variable. The update_todo function updates the todo status.Now, you may have noticed that there is a db named parameter in all of the functions I didn’t talk about. This parameter is of type Session, thus is used to pass the database session instance in the function in order to access and manipulate the data.Finally, we write the main.py file to define the routes utilizing all the work we have done till now.Now this last file has a lot to talk about in it. Let’s start from the topthe models.Base.metadata.create_all(bind=engine) creates the tables that we have defined.Normally you would probably initialize your database (create tables, etc) with Alembic. And you would also use Alembic for “migrations” (that’s its main job).A “migration” is the set of steps needed whenever you change the structure of your SQLAlchemy models, add a new attribute, etc. to replicate those changes in the database, add a new column, a new table, etc.You can find an example of Alembic in a FastAPI project in the templates from Project Generation — Template. Specifically in the alembic directory in the source code.Next, we create a Dependency function. FastAPI has a very powerful but intuitive Dependency Injection system.In our case, it is a function that gets called for every request in order to get database connections. These can be used for may other things, have look at the docs to learn more.Next, we create 3 routes in order to get, create and update todos.And that’s it! Our backend is ready! Go play with them in the auto-generated API docs.I hope this article helped you. If you wanna have a discussion about any doubts you have, feel free to tweet me @kavii_suri and we’ll figure it out together!There are some concepts that may need further reading, I wud recommend to read the docs for them. In particular, Dependency Injection and Pydantic models are some concepts that were hard to grasp for me.Happy Coding!",reactjs,https://medium.com/@surikavii/making-a-todo-app-from-a-beginners-perspective-part-1-intro-to-fastapi-5006abbcb7a2?source=tag_archive---------2-----------------------
Improving SEO with React Helmet,"React Helmet is a tremendously popular library that helps us improve our SEO by “tailoring” our pages’ metadata to each page content, in a dynamic and efficient way. In this article, we’ll take a look at how to use it for React single-page apps as well as Gatsby static websites.As with any other package, we’ll start by installing it:And, we’ll go ahead and use it in our app:In the example above, we added the Helmet component into our App component with the title element — to change the title — and themeta element — to add a meta tag to our app.Additional tags can be the base, link, script, noscript, and style tags.We can also add attributes for body, html and title tags.Note that if more than a single Helemet component exists in the same app, the last use of it will override the others.Publish independent React components from any codebase to a single component hub. Use Bit to document, organize, and even keep track of component updates and usage.Share components with your team to maximize code reuse, speed up delivery and build apps that scale.bit.devGatsby is a static website generated based on React. It has React Helmet included in the default scaffold.If not already installed:And, then in gatsby-config.js, we can add the 'gatsby-plugin-react-helmet' entry to the plugins array. The project should have an SEO component in the componentsfolder.Here’s how that looks:The code above makes a query to the GraphQL API to get the metadata of the website, which includes the title, description, and author. It then passes the title into the title prop and the meta array. Each entry has the name with the meta attribute name and the content for the value.The metadeta can be changed in the gatsby-config.js file.For instance:The siteMetadata object is what gets returned by the GraphQL in seo.js.The SEO component can be used anywhere.For instance, in pages/index.js, we have:We use the SEO component to change the title.If the titles don’t appear when opening in the background when using the gatsby-plugin-offline plugin, then we can set the defer prop to false to change the description synchronously:blog.bitsrc.ioblog.bitsrc.ioblog.bitsrc.io",reactjs,https://blog.bitsrc.io/improving-seo-of-react-apps-with-react-helmet-7b79fb8774f4?source=tag_archive---------0-----------------------
How to hide your API keys in React.,"Puskar AdhikariJun 1, 2020·2 min readAn API key or application programming interface key is a code that gets passed in by computer applications. The program or application then calls the API to identify its user, developer or calling program to a website.Application programming keys are normally used to assist in tracking and controlling how the interface is being utilized. Often, it does this to prevent abuse or malicious use of the API in question.An API key is both incredibly powerful and extremely vulnerable. API key exposure can result in significant damage, both to a company and to the data it holds. As such, hiding and securing keys (as well as a mitigating potential loss) is critical for any security plan in the modern API space2. Inside the .env file, prepend REACT_APP_ to your API key name of choice and assign it.REACT_APP_ is, in fact, a tool that create-react-app uses to identify these variables.3. Add the .env file to your .gitignore file.You don’t want this file to be committed to gitHub!After you’ve saved .gitignore, run $git status to make sure that .env is not on the list of changes to be committed.4. Access the API key via the process.env object.To check that you can access your API key, go to your App.js file and add console.log at the top below the require statements. After saving the file and reloading the page, if the console log does not show your API key, try restarting the react server. And of course, make sure to remove the console.log line before committing your code.The accidental synchronization of secure code is not uncommon. The solutions to hide is very easy to implement and can be done broadly across almost any current implementation. Ensuring the security of your API key should be a major focus for any organization or personal project, and is a first major step for any security plan. What do you think about this implementation?",reactjs,https://medium.com/@puskar.ad/how-to-hide-your-api-keys-in-react-f2b564406c26?source=tag_archive---------12-----------------------
State and Props in ReactJS,"Keshav GoyalJun 1, 2020·4 min readWhile working with ReactJS, you come across the terms State and Props very frequently. Understanding these terms is a big step towards learning ReactJS.In this article, I will discuss State and Props and talk about some of the common questions related to State and Props.Props are used to pass data from the parent component to the child component. In ReactJS, the data flow is unidirectional from parent to child.Now let’s see an example of passing data with props.consider two components 1.Parent 2. ChildThis is the parent component that passes the data “props-explanation” to its child component through prop named data.The constructor for a React component is called before it is mounted. While implementing the constructor for this subclass, you should call super(props) before any other statement; otherwise, this.props will be undefined. this.props is an object which stores all the Props of the component. So this.props.data gives the value of data passed from the parent component to the child component.A component must never update its own props. A component that uses only Props is pure that means given the same input, it will provide the same output.Similar to Props, State also holds information about the component. But unlike props, components cannot pass data with State. The State is entirely private to the component and entirelycontrolled by the component.Let’s walk through an example of how to use State in a component.In the class constructor, this.state is initialized, and then this.state can be used similar to this.props .State can be updated by using the method this.setState() . On triggering this.setState() React re-renders the component and the child components based on the changes to the State. this.state should not be modified directly as this will not re-render the component.Example of using this.setState()This will update the value of cost to 200 and re-render the component.State updates are merged when you call this.setState() React merges the object you provided into the current state.As updates to state may be asynchronous you should not rely on this.state or this.props value for updating the state.To fix this we use the other form of this.setState which accepts a function rather than object. The first parameter to the function is the state of the component at the the time of triggering this.setState and the second parameter is props of the component.The output of this function is merged with the state of the component.The second parameter to this.setState is a callback function that is executed once this.setState is completed, and the component is re-rendered.As State is private to a component, one issue which you may face is how to update State from the child component.This can be solved by passing this.setState as props to the child component.Now let’s see this in use with two components 1. Parent 2. ChildComponent Parent contains an array of books in the state, and here I used the map method for the array elements to go through every element of the array and pass them to Child component as props. Here the prop with name updatecost passes the function, which updates the state when the corresponding event occurs in the Child Component.The button in the child component triggers the updatecost function, which updates the state of parent function and re-renders the parent and child component.So, passing setState through props to the child component, lets us update the state value of Parent Component from Child Component.You can find the code for above example here.I hope this article helps you understand props and state in ReactJS.If you have any doubts feel free to ask in the responses. I would be happy to help.There are also other important things in React about which I will keep writing in my following articles.You can also read about Introduction to ReactJS .",reactjs,https://medium.com/@keshavgoyal2014/state-and-props-in-reactjs-7bb7a57c0dfd?source=tag_archive---------10-----------------------
Using Multiple Databases in Ruby on Rails,"Paul NdemoJun 2, 2020·3 min readOne of the new features that were recently shipped in Rails 6 is the support of multiple database connections. Initially, to use multiple databases in Rails, one had to come up with a custom solution or use gems such as Multiverse and Connection Ninja. In this tutorial, we will be demonstrating configurations and connection switching for multiple database connections in Rails 6.0.Most of the time, a single database should be sufficient, especially if the data to be supported by your Rails application isn’t much and the users aren’t many. However, as your application’s data increases and the user base grows, it may become necessary to invest in multiple databases. For example, consider a situation where your database can handle 1000 concurrent requests and 800 of them are read requests. Having a read-only replica to serve the read requests and a leaving the primary database to serve the write requests will relieve it 80% of the load. Multiple databases are beneficial in this scenario in the sense that, compared to upgrading your database server, it will cost you much less and also enable you to have more fault-tolerant data.Let’s assume we have a school management system built in Ruby on Rails and using MySQL database management system; which has grown to support multiple schools with several users. We want to introduce new features for managing exams and we would want to have a separate database for that. At the same time, we want to have replicas for the initial database called skool and the new database called exams. We plan to have the replicas handle read-only data.In order to use the read-only replicas, we will need to activate the middleware responsible for automatic switching. With automatic switching, the application is able to switch from the primary database to the replica database and vice-versa, based on the HTTP request made and considering the occurrence of a recent write. If the HTTP request made application is a POST, PUT, DELETE or PATCH, the application will automatically write to the primary database. For a certain time, after the recent write, the application will read from the primary database. This is because we need to give our replica database time to execute transactions for the recent writes to the primary database; for both databases to be at the same state. For a GET or HEAD request, the application will read from the replica database. We will need to have the configurations shown below in our config/environments/production.rb file to get the connection switching to work as discussed.Sometimes, we may want to force our application to connect to the primary database or the replica database while executing a certain part of our code, we can achieve this by wrapping the given block of code as shown below.Something to note is, Rails 6.0 currently doesn’t support joins across multiple databases. Hence, queries across multiple databases have to be done independently. You can find more information on using multiple databases, by visiting Rails’ official documentation.Using multiple databases can help to improve your application’s efficiency as it scales. This feature was shipped in Rails 6.0 and it supports both SQL and NoSQL databases.",ruby-on-rails,https://medium.com/@paulndemo/using-multiple-databases-in-ruby-on-rails-316f9040fc6e?source=tag_archive---------1-----------------------
Control Flow Statements,"Broderick Fowler JrJun 4, 2020·2 min readOver the past couple of days, I decided to take some extra time out and re-learn the basics of Ruby.In this article, we’re going to explore the following topics:The if in the statement runs if (n) is either true or falseThe else runs if the statement is falseIn this example, the if statement will print word + a space if the condition is true. If the condition is false that would activate the else statement and the string “REDACTED” would be placed instead of a word.Unless statements work similarly to if/else statements. If the expression is false then the code following the unless statement is executed.In the above example, the expression evaluates to false because of that the else statement will print the string “Time to eat!. If the expression would have evaluated to true then the unless statement would print the string “I’m writing Ruby programs!.The following comparison or relational operators are used in Ruby to compare values.> - greater than; < - less than; >= - greater than or equal to; <= - less than or equal to; == - equal to&& is a logical operator in Ruby which evaluates to true only if both expressions on either side of && evaluates to true.if score1 > score2 && score1 > score3 print “Score 1 is the greatest in value.” else print “Score 1 is not the greatest in value.” endThe || (or) operator is a logical operator which returns true if either of the expressions on left-hand side or right-hand side is true.grade1 = 50 grade2 = 30 grade3 = 80 if grade1 > grade2 || grade1 > grade3 puts “Grade 1 is not the lowest score!” endThe ! (not) operator in Ruby flips a boolean value. If a value is true then applying ! to the value changes it to false and vice versaThank you for taking the time to read this post :-)Feel free to 👏 and share this Medium post if it has been useful for you.",ruby-on-rails,https://medium.com/@broderickfowler08/control-flow-statements-3d18aa09480?source=tag_archive---------7-----------------------
"Ruby on Rails Development: Obstacles, Opportunities, and RubyGems","BoTreeTechnologiesJun 10, 2020·6 min readRuby on Rails is a popular web development framework which has powered some of the world’s best web applications — Airbnb, Shopify, Basecamp, Github, and many more. Ruby on Rails web development framework has highly matured since its inception 15 years ago. BoTree has also built powerful web apps like TFC, Appruv, InspectDate, and others using Ruby on Rails. But does it still make sense to hire Ruby on Rails developers in 2020 for web & mobile applications?Ruby on Rails is facing stiff competition from its competitor frameworks like Django, Laravel, Angular, Express.js, Flask, Bootstrap and many more. The recent advancements in software technology have put these frameworks under the spotlight. It has led developers to question the feasibility of Ruby on Rails application development in comparison to others in 2020.For example, Django’s numerical and computational capabilities keep it ahead of RoR development when it comes to data science applications. But is it faster than Ruby on Rails? Many developers still doubt that.The experienced developers understand that Ruby on Rails has reached its maturity as a framework and is one of the best today for building quality applications. In the post-COVID-19 world, even though there are obstacles, more opportunities lie ahead for Ruby on Rails development companies. Let’s explore such obstacles & opportunities in this article. We will also focus on some of the top RubyGems that make Ruby on Rails what it is today.Ruby on Rails is an older web development framework. With more advanced features, there are frameworks like Django, Flask, and others that can enable companies to build highly-powerful applications.Three of the primary challenges that Ruby on Rails application development faces in 2020 are:-While the framework is facing challenges from others, it is still one of the best to build web and mobile applications. But you have already read about the benefits of Ruby on Rails application development. Let’s explore the opportunities that the framework provide to businesses.Now when we talk about integration in existing applications, RubyGems forms the core behind highly powerful web and mobile applications. RubyGems enables Ruby on Rails engineers toa dd functionalities and feature-rich apps.For most developers, RubyGems is the name of the game when it comes to building high-performing applications. We will list out the best RubyGems for us in 2020 to help you build better apps.Ruby on Rails web development is one of the most preferred application development frameworks in 2020. It provides a lot of opportunity to businesses for building efficient applications. Ruby on Rails will cut down costs and provide high-performing applications — necessary to grow in this era.BoTree Technologies utilizes the Ruby on Rails framework to build high-quality web applications. We have already built a lot of them Have a look here.At BoTree Technologies, we build enterprise applications with our RoR team of 30+ engineers.Originally published at https://www.botreetechnologies.com on June 10, 2020.",ruby-on-rails,https://medium.com/@botreetechnologies/ruby-on-rails-development-obstacles-opportunities-and-rubygems-a4d47015f0ef?source=tag_archive---------2-----------------------
How I Built A Search Bar In React,"Michael MottolaJun 2, 2020·2 min readFor my final project at Flatiron School, I built an app using a Ruby on Rails backend as an API for my frontend which was built in React and Redux. I built an app that rendered MLB Showdown Cards and allowed a user to search thru hundreds of baseball cards and create a deck. MLB Showdown was a popular baseball trading card game from the early 2000s.I found CSV files of all the cards which I used to seed my database for both pitcher and batter cards. I fetched these cards from the backend to render them as cards on the frontend. I knew at that point that I wanted to include a searchbar so that users could easily search through the rendered cards.I wasn’t sure what the best way to create the searchbar was. Should I make new fetch requests each time a user searches a player? I ended up reaching out to friends from my cohort at Flatiron School (shout out to Kelley and Jamie!) and they helped me figure out the best way to implement the search bar for my project.Instead of having a new fetch request each time someone searched for a card, we decided that the best way would be to keep those cards in state and then just search thru the already rendered cards.Here’s a look at some of the code used to create the searchbar and render through the already fetched cards.If you’re looking to create a searchbar in React using a Ruby on Rails backend, I suggest you look into persisting those objects in state and just filtering through them.",ruby-on-rails,https://medium.com/@mottola64/how-i-built-a-search-bar-in-react-3cacb6853cb1?source=tag_archive---------5-----------------------
Containerizing React on Rails 6 (CI/CD Git lab Runner),"Vishwas NaharJun 2, 2020·4 min readRuby version 2.6.3Rails version 6.0.2If you don’t have latest version install rvm or rbenv to get the latest version of ruby. This can help https://rvm.io/rvm/installNow, I assume your rails app is working on local, Mine is here: https://gitlab.com/vishwas1993/react-on-railsNow Install docker https://docs.docker.com/get-docker/Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and deploy it as one packageDocker Compose fileCompose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.database.yml fileHow associate db to host that connect our db image to database.ymlentrypoint.shAn ENTRYPOINT allows you to configure a container that will run as an executable.docker-compose run web bundle exec rake db:createdocker-compose run web bundle exec rake db:migratedocker-compose run web bundle exec rake db:seeddocker-compose up — buildIf already build then rundocker-compose upNow hit http://localhost:3000/ to check site is working, if not debug and comment down below 🎉-d this allows us to daemon modeNow signup Git-lab and create repository, now go to setting to check CI/CD setting. Now install runner — https://docs.gitlab.com/runner/install/Now Register the Gitlab runner to setup CI/CDdocs.gitlab.comNow we have to create .gitlab-ci.yml in which we can put our stages to build, test and deploy using runner. you can commit the this fileNow push the commit and see the CI /CD pipeline build 🎉 🎉",ruby-on-rails,https://medium.com/@vshwsnahar3/containerizing-react-on-rails-6-ci-cd-git-lab-runner-f29bf70142d8?source=tag_archive---------4-----------------------
Metaprogramming is Dangerous and Therefore Cool,"staceykeatingJun 1, 2020·4 min readThe Ruby Robot is officially here. That’s right, coding is we know it is over. Elon Musk’s affectionately named ‘Ruby’ the programming extraordinaire robot is now fully functional and ready to be shipped to start-up companies worldwide!Alright, you can relax junior dev’s, we’re not quite there yet… But if it’s not a coding robot, what is it?‘Metaprogramming is a programming technique in which computer programs have the ability to treat other programs as their data. It means that a program can be designed to read, generate, analyze, or transform other programs, and even modify itself while running.’ — WikipediaIn other words, it’s code that writes code. Through metaprogramming in Ruby, you will be able to manipulate classes, modules, and instance variables all at runtime. But what can we actually accomplish with this? And… is it safe?This first example uses ActiveRecord to make a simple query:Imagining we have a database of different TV shows and their genres we can use this finder method to easily query the database without having to write any of the complicated logic. We get to simply ask for what we want because we know it should be there. All of the complicated logic to make this work has been stored away and ActiveRecord will dynamically search just what is needed.The syntax that we’re left with is intuitive and direct with it’s intent thanks to metaprogramming!The next example of metaprogramming deals with Open Classes.Here we see what is known as the ‘monkey patch’. Through monkey patching, we can dynamically modify a class by opening it up and changing its methods without actually modifying the source code. This sounds like it could get dangerous, right?We define a method called ‘breakingForthWall’ and when called on our string it takes the form of the method called. In this given example, it may be hard to imagine how any conflict could arise from using a method called ‘breakingForthWall’; however, the problem will become much more evident when considering the real-world methods defined in Ruby.When we look at Ruby’s built-in String methods we can start to understand where it might go wrong.So what would happen if you went and redefined ‘delete’? Or ‘index’? Or good ole fashioned ‘count’? If we’re modifying the existing built-in methods, errors and negative side-effects are sure to follow.This can easily become a complicated, time-consuming problem for other developers, as well as a major cause of confusion if they don’t know how or why this is happening. When metaprogramming, we must consider any developers that may be coming into the project later on.This next example of metaprogramming involves defining methods at application runtime.Here we have a class of Show that returns details about various shows by genre. For now, we have only three methods and two arguments, but already the repetition in code is becoming quite evident and frankly, it stinks (code smell, anyone?).Below we see the same class with the same genres, but in this case, we are not defining each one individually. We can see an array of genres, then the looping through of each one by calling ‘define_method’. This approach does exactly the same thing, but it is now much easier to modify. And since all developers are completely obsessed with keeping their code DRY, this form of metaprogramming is extremely useful.Metaprogramming comes in many forms and is definitely worth using and exploring on your own. It can be helpful for making your program module and better at handling any changes that may come along, while still keeping your code clean and concise… regardless of the dangerous (cool) stuff that may come along with it.Of course, it is important to always be aware of the power you hold when monkey-patching or otherwise, and keep in mind best practice is never to override any of the Ruby core methods.Have fun metaprogramming in Ruby!Resources:https://en.wikipedia.org/wiki/Metaprogramminghttps://apidock.com/ruby/v2_5_5/String/https://www.youtube.com/watch?v=lZfv4H-9atohttps://culttt.com/2015/06/17/what-is-monkey-patching-in-ruby/https://www.madetech.com/blog/metaprogramming-in-rubyhttps://thecodeboss.dev/2015/08/metaprogramming-in-ruby-part-1/",ruby-on-rails,https://medium.com/@staceyamberkk/metaprogramming-is-dangerous-and-therefore-cool-9bed6d03eb19?source=tag_archive---------3-----------------------
How to Create the Simplest Friendship Model on Rails,"Kubilay ÇağlayanJun 3, 2020·5 min readEngineering is about dividing problems into smaller partials. And then solving them with the best approach. As an engineer, I enjoy problem-solving.In this article, you will learn how to create friendships on Rails. In the most basic form, this is far away from perfection. But totally working. You can find the GitHub repository link at the bottom of the page to dive deeper into the code. In the repository, you will also find very simple session management.After reading this article you will have an understanding of friendships on Rails.When you want to create a rails app with user friendship interaction, you have to decide what your final product will be. There are many different ways to form a friendship model. In this project we are choosing this model:1-) Users are allowed to send a friendship invitation to other users.2-) Users can accept or reject the invitation that is coming from another user. 2a. Accepting ->Will update a record column from false to true 2b. Rejecting -> Will delete the record.3-) Unfriending will delete the recordWe will have a User model, and an Invitation model.Let’s focus only on friendship details. The invitation model will have only three columns(by the way, keep in your mind that the Rails is always giving you 3 more columns by default: id as primary key, and 2 timestamps).Those columns are: user_id, friend_id, and confirmed. user_id and friend_id are both referencing back to the users table(User model).When I am sending an invitation to my friend: user_id will be my id and friend_id will be my… well, you got that! And confirmed will be false by default.This means an invitation is starting to wait to be accepted or rejected at the moment it got created. When the invitation is rejected, the row is gonna be deleted. When it is accepted the row is gonna be updated, confirmed: from false to true.Any record with true value in the confirmed column means that those two ids are friends now.Any record with a false value in the confirmed column means that user_id is waiting for friend_id to accept/reject his/her invitation.Let’s run these three different commands on command line:After this command we need to addto our migration file. Right after this line:Let’s go to our model files, first: app>models>user.rbHold on there, I am explaining one by one…Our user has many invitations. This means A user object will respond to the “invitations” call.Our user has many pending invitations. Yes, try to stick with plain English, this will help a lot! This means the same thing, A user object should respond to the “pending_invitations” call. And we have options here. First option is a scope, this tells ruby to look for records only has “false” value on pending column. The other option is the “class_name”. We are adding this because rails can’t find the related class from the “pending_invitations”. So I am telling rails “Hey, I want this call on my object and go look for this in that class”. Rails going to the Invitation class and fetch all the records with specific user_id and confirmed values.So what am I going to do with this call? On every users’ page, I want to list the people who sent an invitation to that user. Very useful!We have instance methods. And of course, we have a “friends” method. This will return all the friends of a user in an array. I am not proud of what I do inside this method, but it is working 😃.In the “friend_with?” method, we are returning a boolean value.This is useful when you want to do things with the logic of friendship. For example, show the posts if they are friends, don’t show otherwise.With the “send_invitation” method we are sending invitations.With these very simple 5 interaction tool, we are done in the User model. Let’s go to the second one, the Invitation model.app>models>invitation.rbNothing fancy, in 4 steps we will go through this.Every invitation belongs to a user. Plain English. Very tasty. Haha! Long live DHH! 😅 Long live Matz! 😅After this point, as you realize, we have three class methods. Because I want them to be usable regardless of Invitation objects.Starting with the “reacted?” method we are questioning if two of the given ids have any records in our table.In some situations, you need to know this. For example, you want to show a button for an invitation and you have to know if the two ids have any record in the invitations table. If they have a record then you don’t want to show the button because now you know something! Those two users are in either of the following phases:1-) Some of them sent an invitation to the other one and waiting for a decision.2-) They are already friends.In both cases, we can’t create another record. So we are hiding our button with the information that is coming from this awesome method “.reacted?”.Another class method is the confirmed_record?(..). We are using this method in the User model. This method is basically checking a confirmed record for two given ids.The last method is find_invitation(). We are using this method to find the invitation record of two given ids. Useful when deleting a confirmed relationship.Now we can run the migrations with:And enter the rails console with:You can create new users, send invitations, accept them with update methods, reject them with delete methods.Furthermore, you can see the friends of a specific user, see the pending invitations, see the invitations that user sent, and more.Well, I hope this helps you. Keep in mind this is one of the many approaches. There are more simple ones and more elaborate ones.See The Project RepositoryVisit My School For Software EngineeringVisit My Portfolio Page",ruby-on-rails,https://medium.com/@kubilaycaglayan/how-to-create-the-simplest-friendship-model-on-rails-de0ab51448ec?source=tag_archive---------2-----------------------
"0/1 — Auth with Rails, GraphQL and React — Setup","Jokūbas PučinskasJun 3, 2020·7 min readIt is a Part 0 of step-by-step guide how to secure your Rails, GraphQL API and ReactJS stack.If you have already got your Rails app running you can skip this part. But I strongly recommend to scroll down to the bottom and make sure you have all these steps completed on your app and you are good to continue this journey.Note: All code from this part can be found here.Link to Part 1/1So we are going to build Dashboard app with Login page. Dashboard gonna show our users To-Do’s list and only logged in users will be able to change those lists. Yes… It’s going be very fancy To-Do app.StackTo get GraphQL API running quickly in Rails app I’ll be using graphql_rails gem and Apollo GraphQL client to make things easier on client side.More about graphql_rails docs here (short tutorial from Povilas Jurčys here)Create a PagesController to serve view for React appMake space for our React app to mountModify routes fileRun Rails server.Allow react-router-dom package to handle all non-Rails routes.Initial layout of our fancy app.Do not forget to include assets to our pageAnd create Dashboard and Login components to represent our pagesAt this moment your app should look something like thisCreate Account model. You can name it User if you’d prefer.We will be using has_secure_password from ActiveModel::SecurePassword to make password managment easier. It requires model to have password_digest column and bcrypt gem.Add graphql_rails gem to GemfileRun rails g graphql_rails:install to completely install gem. * this installation provides us with sample GraphqlRails controller which is locatedapp/controllers/graphql/example_users_controller.rb . GraphQL schema must have at least one query and mutation type so you can leave this file untouched for now. But I allways delete this controller and add my own queries as followsGem graphql_rails provides us Rails style GraphQL API which is nice. GraphqlRails::Controller, GraphqlRails::Model, GraphqlRails::Router. Read more about it here.I know… But we need to have something to display on our dashboard and let it be To-Do’s lists.Pro-tip: All sample data can be found in seeds.rb file and copy-paste should do the trick here.As I mentioned before graphql_rails gives us Rails like GraphQL structure within our app.ModelAdd GraphQL types declarations to our models related with To-Do list.ControllerRoutesAnd Tada 🎉Add ApolloApollo v3 beta has been released recently. Let’s try it out!Simple apollo client setup is here.Create table to display our todo lists for public view.Let’s finish it up and add that login formYou will find how to implement login logic with GraphQL in the next chapter. Now just add that form and leave it here.Code https://github.com/pucinsk/reactive-dashboard-tut/tree/v0.3What’s next?We still need to be able to login, logout, modify those to-do things and to know who is authorized to do what.",ruby-on-rails,https://medium.com/@Jpucinsk/0-1-auth-with-rails-graphql-and-react-setup-dbd38b4c7955?source=tag_archive---------4-----------------------
The ‘SameSite’ Issue With Rails 4.2.0 in Chrome,"SameSite is the new recommended way to keep your website secure. It’d make CSRF even more secure. The SameSite attribute allows servers to assert that a cookie shouldn’t be sent along with cross-site requests. This change provides some protection against cross-site request-forgery attacks.In simple terms, cookies that don’t include the SameSite=None and Secure labels won’t be accessible by third parties. This behaviour change was rolled out by Google on February 4, 2020.Let’s understand the scenario in a Rails application. If your Rails application provides the API, which uses cookies for the authentication, then this new behaviour of cookies will break this scenario. Because now the default value for the SameSite is Lax, which won’t send the cookies back to third-party websites.To resolve this issue in Rails, we need to explicitly set the cookies with SameSite=None. To set SameSite=None, we need to create a file, rack.rb, inside config/intializers in the Rails application.If you want to attach it to the application cookie key, then you need to update the rack.rb file with the following code:You need to replace key name with your application key. Where can you find this application cookie key name? You’ll get this cookie key name from the session_store.rb file.That’s it! Your application will work perfectly in Chrome. Cheers!",ruby-on-rails,https://betterprogramming.pub/the-samesite-issue-with-rails-4-2-0-in-chrome-be8deced023e?source=tag_archive---------0-----------------------
Extending Thredded (or any Rails Gem) with ActiveRecord callbacks,"We’ve been using the amazing Thredded forum gem on Notebook.ai for years now. It’s a fully-featured drop-in-place Rails engine that instantly enables a forum in your Rails application and we’ve seen huge usage of it in our little world of writers: with over 20,000 threads created and over two million responses posted to those threads.In the next Notebook.ai release, we’re adding a public event stream for users. Users can follow other users and share the characters, locations, items, and universes they’ve created with their own followers.To keep users updated on the Thredded discussions their friends are creating, we’re also adding new Thredded topics to the stream. To do this, we’re extending the native Thredded::Topic model with an after_create callback that creates the stream events.In order to add an after_create callback to a model that we don’t have direct access to (for example, one in a gem), we need two things.We use the Rails 6 way of creating a Concern that we can extend our model with. This simply defines the after_create trigger and function definition. The code below also adds a few other methods I use in-app (icon and color), but you can add anything a typical Concern would include.As you can see, we have full access to all of the model’s attributes in this after_create, which makes creating a stream event with the proper information easy!Once we have the extension written, we also need to extend the Thredded::Topic model with it.We do this at the end of the config/initializers/thredded.rb initializer we’re already using, with the following code:The code’s pretty straightforward, but has a few nuances that Victor Leong explains really well in the blog post this code came from.And that’s it! Every time a new Thredded::Topic is created, it’ll trigger the after_create and create a ContentPageShare which our stream enumerates to show.That’s all and good, but our use-case has one more problem to tackle: polymorphically linking to the Thredded topics from the stream.Right now, we’re using the polymorphic field content (specifically, the content_type and content_id fields) to use Rails’s built-in polymorphic_path(content) helper for all of our notebook pages; this lets us have a single link in the view that can route to characters, locations, buildings, creatures, and so on — even though they’re all separate models.This gets hairy when the model is a Thredded::Topic, for two reasons:I couldn’t find a “magic” method to define on the Topic model itself to manually set a route to instances to that model (to be used by polymorphic_path). Visiting the page gave me a NoMethodError for the polymorphic route being generated.So I did something a little weird.It worked, but I’d love to hear from readers if there’s a better approach here.I noticed the route Rails uses by default for a Thredded::Topic passed into polymorphic_path isn’t scoped to thredded— it’s unexpectedly looking for topic_pathin the main_app namespace instead of the engine the model is defined in. (Side note: this smells of collision bugs; I’m glad I didn’t already have my own model named Topic!).So I created a ThreddedProxyController that takes in a topic slug, looks up the actual Thredded::Topic model from it, and then builds the full Thredded route to redirect the user to.I added it to config/routes.rb and made sure to name the routetopic so the default route Rails was using before — main_app.topic_path — would route to it.This defines topic_path in the main application namespace, which is successfully routed to from the default route Rails was generating from the originalpolymorphic_path call above: main_app.topic_path .This is what the proxy controller looks like:And now, everything just works!The polymorphic content column on ContentPageShares can continue to function regardless of whether it’s pointing to one of the many page types on Notebook.ai or a Thredded::Topic, and routes to either build just fine.This lets us add new topics to the activity stream that, when clicked, takes users directly to the topic in question!",ruby-on-rails,https://medium.com/indent-labs/extending-thredded-or-any-rails-gem-with-activerecord-callbacks-aa0b5aa4e630?source=tag_archive---------2-----------------------
Displaying your most popular posts,"This post will assume that you have already set up a likes model and controller (and the necessary database migration). If you have not then I recommend a very helpful blog post ‘How to add liking and unliking in rails’. If you have, then let’s continue!Just for information, I am not using any gems such as Acts as Votable or Acts as Favoritor in this example. There are a number of guides and blog posts about using these already so I’m going to try and help those who’ve decided to go it alone.To display your most popular posts you will need to do a search of your likes database, pulling a count of the number of post_ids with the most user_ids and displaying them in descending order (to get the highest number first). To do this purely via SQL your search query (if you were, for example, trying to pull the top 10 posts) would look like this:It’s always an idea to test these things out before proceeding. If you are using “the elephant” (TablePlus) then you can easily test this query out against your database. The example below shows you one I did recently where I was trying to pull the top 10 liked workouts in my recent Rails project. Workout 57 appears to be the winner!That’s all well and good but how does this translate via Rails? The simplest thing to do is to create a named scope in your Likes model.The query above will show the top 10 most popular posts of all time.You may notice that I have wrapped the scope in a lambda. This ensures the contents of the block is evaluated each time the scope is used. Otherwise you’ll get the following error:Now you are able to call this scope elsewhere in your code. For example you may want to attach the scope’s pulled data to an instance variable:Or you may want to call it directly in your view:However you use it, you can now freely display the information you hold on your top liked posts (or in my case, workouts).I hope this guide has been of some small use. Arguably it is easier to use gems for this. The two I mentioned above would probably be your best bet. However I find this to be a relatively clean solution and one that could come in handy in the future.",ruby-on-rails,https://blog.makersacademy.com/displaying-your-most-popular-posts-2ac0956c9c27?source=tag_archive---------1-----------------------
How to implement a simple API for ‘Sign in with Apple’,"Mariusz KapciaJun 1, 2020·5 min readMost of the projects I’m working on right now are related to mobile applications and Ruby on Rails backend. Today we will talk about what needs to be done on your server to support mobile ‘Sign in with Apple’ feature. Our goal is to create a simple API that will allow registration and login process to our system using Apple account and mobile application.The first thing we need to do is to read the documentation to know if there will be any communication between our system and Apple servers and how to implement it. After a quick read, we will know two things:Let’s start by implementing a simple AppleService which will be responsible for all the things described above. We will use the AppleId gem because there is no reason to implement that logic from scratch.Service has one public method fetch_profile where identity token is verified and decoded. The whole process is handled by AppleId gem. Here are a few things worth mentioning:Now we can take care of the necessary things around it. We will use concepts from Domain-Driven Design to make things a little more interesting.Bounded contextThis context will be responsible for managing user accounts. We have many possible names: Access, Users, Identity&Access. Bounded contexts are represented as modules in Ruby on Rails application.CommandWe need one command responsible for registration. We will skip validations to keep things clean but feel free to includeActiveModelor dry-struct.Domain eventWe also need one domain event to represent the fact of user registration. It’s good practice to validate event schema. Here we use ClassyHash gem but dry-struct will work too. In some cases, it’s also worth introducing failure events but we don’t need it here.AggregateDepends on the approach we can treat registration as part of Useraggregate or we can create a separate aggregate for the registration process only and create User aggregate later. We will handle all event-related things with RES gem.There is a simple validation if the user is already registered and if everything is fine thenUserRegisteredFromAppledomain event is published.Command handlerNow we need to load our aggregate and handle the registration command. This part is done by a command handler. Here you can also validate command and run different policies required to check if a command can be performed. We can make sure that users will be unique thanks to using an email address as part of the stream name.Communication between commands and command handlers can be implemented with a command bus.Read modelOur domain model is event-sourced so we need a read model to represent a cached list of registered users. It’s not very efficient to load all aggregates in the system and check who is registered. We also shouldn’t keep view related data inside aggregate. To make this work, the event store should be configured first.Application serviceWe introduce an application service to keep controller action as clean as possible. Service is responsible for checking if there is a need to create a new user account or only generate a session for an existing user. Implementation of GenerateSession service is not part of this blogpost.The only thing left is the controller and routes. It’s pretty straight forward and nobody should have problems with it.What’s more to do?Of course, we need to implement a mobile application part, but that topic is for another time. If we want to send emails to generated Apple aliases then we need to use Private Email Relay Service and we need to properly configure our domain in Apple Developer Center.More DDD contentIf you want to see how to implement other DDD patterns in Ruby on Rails you can check out my sample applications and bounded contexts here.",ruby-on-rails,https://medium.com/@mariuszkapcia/how-to-implement-a-simple-api-for-sign-in-with-apple-ffa7f06e32c7?source=tag_archive---------0-----------------------
Assign a password to new users when they sign up through 3rd party account,"Yingqi ChenJun 4, 2020·3 min readIn my Rails app, there are two ways to sign up a new user in my app: the traditional one that you provide password and email, and the one that you use the third party account, which is Google account here in my app. In the building of the second route, I came across a validation issue, and the way I fixed it is by assigning a password to the user when it is created in the database. Here’s how I did it.So to implement the second feature, I use Ominauth. Omniauth is a Gem in rails that offers another way to authenticate a user’s identity. Here in my app on the Login page, you see a button that says “Login with Google”. Once you click it, Google would usually ask if you want to authorize this operation of the website accessing your information to create a user in their database( or if you haven’t already logged in your Google account, you will be asked to log in first). Once you agree, a user is created in the database with the personal information sent by Google and you will usually be redirected to the page when you are logged in.To make sure a user provides a password and an email when signing up, I put validations for User model. Therefore, if an attempt to sign up a user without providing both of those two items, it will fail.It is not too difficult when a form is provided to capture a password and an email. However, it is hard when I use Omniauth.The good thing of Omniauth is, it is just one “agree” button away before you can sign up. It is very fast because it saves you a lot of trouble filling out a form. On the other side, the bad thing is, how can a user tell the database what the password is if there is not even a form?At first, I was looking for a way to validate two ways of signing up differently. For example, ask for a password ONLY in the traditional route to sign up. But after I talk to one of my instructors, he told me better not to do that because a password is very important, and therefore, we should never give a roundabout route to skip setting up a password.He gave me a suggestion on changing the corresponding action to create a user through third party accounts, which is, new_from_google action in Sessions Controller here.It looks like this before:The key lies in the assignment statement: @user = User.find_or_create_by(email: auth['email']). The method find_or_create_by accepts a block and would pass the params to create if a user is not found.I change it in the following way. When a user with that specific email cannot be found, the block including a password will be passed to create a new user and the problem is solved! And next time, when the same user wants to log in, a google account’s presence is enough.Of course, you can also provide a way for the user to change their password later, just in case they want to log in using the email/password combination. Thanks for reading!",ruby-on-rails,https://medium.com/@chanwingkeihaha/assign-a-password-to-new-users-when-they-sign-up-through-3rd-party-account-e8b473aa8479?source=tag_archive---------4-----------------------
"Why you should avoid nested STI | ActiveRecord, Rails 6","Nested Single Table Inheritance doesn’t work well. Here’s what you must know to make it work or work around it.I recently stumbled across the following scenario.Initial specifications: a project owner creates a project and donors can contribute any amount of money to that project.Later, a little change was made to the specifications: a donor may either be a natural person (an individual human) or a legal person (a corporation or any other kind of legal entity).Since both are donors and will share some significant amount of logic, it seems obvious that they are both a specialization of User::Donor, hence:So far, this is classic OOP and we rely on ActiveRecord’s STI mechanism to do its magic (.find type inference and so forth).Spoiler alert: it doesn’t work.This part is not specific to (nested) STI or to ActiveRecord but it’s worth knowing.Given a recordless database (working on a new project):This is unexpected. I thought User.descendants would give me an array of all subclasses of User (%i[User::ProjectOwner User::Donor User::Donor::Natural User::Donor::Legal]) but I have none of that. Why??You don’t expect a constant to exist unless it has been defined, do you? Well, unless you load the file that defines it, it won’t exist.Here is roughly how it goes:Now you see why lazy loading doesn’t play nice with Single Table Inheritance: unless you’ve already accessed every single one of your STI subclasses const names to preload them, they won’t be known to your app.It’s not that STI doesn’t work at all, it’s just mildly frustrating because oftentimes we need to enumerate the STI hierarchy and there’s no easy, out-of-the-box way to do it.Ruby on Rails’ guide mentions this issue and suggests an (incomplete) solution: https://guides.rubyonrails.org/autoloading_and_reloading_constants.html#single-table-inheritanceTL;DR: use a concern that collects all types from inheritance_column and force-preloads them.Why it’s incomplete: because a subtype that has no record yet won’t be preloaded, which means there are things you won’t be able to do. For instance, you can’t rely on inflection to generate select options because recordless types won’t be listed in your options.Another (really not recommended) solution would be to preload all your app’s classes. It’s killing a fly with a hammer.My solution is based on the concern suggested by Rails’ guide but instead of collecting types from inheritance_column, I use an array that contains all of the STI’s subclasses. This way I can use inflection at will. I agree that it’s not 100% SOLID-complient but it’s a trade-off I’m willing to make.That being said, let’s talk about the main topic of this article.Single Table Inheritance is made for one base class and any number of subclasses you want as long as they all directly inherit from the base class.Take a look at the two following samples. The first one works perfectly fine while the second will give you headaches.Why does the first one work in a predictable manner and not the second? Find out yourself by paying attention to the SQL queries:Now with a nested STI (base class, mid-level subclass and leaf-level subclasses):See? The SQL query to find the donor associated to the contribution looks for the type User::Donor. Since my donor is a User::Donor::Natural, the record is not found. ActiveRecord isn’t aware that User::Donor::Natural is a subclass of User::Donor in the context of an STI unless I preload it first.This is not okay to me. I would rather not take the risk of choosing an architecture whose behavior is uncertain because subject to code preloading.ActiveRecord could’ve been designed to produce the following SQL statement:Which would allow me to:But it behaves otherwise:Only when I preloaded User::Donor’s subclasses does it start allowing me to request User::Donor.all and retrieve records of type: User::Donor, User::Donor::Natural, User::Donor::Legal .One can put the blame on lazy code loading but I don’t. While I agree that inflection and lazy code loading cannot work hand in hand out-of-the-box, and since we can’t have a predictable/stable behavior from a mid-level model, it would be better to have AR’s documentation explicitely discourage nested STIs.I’d rather not have a feature than one I can’t rely on.The answer is found in the source code of ActiveRecord.When accessing the relation, ActiveRecord adds a type condition if needed:To determine whether the type condition is needed, it does a couple of checks regarding the distance between the current class and ActiveRecord::Base as well as the presence of an inheritance column.The type condition is built as follows:To sum up:There always is.We could consider patching ActiveRecord, making it use LIKE in the SQL query as an alternative condition to the actual strict string comparison. Problem: I didn’t run any benchmark but it will certainly slow down database reading. Though it’s a working solution, it is inefficient, requires a lot of work to patch ActiveRecord and, frankly, we’re not even sure the Rails core team would merge such a patch.Another workaround would be to override the default scope of User::Donor to make it use a LIKE statement as described above. I’m not a huge fan of default scopes because the day always comes when we need to use .unscope and voilà it doesn’t work anymore. It’s not a sustainable solution IMO.Yet another solution could be to preload subclasses, for instance with the solution discussed earlier. I guess it’s an acceptable one.One more solution is to roll back to a simpler architecture that does not let any room for behavior changes: no mid-level subclasses, no preloading required. How do I not repeat myself for the common code shared by User::Donor::Natural and User::Donor::Legal, you ask?Using concerns.There is still room for improvement (this code is intentionally oversimplified, no validations whatsoever) to make this article easier to read, my goal being to give you the essential information so that you can choose your own favorite solution in an informed way.When possible, I’d rather have a simpler architecture (no intermediate layers). The less complex it is, the less headaches I have.When I must have this intermediate layer, I’ll preload all subclasses of my STI to avoid any behavior randomness. And I mean all subclasses of my STI, not just the ones having records in the database.Thanks for reading!",ruby-on-rails,https://blog.capsens.eu/why-you-should-avoid-nested-sti-activerecord-rails-6-b180f1bcc029?source=tag_archive---------0-----------------------
VS Code Extensions You Need if You’re a Ruby Developer,"Stop what you’re doing and download these VS Code extensions right nowYou’ve decided to start the journey and learn how to program. Excitedly you’ve downloaded VS Code because you heard it was the best (and you’re not wrong, don’t let the ATOM users tell you otherwise). Or maybe you’ve been coding for a longer period of time, but have you been taking advantage of VS Code extensions?Visual studio code extensions enable you to add tools to the text editor that can help make writing code faster and easier. They are meant to help you with efficiency, productivity, and can spare you from a lot of headaches. Are you excited yet?! I’ll first briefly show you how to install an extension in VS Code just in case you’ve never done it before.You’ll want to click on this icon on the left-hand side of the screen:Once you do, the following should open up:Once there you can search for extensions by their name and once you’ve found the one that you want you just click install. It’s that simple!Okay so let’s get started with an extension that is simple, but oh so very sweet.This lovely extension “wisely” adds the end keyword to your Ruby code. It also maintains proper indentations for the code structure!Below is a demonstration of the extension working to create the end keyword for a method “hello_world” and for an if statement.I can’t tell you how many times I’ve gotten syntax errors from forgetting an end somewhere in my code. I’ve made it a habit to write an “end” as soon as I begin writing a method or an if statement; however, I still somehow forget it a great deal. With this extension, you don’t need to remember to write it! So many headaches and so much time will be saved! Install Endwise here.This extension makes creating and switching between ERB tags (<%= %>, <% %> and <%# %>) a BREEZE!Below is a demonstration of creating and toggling between ERB tags with the keyboard shortcut that this extension provides, ctrl+shift+`. You can also personalize the shortcut if you’d prefer something else!Wow! So quick! So easy ! I know I’ll be using this shortcut ALL THE TIME! You can find out more about this extension here.This extension provides you with A LOT of functionality. From providing you with all the CRUD helpers to writing out the syntax for creating a new table for your database.Below is a demonstration of typing this extensions keyword “crud” and getting (almost) all of the CRUD methods (the “new” action is missing). It’s not perfect, as you might want to tinker with the output it gives you. For example, in the “update” method I would change the .find that it creates for you to .update. But, that’s okay! This could definitely save you a lot of time.Here, I show you how this extension makes creating database tables in your migration files incredibly quick and easy.There’s much more to this extension that you can play around with! You can see all of its features here.There are also so many more extensions to check out. These were the best that I‘ve found in regards to working as a Ruby developer. They help save us time and guard against easily preventable bugs, ultimately making us better programmers. If you want to check out other VS Code extensions that go beyond just Ruby, these are some great blogs to check out — “The 25 Best VS Code Extensions” , “VS Code Extensions For Web Dev Productivity” and “60 Extensions to Supercharge Visual Studio Code”.Thank you for reading and I hope you found this useful!",ruby-on-rails,https://medium.com/weekly-webtips/vs-code-extensions-you-need-if-youre-a-ruby-developer-add2ce8683f6?source=tag_archive---------2-----------------------
"How To Build A Web App, part 19 of ?: API Act ID filtering working at last","Mikey ClarkeJun 11, 2020·12 min readThis is the nineteenth in a series of articles taking you through all the actual steps in building a web app. If you’re an aspiring developer, if mucking around with teensy beginner tutorials frustrates you, if you’d love to build a properly substantial app that does fab things, these articles are for you.Word to the wise: these tutorials don’t depict a polished, pristine workflow. You won’t find “follow directions A, B, C” taking you in a perfectly straight line. Nah. These tutorials depict mess and chaos and grit. They meander. They show what real web-dev is actually like. It’s a standard trait of professionals that you should make your trade look easy. Not here, baby. If I’ve written these articles properly, beginner-to-intermediate devs will read them and think “Oh thank Christ, turns out these self-proclaimed Senior Developers struggle just as much as me! My impostor syndrome is just a syndrome!”Last time, we decided we’d abandon the absurd tyranny of using include() in our tests instead of straight-up true-blue hand-on-heart eq: simply testing that our response JSON include-ed various items of demo Venue json utterly failed to reveal the fact that our API response returned 13 Venues, not five. 13! Geez. After getting to the bottom of, and fixing, that bug, We decided we’d refactor our tests to detecting the entirety of our API response’s Venue JSON: five Venues, ordered by updated_at, descending.Then we ran into a few (more) hiccups. Turned out jsonapi-resources was … let’s say, incomplete. It claims it can serialize its resources. It cannot. At least the latest version can’t. I had to bump its Gemfile version down to 0.9.11 to get it working.Finally, to round off our hilarious fiasco, I discovered I’d forgotten to add t.timestamps to all tables.But that is now sorted and we can continue.After much headbutting, I made these modifications to our “Not supplying any act IDs” test:Two things may jump out at you. First, :params. I’d changedtoNothing too fancy, it just sets our request’s query params; it ensures our HTTP request’s exact URL is this: GET http://localhost:3000/api/v1/venues?filter[acts]=3,4,5.The other things is the actual meat of the test. Neat, right? Two bits: first, generate an array of resources from the array of Venue models we’re testing. List them ordered by updated_at, descending:Then serialize, then test:Awesome. Let’s test it. Testing:Failed? … Ohh, the complete response body has extra fiddly bits: links and so on. I’m not feeling overly strict about testing every last iota of the entire request body: we’re testing only those five Venues, not their extra metadata. Only the JSON-API response’s ""data"" key.In that case, changeto…and test again:Wait, what? Now what’s wrong? What’s this expected: nil business? Why would it expect nil? Is it talking about serialized_resources? That’s not nil! What gives?Remember Byebug? I decided it was high time to inspect serialized_resources. I fired it up and jumped in.Aha. There’s the problem. Can you spot it?That first key. It’s :data. A symbol. It should be ""data"". A string. And :data repeats again on that second-to-last line! Its array’s keys are also symbols. Every key in the response’s entire JSON structure should be strings. And they’re not.Why not, you may ask? That … is a really good question. It’s obviously another bug, though this one is far smaller and more benign and doesn’t tempt me to ragequit. If you consult jsonapi-resources’ Serializer docs (and pay close attention to that URL’s version, we’d fallen back to “v0.9”, remember, not “v0.10”), take a look at its example demo JSON structure. Its keys are all strings and no symbols.Well okay then. We shall have to just simply whack our own patch on too. We’ll manually change serialized_resources' keys to strings.Modifying large and complicated data structures like this can be a horrendous pain in the ass. But! The Rails devs have foreseen this. They have added all kinds of fascinating doohickeys to the Ruby language, all packaged up in a single library. It’s called ActiveSupport. It’s all kinds of awesome. One of its many methods is #deep_stringify_keys!. It does exactly what it says on the tin: converts any big honkin’ hash’s keys into strings, in-place. Including any child hashes within. And their child hashes. Deep as you please. Hence “deep”.(Here’s another Thing: you may be wondering why some Ruby method names have an exclamation mark and others don’t. Here’s the deal. It’s a Ruby convention. A method’s exclamation mark’s presence or absence denotes whether a given method is quote-unquote “destructive”, or dangerous in some way. (Ultra-l33t devs like you and me call a ! a “bang”, so we’d pronounce deep_stringify_keys! “deep-stringify-keys-bang”). Exactly what “destructive” means varies ginormously depending on context: here, deep_stringify_keys calculates and returns an entirely new copy of the hash you’ve called it on, whereasdeep_stringify_keys! instead modifies the existing object, in-place: you’re overwriting your existing data! Similarly, ActiveRecord models have the methods save and save! — if a model is invalid, then save returns false, and save! throws an exception.)But anyway. Behold!And test:Is that not a sight to warm the cockles of your heart?We have our completed test. Let’s commit and diff that.Okay, where were we? Testing our Act ID filtering, that’s where. Onward!Our first search tests were, all along, supposed to test what the API returns when we supply it specific Act IDs: return only Venues and Gigs performed by the Acts we’d supplied. Right.I had a go at rewriting the “Acts 1, 2, 5” test thusly:A return to subject { ... } syntax! I rather like it. The it { is_expected_to eq 'blargh' } syntax tickles me just right.Let us test:Whoa. What happened this time? Why return a 500 Internal Server Error? A 500 error isn’t just any regular old test failure — it means there’s some kind of programming error inside the server that made it fall over.A quick skip and a jump into the [ginormous JSON stack trace] set us straight. The response JSON’s errors.0.meta.exception key equals this:Iiiiinteresting. Malformed SQL. You may recall our earlier chats about ActiveRecord, back in article number … um, umm … number ……Huh. Well I’ll be. I’d just assumed that when I’d started banging out my first actual Rails code for your discernment and delectation in article 13, then naturally I’d have explained the core basics of ActiveRecord. But a perusal tells me that I had not!All right then. Here goes:The whole point of Rails’s ActiveRecord library is to join together databases and programming: it wraps a single database table row with a programming Object. It links together the interactions of these Ruby objects and data with database-friendly SQL: an ActiveRecord query like v = Venue.find(1) becomes SQL’s SELECT * FROM venues WHERE id=1. v.update_attribute 'name', 'blargh' becomes UPDATE venues SET name='blargh' WHERE id=1. That kind of thing.jsonapi-resources does something vaguely similar. Remember when we defined our per-model resource files, in article 17? The Venue resource file was of particular interest. Its filter method, and its apply argument, is the basis for generating the SQL to query our database for our sweet sweet JSON.Just as a refresher, here’s that filter method again:It’s apply that does the heavy lifting. That’s what generates our buggy SQL. Here’s that SQL error message again, cleaned up a bit:Aha. If you’ve ever written SQL manually, you can probably see what’s wrong: our query’s tables and columns, inside SELECT abc FROM xyz mentions only venues, but our filtering code, inside WHERE, mentions only acts. PostgreSQL has no way of knowing the connection between venues and acts.id.We have to tell it. We have to use a thing called an Inner Join. If I was writing my SQL manually instead of getting ActiveRecord and jsonapi-resources to do it, I’d write something like this:Getting déjà vu? We’ve bumped into inner joins before. On that huge bug-fix tangent I charged off on back in article 15….…Wait. Speaking of tangents …Time to mention another Thing! I’d not mentioned something called “technical debt”, had I? See, it’s entirely possible you’d been wondering the following:“Wait wait just a cotton pickin’ minute — all this new jsonapi-resources query/search code —it just looks like we’re re-implementing our SearchController code in app/controllers/api/v1/search_controller.rb with this new stuff? What? We’re doing the same job again? WHY?Yup. Welcome to the messy complexities of real-world dev. We’d written all this API::V1::SearchController code before encountering the glories of jsonapi-resources. Turns out modifying the latter makes for a better program than creating the former from scratch! After we’ve got our Act ID tests into a nice state, I’ll delete the search controller code, and clean up any references to it elsewhere in the app.“Clean Up” is the appropriate phrase here. Some devs don’t. Some managers don’t. Some companies have a “well the app works, doesn’t it? What’s the problem?” attitude to the health and hygiene of their apps’ code bases. They either don’t know or don’t care that more often than not, their code is a horrid apocalyptic abyss of spaghetti and Franken-code and a zillion other abominations. In future articles I’ll go into more detail about this; but for now, yeah, I’m acknowledging that we’re re-implementing our search controller, and we’ll delete it later.Sooo what jsonapi-resources filter code should we write, to shove the right INNER JOIN SQL-code into our SQL? Same as last time: we use a Thing called a join. Simply changetoAnd test again!Oh.Dear god, now what?Um … looks like our Venue resource JSON doesn’t have all its bits. The JSON we’re generating within our tests has all its properly attached Gig IDs for each Venue … but our API’s response JSON doesn’t.Why? Search me. Let’s find out together. Usual story: as I’m typing this, I genuinely have no idea what’s causing this bug. Total mystery.Okay. A day passes. I revisit this apparently insoluble conjecture …… And facepalm.Duh. Obviously the response JSON is showing a reduced subset of each Venue’s Gigs. That’s the whole point of submitting these Act IDs: we’re asking only for Venues and Gigs performed by Acts with the IDs we’re submitting. Good lord I can be a moron sometimes.But all right then. No-one’s perfect. Moving on. What’s our next step?Modify subject to reflect that same Gig-Act filtering, that’s what. After some faffing, I decided this would be just super:Got all that?You can see I’ve whacked a .each loop on the end of .deep_stringify_keys!['data']. It iterates over each “svr”, serialized venue resource; jumps into its gigs data; then filters them, in-place, with Array#select!: retain only those gigs whose act’s ID is one of 1, 2 or 5.Testing:Beautiful ❤But … one thing bugs me. That subject { ... } syntax isn’t really all that readable. Not obvious what’s going on, is it? Very few labels. We can’t properly understand the code execution flow, or what its intentions are. Let’s do proper justice to it.How about …That, I think, is a little better. Two main differences here: (1) we’re letting a hell of a lot more, space out our computations with named variables, splat a lot more labelling this way and that. Though you can see I’m straining a bit on that last one: serialized_vr_4_2_3_data_w_gigs_for_acts_1_2_5 is getting a bit silly. Almost as bad as Hungarian notation. (WARNING Hungarian Notation is a desperate abyss of complexity and despair and wailing and gnashing of dreadful teeth, click on that link at your own risk.)Oh yeah and (2), we’ve let :acts be its own separate standalone array of Act objects, so’s we can do this: acts.include? Gig.find(sgr['id']).act. Neat. Terser.And test, just to be sure …Beautiful-er ❤ ❤And that, I think, will do for today! Let us commit and diff that shit. Awesome.We’ve finally got rudimentary Act ID filtering working. Next time, let us plump out our two tests to be a bit more expansive. I’ll take you through the concept of tests as documentation. Catch you on the flip side.",ruby-on-rails,https://medium.com/@mikey-clarke/how-to-build-a-web-app-part-19-of-api-act-id-filtering-working-at-last-de9e08bb2953?source=tag_archive---------0-----------------------
Using devise in your Ruby on Rails application,"Miguel Angel Dubois PappaJun 4, 2020·7 min readAuthentication. You don’t always want your users to have faceless sessions that open your application without leaving any trace. Of course, there are some applications in which you don’t need accounts for your users, but in the vast majority of the cases, you will need to manage some users and passwords. If you are a beginner with ruby on rails you may already be thinking how to do this. Create a user model, add some fields to your user model, including an encrypted password field. Save your user’s session in some way so they can remain online even after switching application windows, save some cookies so they can remain online even after closing the browser. You know, those things are pretty repetitive if you create new applications often, and if something is repetitive, you can make an algorithm that does it for you. The special thing about the internet though is that most of what you think about creating was already created by someone else. In the case of authentication for Ruby on Rails, there’s a gem for that.Devise is a very complete gem that does all the authentication work for you, or most of it if you are thinking about a very specific feature you would want to implement. When I first used it, I just wanted to make a basic login for my users and add some extra features to the registration process. It took me a couple of hours to read the documentation and add what I wanted, and I wish I could have found a more basic tutorial to guide myself with. I want to do my past self a favor and make a short tutorial on how to create a Rails application, add some extra fields to the user model, and modify the default views of the gem.Let’s begin. Create a project first if you haven’t already.And immediately after, add devise to the gemfile.Then run “bundle install” from your terminal. Wait for everything to install and when it’s done, run “rails generate devise:install” on your terminal.You can see we got some instructions with this one.The first one is referring to the mailer settings. For a development environment, you need to specify your default URL. This won’t give you trouble if you are not going to send mail to your users, but let’s copy and paste the line on the development.rb file just in case we need to send mail to our users.The second point asks us to define a root_url to something. I will leave this for later, you don’t really need anything on the root path of your application for devise to work. This is just the gem reminding you to make a home page.The third point asks us to ensure we have flash messages on our application.html.erb file. Why? Well, this will let the users know if they are doing something wrong. Default messages are already included on the devise gem so you don’t have to write them. Just copy and paste what devise shows you on the terminal wherever you want it to be visible.The last point is, in my opinion, the most important one. It’s telling us to do generate the views of devise for customization. We will run the command now and modify the files later.Great! We can now generate our model. You can call your devise model whatever you want, I will call it “User”. It’s a generic name understandable by anyone. Let’s run the command now. Type “rails generate devise User” on your terminal, you should get something similar to this:And before we migrate our database, let’s go check the migration file.Here, you can uncomment the fields you want to use, for example, if you want to be able to confirm your users sending them an email, you can uncomment the four lines below # Confirmable. The devise gem will do the rest (plus some configuration of your own you will have to investigate, as it’s not going to be covered in this tutorial).Let’s add a first and last name to our users, just to give you an example on how to do it. We will use the same model devise created to add any fields you wish but will need to do some modifications later. Let’s create a new migration now. On your console, write “rails generate migration add_name_to_users name:string surname:string”.This will generate a new migration file which will add some columns to the user table. Let’s go check the migration file.Remember, I didn’t write any of this on the migration file, it was generated by rails. Everything seems to be good, so we can do the database migration. On your terminal, you can either do “rails db:migrate” or you can also write “rake db:migrate”. Either will work. After doing the migration, let’s go check the sign-up form on our app. We haven’t done anything at the moment so we will need to access it manually by typing the route in. First of all, run your server by running “rails server” on your terminal.Then, go to any internet browser you have on your computer and go to this address: http://localhost:3000/users/sign_up . What you will see is your sign-up form, auto-generated by devise. It doesn’t have any special style, but you can take care of that. What I’m concerned about is something else. We added a name and surname to our users and it’s not here! Let’s go change that. Go to the views folder. Remember we ran the command rails g devise:views? It will come in handy now. In our views folder, we have another folder called devise. Open it and search for the registrations folder. Here, you have the new and edit files. Let’s open the new.html.erb file.This is the signup form, yes. But it’s lacking the two fields we wanted to our user. Let’s add them, actually, let’s add these two fields to both the new and edit files. The edit file, by the way, is for old users who want to change their information, like their email or password.I just immitated the stile set by the auto-generated sign-up form from devise. You can style it later with your own css. Let’s see how it looks.You should do the same with the edit.html.erb file while you are at it. For now, let’s test our sign-up form. I haven’t added any more functionality to this app, but we should be able to add something to our database with this form. Let’s first run the console and check if we have something there.You can see we get an empty array when we user the User.all command on the console. Let’s go back to the sign-up form and… sign-up.After clicking on the Sign Up button, we are sent back to the root route. This is the intended behaviour. Let’s go check the console again and see what we get when we search for all of the users. We should only get one user and it sould be mister foo bar.Did you notice something? The name and surname fields on the user record were not saved. This is because we need to explicitly tell rails to accept these two fields in the form. Let’s go do that now. On the controllers folder, open the application_controller.rb file and update it so it looks like this.Let me go to the console and erase the first user from there. Now let’s test this again, after the changes, we should be able to sign-up and have all of our user fields inserted in the database.I’m using the same credentials as last time. Again, I already erased the first mister foo bar from the database so it shouldn’t be a problem. Now let’s click Sign Up and check the database on the console.We got it now.Congratulations, you now know how to install devise in your applications, add a model with devise, add some fields to those models and customize devise’s views. You can now continue with the rest of your web application.Thank you for reading!",ruby-on-rails,https://medium.com/@migueldp4/using-devise-in-your-ruby-on-rails-application-8f06b569efe7?source=tag_archive---------1-----------------------
Event store configuration per bounded context,"Mariusz KapciaJun 4, 2020·4 min readIn most of my projects, I’m using or I’m trying to introduce patterns from Domain-Driven Design, Event Sourcing, and CQRS. Most of those patterns are somehow related to the publish-subscribe pattern and it’s not possible to efficiently use them without it. I’m working with Ruby on Rails and my choice is RailsEventStore. Of course, it’s possible to implement a very basic mechanism by ourselves, but if we can use a well-tested solution then why not use it. The general idea behind this article should be applicable for every tool but in the examples, RES will be used.Let’s start from the beginning. We start a new project or maybe we want to extract our first bounded context from existing legacy code. We need to add event store configuration to the project.With time, we are able to add more and more domain events and at some point, we can start using them with read-models. At this point, we need to add subscribers to our event store.Everything looks good. Domain events are published from Person aggregate. We are able to write tests separately for the write side (commands/command handlers/aggregates) and the read side (read models/handlers). There is also a place for acceptance tests where we want to use our API and check if we can get the expected response body.Now, imagine that our project grows with time and we have not one, but ten bounded contexts and not one read model, but ten per bounded context. In addition, we have different handlers like process managers/sagas or context mappings. Everything is configured inside:It will lead us to a couple of problems. First of all, it’s difficult to manage one big file with the event store configuration. Secondly, there is an even bigger problem that may not be visible at first glance. With one big config, we have only two reasonable ways of using event store in the test environment. We can use all subscribers or none.Of course, it’s possible to manually declare which subscribers should be called for every test but it’s not very efficient. It can be especially problematic if we want to test a bounded context in isolation or maybe only communication between two bounded contexts.Case with all subscribers has two drawbacks. First is that our overall test time execution will increase significantly and that’s due to running many unnecessary subscribers. Second is that we will need to make sure that all necessary dependencies for every subscriber are ready to use. If our goal is to test bounded context A then we don’t want to do anything related to bounded context B.Case without subscribers is not very useful because we are not able to properly test the whole flow.What’s the solution? We can split our event store configuration into small configurations per bounded context. Thanks to that, we will be able to easily create new instances of an event store with subscribers from specific bounded contexts.ImplementationWe want to create a separate file for every bounded context. In this file, we will only have subscribers related to this bounded context.Now we need to put everything together. This part is done in our initializer.Our default configuration is still the same. However, thanks to that change we are able to easily test bounded contexts in isolation. In our test file, we can add a method like below and pass it further to the command handler/process manager/or some other service initializer.It’s also possible to use code like this if we prefer it:SummaryToday we’ve learned common problems with event store configuration and how to tackle it. The proposed solution is very elastic. In addition to bounded contexts, we can introduce separate files for context mappings. It’s also possible to split it even further into specialized subscribers inside one bounded context if we need it.A different solution for this problem is to use separate event stores for every bounded context but that’s a topic for another time.Thanks for reading and have a nice day!",ruby-on-rails,https://medium.com/@mariuszkapcia/eventstore-configuration-per-bounded-context-cf17c40bb01f?source=tag_archive---------3-----------------------
What is Object-Oriented Programming (OOP),"John GuestJun 2, 2020·3 min readOriginally posted on September 16, 2019In OOP, we identify objects for our programs to use. Humans think about objects as things with attributes and behaviors and use these objects based on those attributes and behaviors. In Ruby, these objects become classes, the blueprints and factories for objects. Each instance of an object contains instance variables which are the attributes of the object and the behaviors are described via methods. Take the example of a dog. A dog can be seen as an object which can be realized in Ruby as a class “Dog”. A specific breed is an attribute of the Dog class. The attributes of a breed such as size and color can be stored as instance variables. If you want your dog class to bark, this is a behavior that is described by a method.The way objects are created in Ruby is by calling a new method on a class, as in the example below:What’s going on in the code above?Instance variables can only be accessed from inside the class. In order to access them, we create instance methods that have public access. In order to read and write data, we need “reader” and “writer” methods.Let's look at these methods using our dog example.In Ruby, the reader and writer methods are defined with the same name as the instance that we are dealing with. In the above example when we call my_dog.breed we are calling on the method “breed”. These methods allow us to have more control. But Ruby emplores a little bit of magic to make this a little easier.The “attr” MethodBack to our example:As you can see the attr methods makes things a little easier.In the last example, we did not initialize our instance with any values. This is not a best practice. It’s always best to initialize values using a constructor.Methods and classes define a new scope for variables, and outer scope variables are not carried over to the inner scope. Let’s see what this means.The variable outside of the class is not the same as the variable that is inside the class. These are the very basics of Object-Oriented Programming in Ruby. From here, a promising future of programming can begin.",ruby-on-rails,https://medium.com/@jcguest90/what-is-object-oriented-programming-oop-672b19161141?source=tag_archive---------6-----------------------
How to create a slideshow by looping through uploaded user’s pictures with carousel in your Ruby on Rails app using bootstrap.,"Hai HoJun 1, 2020·3 min readHi everyone, i’ve recently finish my first rails project and when i was building my app, i wanted to loop through the pictures that user has uploaded and make a carousel button where you can click and back and forth for different picture as below. Since i’m only new to programming and haven’t touched on JavaScript, this this the best solution i came up with.So assuming you’re familiar the Rails and know to make a form and a button to upload picture, i’m not going to cover that. Let’s say you have a listing model, in the listing you want to upload multiple pictures to it, you have to go to Listing model (listing.rb file) and add has_many_attached :picture to the model. Now in most rails app you need Action Controller Parameters, basically a list of trusted parameters that allow through, you need to add to add the picture parameter, which is an array of picture to it:params.require(:listing).permit(:picture [])So now that your you have an array of pictures, any time you want to use only it, you have to specify what picture you want to use in the array. For example if you have an image_tag and you want to use the first picture in the array, which is at index 0, you have to put<%= image_tag listing.picture[0]%>First off make sure you have Bootstrap up and running. All the code of how to to make a carousel is available here in the Bootstrap documentation. I’m going to talk about the most important <div> element here which is carousel-inner , and inside it will contain carousel-item .The most important part with this carousel is with a bunch of images, we have to give one and only one picture (here is carousel item) an active class (often the first picture in the array since we can have one or multiple pictures), which is the first picture that is being shown by default. Without the active it is not gonna work. The question is how do we give just first item the active class when we loop through the images?We have to write a function where we only give the class active to the first item, which is at the 0 index.The default of the carousel control arrow is white, and if you have a white or light background , you won’t be able to see it. So how to change it since Bootstrap carousel in the background written by javascript and you have no idea how to modify it? You just need to add these line of code below to your application.scss file.You can change the colour of the carousel by changing the fill field.And Voila, there you have it. Good luck and have fun coding.!",ruby-on-rails,https://medium.com/@hohoanghai14894/how-to-create-a-slideshow-by-looping-through-uploaded-users-pictures-with-carousel-in-your-ruby-28f4ce685322?source=tag_archive---------2-----------------------
The Elephant in the Room,"Carlie AnglemireJun 5, 2020·3 min readIn switching computers for the first time as a developer, I have been experiencing what it is like to have to set up my coding environment all over again. What I have mainly learned from this is how much I do not know, which is always a bewildering realization. There is still a lot of work for me to do, and this post is not going to be a success story. Actually, I just want to share one line of code, and it is related to using PostgreSql as a database for a Ruby on Rails API.While trying to complete the task of setting up my environment, I have also been working on getting my portfolio site up again after being dissatisfied with the initial deployment of it. Because of issues transferring data from one computer to another — at one point I spent a half an hour on hold with Apple Support just to ask how long I might theoretically need to wait for the Migration Assistant to finish before deciding that it wasn’t working — I ended up recreating the backend for my website. When I tried to use PostgreSql as the database, I got an error that said something like: “PG::ConnectionBad: fe_sendauth: no password supplied.” One of the things that was so confusing about this was that because I had just installed postgres on the new computer, I thought that it meant I had not installed it correctly, that I had overlooked some detail that mattered more than I could know. Where it was that I needed to enter the password that had not been “supplied” was a mystery to me.To make a long story short, the mistake was actually familiar, just not one that I had made in awhile. What I needed to do after creating the new Ruby on Rails API with a PostgreSql database and cd’ing into that directory was to enterbefore I could migrate.That was it.This is the blog that included that detail and pointed me in the right direction:dev.toThe reason it did not occur to me to first create the database before migrating was that Ruby on Rails uses sqlite3 as its default database, and when using the default you skip this step.I am tempted to make a connection here. My old computer started to show its age due to its memory being strained after seven years of being crammed with a lot of little things that it was too late for me to go back and delete. I scrambled to rid myself of the detritus that I had accumulated to no avail. Then, once I had acquired a new computer, with all of the other things going on in life, I forgot the one small detail that everything else depended on. I will leave you to draw your own conclusions about what this all means.",ruby-on-rails,https://medium.com/@carlie.anglemire/the-elephant-in-the-room-ebce87d7df5c?source=tag_archive---------3-----------------------
"Rails Routes, from beginner to PRO!","The goal of this article is NOT to provide detailed information about configuration. Here I pretend to give you a better idea of the possibilities that are out there with the Rails Router so that you can start practicing and using different approaches for your applications.So you started creating some basic projects using Ruby on Rails, data is saved, data is rendered, and users are getting to the right pages with the click of a button, and you didn’t have to hard-code any HTML page for this… But, how is all this possible?With this article, I hope to answer the following questions, which will help give you a better understanding of the possibilities you have when using Rails.How do you get your web application to render the right content after a user’s request?The first few projects you built using Rails, you probably just followed along with a tutorial, or read the documentation and made the simplest implementation that could get your project to work.In this article, I want to show how the Rails Router is working for your application, and how you can customize its behavior to make your website feel more professional and easier to navigate, and even scale.First, let us define a few concepts to get you up to speed.Rails RouterThis is the core of this article. The router is what takes care of how and where data is sent, or where data is requested from.When a user makes a request to your page (i.e. “load my list of friends”) the request is sent over HTTP(s) to your server with an HTTP method that could be /GET (read), /POST (create), PUT (update/replace) or DELETE.In Rails, depending on the URL that receives the request and the method being used, a controller action is called, they are index, new, create, show, edit, update, and destroy. Visit the Rails Guides for more detail.Rails ControllersWhen the controller actions are called, they could provide an HTML response (rendering a view you create) or a JSON response (read below for more on this one).What is an API?Application Programming Interface, clearly…You probably know this one, you may have even called one in the past, maybe to get some weather data, or to create a personalized map using the Google Maps API. But, how can our Rails projects relate to this?Well, one teammate once told me, “Everything on the internet is an API”, and all I could think was that I was not using an API from anyone to get my websites working, so why did he say that to me?The answer is, almost everything on the internet is an API. Why? Using an API gives you great flexibility if you want to completely swap out your front-end without having to worry about how your data is processed by the back end of your application.Most likely, the Rails projects you have done or have seen from tutorials include creating models and a DB migration, setting up the routes (which we’ll get to in a minute), creating methods in a controller in order to handle actions based on HTTP requests (REST), then creating the views that get rendered when one of this controller actions gets called.You can create an internal API (your Rails server) that will take care of the same HTTP requests but instead of having your controllers render something, you would have them return a JSON object that can then be used by your front-end, created however you want, without worrying about ERB at all, and with the possibility of adding more APIs to the mix, thus making it a very flexible way of building your website, as you can scale easily and add more services in a decoupled way.Finally, ROUTESNow it is clear that you have a router passing requests to controllers, we know the controller actions, and we know that having an API (using controller actions to return JSON responses) gives your application great advantages when compared to building the whole thing on Rails. The Rails Guides provide great reasoning for this approach.How can you use the Router to build with any of the above approaches?The above is a very basic way of telling the Rails Router to create all the routes needed for our previously mentioned controller actions. Behind the scenes, it creates separate rules, that look like:which allows you to get the patient with the id passed in from the URL (:id). Whether this returns an HTML response and loads your views, or instead returns a JSON response, it is up to you now that you know your possibilities.What if you have a contact page that doesn’t connect to your DB, it just sends you an email with the contact form text? How would you send the user to that page?Any page that is not within the RESTful requests and is not handled by any existing controller, would require a custom route. Let’s look at one:Exactly, where to? Now the users can send us a request to get the contact page by visiting “http://yourwebsite.com/contact”, but since HTTP requests are routed to a controller action, we need to create a controller for handling calls to this page, that would then load the corresponding view, let’s say you called it “pages” and you created a method “contact” that just renders the contact view, then the route would look like:Now, what if you have a static home page? Then you can use the controller you just created, but instead of contact use a “homepage” method and have your root directory be the home page:Let’s suppose we have users that have many articles in our blog, and you want to have a dedicated page for showing that user’s articles, and not just render them within the user’s profile (which is what’s done in most tutorials).A route for that would look like:And you also get a helper named path:The above are just some examples, again, if you want to review in detail how it works, I encourage you to visit the Rails Guides, they are probably comprehensible guides out there. The problem is, sometimes we just find what we need to get the project going and don’t even pay attention to the options we have out there.With the information I have provided here you can now start exploring different ways of using the Rails Router depending on your application needs. You don’t need to be constrained by the basics; you can take full advantage of the customization options that are provided.We talked about APIs, but why? Even though this article is mostly focused on the routes themselves, consider the fact that Rails Routes and Controllers help you define an API. You can easily set one up and use your knowledge about how each of the HTTP requests you send to your API gets routed.I hope this article has opened a door for exploring more about Rails, and so you know where you can continue your exploration, here are some useful guides:Cheers, happy coding!",ruby-on-rails,https://blog.devgenius.io/rails-routes-from-beginner-to-pro-1381e5567a27?source=tag_archive---------0-----------------------
"Creating a IOT Bedroom Climate Controller with ESP8266, Ruby on Rails and Arduino Uno.","Saurav KumarJun 8, 2020·6 min readMy bedroom has a problem… recently my neighbour has constructed a new shed with a tin corrugated roof which is at the precise angle to bounce all all of the sunlight directly through my bedroom window. The amount of sunlight hitting the window basically turns my room into an oven. It is by far the hottest room in the house even with the blinds fully closed.This project is going to attempt to resolve this problem by designing an IOT device that will check the temperature of the room and adjust my ceiling fan accordingly. This way i’ll always be at a comfortable temperature and can finally open my blinds.All the code used in this project can be found on my GITHUB repository: https://github.com/sauravkumar173/Climate_Controller.gitThe first step in the project was to create a server using ruby on rails. In my previous experiences, I created the CRUD framework server by coding the Model, View and Controller from scratch. This is actually not necessary for Rails as there exists a scaffold feature that will auto generate a lot of the setup code. To become more familiar with all of the functionality of Rails I decided that for this project I would use the scaffold feature — saving a bunch of time was also a major plus.After generating the MVC Rails application, the following functionality was added to the controller to request data from the ESP board. It should be noted that two calls are made — one to update the device and the other to retrieve the latest data. This was to allow flexibility for future development so that data can be requested without being updated and vice versa.In order to have the server update with a partial refresh and adjust the fans, the following JavaScript was used in the HTML index page.With the Rails server fully setup, the website was loaded up and included a section to manually control the fan speed as well as a partial refresh section that displayed the latest temperate and speed.Every 2 minutes, the server would send a GET request the ESP8266 to update the latest temperature reading and to adjust the fan position accordingly. The change in fan speed and ambient temperature of the room was then sent as a JSON packet to the server and the website was updated through the JavaScript.Furthermore, a log is then created and added to the SQL database and displayed on the site. Since the server was created under the CRUD framework any log can be created, read, updated or deleted. Looking at the times between the logs it can be seen they are 2 mins apart which was the reload interval time specified. The time between GET requests can be adjusted depending on the users preference.On the embedded systems side of this project, I decided to use two board — the ESP8266 and the Arduino UNO. This would allow me to place the temperature sensor anywhere in the room meaning and out of the way of directly sunlight or directly under the fan which would change the temperature reading. Communication between both the board would be accomplished through RF transceivers.In order for fan speed to be changed, a servo motor was used to rotate the fan controller knob. This required the Servo motor to be able to rotate continuously so it needed to be modified. I wrote a previous tutorial on how to modify a servo motor to be continuous which can be viewed here.For the servo motor to actually rotate the knob without causing damage to the housing, I modelled and 3D printed a connector that can be attached to the wall. This meant the device could rotate the fan knob without causing irreparable damage.Previously my project development was conducted on the Arduino IDE which was extremely limited in functionality, debugging and library files. Since this project is more complex than my previous devices, I needed more control so I began to use PlatformIO. Using PlatformIO allowed me to create my own driver libraries for each peripherals making my code more modular.In order to communicate with RF, a HCI protocol was used between the host (ESP8266) and controller (Arduino). The table below details the encoding procedure to send lightweight encoded RF packets.On the Arduino UNO (Controller) side, the following state machine was used. Firstly the RF address is constantly listened to until a packet is received that requests temperature data. Once a request is obtained, the temperature sensor is read through an analogue pin and the temperature value is attached to a new HCI packet and sent back to the ESP8266.The host side on the ESP8266 was much more complex and required a more rigorous state machine. When the server sends a GET request to the ESP8266, A JSON packet is sent back to signify the change in fan position or temperature. The following code, details the response from the ESP8266.If the server sends an update request, a RF request packet encoded in the custom HCI protocol. Once the UNO sends the response packet with the payload, the following code is run to determine if the fan speed should change in order to keep the temperature at 24 degrees.Once the fan speed has been changed, a log is created and sent to the server through JSON. If an error occurs, a log detailing the error is also sent to the server.With these systems and state machines, the fan speed and by extension, temperature of the room is automatically controlled. Every 2 mins, the temperature is checked and the fan speed adjusted. The boards are able to communicate with each other wirelessly meaning the location where the temperature is being checked can be anywhere in the room. Due to the current COVID-19 situation, I was not able to create a custom PCB for the device which would have significantly reduced the devices size.This first video demonstrates how the speed of the fan can be manually controlled through the website.This next video shows how the speed of the fan is automatically adjusted based on the temperature of the room. To simulate the ambient temperature increasing, I heated up the temperature sensor with my hand. In order to make this demonstration video concise, I changed the update rate from every 2 minutes to every 10 seconds.It is really clear from the demo logs how the fan adjusted itself through all speed setting to keep the temperature at a comfortable rangeIn conclusion I am very happy with how this project turned out. It was by far the most complicated personal IOT project I have undertaken and I learnt alot about the communication aspect with multiple boards. Now I can rest easy knowing my room will always be a comfortable temperature without me ever having to get up and change it manually. For future projects I would like to connect more parts of my room up to the ESP8266, all controllable from one single website. Stay tuned for more!GITHUB: https://github.com/sauravkumar173/Climate_Controller.gitLinkedIn: https://www.linkedin.com/in/saurav-kumar-756406181/",ruby-on-rails,https://medium.com/@sauravkumar173/creating-a-iot-bedroom-climate-controller-with-esp8266-ruby-on-rails-and-arduino-uno-18a6857e2c22?source=tag_archive---------2-----------------------
"Wait, No… Am I a Developer?","Jeremy BissonnetteJun 8, 2020·5 min readHi, my name is Jeremy and I’m a Junior Full-Stack Software Developer. Wait, no… I’m a registered massage therapist and have been for 15 years, twelve of which I have run my own business, KW Massage. But I write code and I have completed projects that I’m proud of. I’m motivated, curious, passionate, I love working with and learning new technologies and I have a demonstrable capacity to learn. In fact, I have learned a great many things, with very little guidance, no formal training and mainly just the world wide web as a teacher. I know I can more than succeed but excel in a professional software development environment. In this blog I will try to sum up my learnings as a full-stack web developer by chronicling my progress since I first logged onto Code Academy and wrote, <p>Hello World!</p> into my first HTML tag.I’m motivated, curious, passionate, I love working with and learning new technologies and I have a demonstrable capacity to learn.My curiosity in software development was initially sparked while working as a massage therapist. Although I love my work as a massage therapist and I’m very good at it, the job gets a little monotonous at times. It’s the people that make my work interesting. I ask them about themselves and I always find it interesting to hear about other people’s work; whether they are lawyers, doctors, nurses, mechanics or factory workers everyone is good at something and I find that super interesting. However, what really strikes me is the technology field.I remember being 10 years old, in a friend’s backyard, playing with our Transformers. We had to know how they worked. With screwdrivers in-hand we took those Transformers completely apart. That was the easy part, it’s easy to be curious. The hard part was putting them back together again, it took us all day and into the night. The determination is the part I look back on with pride, the problem solving and collaboration with a peer. This was a challenge for 10 year old Jeremy but also one of my fondest memories as a child.I’ve always been interested in how things work. I’v always been gifted with hands-on, interactive on learning.In the small town high school I went to there was only one computer course. I didn’t even think I would like it. We learned about email and the basics of programming (booleans, if statements and variables mainly). For the final project in that class I made a basic drawing program using Logo, an educational programming language. It tracked your mouse. You could select your colour, pick from a few drawing tools and use the erase function to draw a picture and then save your file. Although it worked, it wasn’t written well and it was the farthest thing from adhering to the DRY (do not repeat yourself) principle. However, my teacher couldn’t figure out how I did it so he was forced to give me 100% in the class. Another moment I remember was buying my first computer in grade 10, staring at the black and white screen of the DOS prompt wondering now what, where are all the games? Two months later, after reading the Microsoft DOS user manual front-to-back I was the guy my friend’s fathers would call upon for help with their old 386 machines.Unfortunately, my teacher couldn’t figure out how I did it so he was forced to give me 100% in the classI ended up having several clients during the fall of 2013 who were computer programmers and who were kind enough to humor me by talking about their work and allowing me to ask questions. Eventually they started sending me to online resources like Code Academy, where I learned the basics of HTML and CSS and played with JavaScript and Ruby for the first time. This kept me busy until the spring of 2014 when I started wanting more. I found edX.com, an online platform for universities like Harvard, MIT and Berkeley to host their online courses. You could pay the universities to take the courses and get credit for your work, or if you were only interested in learning you could monitor the courses for your own interest without the fee. This seemed too good to be true. I started a few courses that seemed interesting but mostly the instructors made the content much drier than it needed to be. Until I found Harvard’s CS50, taught by David J. Malan. This man was incredible, running around the stage like he was a rock star, bringing volunteers up like it was a magic show, his enthusiasm was addictive, he made computer science interesting in a way that I could relate to.Introduction to Computer Science — CS50— HarvardX — 2014Familiarity in a number of languages, including C, PHP, SQL, and JavaScript plus CSS and HTMLCS50 was taught primarily in ‘C’. I learnt a bunch of fundamental stuff like variables, arrays and IF statements using scratch. Spent some time getting comfortable with Linux and the Virtual Machine the course used to host the development environment, then quickly jumped into ‘Hello World!’ with ‘C’, having to compile code in the process. Variable types, arrays, nested arrays, pointers, functions, libraries, recursion, debugging, reading and writing files, debugging. Data structures, sorting algorithms, calculating ‘Big O’ and hexadecimals, CS50 was a crash course in computer science learning the fundamentals the hard way. All this lead into ‘PHP’, with a demonstration of a sorting function in just a few lines. Compared to what I had to do in ‘C’ to sort an Array ‘PHP’ seemed amazing. HTML, CSS, PHP, SQL, JavaScript, Ajax, JSON and working with API’s . On top of the fundamentals CS50 taught me how to learn new languages.As a next step I found Ruby on Rails and got really interested in it. Engineering Software as a Service by BerkeleyX through edX then caught my attention. Although the courses seem to have changed names since I took them.Engineering Software as a Service — BerkeleyX — 2015Developing Software as a Service using the Agile development processes with Ruby on RailsObject oriented programming with Ruby and Ruby on Rails — Git, GitHub and Heroku — Agile process, scrum, pair programming — Test Driven Design(TDD), RSpec, mocks, stubs — Behaviour Driven Design(BDD), Cucumber, Capybara — Model View Controller(MVC) — Representational State Transfer(REST) and routes — Databases, ActiveRecord and migrations — Templates, Views, ERB and HAMLEngineering Software as a Service, part 2 — BerkeleyX — 2015Developing Software as a Service using the Agile development processes with Ruby on Rails, advanced topics!Single Sign-on and Third-party Authentication — Relational database, foreign keys, one-to-one relationships, one-to-many relationships, many-to-many relationships, SQLite — Refactoring, DRYing out MVC — Validations — Design Patterns, anti-patterns, single responsibility, open/closed principle, substitution principle, dependency injection, demeter principle — JavaScript for Rails, Document Object Model(DOM), events, callbacks, Ajax — TDD for JavaScript, Jasmin — Single page applicationsWhys (poignant) guide to Ruby | When you wish Upon a BeardAlso an interesting read",ruby-on-rails,https://medium.com/@jeremybissonnette/wait-no-am-i-a-developer-a043d621e6fd?source=tag_archive---------3-----------------------
Rails 6 API Development and GitHub Actions CI with Docker,"F.S0k0mataJun 5, 2020·9 min readI often see the following cases for Rails CI on GitHub Actions.I want to run CI using Docker!. But I don’t see that case very often. So, I would like to introduce a case study of a project I was involved in.is here (github gist)We should not write some COPY or ADD commands in our Dockerfile if you want to minimize the docker image size.Instead of CMD or ENTRYPOINT command, operations such as bundle install or rails s or others are executed in the container via docker-compose {run, exec}, and library and application files are mounted as volumes and copied to the container (not included in the image).ufoscout/docker-compose-wait is installed in the last 3 lines of Dockerfile.It is a tool for waiting port listening with Rust. It’s intended to be used with docker-compose.yml.How to wait for dependent middleware startup? Regarding, there are examples using netcat and dockerize, and writing shell scripts such as the official Postgres example. On the other hand, ufoscout/docker-compose-wait can handle the addition and deletion of middleware by adding a simple code to docker-compose.yml. And the operation is also reliable and easy to use.is here (github gist)The “base” service does not perform any processing by command or entrypoint itself, and is defined as a service only for building.As you can see from the fact that the alias is defined as “&base” , this is merged and used in the subsequent service (described later). Since the settings related to build are aggregated only in this “base” service, the settings in the build section will not appear in the services thereafter.Notice that cache_from rails6api-development-cache is defined in this definition.“wait-middleware” service definition is a service to wait for the start of “db” service using ufoscout/docker-compose-wait installed at the end of Dockerfile.The “base” service defined earlier is merged, and the settings required by docker-compose-wait and the relationship with the “db” service are defined. If you want to run it independently, just run “docker-compose run”.When you run it on a Mac, “db” starts up with almost no wait. At this speed, it’s unlikely that your application will run even though “db” isn’t running.However, on GitHub Actions, it does not start at the same speed as the Mac, so it makes sense to wait for the port to be listened by “wait-middleware”.“backend” is the service definition for executing prompt work after logging in to bash or executing rails s, and the definition part of volume.Both “console” and “server” services merge the “backend” service definition. The volume defined as ${GEMS_CACHE_DIR:-bundle-cache}:/bundle in the volumes of this backend is synonymous with the directory of the bundle install destination. It is intended to “mount if the environment variable GEMS_CACHE_DIR is set, and if it is not set, mount it with a named volume named “bundle-cache”.The contents of .env are expanded for the environment variables of the “db” service.is here (github gist)In Global environment variables, set the environment variables to enable BuildKit for docker-compose.This Dockerfile does not include multi-stage definition, but enabling BuildKit will speed up the build time (around 12% reduction).First, be sure to restore the cache of the Docker image. Subsequent app testing and image vulnerability scanning should be performed using this restored image. App testing and image vulnerability scanning can be run in parallel.Cache restoration of Docker images is done with actions/cache which is the official action for cache processing.Why is this example including ${{ hashFiles(‘Dockerfile’) }} in the cache key? This is to invalidate the cache when there is a change in the Dockerfile. If there is no COPY or ADD command in the Dockerfile, the image depends only on the contents of the Dockerfile.Conversely, if the Dockerfile includes COPY and ADD processing, you can’t manage the cache with ${{ hashFiles(‘Dockerfile’) }}. This is because in this case, the generated image can be changed without changing the Dockerfile itself.In the above step, add the tag “rails6api-development-cache” to the built image and save it in tar, and save it in the actions/cache cache destination directory as “image.tar”.When the cache DOES NOT hits…When the cache hits…When the job that builds and caches the image is completed, the “test-app” step that tests the application starts.In the “docker-load” step, the image cache with the tag “rails6api-development-cache” is expanded, and in the “docker-build” step, this image cache is fetched with cache_from and the “base” service image is built.Wait for db service startup using ufoscout/docker-compose-wait.The above log is an execution example on the runner instance of GitHub Actions (sleep is executed every 1 second and waiting). It takes more than 10 seconds to listen to port 3306.Caching and restoring dependent gems is also done by actions/cache.The inclusion of ${{ hashFiles(‘Gemfile.lock’) }} in the cache key is intended to prevent a cache hit when a Gemfile.lock (Gemfile) is changed.What does ${GEMS_CACHE_DIR:-bundle-cache}:/bundle mean? If the environment variable GEMS_CACHE_DIR is set, specify the mount destination path in the environment variable. If not set, it will be mounted on the named valume.In this example, bundle install is also executed inside the container. So if you want to cache the installed gems, you have to get the installation result from the volume mount.If the installation result is extracted to a directory mounted with named volume, the real path will be /var/lib/docker/volumes/xxx. If I try to cache this real path with actions/cache, We get a “permission denied” error.To avoid this error, set the environment variable GEMS_CACHE_DIR to any directory writable by non-root users (/tmp/cache/bundle in this example). This directory can be cached as is using actions/cache.Execute the processes in order ofInstead of running each command directly on the runner instance, I run it via docker-compose run.",ruby-on-rails,https://medium.com/@s0k0mata/rails-6-api-development-and-github-actions-ci-with-docker-22f28fd2a7a0?source=tag_archive---------1-----------------------
Upgrading old Rails 4.0 to Rails 6 with webpacker and React.js,"Rifki (Kubid) FauziJun 3, 2020·1 min readIt has been a while since I worked with ruby on rails… for the past couple of years, my main focus programming language is javascript. Node.js in the backend, and React on the frontend. so let’s start how we update our old rails app into the latest Rails app with React.update your rails version inside Gemfile:and run bundle update railsIn your Gemfilethen run:Once your webpacker is setup. you are ready to put the bundle pack, you can say a pack is the entry point of your main js file. so all the js file required by rails views should be put inside packsfolder. and put inside your views /layouts/application.html.erbby default, this will refer to /app/javascript/packs/application.js, you can change this path inside webpacker.yml by changing the source_entry_pathinitialize react application with webpacker :it will install all related dependency using yarnI prefer to mount all my component into a single div, so I put empty div inside my /layouts/application.html.erbthen inside my packs/application.jsDone, and you ready to go build your React app inside Ruby on rails application",ruby-on-rails,https://medium.com/@kubid/upgrading-old-rails-4-0-to-rails-6-with-webpacker-and-rails-280313235bb4?source=tag_archive---------6-----------------------
Ruby on Rails vs. Django — Which Framework is Best For 2020?,"We all know that in the programming world, there are many web development frameworks in the market. And we are witnessing two frameworks that stand out the most are Ruby on Rails and Django. Both these frameworks are evolving as famous web frameworks, and the popularity is expected to grow more in 2020 as well.That is the reason that makes a hot debate in front of us. Have a look;Person A: “Ruby on Rails offers an extra edge with the lightning development process.”Person B: “ Well, Django offers an easy and understandable code thus, it is the best web development framework.”Such unending debates are going on as the discussions regarding Ruby on Rails and Django are becoming more popular.As both RoR and Django are loved by top mobile app and web developers, let’s compare them and discover which is better.Ruby on Rails abbreviated as RoR is an open-source and server-side web application development framework. It offers fantastic default structures for web pages, services, and databases. It is regarded as a time-saving technique for programmers to write code.Pros of Ruby on Rails:-Django is a Python-based web application framework that is the primary choice for Python app development. The reason for the popularity of Django is that it is a free, open-source, and general-purpose framework that makes it easily accessible. It fosters faster development with a pragmatic and neat design.Pros of Django:-In the head to head comparison of Ruby on Rails Vs. Django, it has been observed that Ruby on Rails is 0.7% faster than Django. The reason is that RoR comes with a rich repository of fantastic plug-ins and libraries for enhancing the speed and, ultimately, the performance. Django also offers a faster development process.Verdict: RoR wins hereStability and creativity are two essential pillars that are required for development. We can announce the one as the winner who is capable of managing both effectively.RoR can juggle both stability and creativity as it enables users to reuse the code for minimizing dependencies. Moreover, coders are not required to put any additional efforts as it uses the Convention over Configuration approach.On the other hand, Django practices a more traditional approach. For solving the problems to provide stability, it follows any proven technique.Verdict: RoR can beat Django anytime for stabilityDjango still lags behind a little compared to Ruby on Rails, despite its inherent scalability from Python. Ruby on Rails and Django both are heavyweight web development frameworks; therefore they are certainly developed considering scalability.Verdict: RoR takes the crown hereEvery web app owner looks for the best of the line User Interface for their application. Ruby on Rails and Django both emerge as absolute winners when we compare them on the grounds of the user interface. The reason is that both of these frameworks are developed for offering a high-class experience.Verdict: Both are winnersAs the name suggests, Ruby on Rails is developed using Ruby language, whereas Django is developed using Python. Python is known as the top programming language because it focuses on readability and clarity of the code.On the other hand, Ruby is known for its attributes like understandable syntax, freedom, and flexibility. Ruby was developed with the focus on “enjoying” writing the language that makes it fun, whereas Python is the easiest programming language to learn and write.Verdict: Ruby on Rails is more preferredIt is not difficult to compare Ruby on Rails vs. Django based on the installation process. Django has a very easy installation process, it will only take around a minute to install completely.We can not say the same thing about Ruby on Rails. Because there is a need to understand what Gems and bundles are required for installing the specific Ruby packages. So, you need to install these gems and bundle first and then run Command Gem Install Rails. After that, you will be able to install the latest version of the Ruby on Rails.Verdict: Django is the winner for sureA comparison of security is inevitable in this comparison because it is an indispensable part of any application or website. Django has received its security from Python. In fact, Django is used by NASA to prove how secure it is.Rails also offers robust security as it is supported with active records. Still, Django has got the upper hand here.Verdict: Again, Django you got the crownHere, it’s a clear cut tie between Ruby on Rails and Django when we talk about the documentation. You can find the popular answers to the various queries & FAQs easily as both frameworks are well documented. And even the language of the documentation of both frameworks is very easy, straightforward and understandable that does not put readers into any kind of confusion.Verdict: It’s tieIt is a fact that Python is a very easy language compared to other languages. Therefore Django has a very small learning curve. Moreover, you can also find online as well as offline resources that make it easy to resolve your queries.On the other hand, Ruby on Rails has a steep learning curve. The reason is that programmers are required to hone its various independent concepts to become proficient in Railṣ.Verdict: Django beats RoRGenerally, Ruby on Rails and Django both web frameworks are at the top of their category, giving each other a tough competition. And there are many areas where one overrides the other.Also Read:Top 8 Amazing Reasons for Ruby on Rails | Why RoR?",ruby-on-rails,https://medium.com/quick-code/ruby-on-rails-vs-django-which-framework-is-best-for-2020-1a0f1fc7ac4b?source=tag_archive---------0-----------------------
How to handle one big request as a batch of small commands.,"Mariusz KapciaJun 2, 2020·6 min readThis is a rather common situation in my projects. We have a back-office web application to give our clients the option to manage mobile application content. The specification is very similar between projects. There are many tabs, tables, and plenty of big forms with 10 or even more attributes. It’s a common practice to combine unrelated attributes into one huge form with one save button. That leads to significant differences between UI and our domain.Today we will talk about how to fit this into Domain-Driven Design patterns. At first, it may look like there is no problem. We can create one big command with all attributes received from the front-end via API and pass it to the command handler. Unfortunately in most cases, attributes are related to different domain models and we need more than one command to properly handle this situation. We can also have a situation where all attributes are from one domain model but there is no strong connection between them e.g., avatar and password. In such cases, we also want to split attributes into independent commands.Let’s define our goal. Following actions needs to be performed:Now we need to think about how this should be implemented. There are many options, but we would like to use something similar to FormObject pattern with few additions. We can continue with a profile example and try to imagine end result.We want to use standard ActiveModel validations and be able to specify a list of commands related to this form. There is also an option to skip validation of unchanged attributes. In one case we need to validate such attributes and it’s when we adding new domain models. The default value of this option can be set to true or false depending on our needs and we can change it anytime we need. The last difference is with validate method. Normally this method is used with one parameter but we need to know which attribute is validated by the block. That’s the reason for the additional attribute name.What about usage and public interface? We have two use cases. One is for creating a completely new domain model. In this case, we don’t have any existing domain models to use with our form. We will apply changes to the form, verify if all validations passed, build a command list, and perform them on our system.In the second case, we want to update the existing domain model. There is one difference in the code. We will create our form with existing domain model. Thanks to that we will be able to tell if something changed.Let’s implement it! We will split it into smaller pieces to make things easier.CommandWe need to start with a command implementation. We will use the dry-struct gem for this purpose. It’s pretty straight forward. Command is a base class with simple error handling and ChangeAvatar with ChangePassword are our commands.FormLet’s start with a very basic class. We include ActiveModel validations and declaring custom validation error class.Now let’s take care of a command list. We need to build setters and getters for every command attribute and save list of attributes and commands for later use.Command is a class method but validations are done on an instance so we need a way to pass this data to the instanceNext are validation options. We need to set default values and implement setter with getter.Now we need to override validates and validate methods from ActiveModel. The reason for this is that we want to be able to validate only changed attributes. To achieve this we need to dynamically add and remove validations. The idea is to keep aliases for original methods and save all declared validations for later use.We also need getters to get those validations when the time comes right.Okay, we are about halfway through. We have all the necessary things on a class level. Now let’s focus on instance methods. First is initialization. We want to assign a default value for every attribute we have in our form. Of course, it will happen only when an object is provided.Next thing is to apply changes from API parameters. We want to skip unknown attributes from parameters and save which attributes actually changed the value. For those attributes, we will create commands later.Right now we can check if all validations are passing. This is the part where we need to check if we want to validate all attributes or only changed and decide which validations should be called. ActiveModel method clear_validators! is needed because we can use the same instance of our form many times with different params. If everything is fine we will return nil. In case of any errors, we will raise an exception.The last step is to prepare a list of commands. If any of the command attributes have changed then we want to create an instance of this command.That’s it! That was quite a journey. The whole code can be seen here.ExtensionsThat’s the first iteration of this idea. It can be extended with new features:SummaryAs we said at the beginning, there are many ways to handle those requirements. The nice thing about this solution is that logic is in one place and we don’t have to copy/paste the same thing over and over from one application service to another. We are able to reuse validations and we don’t need to think about making sure that any unnecessary command was performed.Thanks for reading and have a nice day!ExamplesHere you can find my Github.",ruby-on-rails,https://medium.com/@mariuszkapcia/how-to-handle-one-big-request-as-a-batch-of-small-commands-f45dc4107865?source=tag_archive---------2-----------------------
Server-side user account verification with a text message and a one-time password.,"Mariusz KapciaJun 3, 2020·3 min readRecently one of our clients asked us to improve the registration process in the mobile application. The current solution is based on text messages and deep links. Users need to click on a link in a text message to verify the account and be redirected to the mobile application. This solution has a few disadvantages:So, what’s the modern way of solving this problem? It’s called one-time passwords. Instead of a deep link, we send a special password/code inside the text message. This code is later parsed by the mobile application and automatically used in a verification process. No action is required from the user.To achieve this we need two things:TwilioOne possible way of sending text messages is to integrate our server with Twilio API. It’s super easy thanks to Twilio gem. All we need is to create a new account on the Twilio webpage, create one phone number from which text messages will be sent, and copy API credentials. The process is very easy and intuitive so we will skip it. Let’s see how we can use this gem.The credentials needed to create an instance of the Twilio client consists of account_sid and auth_token. We can find both on the Twilio account panel webpage. We also need our created phone number from which we want to send our messages. It should be used with from attribute. For obvious reasons, there is an empty string above. The last thing is to attribute which represents the phone number of our recipient.Message formatMessage format requirements are a little different for Android and iOS but it’s possible to combine them into one message format. In the case of iOS, there is a lot of magic going on. It’s not possible to find any details in the documentation regarding the message format but it’s a good idea to use keywords passcode or code. Android is much better. We need to generate a hash string that will represent our mobile application and include it somewhere in our message. Detailed instructions on how to generate that hash can be found here. The overall length of the message shouldn’t be longer than 140 characters. Here is one example of a generated message with a password 21562624:Password/code itself can consist of anything we want, but it’s a good idea to generate a possibly user-friendly password because in some cases the user will need to type it manually.How to use it?We need two API endpoints. One for creating a user session and user account, if this is the first time user is in our system, we will create a user account. Second for verifying the password if it’s correct. The simplified version can be seen below.It would also be very nice to add a few security additions:SummaryWe have all the necessary information to build API for the mobile application with one-time password support. To quickly recap, we know how to send text messages via Twilio API, how to generate properly formatted messages with passwords, and how to handle the basic flow of requests.Thanks for reading and have a nice day!",ruby-on-rails,https://medium.com/@mariuszkapcia/server-side-user-account-verification-with-a-text-message-and-a-one-time-password-8596229f7b76?source=tag_archive---------5-----------------------
Ruby vs Python: What are the differences and how they matter to you?,"Coding NinjasJun 3, 2020·5 min readThe digital era is on a constant dynamic change, and so are the expectations of IT companies. As a developer, you need to think, program and execute tasks in a manner that was way different than a decade ago, all the while striving to continue evolving and upgrading skill sets as per the rising demand of hiring organisations.Of these, Python and Ruby are the new, next-generation, server-side scripting languages that focus on creating applications that are simpler, crispier and high-performance codes. If you’re looking at picking out one of these languages to add your skillset, read on to find out the differences between them and how these differences matter to you.Ruby is a dynamic, open-source, reflective, object-oriented and general-purpose programming language that can be used to create web applications. Created by Yukihiro “Matz” Matsumoto in 1995, Ruby is made up of blending parts of its creator’s favourite languages, including Perl, Smalltalk, Eiffel, Ada and Lisp, to form a brand new language that balanced functional programming with imperative programming.Ruby is just like a human body, simple appearance outside, but inside is very complex, just like our human body. In Ruby, everything is treated as an object and every bit of information and code is assigned its own properties and actions. A very flexible language, Ruby allows its users to freely alter its parts. Essential parts of Ruby can be removed or redefined, depending on will. Existing parts can be added upon. Ruby tries to not restrict the coder.For example, to perform the addition operation, the plus (+) operator is employed. However, if you’d rather use the readable word plus, you could add such a method to Ruby’s built-in numeric class.Ruby is designed to be infinitely flexible and empowering for programmers. Amazing and elegant web frameworks can be created using small tricks allowed by Ruby on Rails. The number one choice of companies such as Apple and GitHub, Ruby is a popular language when it comes to application development.Some of the Popular Rudy editors are RubyWin, Ruby Development Environment (RDE) and Vi. These editors make coding in Ruby much easier, more expressive and simple enough for a layman to understand the code and derive its meaning.A simple Hello World program requires only a single line:puts “Hello World!”;Python is a high-level, interpreted, interactive and object-oriented scripting language. It was created by Guido Van Rossum in 1991.Python uses English keywords more frequently than punctuation marks and also has fewer syntactical constructions as compared to other languages. Python takes a more direct approach to programming. Its primary goal is to make everything visible to the programmer. It makes understanding and debugging of the code easier. Python is a great language for beginner-level programmers and it supports a wide range of applications from simple text processing to creating high-end gaming.Python has a broad standard library that is very portable and comes with cross-platform compatibility. Also, it allows for adding low-level modules to the Python interpreter that can enable programmers to add or customise their tools more efficiently. Data scientists prefer Python over other languages, mainly due to its extensive set of statistical libraries such as NumPy, pandas, matplotlib, TensorFlow, scikit learn and so on.Most preferred by giants like Google and Instagram, Python is used for developing complex, fast-performing applications.Popular Python IDE’s are PyCharm, IDLE, Spyder and Visual Studio Code. Django and Flask frameworks help the developers to build a robust web application.In Python same simple “Hello World” program looks like this:print(“Hello World!”)Remember that, when you’re programming in Python, pay close attention to the indentation and extra white spaces.Still, confused? Look at these neat little numbers we’ve created to help you decide!The following table lists the code-level differences between Ruby and Python:In this table, we attempt a more head-to-head comparison and contrast between the two languages:If you’re a beginner, it might make more sense to pick up Python, because it focuses greatly on understanding the code. It takes a more direct approach to programming and it’s easier because the main aim of Python is to make everything visible to the programmer.Though Python and Ruby are visually similar, both languages have different approach to problem-solving. If you have a specific project lined up that requires you to build only web applications, then you could focus on learning Ruby.Although both, Python and Ruby are efficient and enjoy the loyalty of a large and dedicated community, Python seems the obvious choice in cases where a lot of data computing and processing are involved.Ruby fits in scenarios where rapid prototyping of traffic-heavy applications is required. In Ruby, there’s more than one way to do the same thing whereas in Python, there’s a singular approach but that makes it the most comprehensive, especially if you’re only just beginning.",ruby-on-rails,https://medium.com/@codingninjasindia/ruby-vs-python-what-are-the-differences-and-how-they-matter-to-you-7ecd232513c9?source=tag_archive---------8-----------------------
What the hell is a Snippet?,"Greem JellyfishJun 7, 2020·3 min readHave you ever seen any experienced programmers, typing code extremely fast? You look at their screens and wonder how lines of code just pop up, out of no where at the perfect time, at the perfect location? What the heck did they just do? It’s called Snippet.Let’s talk about snippets for Ruby on Rails and erb file on VScode!A snippet is a small code that can be inserted into the code of a program. Snippets provide a quick path to implement commonly used code into a larger section of code. Instead of re-writing the same code over and over again, a programmer can save the code as a snippet and simply type the prefixwherever it is needed. Simply explained, let’s say, push the button or a shortcut .when you type the prefix the commonly used code appears in your editor.Let me break the snippets down for you.Open User Snippets to add snippets onto your VScode. Code > Preferences > User Snippets the type ruby.json to add snippets into ruby.json file.$1, $2 are for tab stops. $0 is for the final cursor position. and ${1:label}, ${2:another} for placeholders.Now you are an expert at Snippets!",ruby-on-rails,https://medium.com/@greemjellyfish/what-the-hell-is-a-snippet-e6e7e9730e82?source=tag_archive---------1-----------------------
Befriending Rails Generators,"Sid JagarlamudiJun 9, 2020·4 min readYou just came up with a new app idea and have your mind set on building it from scratch. Unfortunately your dreams hit a brick wall when you realize you don’t even know where to start. Ideally it would be nice to just open up your laptop and do this…Instead you stare at a blank text editor as you contemplate on what to do? You know the app must follow the recipe for success being; CRUD functionality supplied with RESTful convention. Let’s take a look at how you may approach this. The first thing that comes to mind is following an MVC design pattern (Model-View-Controller). Well thanks to Rails, with one line of code you are about to think your dream into existence.That’s it? Doesn’t really seem like much. Well let’s take a look at what this produces. As the word scaffold implies, we now have the structure laid out for us to expound upon and flesh out our application. If we take a look behind the scenes, we now have:Now that’s impressive but maybe a little too impressive. When we use a rails generator, we are automatically generating files and lines of code to form the basic structure of any Rails project. But with great power comes great responsibility. What if we don’t need all these features within our app? This can becomes an issue in the future as there will be files/folders/code that will never get used and make you scratch your head as to why it’s even there? It also makes you question, if these generators are doing all this work, am I the coder or is Rails?The good news is that Ruby on Rails offers many generators to find that balance of not having to build your infrastructure from scratch yet still having control over what you want to create.If we run rails generate in the terminal, we can see all the different types of generators at our disposal.The most popular generators include migration, model, controller, and resource. Let’s take a look at the model generator for starters.Juxtaposed with scaffolding, the model generator gives you much more control. It is most beneficial when it comes to creating the back-end part of your app. Let’s say we want a musician model to be created with certain attributes. We can use the model generator here.Running this code will produceWe now have a database migration that adds a musicians table and columns name, genre, and biography.We also have created a model file for musician that will inherit from ApplicationRecord which inherits from Active Record. It’s pretty much a 2 for 1 deal as it creates and model while also doing the work of generate migration. And just like that, with one line of code again, our model is complete!Just like in their names, you could probably take an educated guess as to what generate controller. Controller works the same way as the others as it generates certain folders, and establishes connections. More specifically it will create a controller file that inherits from ApplicationController, a set of routes based on the generator arguments, and a new directory for the view files which link to the controller methods declared. Pretty cool but it can be a tricky one to use like scaffold as sometimes you can unintentionally create files and templates that you don’t intend to use.Finally we have the generator Resource which is probably the best bang for your buck.This will create several files and lines of code for you.Break it all down, resource does the work of migrate, model, and controller generators while also providing full RESTful route capability and creating a corresponding views folder and templates based on controller actions. This is super beneficial to hit the ground running while still having control over your app to tweak it as you wish.This may seem pretty overwhelming at first but you just have to trust rails and know that these tools are here to help you speed up the process, avoid naming errors, make life easier, and avoid looking like this guy…Happy Coding!",ruby-on-rails,https://medium.com/@jaglings/befriending-rails-generators-d53ebaee1337?source=tag_archive---------3-----------------------
react-rails not showing component bug 【Error handling】,"Tomoharu TsutsumiJun 7, 2020·2 min readHi, I’ve been stuck in an error, it took 2 days to solve it. I want to write down how to solve it for me to solve the error when I see it again.I tried to develop a Rails and Redux application, and installed “react-rails”. https://github.com/reactjs/react-railsI wrote codes as the Github page said. However, the react component I made didn’t show up.I couldn’t understand the reason, and the way to solve it. Anyway, I opened console. There was an error.I have never seen this… It has been 2 days since I started to look for the way, and I found it finally.It is seemed that you can solve it by deleting cash and cookie on your browser. This error happens when the many cashes and cookies remain on your browser. However, It was scary to delete all of them because I don’t remember all the passwords in the services I use. I opened this application on another browser(In my case, it is FireFox). The component showed up finally!!",ruby-on-rails,https://medium.com/@tomoharutsutsumi/react-rails-not-showing-component-bug-error-handling-48d70a4ef378?source=tag_archive---------2-----------------------
